{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9abf340-0060-4bcd-a880-0f983a5243ab",
   "metadata": {},
   "source": [
    "**Tutorial 4: Train SimSiam on Satellite Images**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f232a1-0565-439c-8041-0f04e1c054cf",
   "metadata": {},
   "source": [
    "https://docs.lightly.ai/tutorials/package/tutorial_simsiam_esa.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4e3c2e-243b-4ad8-acc0-f2e70631cc2a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bc7df3-9770-40e3-89e6-e8e6128835d1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98bda6-5d7f-4889-ad57-34f9fbea69d5",
   "metadata": {},
   "source": [
    "# Try: balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c33bab9-5560-4394-9e50-d0e315ee4bb2",
   "metadata": {},
   "source": [
    "https://www.maskaravivek.com/post/pytorch-weighted-random-sampler/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48fd6c8-6ac0-4090-8a0f-e426865a26d6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8143c017-c0b6-42b0-ac90-27d7a9ce0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27157ed9-5472-43fe-9190-af36c575127e",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "022c9b24-671a-481d-a5dc-be7166ede5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "batch_size = 32\n",
    "data_dir = 'datasets/Sentinel2GlobalLULC-reduced/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d32112-6cb8-485a-8206-16b44a6d655a",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae23809-3131-4df8-9be4-b5ebda3a0061",
   "metadata": {},
   "source": [
    "### ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e7ebf62-10d0-48d5-8a3e-37a68b257dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchvision custom transforms.\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((input_size, input_size)),\n",
    "    torchvision.transforms.RandomResizedCrop(size=input_size, scale=(0.2, 1.0)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "    torchvision.transforms.GaussianBlur(21),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create the dataset using ImageFolder.\n",
    "dataset = torchvision.datasets.ImageFolder(root=data_dir,\n",
    "                                           transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68471c3b-5832-4192-a40b-0c3a6e339db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset: 27820\n"
     ]
    }
   ],
   "source": [
    "print('Full dataset: ' + str(len(dataset.targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f20074c2-3ebb-4090-9b23-d2ea0f6678dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes and samples per class: (array([0, 1, 2, 3, 4, 5, 6]), array([4656, 4437, 1348, 6380, 2880, 3914, 4205]))\n"
     ]
    }
   ],
   "source": [
    "print('Classes and samples per class: ' + str(np.unique(dataset.targets, return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705b607-73b4-434b-82ca-0b8549864efd",
   "metadata": {},
   "source": [
    "### Train-val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52c3990a-a96f-4b20-a500-2bfafec043f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 19474\n",
      "Valid samples: 8346\n"
     ]
    }
   ],
   "source": [
    "# Size of the dataset.\n",
    "dataset_size = dataset.__len__()\n",
    "\n",
    "# Train and validation samples.\n",
    "train_count = int(dataset_size * 0.7)\n",
    "val_count = dataset_size - train_count\n",
    "print('Train samples: ' + str(train_count))\n",
    "print('Valid samples: ' + str(val_count))\n",
    "\n",
    "# Random split.\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                           [train_count, val_count])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ca8ff-cf72-4233-9869-bb97b7776a33",
   "metadata": {},
   "source": [
    "## Dealing with imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc992f5-232b-44dc-991e-e1aeae25448b",
   "metadata": {},
   "source": [
    "### Computing weights per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3ed3f021-d5e8-4f95-8858-2d143c989ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3306 3093  906 4491 2033 2710 2935]\n"
     ]
    }
   ],
   "source": [
    "# Selecting the indices that form the training dataset.\n",
    "y_train_indices = train_dataset.indices\n",
    "\n",
    "# Creating a list with the labels of training samples.\n",
    "y_train = [dataset.targets[i] for i in y_train_indices]\n",
    "\n",
    "# Counting the number of samples per class.\n",
    "class_sample_count = np.unique(y_train,\n",
    "                               return_counts=True)[1]\n",
    "print(class_sample_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13d089-88e3-4a12-aeeb-b2ec78dfa98c",
   "metadata": {},
   "source": [
    "### WeightedRandomSampler & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "063d1d8f-167d-4273-91bc-747057e21dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0003, 0.0002, 0.0003,  ..., 0.0002, 0.0002, 0.0004],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Weight per sample not per class.\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in y_train])\n",
    "\n",
    "# Casting.\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "samples_weigth = samples_weight.double()\n",
    "print(samples_weight)\n",
    "\n",
    "# Sampler, imbalanced data.\n",
    "sampler = torch.utils.data.WeightedRandomSampler(samples_weight,\n",
    "                                                 len(samples_weight))\n",
    "\n",
    "# Dataloader.\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=sampler,\n",
    "                                               drop_last=True)    # CAREFUL, removes the last batch if it is incomplete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef498f-23c5-4498-b25e-9293dcad15e2",
   "metadata": {},
   "source": [
    "### Checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "24fe2e8f-b282-446b-874c-d02d7a0ccfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5, 6]), array([2862, 2756, 2753, 2775, 2734, 2800, 2776]))\n"
     ]
    }
   ],
   "source": [
    "ds_labels = []\n",
    "\n",
    "# Accessing Data and Targets in a PyTorch DataLoader\n",
    "for imgs_batch, labels_batch in train_dataloader:\n",
    "    for i in range(batch_size):\n",
    "        img = imgs_batch[i]\n",
    "        label = labels_batch[i]\n",
    "        # print(label)\n",
    "        ds_labels.append(int(label))\n",
    "        # plt.title(\"Label: \" + str(int(label)))\n",
    "        # plt.imshow(torch.permute(img,(1, 2, 0)))\n",
    "        # plt.show()\n",
    "        # break\n",
    "    # break\n",
    "\n",
    "print(np.unique(ds_labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24ef49a-f1be-4214-8ad1-4a3f9c213998",
   "metadata": {},
   "source": [
    "## TO BE CONTINUED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4201b4fd-8b81-4dd0-b344-2bf592d70317",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde6873-6653-4877-94f2-0c5ec50e7abc",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a86cc-5385-42b8-9d38-2cef583f07fb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d2950-4abb-4315-ac13-b00dea2af015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import lightly\n",
    "from lightly.models.modules.heads import SimSiamPredictionHead\n",
    "from lightly.models.modules.heads import SimSiamProjectionHead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba2d60-9846-4b99-a1d4-425896b9f048",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fa0345-e25c-4c6d-8550-f5f1a512bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters.\n",
    "num_workers = 8\n",
    "batch_size = 128\n",
    "seed = 1\n",
    "epochs = 15\n",
    "# input_size = 256\n",
    "input_size = 224\n",
    "\n",
    "# Dimension of the embeddings.\n",
    "num_ftrs = 512\n",
    "# Dimension of the output of the prediction and projection heads.\n",
    "out_dim = proj_hidden_dim = 512\n",
    "# The prediction head uses a bottleneck architecture.\n",
    "pred_hidden_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0686a274-7755-40e2-9bd8-7eb356e4e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed torch and numpy.\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set the path to the dataset.\n",
    "path_to_data = 'datasets/Sentinel2GlobalLULC-reduced/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7927a-4332-405d-ac00-d9da4dd34b04",
   "metadata": {},
   "source": [
    "# Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428041c5-38e2-4154-9491-f358c0786058",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = torchvision.datasets.ImageFolder(root=path_to_data)\n",
    "dataset_train_simsiam = lightly.data.LightlyDataset.from_torch_dataset(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8865f-7f89-4ac2-9162-dd17c87f72bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "class_sample_count = [4656, 4437, 1348, 6380, 2880, 3914, 4205] # dataset has 10 class-1 samples, 1 class-2 samples, etc.\n",
    "weights = 1 / torch.Tensor(class_sample_count)\n",
    "weights = weights.double()\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, batch_size)\n",
    "trainloader = torch.utils.data.DataLoader(dataset_train_simsiam, batch_size = batch_size, sampler = sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "806b5c8d-1913-4ba7-93b1-8fb55b90df96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mbase\u001b[49m\u001b[38;5;241m.\u001b[39mtargets)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "len(base.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a0ce0-5abb-43f3-8b86-19d350fa255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None):\n",
    "    # def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        # self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c5e99-bd30-433e-af97-c8834dcd0c62",
   "metadata": {},
   "source": [
    "# Setup data augmentations and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed4197-cd56-4234-9c4f-6d1a94a27942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the augmentations for self-supervised learning.\n",
    "# collate_fn = lightly.data.ImageCollateFunction(\n",
    "#     input_size=input_size,\n",
    "#     # # Require invariance to flips and rotations.\n",
    "#     # hf_prob=0.5,\n",
    "#     # vf_prob=0.5,\n",
    "#     # rr_prob=0.5,\n",
    "#     # # Satellite images are all taken from the same height\n",
    "#     # # so we use only slight random cropping.\n",
    "#     # min_scale=0.5,\n",
    "#     # # Use a weak color jitter for invariance w.r.t small\n",
    "#     # # color changes.\n",
    "#     # cj_prob=0.2,\n",
    "#     # cj_bright=0.1,\n",
    "#     # cj_contrast=0.1,\n",
    "#     # cj_hue=0.1,\n",
    "#     # cj_sat=0.1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fe091-4a27-44cf-9d13-21c39045a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose the custom augmentations with available augmentations\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((input_size, input_size)),\n",
    "    torchvision.transforms.RandomResizedCrop(size=input_size, scale=(0.2, 1.0)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "    torchvision.transforms.GaussianBlur(21),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# create a collate function which performs the random augmentations\n",
    "collate_fn = lightly.data.BaseCollateFunction(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c45f2-40a6-4871-b723-5d87e93c66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# Create a lightly dataset for training, since the augmentations are handled\n",
    "# by the collate function, there is no need to apply additional ones here.\n",
    "# dataset_train_simsiam = lightly.data.LightlyDataset(\n",
    "#     input_dir=path_to_data\n",
    "# )\n",
    "\n",
    "# dataset_train_simsiam = torchvision.datasets.ImageFolder(root=path_to_data,\n",
    "#                                                          transform=transform)\n",
    "\n",
    "base = torchvision.datasets.ImageFolder(root=path_to_data)\n",
    "dataset_train_simsiam = lightly.data.LightlyDataset.from_torch_dataset(base)\n",
    "\n",
    "# Create a dataloader for training.\n",
    "dataloader_train_simsiam = torch.utils.data.DataLoader(\n",
    "    dataset_train_simsiam,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e2b58-92ed-4b1d-98c9-4654f69a142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(base.targets, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73f9384-fb0a-47f7-af07-9f980c153104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Accessing Data and Targets in a PyTorch DataLoader\n",
    "for images, labels, names in dataloader_train_simsiam:\n",
    "    img = images[0][0]\n",
    "    print(img.shape)\n",
    "    # img = images[0].squeeze()\n",
    "    # print(img)\n",
    "    label = labels[0]\n",
    "    plt.title(\"Label: \" + str(int(label)))\n",
    "    plt.imshow(torch.permute(img,(1, 2, 0)))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a093e-3940-495b-8420-977a7b541736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torchvision transformation for embedding the dataset after training\n",
    "# here, we resize the images to match the input size during training and apply\n",
    "# a normalization of the color channel based on statistics from imagenet.\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((input_size, input_size)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # torchvision.transforms.Normalize(\n",
    "    #     mean=lightly.data.collate.imagenet_normalize['mean'],\n",
    "    #     std=lightly.data.collate.imagenet_normalize['std'],\n",
    "    # )\n",
    "])\n",
    "\n",
    "# Create a lightly dataset for embedding.\n",
    "base = torchvision.datasets.ImageFolder(root=path_to_data)\n",
    "dataset_test = lightly.data.LightlyDataset.from_torch_dataset(base,\n",
    "                                                              transform=test_transforms)\n",
    "\n",
    "# Create a dataloader for embedding.\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118db098-8cc2-4616-a295-ed05189dfed0",
   "metadata": {},
   "source": [
    "# Create the SimSiam model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a700d74-a184-4c25-aa6a-8dbb59b75586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimSiam(nn.Module):\n",
    "    def __init__(\n",
    "        self, backbone, num_ftrs, proj_hidden_dim, pred_hidden_dim, out_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = SimSiamProjectionHead(\n",
    "            num_ftrs, proj_hidden_dim, out_dim\n",
    "        )\n",
    "        self.prediction_head = SimSiamPredictionHead(\n",
    "            out_dim, pred_hidden_dim, out_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get representations\n",
    "        f = self.backbone(x).flatten(start_dim=1)\n",
    "        # get projections\n",
    "        z = self.projection_head(f)\n",
    "        # get predictions\n",
    "        p = self.prediction_head(z)\n",
    "        # stop gradient\n",
    "        z = z.detach()\n",
    "        return z, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9acc4fb-2be4-4d13-8657-274934d59f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a pretrained resnet for this tutorial to speed\n",
    "# up training time but you can also train one from scratch.\n",
    "resnet = torchvision.models.resnet18(weights=None) # ADDED\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "model = SimSiam(backbone, num_ftrs, proj_hidden_dim, pred_hidden_dim, out_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c40e9-7539-49c6-8203-52539f7113d4",
   "metadata": {},
   "source": [
    "SimSiam uses a symmetric negative cosine similarity loss and does therefore not require any negative samples. We build a criterion and an optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a886f-cd56-4751-8d44-2ecb67eba73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimSiam uses a symmetric negative cosine similarity loss\n",
    "criterion = lightly.loss.NegativeCosineSimilarity()\n",
    "\n",
    "# scale the learning rate\n",
    "lr = 0.05 * batch_size / 256\n",
    "# use SGD with momentum and weight decay\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db71ca-aab9-4534-82e6-31dfa47398f1",
   "metadata": {},
   "source": [
    "# Train SimSiam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3272ff-0d1c-4958-9b90-92a141e37d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "avg_loss = 0.\n",
    "avg_output_std = 0.\n",
    "for e in range(epochs):\n",
    "\n",
    "    for (x0, x1), _, _ in dataloader_train_simsiam:\n",
    "\n",
    "        # move images to the gpu\n",
    "        x0 = x0.to(device)\n",
    "        x1 = x1.to(device)\n",
    "\n",
    "        # run the model on both transforms of the images\n",
    "        # we get projections (z0 and z1) and\n",
    "        # predictions (p0 and p1) as output\n",
    "        z0, p0 = model(x0)\n",
    "        z1, p1 = model(x1)\n",
    "\n",
    "        # apply the symmetric negative cosine similarity\n",
    "        # and run backpropagation\n",
    "        loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculate the per-dimension standard deviation of the outputs\n",
    "        # we can use this later to check whether the embeddings are collapsing\n",
    "        output = p0.detach()\n",
    "        output = torch.nn.functional.normalize(output, dim=1)\n",
    "\n",
    "        output_std = torch.std(output, 0)\n",
    "        output_std = output_std.mean()\n",
    "\n",
    "        # use moving averages to track the loss and standard deviation\n",
    "        w = 0.9\n",
    "        avg_loss = w * avg_loss + (1 - w) * loss.item()\n",
    "        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\n",
    "\n",
    "    # the level of collapse is large if the standard deviation of the l2\n",
    "    # normalized output is much smaller than 1 / sqrt(dim)\n",
    "    collapse_level = max(0., 1 - math.sqrt(out_dim) * avg_output_std)\n",
    "    # print intermediate results\n",
    "    print(f'[Epoch {e:3d}] '\n",
    "        f'Loss = {avg_loss:.2f} | '\n",
    "        f'Collapse Level: {collapse_level:.2f} / 1.00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17856de-b06e-425f-a5dd-ffc108c3f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "filenames = []\n",
    "\n",
    "# disable gradients for faster calculations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (x, _, fnames) in enumerate(dataloader_test):\n",
    "        # move the images to the gpu\n",
    "        x = x.to(device)\n",
    "        # embed the images with the pre-trained backbone\n",
    "        y = model.backbone(x).flatten(start_dim=1)\n",
    "        # store the embeddings and filenames in lists\n",
    "        embeddings.append(y)\n",
    "        filenames = filenames + list(fnames)\n",
    "\n",
    "# concatenate the embeddings and convert to numpy\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "embeddings = embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca137558-2f25-40dd-a645-d2120d675f1b",
   "metadata": {},
   "source": [
    "# Scatter Plot and Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e72dc-f4fb-4444-9704-65c01d315fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.offsetbox as osb\n",
    "from matplotlib import rcParams as rcp\n",
    "\n",
    "# for resizing images to thumbnails\n",
    "import torchvision.transforms.functional as functional\n",
    "\n",
    "# for clustering and 2d representations\n",
    "from sklearn import random_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681fc95f-4354-43b8-b710-744d8e3ebc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the scatter plot we want to transform the images to a two-dimensional\n",
    "# vector space using a random Gaussian projection\n",
    "projection = random_projection.GaussianRandomProjection(n_components=2)\n",
    "embeddings_2d = projection.fit_transform(embeddings)\n",
    "\n",
    "# normalize the embeddings to fit in the [0, 1] square\n",
    "M = np.max(embeddings_2d, axis=0)\n",
    "m = np.min(embeddings_2d, axis=0)\n",
    "embeddings_2d = (embeddings_2d - m) / (M - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783eca63-3acc-4576-8927-3e2da055128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scatter_plot_with_thumbnails():\n",
    "    \"\"\"Creates a scatter plot with image overlays.\n",
    "    \"\"\"\n",
    "    # initialize empty figure and add subplot\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    fig.suptitle('Scatter Plot of the Sentinel-2 Dataset')\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    # shuffle images and find out which images to show\n",
    "    shown_images_idx = []\n",
    "    shown_images = np.array([[1., 1.]])\n",
    "    iterator = [i for i in range(embeddings_2d.shape[0])]\n",
    "    np.random.shuffle(iterator)\n",
    "    for i in iterator:\n",
    "        # only show image if it is sufficiently far away from the others\n",
    "        dist = np.sum((embeddings_2d[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < 2e-3:\n",
    "            continue\n",
    "        shown_images = np.r_[shown_images, [embeddings_2d[i]]]\n",
    "        shown_images_idx.append(i)\n",
    "\n",
    "    # plot image overlays\n",
    "    for idx in shown_images_idx:\n",
    "        thumbnail_size = int(rcp['figure.figsize'][0] * 10.)\n",
    "        path = os.path.join(path_to_data, filenames[idx])\n",
    "        img = Image.open(path)\n",
    "        img = functional.resize(img, thumbnail_size)\n",
    "        img = np.array(img)\n",
    "        img_box = osb.AnnotationBbox(\n",
    "            osb.OffsetImage(img, cmap=plt.cm.gray_r),\n",
    "            embeddings_2d[idx],\n",
    "            pad=0.2,\n",
    "        )\n",
    "        ax.add_artist(img_box)\n",
    "\n",
    "    # set aspect ratio\n",
    "    ratio = 1. / ax.get_data_ratio()\n",
    "    ax.set_aspect(ratio, adjustable='box')\n",
    "\n",
    "\n",
    "# get a scatter plot with thumbnail overlays\n",
    "get_scatter_plot_with_thumbnails()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29622fb9-fdc3-4cfe-8ccc-d35bf5b59804",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_images = [\n",
    "    '02_MossAndLichen_jpeg/2_MossAndLichen_90.0__4618_0.2_(+67.4397623138,+169.7815886985)_RU_Chukotskiy-Avtonomnyy-Okrug_nan_Pevek.jpg', # water 1\n",
    "    '06_ForestsOpDeBr_jpeg/6_ForestsOpDeBr_80.0__4363_15.8_(+7.5760317801,+17.1542286655)_TD_Logone-Oriental_nan_Beboto.jpg', # water 2\n",
    "    # 'S2B_MSIL1C_20200526T101559_N0209_R065_T32TNL/tile_00556.png', # land\n",
    "    # 'S2B_MSIL1C_20200526T101559_N0209_R065_T31SGD/tile_01731.png', # clouds 1\n",
    "    # 'S2B_MSIL1C_20200526T101559_N0209_R065_T32SMG/tile_00238.png', # clouds 2\n",
    "]\n",
    "\n",
    "\n",
    "def get_image_as_np_array(filename: str):\n",
    "    \"\"\"Loads the image with filename and returns it as a numpy array.\n",
    "\n",
    "    \"\"\"\n",
    "    img = Image.open(filename)\n",
    "    return np.asarray(img)\n",
    "\n",
    "\n",
    "def get_image_as_np_array_with_frame(filename: str, w: int = 5):\n",
    "    \"\"\"Returns an image as a numpy array with a black frame of width w.\n",
    "\n",
    "    \"\"\"\n",
    "    img = get_image_as_np_array(filename)\n",
    "    ny, nx, _ = img.shape\n",
    "    # create an empty image with padding for the frame\n",
    "    framed_img = np.zeros((w + ny + w, w + nx + w, 3))\n",
    "    framed_img = framed_img.astype(np.uint8)\n",
    "    # put the original image in the middle of the new one\n",
    "    framed_img[w:-w, w:-w] = img\n",
    "    return framed_img\n",
    "\n",
    "\n",
    "def plot_nearest_neighbors_3x3(example_image: str, i: int):\n",
    "    \"\"\"Plots the example image and its eight nearest neighbors.\n",
    "\n",
    "    \"\"\"\n",
    "    n_subplots = 9\n",
    "    # initialize empty figure\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(f\"Nearest Neighbor Plot {i + 1}\")\n",
    "    #\n",
    "    example_idx = filenames.index(example_image)\n",
    "    # get distances to the cluster center\n",
    "    distances = embeddings - embeddings[example_idx]\n",
    "    distances = np.power(distances, 2).sum(-1).squeeze()\n",
    "    # sort indices by distance to the center\n",
    "    nearest_neighbors = np.argsort(distances)[:n_subplots]\n",
    "    # show images\n",
    "    for plot_offset, plot_idx in enumerate(nearest_neighbors):\n",
    "        ax = fig.add_subplot(3, 3, plot_offset + 1)\n",
    "        # get the corresponding filename\n",
    "        fname = os.path.join(path_to_data, filenames[plot_idx])\n",
    "        if plot_offset == 0:\n",
    "            ax.set_title(f\"Example Image\")\n",
    "            plt.imshow(get_image_as_np_array_with_frame(fname))\n",
    "        else:\n",
    "            plt.imshow(get_image_as_np_array(fname))\n",
    "        # let's disable the axis\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "# show example images for each cluster\n",
    "for i, example_image in enumerate(example_images):\n",
    "    plot_nearest_neighbors_3x3(example_image, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39e9cb-9a4d-4785-88d1-ea246f18dff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lulc-venv",
   "language": "python",
   "name": "lulc-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

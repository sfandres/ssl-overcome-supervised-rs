{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92a45666-0767-454a-95c0-cb0bba962a9c",
   "metadata": {},
   "source": [
    "**PLAYING WITH THE TENSORFLOW API**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857047b6-0c47-4db4-8565-8be6490a4830",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e028b-734a-4717-bb86-4fb7c25234b0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b840970f-1d7a-4268-b519-00959104eaac",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f240f-3d46-4aa5-8b2d-68c80699d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Modules.\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
    "\n",
    "# Disable GPU.\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n",
    "\n",
    "# Information.\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb513eb-7bab-454b-9908-9362f2a64a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33efd72-429d-4fa7-b6d4-a244d4a36a23",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be791cf0-76f7-428b-b06a-e8c7e15df019",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461355d-fc35-41f2-9ff4-a70cf3ad9354",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0ec6c0-b484-4acb-819e-1f549bfc526f",
   "metadata": {},
   "source": [
    "## Split into training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5fcce3-1187-4dcf-a9f9-5c8c513a6d60",
   "metadata": {},
   "source": [
    "When training and evaluating deep learning models in Keras, generating a dataset from image files stored on disk is simple and fast. Call `image_data_set_from_directory()` to read from the directory and create both training and validation datasets. \n",
    "\n",
    "If you're specifying a validation split, you'll also need to specify the subset for each portion. Just set the training set to `subset='training'` and the validation set to `subset='validation'`.\n",
    "\n",
    "You'll also set your seeds to match each other, so your training and validation sets don't overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d026eadb-b788-49fc-93eb-c69a29d34971",
   "metadata": {},
   "source": [
    "Then calling image_dataset_from_directory(main_directory, labels='inferred') will return a tf.data.Dataset that yields batches of images from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f540047d-4f3d-4b1a-a233-4c98e12a8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_size = (224, 224)\n",
    "directory = 'Sentinel2GlobalLULC/Sentinel2LULC_JPEG/'\n",
    "\n",
    "split_train_val = 0.2\n",
    "shuffle = True\n",
    "\n",
    "# Train set.\n",
    "train_dataset = image_dataset_from_directory(directory,\n",
    "                                             label_mode='categorical',\n",
    "                                             batch_size=batch_size,\n",
    "                                             image_size=img_size,\n",
    "                                             shuffle=shuffle,\n",
    "                                             seed=42,\n",
    "                                             validation_split=split_train_val,\n",
    "                                             subset='training')\n",
    "\n",
    "# Validation set.\n",
    "validation_dataset = image_dataset_from_directory(directory,\n",
    "                                                  label_mode='categorical',\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  image_size=img_size,\n",
    "                                                  shuffle=shuffle,\n",
    "                                                  seed=42,\n",
    "                                                  validation_split=split_train_val,\n",
    "                                                  subset='validation')\n",
    "\n",
    "# Test set extracted from the validation one.\n",
    "split_val_test = 0.2\n",
    "n_test_samples = int(split_val_test * len(validation_dataset))\n",
    "test_dataset = validation_dataset.take(n_test_samples)\n",
    "validation_dataset = validation_dataset.skip(n_test_samples)\n",
    "\n",
    "# Show stats.\n",
    "print('Training samples: ' + str(len(train_dataset) * batch_size))\n",
    "print('Validation samples: ' + str(len(validation_dataset) * batch_size))\n",
    "print('Test samples: ' + str(len(test_dataset) * batch_size))\n",
    "print('Total approx.: ' + str((len(train_dataset) + len(validation_dataset) + len(test_dataset)) * batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14df75-aa30-442a-889c-ac027595b76e",
   "metadata": {},
   "source": [
    "## Show some examples from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96ffa2-d9e9-4655-bb8d-38d0be074c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with the different classes.\n",
    "class_names = train_dataset.class_names\n",
    "n_classes = len(class_names)\n",
    "\n",
    "# Show more stats.\n",
    "print('Classes: ', class_names)\n",
    "print('Number of classes: ', n_classes)\n",
    "\n",
    "# Example.\n",
    "plt.figure(figsize=(15, 10))\n",
    "for images, labels in train_dataset.take(1):    # Take a batch.\n",
    "    for i in range(9):                          # Take nine images.\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        # plt.title(class_names[labels[i]])    # no one-hot\n",
    "        plt.title(class_names[np.argmax(labels[i])])    # one-hot\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c0240-b663-4ed1-a9cb-dbbf8c6b2886",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03582380-7d99-4ea0-aba0-f7e45c05283e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d560de5-758d-448b-bf59-4943b9ce69a6",
   "metadata": {},
   "source": [
    "# Preprocesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57e382-d670-410f-9a71-88c2560ce96c",
   "metadata": {},
   "source": [
    "## Prefetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99500df-1716-44fb-8da7-fcb4de4bd3b2",
   "metadata": {},
   "source": [
    "You may have encountered `dataset.prefetch` in a previous TensorFlow assignment, as an important extra step in data preprocessing. \n",
    "\n",
    "Using `prefetch()` prevents a memory bottleneck that can occur when reading from disk. It sets aside some data and keeps it ready for when it's needed, by creating a source dataset from your input data, applying a transformation to preprocess it, then iterating over the dataset one element at a time. Because the iteration is streaming, the data doesn't need to fit into memory.\n",
    "\n",
    "You can set the number of elements to prefetch manually, or you can use `tf.data.experimental.AUTOTUNE` to choose the parameters automatically. Autotune prompts `tf.data` to tune that value dynamically at runtime, by tracking the time spent in each operation and feeding those times into an optimization algorithm. The optimization algorithm tries to find the best allocation of its CPU budget across all tunable operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512a10c-68bf-4df2-8d28-8218e95d078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "autotune = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size=autotune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5bb3c-e0ab-44f3-bfa7-cba93b2c0c5a",
   "metadata": {},
   "source": [
    "## Augmenting training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b3bd70-bff3-4ecd-8e33-7f23caa0d185",
   "metadata": {},
   "source": [
    "To increase diversity in the training set and help your model learn the data better, it's standard practice to augment the images by transforming them, i.e., randomly flipping and rotating them. Keras' Sequential API offers a straightforward method for these kinds of data augmentations, with built-in, customizable preprocessing layers. These layers are saved with the rest of your model and can be re-used later.  Ahh, so convenient! \n",
    "\n",
    "As always, you're invited to read the official docs, which you can find for data augmentation [here](https://www.tensorflow.org/tutorials/images/data_augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db775bd-a4c2-489f-8dca-a1e483e2eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmenter():\n",
    "    \"\"\"\n",
    "    Create a Sequential model composed of 2 layers\n",
    "    Returns:\n",
    "        tf.keras.Sequential\n",
    "    \"\"\"\n",
    "    data_augmentation = tf.keras.Sequential()\n",
    "    data_augmentation.add(RandomFlip('horizontal_and_vertical'))\n",
    "    data_augmentation.add(RandomRotation(0.2))\n",
    "\n",
    "    return data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf957b-410a-43b0-8e0f-4e3808a838f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = data_augmenter()\n",
    "\n",
    "for image, label in train_dataset.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image, first_label = image[0], label[0]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "        plt.imshow(augmented_image[0] / 255)\n",
    "        plt.title(class_names[np.argmax(first_label)])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bce951-5b38-40ed-ad35-a534bd4e6128",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7a6c9-2d6a-4536-9b4e-c67c4aa19428",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb62e3-2724-4108-aac2-32a8cbd893b3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3bf02-bc4d-4523-a57c-97b8d9279cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b312e3-cf3a-452c-a886-20b46ed4c273",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4e4b1-fbb0-4fce-a88d-f215663e0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "    # Set 3D input shape.\n",
    "    input_shape = img_size + (3,)\n",
    "\n",
    "    # Loading the pre-trained model without the top layers.\n",
    "    base_model = VGG16(include_top=False,\n",
    "                       weights='imagenet',\n",
    "                       input_shape=input_shape)\n",
    "\n",
    "    # Freeze the base model.\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create the input layer.\n",
    "    inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "    # Apply data augmentation to the inputs. APPLYYYYYYY\n",
    "    # x = data_augmentation(inputs)\n",
    "\n",
    "    # Data preprocessing using the same weights the model was trained on.\n",
    "    x = preprocess_input(inputs)\n",
    "\n",
    "    # Set training to False to avoid keeping track of statistics in the batch norm layer.\n",
    "    x = base_model(x, training=False)\n",
    "\n",
    "    # Add the new Binary classification layers.\n",
    "    # Use global avg pooling to summarize the info in each channel.\n",
    "    x = tfl.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Include dropout to avoid overfitting.\n",
    "    x = tfl.Dropout(0.2)(x)\n",
    "\n",
    "    # Output layer.\n",
    "    x = tfl.Flatten()(x)\n",
    "    outputs = tfl.Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create model.\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c932ce6-4e4e-4e15-a23b-109a8a4ecb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Include the epoch in the file name (uses `str.format`)\n",
    "# checkpoint_path = \"results/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=\"results/best_keras_model.h5\", \n",
    "#     verbose=1, \n",
    "#     save_weights_only=True,\n",
    "#     save_freq=5*batch_size)\n",
    "\n",
    "# Include the epoch in the file name (uses `str.format`)\n",
    "checkpoint_path = 'model-{epoch:02d}-{val_loss:.4f}.h5'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Custom callback: New print during fit.\n",
    "class PrintValTrainRatioCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"val/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "\n",
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "\n",
    "# Checkpoint callback.\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch')\n",
    "\n",
    "# TensorBoard logs callback.\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "# Create a new model instance.\n",
    "model = get_model()\n",
    "\n",
    "# Compile.\n",
    "base_learning_rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "# base_learning_rate = 0.001\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "#            loss=tf.keras.losses.categorical_crossentropy,\n",
    "#            metrics=['accuracy'])\n",
    "\n",
    "# Train the model with the new callback.\n",
    "initial_epochs = 5\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset,\n",
    "                    callbacks=[cp_callback, tensorboard_cb, val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76d839-c457-4d18-a874-1e9cdf2a7cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6b26d-b568-4dad-9d1d-983e79129a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate current model\n",
    "model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f02aa5-ef9b-406a-93a8-e78a8f9897fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the resulting checkpoints and choose the latest one.\n",
    "os.listdir(checkpoint_dir)\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "# Create a new model instance\n",
    "new_model = create_model()\n",
    "\n",
    "# Load the previously saved weights\n",
    "new_model.load_weights(latest)\n",
    "\n",
    "# Re-evaluate the model\n",
    "new_model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6dc899-18b7-4240-a8c8-e40afd2e2932",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [0.] + history.history['accuracy']\n",
    "val_acc = [0.] + history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9795c112-5c14-4651-b0c0-445e70f50669",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f0045-b282-40b7-ae5f-83fc6646889a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074ab1d-d5a6-4883-b4b3-43dfbf261742",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acede657-936c-4ca5-a1d1-dc91a26bb7f3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f33bc1b-3295-427c-bfee-b94bb0a53214",
   "metadata": {},
   "source": [
    "## Save the whole model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49c799-ba3d-475b-bcc5-9f17abebffe5",
   "metadata": {},
   "source": [
    "Keras also supports saving a single HDF5 file containing the model's architecture, weights values, and compile() information. It is a light-weight alternative to SavedModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3839f43d-66e2-45d4-b5b6-64fc8b2d3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\n",
    "model.save(\"my_model\")\n",
    "\n",
    "# Model summary.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd537b-60ed-4b19-90c7-fe33c4cad549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be used to reconstruct the model identically.\n",
    "new_model = tf.keras.models.load_model(\"my_model\")\n",
    "\n",
    "# Reconstructed model summary.\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec26e52-3ff7-42d4-a8ed-d34294eb6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 1\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = new_model.fit(train_dataset,\n",
    "                             epochs=total_epochs,\n",
    "                             initial_epoch=history.epoch[-1],\n",
    "                             validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b95bc-b254-445d-b31f-74dd0690b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6df21-c9e1-4267-b06f-b8464fcf386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a559eff8-6c36-4b7c-8ecf-163be4b18292",
   "metadata": {},
   "source": [
    "## Save only the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b632f6-8773-4e1b-b868-c896f5a74d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only weights.\n",
    "model.save_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebeea3-5fb1-438f-8595-a317e2606190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only weights.\n",
    "reconstructed_model = get_model()\n",
    "reconstructed_model.load_weights(\"weights.h5\")\n",
    "\n",
    "# Reconstructed model summary.\n",
    "reconstructed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e764c-96ae-44f5-a143-42c53590f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all of the pretrained weights have been loaded.\n",
    "for a, b in zip(model.weights, reconstructed_model.weights):\n",
    "    np.testing.assert_allclose(a.numpy(), b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94550516-0310-4ba8-8c68-d6ed5d99042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check:\n",
    "# np.testing.assert_allclose(\n",
    "#     model.predict(validation_dataset), reconstructed_model.predict(validation_dataset)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430a7fe-4c66-4195-8446-a3d7be6ac37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(validation_dataset)\n",
    "reconstructed_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "                            loss=tf.keras.losses.categorical_crossentropy,\n",
    "                            metrics=['accuracy'])\n",
    "reconstructed_model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9c62a2-9bd9-4592-8bff-4a77c20f4edb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd4baf3-7a3d-4c7c-b748-b366ab8cd1f7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714fd1c2-36d8-4940-bbfe-ac6ebfa70840",
   "metadata": {},
   "source": [
    "# Test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c1a23-cc79-4efa-a260-19daf8adb330",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in validation_dataset.take(1):    # only take first element of dataset, batch in this case as it is prefetched\n",
    "    \n",
    "    # Batch id.\n",
    "    idx = 2\n",
    "    numpy_batch_images = images.numpy()\n",
    "    numpy_batch_labels = labels.numpy()\n",
    "    \n",
    "    # Plot target img.\n",
    "    plt.imshow(numpy_batch_images[idx].astype(\"uint8\"))\n",
    "    print(class_names[np.argmax(numpy_batch_labels[idx])])\n",
    "    \n",
    "    # Prediction.\n",
    "    predictions = model.predict(numpy_batch_images)[idx]\n",
    "    print('\\n', predictions)\n",
    "    print(class_names[np.argmax(predictions)], predictions[np.argmax(predictions)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304018f7-443b-4fd1-b33b-7faec6e9bd11",
   "metadata": {},
   "source": [
    "# Fine-tuning the model by unfreezing more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af9c5f-637f-432b-b97a-5945405e9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = new_model.layers[3]\n",
    "base_model.trainable = True\n",
    "\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525bbd1-9ab1-4ab0-9fa2-c83950336a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 10\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "history_fine2 = new_model.fit(train_dataset,\n",
    "                              epochs=5,\n",
    "                              initial_epoch=history.epoch[-1],\n",
    "                              validation_data=validation_dataset)\n",
    "\n",
    "initial_epochs = total_epochs\n",
    "new_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "                  loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ccba5-8470-4d6e-a2bd-b3c02e4907ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine2.history['accuracy']\n",
    "val_acc += history_fine2.history['val_accuracy']\n",
    "\n",
    "loss += history_fine2.history['loss']\n",
    "val_loss += history_fine2.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ceeffa-038f-4a56-a728-af3ac84ecad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17910d0d-4d7c-403e-9023-a529623da9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lulc-venv",
   "language": "python",
   "name": "lulc-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92a45666-0767-454a-95c0-cb0bba962a9c",
   "metadata": {},
   "source": [
    "**PLAYING WITH THE TENSORFLOW API**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857047b6-0c47-4db4-8565-8be6490a4830",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e028b-734a-4717-bb86-4fb7c25234b0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b840970f-1d7a-4268-b519-00959104eaac",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0f240f-3d46-4aa5-8b2d-68c80699d6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n",
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 13:46:52.228725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:46:52.259814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:46:52.260061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# Essential libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Modules.\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
    "\n",
    "# Disable GPU.\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n",
    "\n",
    "# Information.\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fb513eb-7bab-454b-9908-9362f2a64a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33efd72-429d-4fa7-b6d4-a244d4a36a23",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be791cf0-76f7-428b-b06a-e8c7e15df019",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461355d-fc35-41f2-9ff4-a70cf3ad9354",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0ec6c0-b484-4acb-819e-1f549bfc526f",
   "metadata": {},
   "source": [
    "## Split into training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5fcce3-1187-4dcf-a9f9-5c8c513a6d60",
   "metadata": {},
   "source": [
    "When training and evaluating deep learning models in Keras, generating a dataset from image files stored on disk is simple and fast. Call `image_data_set_from_directory()` to read from the directory and create both training and validation datasets. \n",
    "\n",
    "If you're specifying a validation split, you'll also need to specify the subset for each portion. Just set the training set to `subset='training'` and the validation set to `subset='validation'`.\n",
    "\n",
    "You'll also set your seeds to match each other, so your training and validation sets don't overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d026eadb-b788-49fc-93eb-c69a29d34971",
   "metadata": {},
   "source": [
    "Then calling image_dataset_from_directory(main_directory, labels='inferred') will return a tf.data.Dataset that yields batches of images from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1719085-284a-4666-88e1-d4f4f1b82712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 194877 files belonging to 29 classes.\n",
      "Using 175390 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 13:46:59.100175: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-21 13:46:59.101184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:46:59.101345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:46:59.101446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:46:59.699292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:46:59.699445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:46:59.699550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:46:59.699853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6065 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 194877 files belonging to 29 classes.\n",
      "Using 19487 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Samples info.\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "path_dir = 'Sentinel2GlobalLULC/Sentinel2LULC_JPEG/' # /home/sfandres/Downloads/Sentinel-dataset/'    \n",
    "\n",
    "# Dataset info.\n",
    "split_train_val = 0.1\n",
    "shuffle = True\n",
    "\n",
    "# Train set.\n",
    "train_dataset = image_dataset_from_directory(path_dir,\n",
    "                                             label_mode='categorical',\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=IMG_SIZE,\n",
    "                                             shuffle=shuffle,\n",
    "                                             seed=42,\n",
    "                                             validation_split=split_train_val,\n",
    "                                             subset='training')\n",
    "\n",
    "# Validation set.\n",
    "validation_dataset = image_dataset_from_directory(path_dir,\n",
    "                                                  label_mode='categorical',\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  image_size=IMG_SIZE,\n",
    "                                                  shuffle=shuffle,\n",
    "                                                  seed=42,\n",
    "                                                  validation_split=split_train_val,\n",
    "                                                  subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88172005-9527-4ad3-9752-d488b293acea",
   "metadata": {},
   "source": [
    "As the original dataset doesn't contain a test set, you will create one. To do so, determine how many batches of data are available in the validation set using tf.data.experimental.cardinality, then move 20% of them to a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5883e8eb-3354-4c3e-be36-e7cf518d2bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 5481\n",
      "Training samples: 175392\n",
      "\n",
      "Validation batches: 203\n",
      "Validation samples: 6496\n",
      "\n",
      "Test batches: 406\n",
      "Test samples: 12992\n",
      "\n",
      "Total approx.: 194880\n"
     ]
    }
   ],
   "source": [
    "# Test set.\n",
    "ratio_val_test = 3\n",
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // ratio_val_test)\n",
    "validation_dataset = validation_dataset.skip(val_batches // ratio_val_test)\n",
    "\n",
    "validation_dataset, test_dataset = test_dataset, validation_dataset\n",
    "\n",
    "# Show stats.\n",
    "print('Training batches: %d' % tf.data.experimental.cardinality(train_dataset))\n",
    "print('Training samples: ' + str(len(train_dataset) * BATCH_SIZE))\n",
    "\n",
    "print('\\nValidation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Validation samples: ' + str(len(validation_dataset) * BATCH_SIZE))\n",
    "\n",
    "print('\\nTest batches: %d' % tf.data.experimental.cardinality(test_dataset))\n",
    "print('Test samples: ' + str(len(test_dataset) * BATCH_SIZE))\n",
    "\n",
    "print('\\nTotal approx.: ' + str((len(train_dataset) + len(validation_dataset) + len(test_dataset)) * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834b28e-b435-4305-8a64-2c22dbb597aa",
   "metadata": {},
   "source": [
    "## Check if the dataset is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf5f9f61-e861-4448-b019-213f50c82b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for x, y in train_dataset:\n",
    "    # If one hot encoded, then apply argmax.\n",
    "    labels.append(np.argmax(y, axis=-1))\n",
    "    \n",
    "    # Not one hot encoded.\n",
    "    # labels.append(y.numpy())\n",
    "\n",
    "# Concatenate asuming dataset was batched.\n",
    "labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "# Count unique labels.\n",
    "labels, samples = np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f26a591f-7880-4b9a-9ed5-ae4282b31024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 29 artists>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel('Samples')\n",
    "plt.bar(x=labels, height=samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14df75-aa30-442a-889c-ac027595b76e",
   "metadata": {},
   "source": [
    "## Show some examples from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96ffa2-d9e9-4655-bb8d-38d0be074c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with the different classes.\n",
    "class_names = train_dataset.class_names\n",
    "n_classes = len(class_names)\n",
    "\n",
    "# Show more stats.\n",
    "print('Classes: ', class_names)\n",
    "print('Number of classes: ', n_classes)\n",
    "\n",
    "# Example.\n",
    "plt.figure(figsize=(15, 10))\n",
    "for images, labels in train_dataset.take(1):    # Take a batch.\n",
    "    for i in range(9):                          # Take nine images.\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        # plt.title(class_names[labels[i]])                # no one-hot\n",
    "        plt.title(class_names[np.argmax(labels[i])])       # one-hot\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c0240-b663-4ed1-a9cb-dbbf8c6b2886",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03582380-7d99-4ea0-aba0-f7e45c05283e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d560de5-758d-448b-bf59-4943b9ce69a6",
   "metadata": {},
   "source": [
    "# Preprocesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57e382-d670-410f-9a71-88c2560ce96c",
   "metadata": {},
   "source": [
    "## Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99500df-1716-44fb-8da7-fcb4de4bd3b2",
   "metadata": {},
   "source": [
    "Using `prefetch()` prevents a memory bottleneck that can occur when reading from disk. It sets aside some data and keeps it ready for when it's needed, by creating a source dataset from your input data, applying a transformation to preprocess it, then iterating over the dataset one element at a time. Because the iteration is streaming, the data doesn't need to fit into memory.\n",
    "\n",
    "You can set the number of elements to prefetch manually, or you can use `tf.data.experimental.AUTOTUNE` to choose the parameters automatically. Autotune prompts `tf.data` to tune that value dynamically at runtime, by tracking the time spent in each operation and feeding those times into an optimization algorithm. The optimization algorithm tries to find the best allocation of its CPU budget across all tunable operations.\n",
    "\n",
    "Use buffered prefetching to load images from disk without having I/O become blocking. To learn more about this method see the data performance guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512a10c-68bf-4df2-8d28-8218e95d078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5bb3c-e0ab-44f3-bfa7-cba93b2c0c5a",
   "metadata": {},
   "source": [
    "## Use data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b3bd70-bff3-4ecd-8e33-7f23caa0d185",
   "metadata": {},
   "source": [
    "To increase diversity in the training set and help your model learn the data better, it's standard practice to augment the images by transforming them, i.e., randomly flipping and rotating them. Keras' Sequential API offers a straightforward method for these kinds of data augmentations, with built-in, customizable preprocessing layers. These layers are saved with the rest of your model and can be re-used later.  Ahh, so convenient! \n",
    "\n",
    "As always, you're invited to read the official docs, which you can find for data augmentation [here](https://www.tensorflow.org/tutorials/images/data_augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31390e9-5da5-4887-8f6e-fdad5d8608ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does not work but I think it is because\n",
    "# of the nature of the dataset samples.\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tfl.RandomFlip(\"horizontal_and_vertical\", seed=42),\n",
    "    tfl.RandomRotation(0.2, seed=42),\n",
    "    tfl.RandomZoom(0.1, seed=42),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368cefb-6150-47f9-914c-f912decf49c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in train_dataset.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image, first_label = image[0], label[0]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "        plt.imshow(augmented_image[0] / 255)\n",
    "        plt.title(class_names[np.argmax(first_label)])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97006836-0f7d-4570-baaa-55fea03a8247",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226f2bbe-9cbc-459e-85f5-5b540c874f76",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3aff0-df49-4c03-b164-4a1e80cc9bac",
   "metadata": {},
   "source": [
    "# Create the base model from the pre-trained CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d863cb0e-447e-4d19-acd1-1dd6cac113eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b7d2ff-8c98-437e-80f4-248b95163aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = False\n",
    "model_name = 'ResNet50'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d3d782-5f3f-4d5f-8a81-87bc04c1ace6",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d861457-ddcc-4d3b-b8d1-c2856a0828ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(data_aug=False):\n",
    "\n",
    "    # Create the base model from the pre-trained model ResNet50\n",
    "    input_shape = IMG_SIZE + (3,)\n",
    "\n",
    "    # Loading the pre-trained model without the top layers.\n",
    "    base_model = ResNet50(include_top=False,\n",
    "                          weights='imagenet',\n",
    "                          input_shape=input_shape)\n",
    "\n",
    "    # Freeze the base model.\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create the input layer.\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Apply data augmentation to the inputs.\n",
    "    if data_aug:\n",
    "        inputs = data_augmentation(inputs, training=True)\n",
    "\n",
    "    # Data preprocessing using the same weights the model was trained on.\n",
    "    x = preprocess_input(inputs)\n",
    "\n",
    "    # Set training to False to avoid keeping track of statistics in the batch norm layer.\n",
    "    x = base_model(x, training=False)\n",
    "\n",
    "    # Add the new Binary classification layers.\n",
    "    # Use global avg pooling to summarize the info in each channel.\n",
    "    x = tfl.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Include dropout to avoid overfitting.\n",
    "    x = tfl.Dropout(0.2)(x)\n",
    "\n",
    "    # Output layer.\n",
    "    x = tfl.Flatten()(x)\n",
    "    outputs = tfl.Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create model.\n",
    "    model = tf.keras.Model(inputs, outputs, name=model_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d384a-1a00-4c03-a65d-4649c8c2f33d",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e5303-a3bd-420f-b353-a0fb21d71398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback: New print during fit.\n",
    "class PrintValTrainRatioCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\" - val/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "\n",
    "cb_val_train_ratio = PrintValTrainRatioCallback()\n",
    "\n",
    "# TensorBoard logs callback.\n",
    "def get_run_logdir(root_logdir):\n",
    "    import time\n",
    "    run_id = time.strftime('run-%Y_%m_%d-%H_%M_%S')\n",
    "    run_id = run_id + '-' + (model_name) + '-Data_aug_' + str(data_aug)\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "run_logdir = get_run_logdir(root_logdir) # e.g., './my_logs/run_2019_06_07-15_15_22'\n",
    "\n",
    "cb_tensorboard = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "# Checkpoint callback: epoch in the file name.\n",
    "checkpoint_path =  model_name + '-epoch-{epoch:02d}-loss-{loss:.3f}-val_loss-{val_loss:.3f}-val_acc-{val_categorical_accuracy:.3f}.h5'\n",
    "checkpoint_filepath = os.path.join(run_logdir, checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61387e-3b42-4685-9eb7-a4fe3fe59e03",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a989bbcc-510d-4e78-8c3d-24ecda7c72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model instance.\n",
    "model = get_model(data_aug)\n",
    "\n",
    "# Compile.\n",
    "base_learning_rate = 0.001    # 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "# Summary.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f60f9-b2c9-46f1-9524-c4e67c104bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the new callback.\n",
    "initial_epochs = 10\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset,\n",
    "                    callbacks=[cb_val_train_ratio,\n",
    "                               cb_tensorboard,\n",
    "                               cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1a7e6-d3fa-4042-b808-ba6a26971503",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(color='gray', linestyle=':', linewidth=.8)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.grid(color='gray', linestyle=':', linewidth=.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c80d3-d00c-4bc5-9ea4-a2ac11f3c51c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a139673-9b85-4038-abde-4ff3b3c275d2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c3fc8-0055-4cd9-8929-b5e4470585e8",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e5885-21b4-4a1d-8537-89e512c724fb",
   "metadata": {},
   "source": [
    "## Un-freeze the top layers of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa291f1-e4d2-4976-9482-2cd004bd4323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-freeze the base_model.\n",
    "base_model = model.get_layer('resnet50')    # model.layers[3]\n",
    "base_model.trainable = True\n",
    "\n",
    "# Let's take a look to see how many layers are in the base model.\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards.\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the 'fine_tune_at' layer.\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Summary again.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7fa8b-90ea-4bd4-a2e7-afbfdb806218",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185d99fb-207c-4648-8a9e-895db7f341e9",
   "metadata": {},
   "source": [
    "As you are training a much larger model and want to readapt the pretrained weights, it is important to use a lower learning rate at this stage. Otherwise, your model could overfit very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee99f0-9ce5-40f5-a97e-96b7d11467da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New compilation with lower lr.\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate / 10),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afdadf-1468-42bf-a42d-a6a57f58d775",
   "metadata": {},
   "source": [
    "## Continue training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db16cf-72bc-455e-addc-c468d6595d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset,\n",
    "                         callbacks=[cb_val_train_ratio,\n",
    "                                    cb_tensorboard,\n",
    "                                    cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f835cf03-3e86-4d0d-b8fb-c40903937c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine.history['categorical_accuracy']\n",
    "val_acc += history_fine.history['val_categorical_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(color='gray', linestyle=':', linewidth=.8)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.grid(color='gray', linestyle=':', linewidth=.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a930e0-9eeb-472b-9648-82c86d01975e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494174fa-b39c-4a32-b46b-79fdf11ec768",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ba827-cfe4-4b9d-bbc8-322306af4165",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec5ec02-4835-4468-b2ce-416edf7427bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1fc6a2-38f9-4775-b047-523f3a1fb66f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ab5af1-c345-41ec-bfc9-a5c49cdc5ee6",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbef33e-0823-4cfb-9746-3a5f8daadb87",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b9f28-3601-47c5-88d7-b6de5d456e35",
   "metadata": {},
   "source": [
    "## When only the weights had been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6026f561-3e92-4a23-87b6-a4a4f0014687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually save only weights.\n",
    "# model.save_weights(\"weights.h5\")\n",
    "\n",
    "# Load only weights.\n",
    "# reconstructed_model = get_model()\n",
    "# reconstructed_model.load_weights(\"weights.h5\")\n",
    "# reconstructed_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "#                             loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "#                             metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9242b2-20cd-49b9-97a9-5f40478340d2",
   "metadata": {},
   "source": [
    "## When the whole model had been sabed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936bda7-61f4-4694-a957-f9fb342b0bc5",
   "metadata": {},
   "source": [
    "Keras also supports saving a single HDF5 file containing the model's architecture, weights values, and compile() information. It is a light-weight alternative to SavedModel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95330c47-33f0-448d-ab93-f8fafa209d07",
   "metadata": {},
   "source": [
    "If saving only weights I get an error related to saving trainable and non-trainable weights all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b1dfd-744b-4792-9e04-661629b12e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\n",
    "# model.save(\"my_model\")\n",
    "\n",
    "# It can be used to reconstruct the model identically.\n",
    "# new_model = tf.keras.models.load_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b3353a-2450-4005-8a15-31eb31d1a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the resulting checkpoints (whole model with .h5) and choose the latest one.\n",
    "files_in_logdir = os.listdir(run_logdir)\n",
    "models = [file for file in files_in_logdir if file.endswith(\".h5\")]\n",
    "latest_model_name = sorted(models, reverse=True)[0]\n",
    "model_path = os.path.join(run_logdir, latest_model_name)\n",
    "print(model_path)\n",
    "\n",
    "# Load again the model.\n",
    "reconstructed_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Reconstructed model summary.\n",
    "reconstructed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d3d1c-e785-46da-87c2-12cd8b0e097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if necessary.\n",
    "# loss, accuracy = reconstructed_model.evaluate(test_dataset)\n",
    "# print('Reconstructed model\\n'\n",
    "#       + '- Loss on test: ' + str(round(loss, 6))\n",
    "#       + '\\n- Acc on test: ' + str(round(accuracy, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00316b45-d5c6-4a4e-960c-9d1cdee3042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if necessary.\n",
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print('Current model\\n'\n",
    "      + '- Loss on test: ' + str(round(loss, 6))\n",
    "      + '\\n- Acc on test: ' + str(round(accuracy, 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9220137-96f1-4eec-98fe-cf6f32d957f6",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beebf76-1963-444a-80e6-7f33b761a94f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714fd1c2-36d8-4940-bbfe-ac6ebfa70840",
   "metadata": {},
   "source": [
    "# Test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c1a23-cc79-4efa-a260-19daf8adb330",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in validation_dataset.take(1):    # only take first element of dataset, batch in this case as it is prefetched\n",
    "    \n",
    "    # Batch id.\n",
    "    idx = 2\n",
    "    numpy_batch_images = images.numpy()\n",
    "    numpy_batch_labels = labels.numpy()\n",
    "    \n",
    "    # Plot target img.\n",
    "    plt.imshow(numpy_batch_images[idx].astype(\"uint8\"))\n",
    "    print(class_names[np.argmax(numpy_batch_labels[idx])])\n",
    "    \n",
    "    # Prediction.\n",
    "    predictions = model.predict(numpy_batch_images)[idx]\n",
    "    print('\\n', predictions)\n",
    "    print(class_names[np.argmax(predictions)], predictions[np.argmax(predictions)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497cd1a2-d044-4d45-925f-07e21ebf75cb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071792e-b3a4-4769-97e8-fb951d4ee306",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4991301-7995-4608-acf1-6f5c46e70ff3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da501134-77ca-4bb1-a0dd-eb76aefd1df2",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lulc-venv",
   "language": "python",
   "name": "lulc-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

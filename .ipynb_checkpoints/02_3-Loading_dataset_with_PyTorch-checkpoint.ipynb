{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924096aa-e3ec-437c-81fe-03dd73fbfce5",
   "metadata": {},
   "source": [
    "**Loading the dataset and custom transforms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49adbc03-f1d2-446f-b2aa-028c144e8a4a",
   "metadata": {},
   "source": [
    "https://docs.lightly.ai/tutorials/package/tutorial_simsiam_esa.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7fc52-07dd-41f2-9fb4-510f27a20e82",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de16ef-fd9a-45fb-b620-a514366d9db4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb897a77-864f-4adb-959c-ab0e4bb2f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398826b-0d08-4e22-9144-9dfb3926f7f7",
   "metadata": {},
   "source": [
    "# Split the dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac795d71-e68b-45fe-b4e6-d73bb7005477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "801fc948-be0d-4504-9cb1-f37bad083cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying files: 0 files [00:00, ? files/s]\u001b[A\n",
      "Copying files: 3 files [00:00, 17.91 files/s]\u001b[A\n",
      "Copying files: 23 files [00:00, 100.88 files/s]\u001b[A\n",
      "Copying files: 36 files [00:00, 78.97 files/s] \u001b[A\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'datasets/Sentinel2GlobalLULC_full_raw/Sentinel2LULC_JPEG/'\n",
    "\n",
    "# Split with a ratio.\n",
    "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "splitfolders.ratio(data_dir, output=\"datasets/Sentinel2GlobalLULC_ratio\",\n",
    "                   seed=seed, ratio=(.7, .1, .2), group_prefix=None, move=False) # default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c0fcf4-ea66-4a25-9e6d-0e3158217588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split val/test with a fixed number of items, e.g. `(100, 100)`, for each set.\n",
    "# To only split into training and validation set, use a single number to `fixed`, i.e., `10`.\n",
    "# Set 3 values, e.g. `(300, 100, 100)`, to limit the number of training values.\n",
    "# splitfolders.fixed(data_dir, output=\"datasets/Sentinel2GlobalLULC_balanced\",\n",
    "#                    seed=seed, fixed=(7250, 1160, 1740), oversample=True, group_prefix=None, move=False) # default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f0e4687-8742-460e-b729-1248480210bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying files: 0 files [00:00, ? files/s]\u001b[A\n",
      "Copying files: 1166 files [00:00, 11656.87 files/s]\u001b[A\n",
      "Copying files: 2827 files [00:00, 14568.32 files/s]\u001b[A\n",
      "Copying files: 4405 files [00:00, 15118.39 files/s]\u001b[A\n",
      "Copying files: 5917 files [00:00, 11928.17 files/s]\u001b[A\n",
      "Copying files: 7936 files [00:00, 14470.92 files/s]\u001b[A\n",
      "Copying files: 9888 files [00:00, 16010.56 files/s]\u001b[A\n",
      "Copying files: 11939 files [00:00, 17377.33 files/s]\u001b[A\n",
      "Copying files: 14016 files [00:00, 18401.88 files/s]\u001b[A\n",
      "Copying files: 15914 files [00:00, 18576.25 files/s]\u001b[A\n",
      "Copying files: 18795 files [00:01, 16850.15 files/s]\u001b[A\n",
      "\n",
      "Oversampling: 0 classes [00:00, ? classes/s]\u001b[A\n",
      "Oversampling: 1 classes [00:00,  1.16 classes/s]\u001b[A\n",
      "Oversampling: 2 classes [00:01,  1.29 classes/s]\u001b[A\n",
      "Oversampling: 4 classes [00:02,  1.63 classes/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "#### Split val/test with a fixed number of items, e.g. `(100, 100)`, for each set.\n",
    "# To only split into training and validation set, use a single number to `fixed`, i.e., `10`.\n",
    "# Set 3 values, e.g. `(300, 100, 100)`, to limit the number of training values.\n",
    "# splitfolders.fixed(data_dir, output=\"datasets/Sentinel2GlobalLULC_balanced\",\n",
    "#                    seed=seed, fixed=(400, 100), oversample=True, group_prefix=None, move=False) # default values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a342e-d7c1-4999-8ed0-0a3118f5f3bf",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb94f55-b0da-4d7e-b461-f6a478166de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfandres/Documents/Git/lulc/lulc-venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import lightly\n",
    "from lightly.models.modules.heads import SimSiamPredictionHead,SimSiamProjectionHead\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e2c6c-4e75-4708-88d2-1ef7da79a9f0",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e327f67-c943-482a-8294-2c90685a1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'datasets/Sentinel2GlobalLULC-reduced/'\n",
    "input_size = 222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6847148a-96e4-4aec-9b2c-df7b83123446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchvision custom transforms.\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((input_size, input_size)),\n",
    "    torchvision.transforms.RandomResizedCrop(size=input_size, scale=(0.2, 1.0)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "    torchvision.transforms.GaussianBlur(21),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "188d464c-4f0c-4463-b28c-84dfc30ee0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchvision custom transforms.\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((input_size, input_size)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50bcc5-7e50-4d9e-962f-9a21343618a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(base_path + '/train/',\n",
    "                                           transform=train_transform)\n",
    "valid_data = datasets.ImageFolder(base_path + '/train/',\n",
    "                                           transform=test_transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lulc-venv",
   "language": "python",
   "name": "lulc-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

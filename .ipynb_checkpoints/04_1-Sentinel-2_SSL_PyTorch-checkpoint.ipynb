{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924096aa-e3ec-437c-81fe-03dd73fbfce5",
   "metadata": {},
   "source": [
    "**FIRST ATTEMP TO APPLY SSL TO THE SENTINEL-2 DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49adbc03-f1d2-446f-b2aa-028c144e8a4a",
   "metadata": {},
   "source": [
    "Reference tutorial: https://docs.lightly.ai/tutorials/package/tutorial_simsiam_esa.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f2f9be-dc8e-4b2c-ab6e-94407466cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c01ea34-c899-40a9-8a7f-e6cc346cee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74a41df-66ca-4a6c-935e-1277e006b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility.\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7fc52-07dd-41f2-9fb4-510f27a20e82",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de16ef-fd9a-45fb-b620-a514366d9db4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398826b-0d08-4e22-9144-9dfb3926f7f7",
   "metadata": {},
   "source": [
    "# Split the dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac795d71-e68b-45fe-b4e6-d73bb7005477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "\n",
    "data_dir_initial = 'datasets/Sentinel2GlobalLULC_full_raw/Sentinel2LULC_JPEG/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35055a2e-a266-4d21-bc85-51f6ce9716f1",
   "metadata": {},
   "source": [
    "## Ratio (imbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "801fc948-be0d-4504-9cb1-f37bad083cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 194877 files [00:46, 4158.47 files/s]\n"
     ]
    }
   ],
   "source": [
    "# Split with a ratio.\n",
    "# To only split into training and validation set,\n",
    "# set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "splitfolders.ratio(data_dir_initial,\n",
    "                   output=\"datasets/Sentinel2GlobalLULC_full_ratio_seed=\"\n",
    "                   + str(SEED),\n",
    "                   seed=SEED,\n",
    "                   ratio=(.7, .1, .2),\n",
    "                   group_prefix=None,\n",
    "                   move=False)  # Default values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5891bb1-003e-475e-810c-62cde7bb62f7",
   "metadata": {},
   "source": [
    "## Fixed (balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f0e4687-8742-460e-b729-1248480210bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 194877 files [00:28, 6776.75 files/s] \n",
      "Oversampling: 29 classes [00:25,  1.12 classes/s]\n"
     ]
    }
   ],
   "source": [
    "# Split val/test with a fixed number of items, e.g. `(100, 100)`, for each set.\n",
    "# To only split into train-val set, use a single number to `fixed`, i.e., `10`.\n",
    "# Set 3 values, e.g. `(300, 100, 100)`, to limit the number of training values.\n",
    "splitfolders.fixed(data_dir_initial,\n",
    "                   output=\"datasets/Sentinel2GlobalLULC_full_balanced_seed=\"\n",
    "                   + str(SEED),\n",
    "                   seed=SEED,\n",
    "                   fixed=(250, 100),\n",
    "                   oversample=True,\n",
    "                   group_prefix=None,\n",
    "                   move=False)  # Default values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9473fee0-fe3a-4211-ba88-837cca8424e8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f99239-de1b-4fa8-bb68-e7c4e4672f81",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a342e-d7c1-4999-8ed0-0a3118f5f3bf",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb94f55-b0da-4d7e-b461-f6a478166de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import lightly\n",
    "from lightly.models.modules.heads import SimSiamPredictionHead,SimSiamProjectionHead\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e7cab6-59ae-409b-ba11-2e857ab8fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamenters.\n",
    "input_size = 224  # input_size = 256\n",
    "batch_size = 32   # batch_size = 128\n",
    "num_workers = 8\n",
    "epochs = 5\n",
    "\n",
    "# Dimension of the embeddings.\n",
    "num_ftrs = 512\n",
    "# Dimension of the output of the prediction and projection heads.\n",
    "out_dim = proj_hidden_dim = 512\n",
    "# The prediction head uses a bottleneck architecture.\n",
    "pred_hidden_dim = 128\n",
    "\n",
    "# Seed torch and numpy.\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce08d1-0293-4830-a366-c149e95f0515",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6745ac7c-9497-4bf7-90b8-18a3dae2c27b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e2c6c-4e75-4708-88d2-1ef7da79a9f0",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e327f67-c943-482a-8294-2c90685a1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_target = 'datasets/Sentinel2GlobalLULC_ratio'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ef555-daa7-48d6-be77-9d069ce7404d",
   "metadata": {},
   "source": [
    "## Custom tranforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b451709-864e-42fe-841e-b30157619385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentations for self-supervised learning.\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((input_size, input_size)),\n",
    "    torchvision.transforms.RandomResizedCrop(size=input_size, scale=(0.2, 1.0)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "    torchvision.transforms.GaussianBlur(21),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((input_size, input_size)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff192ed-3aa6-4ee3-aab1-76d8a737d6c6",
   "metadata": {},
   "source": [
    "## ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50bcc5-7e50-4d9e-962f-9a21343618a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading both datasets.\n",
    "train_data = torchvision.datasets.ImageFolder(data_dir + '/train/')\n",
    "\n",
    "val_data = torchvision.datasets.ImageFolder(data_dir + '/val/')\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(data_dir + '/test/')\n",
    "\n",
    "train_data_lightly = lightly.data.LightlyDataset.from_torch_dataset(train_data)\n",
    "test_data_lightly = lightly.data.LightlyDataset.from_torch_dataset(test_data,\n",
    "                                                                   transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c7a8d-f598-417d-92f2-3b411a68dcc0",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b791f95-bbe1-4296-a8dc-9a1e3bb55923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentations for self-supervised learning.\n",
    "collate_fn_train = lightly.data.collate.BaseCollateFunction(train_transform)\n",
    "\n",
    "# Create a dataloader for training and embedding.\n",
    "dataloader_train_simsiam = torch.utils.data.DataLoader(\n",
    "    train_data_lightly,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_train,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    test_data_lightly,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967c24a-aa41-4dd3-b183-2232799c16a4",
   "metadata": {},
   "source": [
    "## Check balance and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa6845-e34c-4016-983d-688c1c0928cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check samples per class in train dataset.\n",
    "print(np.unique(train_data.targets, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b964a0-5d99-48aa-8108-fad4a1fdf2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check samples per class in val dataset.\n",
    "print(np.unique(val_data.targets, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52070963-a543-484f-a134-b5a1dde41fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data.targets))\n",
    "print(len(val_data.targets))\n",
    "print(len(test_data.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366bf8d4-6053-4115-8fd7-b628a5b9a284",
   "metadata": {},
   "source": [
    "## See some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d64513-dda3-4e75-853c-2290bcf40452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # Accessing Data and Targets in a PyTorch DataLoader\n",
    "# for images, labels, names in dataloader_train_simsiam:\n",
    "#     img = images[0][0]\n",
    "#     print(img.shape)\n",
    "#     # img = images[0].squeeze()\n",
    "#     # print(img)\n",
    "#     label = labels[0]\n",
    "#     plt.title(\"Label: \" + str(int(label)))\n",
    "#     plt.imshow(torch.permute(img,(1, 2, 0)))\n",
    "#     plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b873d6d4-6417-4c09-b6a1-a5b8545fc10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display image and label.\n",
    "# train_features, train_labels = next(iter(dataloader_train_simsiam))\n",
    "\n",
    "# print(f\"Features shape of the current batch is {train_features.size()}\")\n",
    "# print(f\"Labels shape of the current batch shape is {train_labels.size()}\")\n",
    "\n",
    "# img = train_features[0].squeeze()\n",
    "# label = train_labels[0]\n",
    "# plt.title(\"Label: \" + str(int(label)))\n",
    "# plt.imshow(torch.permute(img,(1, 2, 0)), cmap=\"gray\")\n",
    "# plt.show()\n",
    "# print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a4a6d8-28c2-422a-926d-3d5bd9aa69cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SimSiam model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c24a8-5d44-4fa9-ade5-a51c2517e991",
   "metadata": {},
   "source": [
    "## Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60885a39-131d-4def-86f0-2548aadfe7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimSiam(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, backbone, num_ftrs, proj_hidden_dim, pred_hidden_dim, out_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = SimSiamProjectionHead(\n",
    "            num_ftrs, proj_hidden_dim, out_dim\n",
    "        )\n",
    "        self.prediction_head = SimSiamPredictionHead(\n",
    "            out_dim, pred_hidden_dim, out_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get representations\n",
    "        f = self.backbone(x).flatten(start_dim=1)\n",
    "        # get projections\n",
    "        z = self.projection_head(f)\n",
    "        # get predictions\n",
    "        p = self.prediction_head(z)\n",
    "        # stop gradient\n",
    "        z = z.detach()\n",
    "        return z, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb30195-100c-428e-97c5-6941d5bcdd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a pretrained resnet for this tutorial to speed\n",
    "# up training time but you can also train one from scratch.\n",
    "resnet = torchvision.models.resnet18(weights=None) # ADDED\n",
    "backbone = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "model = SimSiam(backbone, num_ftrs, proj_hidden_dim, pred_hidden_dim, out_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529c018-fe2b-40f5-91d0-1708f4925e5f",
   "metadata": {},
   "source": [
    "SimSiam uses a symmetric negative cosine similarity loss and does therefore not require any negative samples. We build a criterion and an optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27a0c0-75d0-473e-9d6a-44075eaa6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(backbone, input_size=(batch_size, 3, input_size, input_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4308cc-0649-4f87-b574-bd306b931b21",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c498b42-1e1d-4740-b3ad-2daaecc0908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimSiam uses a symmetric negative cosine similarity loss\n",
    "criterion = lightly.loss.NegativeCosineSimilarity()\n",
    "\n",
    "# scale the learning rate\n",
    "lr = 0.05 * batch_size / 256\n",
    "# use SGD with momentum and weight decay\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665beb28-df9a-4010-8781-50423d27dda7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ac90b0-5240-462f-ba7b-15c3adcd9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745b5bb-e4a8-4886-83fd-e2ecbfbc6148",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f9d73-0fbc-4635-9fe5-60ebd16178de",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "avg_loss = 0.\n",
    "avg_output_std = 0.\n",
    "for e in range(epochs):\n",
    "    \n",
    "    t0 = time.time()  # Added by me.\n",
    "\n",
    "    for (x0, x1), _, _ in dataloader_train_simsiam:\n",
    "\n",
    "        # move images to the gpu\n",
    "        x0 = x0.to(device)\n",
    "        x1 = x1.to(device)\n",
    "\n",
    "        # run the model on both transforms of the images\n",
    "        # we get projections (z0 and z1) and\n",
    "        # predictions (p0 and p1) as output\n",
    "        z0, p0 = model(x0)\n",
    "        z1, p1 = model(x1)\n",
    "\n",
    "        # apply the symmetric negative cosine similarity\n",
    "        # and run backpropagation\n",
    "        loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculate the per-dimension standard deviation of the outputs\n",
    "        # we can use this later to check whether the embeddings are collapsing\n",
    "        output = p0.detach()\n",
    "        output = torch.nn.functional.normalize(output, dim=1)\n",
    "\n",
    "        output_std = torch.std(output, 0)\n",
    "        output_std = output_std.mean()\n",
    "\n",
    "        # use moving averages to track the loss and standard deviation\n",
    "        w = 0.9\n",
    "        avg_loss = w * avg_loss + (1 - w) * loss.item()\n",
    "        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\n",
    "\n",
    "    # the level of collapse is large if the standard deviation of the l2\n",
    "    # normalized output is much smaller than 1 / sqrt(dim)\n",
    "    collapse_level = max(0., 1 - math.sqrt(out_dim) * avg_output_std)\n",
    "    # print intermediate results\n",
    "    print(f'[Epoch {e:3d}] '\n",
    "          f'Loss = {avg_loss:.2f} | '\n",
    "          f'Collapse Level: {collapse_level:.2f} / 1.00 | '\n",
    "          f'Duration: {(time.time()-t0):.2f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7647762-b42b-40f0-b88e-cd5f487c7d13",
   "metadata": {},
   "source": [
    "### Saving the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91357f4f-c878-497a-b39a-c341caba885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(backbone.state_dict(), 'pytorch_models/simsiam_backbone_resnet18')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91de858-a1f9-4f98-90c4-586d9dee8ac1",
   "metadata": {},
   "source": [
    "### Embeddings for the samples of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db33812-0e52-4ba7-82ca-11da3895677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "filenames = []\n",
    "\n",
    "# disable gradients for faster calculations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (x, _, fnames) in enumerate(dataloader_test):\n",
    "        # move the images to the gpu\n",
    "        x = x.to(device)\n",
    "        # embed the images with the pre-trained backbone\n",
    "        y = model.backbone(x).flatten(start_dim=1)\n",
    "        # store the embeddings and filenames in lists\n",
    "        embeddings.append(y)\n",
    "        filenames = filenames + list(fnames)\n",
    "\n",
    "# concatenate the embeddings and convert to numpy\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "embeddings = embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e176a-477a-4732-bd6a-960e98170a13",
   "metadata": {},
   "source": [
    "# Scatter Plot and Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c2198-cf59-4222-9cdb-ec438c998775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.offsetbox as osb\n",
    "from matplotlib import rcParams as rcp\n",
    "\n",
    "# for resizing images to thumbnails\n",
    "import torchvision.transforms.functional as functional\n",
    "\n",
    "# for clustering and 2d representations\n",
    "from sklearn import random_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb003c-70f0-46be-ade6-37e35e453aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the scatter plot we want to transform the images to a two-dimensional\n",
    "# vector space using a random Gaussian projection\n",
    "projection = random_projection.GaussianRandomProjection(n_components=2)\n",
    "embeddings_2d = projection.fit_transform(embeddings)\n",
    "\n",
    "# normalize the embeddings to fit in the [0, 1] square\n",
    "M = np.max(embeddings_2d, axis=0)\n",
    "m = np.min(embeddings_2d, axis=0)\n",
    "embeddings_2d = (embeddings_2d - m) / (M - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e880d4-d7a2-461c-8f9a-ee40a26cc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scatter_plot_with_thumbnails():\n",
    "    \"\"\"Creates a scatter plot with image overlays.\n",
    "    \"\"\"\n",
    "    # initialize empty figure and add subplot\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    fig.suptitle('Scatter Plot of the Sentinel-2 Dataset')\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    # shuffle images and find out which images to show\n",
    "    shown_images_idx = []\n",
    "    shown_images = np.array([[1., 1.]])\n",
    "    iterator = [i for i in range(embeddings_2d.shape[0])]\n",
    "    np.random.shuffle(iterator)\n",
    "    for i in iterator:\n",
    "        # only show image if it is sufficiently far away from the others\n",
    "        dist = np.sum((embeddings_2d[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < 2e-3:\n",
    "            continue\n",
    "        shown_images = np.r_[shown_images, [embeddings_2d[i]]]\n",
    "        shown_images_idx.append(i)\n",
    "\n",
    "    # plot image overlays\n",
    "    for idx in shown_images_idx:\n",
    "        thumbnail_size = int(rcp['figure.figsize'][0] * 10.)\n",
    "        path = os.path.join(data_dir + '/test/', filenames[idx])\n",
    "        img = Image.open(path)\n",
    "        img = functional.resize(img, thumbnail_size)\n",
    "        img = np.array(img)\n",
    "        img_box = osb.AnnotationBbox(\n",
    "            osb.OffsetImage(img, cmap=plt.cm.gray_r),\n",
    "            embeddings_2d[idx],\n",
    "            pad=0.2,\n",
    "        )\n",
    "        ax.add_artist(img_box)\n",
    "\n",
    "    # set aspect ratio\n",
    "    ratio = 1. / ax.get_data_ratio()\n",
    "    ax.set_aspect(ratio, adjustable='box')\n",
    "\n",
    "\n",
    "# get a scatter plot with thumbnail overlays\n",
    "get_scatter_plot_with_thumbnails()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998cde14-fb59-4330-b070-467a54537e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subdirectories (classes)\n",
    "directory_list = []\n",
    "for root, dirs, files in os.walk(data_dir + '/test/'):\n",
    "    for dirname in sorted(dirs):\n",
    "        directory_list.append(os.path.join(root, dirname))\n",
    "        print(dirname)\n",
    "\n",
    "# print(directory_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae54d2-c622-493a-96fa-f98fa931c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_images = []\n",
    "for classes in directory_list:\n",
    "    random_file = np.random.choice(os.listdir(classes))\n",
    "    path_to_random_file = classes + '/' + random_file\n",
    "    example_images.append(path_to_random_file[40:])\n",
    "    print(path_to_random_file)\n",
    "\n",
    "# example_images = example_images[:3]\n",
    "# print(example_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45da9b-2032-4bf1-b00c-6515ecf963c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_as_np_array(filename: str):\n",
    "    \"\"\"\n",
    "    Loads the image with filename and returns it as a numpy array.\n",
    "    \n",
    "    \"\"\"\n",
    "    img = Image.open(filename)\n",
    "    return np.asarray(img)\n",
    "\n",
    "\n",
    "def get_image_as_np_array_with_frame(filename: str, w: int = 5):\n",
    "    \"\"\"\n",
    "    Returns an image as a numpy array with a black frame of width w.\n",
    "\n",
    "    \"\"\"\n",
    "    img = get_image_as_np_array(filename)\n",
    "    ny, nx, _ = img.shape\n",
    "    # create an empty image with padding for the frame\n",
    "    framed_img = np.zeros((w + ny + w, w + nx + w, 3))\n",
    "    framed_img = framed_img.astype(np.uint8)\n",
    "    # put the original image in the middle of the new one\n",
    "    framed_img[w:-w, w:-w] = img\n",
    "    return framed_img\n",
    "\n",
    "\n",
    "def plot_nearest_neighbors_nxn(example_image: str, i: int):\n",
    "    \"\"\"\n",
    "    Plots the example image and its eight nearest neighbors.\n",
    "\n",
    "    \"\"\"\n",
    "    n_subplots = 6\n",
    "    # initialize empty figure\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    fig.suptitle(f\"Nearest Neighbor Plot {i + 1}\")\n",
    "    #\n",
    "    example_idx = filenames.index(example_image)\n",
    "    # get distances to the cluster center\n",
    "    distances = embeddings - embeddings[example_idx]\n",
    "    distances = np.power(distances, 2).sum(-1).squeeze()\n",
    "    # sort indices by distance to the center\n",
    "    nearest_neighbors = np.argsort(distances)[:n_subplots]\n",
    "    # show images\n",
    "    for plot_offset, plot_idx in enumerate(nearest_neighbors):\n",
    "        ax = fig.add_subplot(3, 3, plot_offset + 1)\n",
    "        # get the corresponding filename\n",
    "        fname = os.path.join(data_dir + '/test/', filenames[plot_idx])\n",
    "        if plot_offset == 0:\n",
    "            ax.set_title(f\"Example Image\")\n",
    "            plt.imshow(get_image_as_np_array_with_frame(fname))\n",
    "        else:\n",
    "            plt.imshow(get_image_as_np_array(fname))\n",
    "        # let's disable the axis\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "# Show example images for each cluster.\n",
    "for i, example_image in enumerate(example_images):\n",
    "    plot_nearest_neighbors_nxn(example_image, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f809fc8-79ab-4389-a8ed-5e6a98421fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lulc-venv",
   "language": "python",
   "name": "lulc-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

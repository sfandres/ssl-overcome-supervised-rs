{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924096aa-e3ec-437c-81fe-03dd73fbfce5",
   "metadata": {},
   "source": [
    "**TRAINING SSL MODELS USING THE ENTIRE SENTINEL-2 DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49adbc03-f1d2-446f-b2aa-028c144e8a4a",
   "metadata": {},
   "source": [
    "Reference: https://docs.lightly.ai/tutorials/package/tutorial_simsiam_esa.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7fc52-07dd-41f2-9fb4-510f27a20e82",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de16ef-fd9a-45fb-b620-a514366d9db4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a342e-d7c1-4999-8ed0-0a3118f5f3bf",
   "metadata": {},
   "source": [
    "# Initial configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c506cd5d-71b7-47d7-b157-1b58caee0936",
   "metadata": {},
   "source": [
    "## Libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c34d98-df6d-4d36-be24-f18247520ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.other import is_notebook, build_paths\n",
    "\n",
    "# Load notebook extensions.\n",
    "if is_notebook():\n",
    "    %load_ext tensorboard\n",
    "    %load_ext pycodestyle_magic\n",
    "    %pycodestyle_on\n",
    "    %env RAY_PICKLE_VERBOSE_DEBUG=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb94f55-b0da-4d7e-b461-f6a478166de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom modules.\n",
    "from utils.reproducibility import set_seed, seed_worker\n",
    "from utils.dataset import load_dataset_based_on_ratio, GaussianBlur\n",
    "from utils.computation import pca_computation, tsne_computation\n",
    "from utils.simsiam import SimSiam\n",
    "from utils.simclr import SimCLR\n",
    "from utils.mocov2 import MoCov2\n",
    "from utils.barlowtwins import BarlowTwins\n",
    "from utils.graphs import simple_bar_plot\n",
    "\n",
    "# Arguments and paths.\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "# PyTorch.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models import (\n",
    "    resnet18,\n",
    "    ResNet18_Weights,\n",
    "    resnet50,\n",
    "    ResNet50_Weights\n",
    ")\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "# For resizing images to thumbnails.\n",
    "import torchvision.transforms.functional as functional\n",
    "\n",
    "# Data management.\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# SSL library.\n",
    "import lightly\n",
    "from lightly.utils.scheduler import cosine_schedule\n",
    "\n",
    "# Training checks.\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Hyperparameter tunning.\n",
    "from functools import partial\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# For plotting.\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.offsetbox as osb\n",
    "from matplotlib import rcParams as rcp\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# For clustering and 2d representations.\n",
    "from sklearn import random_projection\n",
    "\n",
    "# Showing images in the notebook.\n",
    "# from IPython.display import Image\n",
    "# from IPython.core.display import HTML\n",
    "\n",
    "# Other imports.\n",
    "# import copy\n",
    "# from lightly.utils.debug import std_of_l2_normalized\n",
    "# import matplotlib.font_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3eada4-fb42-434c-94d6-c4a6fb73aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAIL_SSL_MODELS = ['SimSiam', 'SimCLR', 'SimCLRv2', 'BarlowTwins', 'MoCov2']\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a2c0f8-81f3-4fde-9419-78785a46378f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Enable reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9265fd01-199d-4958-af80-bc2d88d89d8c",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/docs/stable/notes/randomness.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4cba4e-5fd7-4dd7-9640-25628df69dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'torch initial seed:'.ljust(20)} {torch.initial_seed()}\")\n",
    "g = set_seed(SEED)\n",
    "print(f\"{'torch current seed:'.ljust(20)} {torch.initial_seed()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa973f55-8303-4ba9-8e27-5154a13423ef",
   "metadata": {},
   "source": [
    "## Check torch CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf235e1-2c3a-4ca5-9119-aaf53d5e1316",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'torch.cuda.is_available():'.ljust(32)}\"\n",
    "      f\"{torch.cuda.is_available()}\")\n",
    "print(f\"{'torch.cuda.device_count():'.ljust(32)}\"\n",
    "      f\"{torch.cuda.device_count()}\")\n",
    "print(f\"{'torch.cuda.current_device():'.ljust(32)}\"\n",
    "      f\"{torch.cuda.current_device()}\")\n",
    "print(f\"{'torch.cuda.device(0):'.ljust(32)}\"\n",
    "      f\"{torch.cuda.device(0)}\")\n",
    "print(f\"{'torch.cuda.get_device_name(0):'.ljust(32)}\"\n",
    "      f\"{torch.cuda.get_device_name(0)}\")\n",
    "print(f\"{'torch.backends.cudnn.benchmark:'.ljust(32)}\"\n",
    "      f\"{torch.backends.cudnn.benchmark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9b774-045a-41ba-bf50-2af289850e99",
   "metadata": {},
   "source": [
    "## Command line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638cc5c-5615-4691-9f74-3f694370d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get arguments.\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Script for training the self-supervised learning models.\"\n",
    ")\n",
    "\n",
    "parser.add_argument('model_name', type=str,\n",
    "                    choices=AVAIL_SSL_MODELS,\n",
    "                    help=\"target SSL model.\")\n",
    "\n",
    "parser.add_argument('--backbone_name', '-bn', type=str, default='resnet18',\n",
    "                    choices=['resnet18', 'resnet50'],\n",
    "                    help=\"backbone model name (default: resnet18).\")\n",
    "\n",
    "parser.add_argument('--dataset_name', '-dn', type=str,\n",
    "                    default='Sentinel2GlobalLULC_SSL',\n",
    "                    help='dataset name for training '\n",
    "                         '(default: Sentinel2GlobalLULC_SSL).')\n",
    "\n",
    "parser.add_argument('--dataset_ratio', '-dr', type=str,\n",
    "                    default='(0.900,0.0250,0.0750)',\n",
    "                    help='dataset ratio for evaluation '\n",
    "                         '(default: (0.900,0.0250,0.0750)).')\n",
    "\n",
    "parser.add_argument('--epochs', '-e', type=int, default=25,\n",
    "                    help='number of epochs for training (default: 25).')\n",
    "\n",
    "parser.add_argument('--batch_size', '-bs', type=int, default=64,\n",
    "                    help='number of images in a batch during training '\n",
    "                         '(default: 64).')\n",
    "\n",
    "parser.add_argument('--ini_weights', '-iw', type=str, default='random',\n",
    "                    choices=['random', 'imagenet'],\n",
    "                    help=\"initial weights (default: random).\")\n",
    "\n",
    "parser.add_argument('--show', '-s', action='store_true',\n",
    "                    help='the images should appear.')\n",
    "\n",
    "parser.add_argument('--balanced_dataset', '-bd', action='store_true',\n",
    "                    help='whether the dataset should be balanced.')\n",
    "\n",
    "parser.add_argument('--reduced_dataset', '-rd', action='store_true',\n",
    "                    help='whether the dataset should be reduced.')\n",
    "\n",
    "parser.add_argument('--cluster', '-c', action='store_true',\n",
    "                    help='the script runs on a cluster (large mem. space).')\n",
    "\n",
    "parser.add_argument('--torch_compile', '-tc', action='store_true',\n",
    "                    help='PyTorch 2.0 compile enabled.')\n",
    "\n",
    "parser.add_argument('--resume_training', '-r', action='store_true',\n",
    "                    help='training is resumed from the latest checkpoint.')\n",
    "\n",
    "parser.add_argument('--ray_tune', '-rt', type=str,\n",
    "                    choices=['gridsearch', 'loguniform'],\n",
    "                    help='enables Ray Tune (tunes everything or only lr).')\n",
    "\n",
    "parser.add_argument('--grace_period', '-gp', type=int, default=5,\n",
    "                    help='only stop trials at least this old in time.')\n",
    "\n",
    "parser.add_argument('--num_samples_trials', '-nst', type=int, default=10,\n",
    "                    help='number of samples to tune the hyperparameters.')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c23fc2-82d4-49b9-8fb3-d02655df2c3c",
   "metadata": {},
   "source": [
    "## Simulate and get input arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d777d3-067b-4f7d-9c5e-a70091683dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input arguments.\n",
    "if is_notebook():\n",
    "    args = parser.parse_args(\n",
    "        args=[\n",
    "            'SimSiam',\n",
    "            '--backbone_name=resnet18',\n",
    "            '--dataset_name=Sentinel2GlobalLULC_SSL',\n",
    "            '--dataset_ratio=(0.900,0.0250,0.0750)',\n",
    "            '--epochs=10',\n",
    "            '--batch_size=64',\n",
    "            '--ini_weights=random',\n",
    "            '--show',\n",
    "            # '--resume_training',\n",
    "            # '--reduced_dataset',\n",
    "            # '--ray_tune=gridsearch',\n",
    "            # '--grace_period=1',\n",
    "            # '--num_samples_trials=1',\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    args = parser.parse_args(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ddea1-bb6a-4c9c-b848-d710f9475c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the parsed arguments into a dictionary and declare\n",
    "# variables with the same name as the arguments.\n",
    "args_dict = vars(args)\n",
    "for arg_name in args_dict:\n",
    "    globals()[arg_name] = args_dict[arg_name]\n",
    "\n",
    "# Iterate over the keys of the dictionary and check whether\n",
    "# the corresponding variables have been declared.\n",
    "print()\n",
    "for arg_name in args_dict:\n",
    "    if arg_name in globals():\n",
    "        arg_name_col = f'{arg_name}:'\n",
    "        print(f'{arg_name_col.ljust(20)} {globals()[arg_name]}')\n",
    "    else:\n",
    "        print(f'{arg_name} has not been declared')\n",
    "\n",
    "# Avoiding the runtimeError: \"Too many open files.\n",
    "# Communication with the workers is no longer possible.\"\n",
    "if is_notebook() or cluster:\n",
    "    print(' - Torch sharing strategy set to file_descriptor (default)')\n",
    "    torch.multiprocessing.set_sharing_strategy('file_descriptor')\n",
    "else:\n",
    "    print(' - Torch sharing strategy set to file_system (less memory)')\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "# Setting the device.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"{'Device:'.ljust(23)} {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216018d-6b4c-4d41-b862-898e571ee07d",
   "metadata": {},
   "source": [
    "## Build paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bbaf91-f497-4edb-83a8-0b2af015ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current directory.\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Build paths.\n",
    "paths = build_paths(cwd, model_name)\n",
    "\n",
    "# Show built paths.\n",
    "print()\n",
    "for path in paths:\n",
    "    path_name_col = f'{path}:'\n",
    "    print(f'{path_name_col.ljust(20)} {paths[path]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2265f6b-9854-4e2e-85bd-3e7c88792e77",
   "metadata": {},
   "source": [
    "## Settings and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e55b00-2ff7-4334-a047-8abd7edae0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the images.\n",
    "input_size = 224\n",
    "\n",
    "# Format of the saved images.\n",
    "fig_format = '.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897cdadd-b624-406c-8c2d-d4f57d796a21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load two pretrained models (ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac343186-a680-43d6-bf92-e6d8e3105651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nModel with pretrained weights using SSL')\n",
    "# resnet18 = torchvision.models.resnet18(weights=None)\n",
    "\n",
    "# # Only backbone.\n",
    "# pt_backbone = torch.nn.Sequential(*list(resnet18.children())[:-1])\n",
    "\n",
    "# # List of trained models.\n",
    "# model_list = []\n",
    "# print()\n",
    "# for root, dirs, files in os.walk(input_dir_models):\n",
    "#     for i, filename in enumerate(sorted(files, reverse=True)):\n",
    "#         model_list.append(os.path.join(root, filename))\n",
    "#         print(f'{i:02} --> {filename}')\n",
    "\n",
    "# # Loading model.\n",
    "# idx = 0\n",
    "# print(f'\\nLoaded: {model_list[idx]}')\n",
    "# pt_backbone.load_state_dict(torch.load(model_list[idx]))\n",
    "\n",
    "# # Adding a linear layer on top of the model (linear classifier).\n",
    "# model = torch.nn.Sequential(\n",
    "#     pt_backbone,\n",
    "#     torch.nn.Flatten(),\n",
    "#     torch.nn.Linear(in_features=512, out_features=10, bias=True),\n",
    "#     # torch.nn.Softmax(dim=1)\n",
    "# )\n",
    "\n",
    "# print()\n",
    "# print(model[0][0])\n",
    "# print(model[0][0].weight[3])\n",
    "\n",
    "# # Loading model.\n",
    "# idx = 5\n",
    "# print(f'\\nLoaded: {model_list[idx]}')\n",
    "# pt_backbone.load_state_dict(torch.load(model_list[idx]))\n",
    "\n",
    "# # Adding a linear layer on top of the model (linear classifier).\n",
    "# model = torch.nn.Sequential(\n",
    "#     pt_backbone,\n",
    "#     torch.nn.Flatten(),\n",
    "#     torch.nn.Linear(in_features=512, out_features=10, bias=True),\n",
    "#     # torch.nn.Softmax(dim=1)\n",
    "# )\n",
    "\n",
    "# print()\n",
    "# print(model[0][0])\n",
    "# print(model[0][0].weight[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a51e2-5a68-46c6-9af3-ea0b2c00b0b3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3306c7fc-799f-4cf2-9829-5c1549b2c641",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e2c6c-4e75-4708-88d2-1ef7da79a9f0",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c800357-7691-4df2-bb86-075b15691d3e",
   "metadata": {},
   "source": [
    "## Load normalization values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963bb28a-99f4-46fa-864f-12892332b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the path, mean and std values of each split from\n",
    "# a .txt file previously generated using a custom script.\n",
    "paths[dataset_name], mean, std = load_dataset_based_on_ratio(\n",
    "    paths['datasets'],\n",
    "    dataset_name,\n",
    "    dataset_ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ef555-daa7-48d6-be77-9d069ce7404d",
   "metadata": {},
   "source": [
    "## Custom transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a8172-7c2c-49a0-99de-223d3b0ebbca",
   "metadata": {},
   "source": [
    "Define the augmentations for self-supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e8826-babc-4e00-b507-7fc6791919cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "# Normalization transform (val and test).\n",
    "transform = {x: transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean[x],\n",
    "                         std=std[x])\n",
    "]) for x in splits[1:]}\n",
    "\n",
    "# Normalization transform (train).\n",
    "# from https://github.com/facebookresearch/simsiam/blob/main/main_simsiam.py\n",
    "# MoCo v2's aug: similar to SimCLR https://arxiv.org/abs/2002.05709\n",
    "transform['train'] = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.2, 1.)),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(.4, .4, .4, .1)  # not strengthened\n",
    "    ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean['train'],\n",
    "                         std['train'])\n",
    "])\n",
    "\n",
    "for t in transform:\n",
    "    print(f'\\n{t}: {transform[t]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff192ed-3aa6-4ee3-aab1-76d8a737d6c6",
   "metadata": {},
   "source": [
    "## ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc75351-a81d-4a3e-99da-5408b47f3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the three datasets with ImageFolder.\n",
    "dataset = {x: torchvision.datasets.ImageFolder(\n",
    "    os.path.join(paths[dataset_name], x)) for x in splits}\n",
    "\n",
    "# for d in dataset:\n",
    "#     print(f'\\n{d}: {dataset[d]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1aa1d-3350-4786-8bd1-7e45ca32a1da",
   "metadata": {},
   "source": [
    "## Dealing with imbalanced data (option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf7498-f773-4b39-b236-86d8957a691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if balanced_dataset:\n",
    "\n",
    "    # Creating a list of labels of samples.\n",
    "    train_sample_labels = dataset['train'].targets\n",
    "\n",
    "    # Calculating the number of samples per label/class.\n",
    "    class_sample_count = np.unique(train_sample_labels,\n",
    "                                   return_counts=True)[1]\n",
    "    print(class_sample_count)\n",
    "\n",
    "    # Weight per sample not per class.\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in train_sample_labels])\n",
    "\n",
    "    # Casting.\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    samples_weigth = samples_weight.double()\n",
    "\n",
    "    # Sampler, imbalanced data.\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        samples_weight,\n",
    "        len(samples_weight)\n",
    "    )\n",
    "    shuffle = False\n",
    "\n",
    "else:\n",
    "    sampler = None\n",
    "    shuffle = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23aae57-f332-47ed-aef8-261fb3a33bef",
   "metadata": {},
   "source": [
    "## Creating a reduced subset (option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ceeaa4-b031-46c9-a42c-8004b6ab76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reduced_dataset:\n",
    "\n",
    "    # Get the number of samples in the full dataset.\n",
    "    num_samples = len(dataset['train'])\n",
    "\n",
    "    # Get the labels.\n",
    "    labels = dataset['train'].targets\n",
    "\n",
    "    # Get the unique labels and their corresponding counts in the dataset.\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    # Set the percentage of samples you want to keep.\n",
    "    percent_keep = 0.05\n",
    "\n",
    "    # Calculate the number of samples to keep for each label.\n",
    "    num_keep = np.ceil(percent_keep * label_counts).astype(int)\n",
    "\n",
    "    # Create a list of indices for the samples to keep.\n",
    "    keep_indices = []\n",
    "    for i in range(len(unique_labels)):\n",
    "        label_indices_i = np.where(labels == unique_labels[i])[0]\n",
    "        np.random.shuffle(label_indices_i)\n",
    "        keep_indices_i = label_indices_i[:num_keep[i]]\n",
    "        keep_indices.extend(keep_indices_i)\n",
    "\n",
    "    # Create a SubsetRandomSampler using the keep indices.\n",
    "    sampler = SubsetRandomSampler(keep_indices)\n",
    "    shuffle = False\n",
    "\n",
    "else:\n",
    "    sampler = None\n",
    "    shuffle = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444f059-271c-4afa-84a0-5627885ef989",
   "metadata": {},
   "source": [
    "## Cast to Lightly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c08391-8cbf-467f-9769-a7e9bed8d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a LightlyDataset from a PyTorch (or torchvision) dataset.\n",
    "# Returns a tuple (sample, target, fname) when accessed using __getitem__.\n",
    "lightly_dataset = {x: lightly.data.LightlyDataset.from_torch_dataset(\n",
    "    dataset[x]) for x in splits}\n",
    "\n",
    "# print()\n",
    "# for d in lightly_dataset:\n",
    "#     print(f'{d}:\\t{lightly_dataset[d]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c1996-9ed9-4597-bb3a-3819f70bb22d",
   "metadata": {},
   "source": [
    "## Collate functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6088d6-ddbd-4829-a1ee-dbbc60b5d522",
   "metadata": {},
   "source": [
    "PyTorch uses a Collate Function to combine the data in your batches together.\n",
    "\n",
    "BaseCollateFunction (base class) takes a batch of images as input and <b>transforms each image into two different augmentations</b> with the help of random transforms. The images are then concatenated such that the output batch is exactly twice the length of the input batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33670d-1bed-4d12-a117-279d43b98089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class for other collate implementations.\n",
    "# This allows training.\n",
    "collate_fn = {x: lightly.data.collate.BaseCollateFunction(\n",
    "    transform[x]) for x in splits}\n",
    "\n",
    "# print()\n",
    "# for c in collate_fn:\n",
    "#     print(f'{c}:\\t{collate_fn[c]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba89a4-1ae0-4411-a958-43c0ffe1fc8c",
   "metadata": {},
   "source": [
    "**Important note:** These functions could be removed if I implement a custom load dataset with a get_item that gets and tranforms two batches of images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c7a8d-f598-417d-92f2-3b411a68dcc0",
   "metadata": {},
   "source": [
    "## PyTorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4beea0f-2621-495a-b479-56efdf6bc0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nSampler: {sampler}')\n",
    "print(f'Shuffle:   {shuffle}')\n",
    "\n",
    "# Dataloader for validating and testing.\n",
    "dataloader = {x: torch.utils.data.DataLoader(\n",
    "    lightly_dataset[x],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn[x],\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    worker_init_fn=seed_worker if not ray_tune else None,\n",
    "    generator=g if not ray_tune else None\n",
    ") for x in splits[1:]}\n",
    "\n",
    "# Dataloader for training.\n",
    "dataloader['train'] = torch.utils.data.DataLoader(\n",
    "    lightly_dataset['train'],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    sampler=sampler,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn['train'],\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    worker_init_fn=seed_worker if not ray_tune else None,\n",
    "    generator=g if not ray_tune else None\n",
    ")\n",
    "\n",
    "# Check if shuffle is enabled.\n",
    "if isinstance(dataloader['train'].sampler, torch.utils.data.RandomSampler):\n",
    "    print('\\nShuffle enabled in training!')\n",
    "else:\n",
    "    print('\\nShuffle disabled in training!')\n",
    "\n",
    "# for d in dataloader:\n",
    "#     print(f\"\\n{d}:\\t{vars(dataloader[d])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967c24a-aa41-4dd3-b183-2232799c16a4",
   "metadata": {},
   "source": [
    "## Check the balance and size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049b932-ae39-409d-9cdc-0eccf7c4b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check samples per class, total samples and batches of each dataset.\n",
    "for d in dataset:\n",
    "    samples = np.unique(dataset[d].targets, return_counts=True)[1]\n",
    "    print(f'\\n{d}:')\n",
    "    print(f'  - #Samples/class (from dataset):\\n{samples}')\n",
    "    print(f'  - #Samples (from dataset):  {len(dataset[d].targets)}')\n",
    "    print(f'  - #Batches from dataloader: {len(dataloader[d])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc5860-964d-4847-bc93-11327f9a2bd2",
   "metadata": {},
   "source": [
    "## Check the distribution of samples in the dataloader (lightly dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd5b5f1-f14d-4c9e-addd-e8e0eae9d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List to save the labels.\n",
    "# labels_list = []\n",
    "\n",
    "# # Accessing Data and Targets in a PyTorch DataLoader.\n",
    "# t0 = time.time()\n",
    "# for i, (images, labels, names) in enumerate(dataloader['train']):\n",
    "#     labels_list.append(labels)\n",
    "\n",
    "# # Concatenate list of lists (batches).\n",
    "# labels_list = torch.cat(labels_list, dim=0).numpy()\n",
    "# print(f'\\nSample distribution computation in train dataset (s): '\n",
    "#       f'{(time.time()-t0):.2f}')\n",
    "\n",
    "# # Count number of unique values.\n",
    "# data_x, data_y = np.unique(labels_list, return_counts=True)\n",
    "\n",
    "# # New function to plot (suitable for execution in shell).\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "# simple_bar_plot(ax,\n",
    "#                 data_x,\n",
    "#                 'Class',\n",
    "#                 data_y,\n",
    "#                 'N samples (dataloader)')\n",
    "\n",
    "# plt.gcf().subplots_adjust(bottom=0.15)\n",
    "# plt.gcf().subplots_adjust(left=0.15)\n",
    "# fig_name_save = (f'sample_distribution'\n",
    "#                  f'-ratio={dataset_ratio}'\n",
    "#                  f'-balanced_dataset={balanced_dataset}'\n",
    "#                  f'-reduced_dataset={reduced_dataset}')\n",
    "# fig.savefig(os.path.join(paths['images'], fig_name_save+fig_format),\n",
    "#             bbox_inches='tight')\n",
    "\n",
    "# plt.show() if show else plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366bf8d4-6053-4115-8fd7-b628a5b9a284",
   "metadata": {},
   "source": [
    "## Look at some training samples (lightly dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4a8eb8-0133-4f1d-954a-6228f05cb6cd",
   "metadata": {},
   "source": [
    "### Only one sample from the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d64513-dda3-4e75-853c-2290bcf40452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing Data and Targets in a PyTorch DataLoader.\n",
    "if show:\n",
    "    for i, (images, labels, names) in enumerate(dataloader['train']):\n",
    "        img = images[0][0]\n",
    "        label = labels[0]\n",
    "        print(images[0].shape)\n",
    "        print(labels.shape)\n",
    "        plt.title(\"Label: \" + str(int(label)))\n",
    "        plt.imshow(torch.permute(img, (1, 2, 0)))\n",
    "        plt.show() if show else plt.close()\n",
    "        if i == 0:\n",
    "            break  # Only a few batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeaf97d-4427-4093-9b8d-85e8ea574ab7",
   "metadata": {},
   "source": [
    "### Two batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a80b14-6499-47dc-8aa4-d612aba00d14",
   "metadata": {},
   "source": [
    "Note: Comment out the normalization augmentation first to view the images below properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1bec3-ccfd-4947-b06d-abd7ab3a4cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(batch, batch_id):\n",
    "    \"\"\"\n",
    "    Shows the images in the batch.\n",
    "\n",
    "    Attributes:\n",
    "        batch: Batch of images.\n",
    "        batch_id: Batch identification number.\n",
    "    \"\"\"\n",
    "\n",
    "    columns = 8\n",
    "    rows = 2\n",
    "    width = 30\n",
    "    height = 5\n",
    "\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    fig.suptitle(f'Batch {batch_id}')\n",
    "    for i in range(1, columns * rows + 1):\n",
    "        if i < batch_size:\n",
    "            img = batch[i]\n",
    "            fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(torch.permute(img, (1, 2, 0)))\n",
    "\n",
    "    plt.show() if show else plt.close()\n",
    "\n",
    "\n",
    "# Train loop.\n",
    "if show:\n",
    "    for b, ((x0, x1), _, _) in enumerate(dataloader['train']):\n",
    "\n",
    "        # Show the images within the first batch.\n",
    "        show_batch(x0, 0)\n",
    "        show_batch(x1, 1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3352095f-6c8e-4b28-88cf-ed2625b7673e",
   "metadata": {},
   "source": [
    "Each image is augmented differently in the two batches that are loaded at the same time during training. The dataloader from lightly is capable of providing two batches in one iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5266e3-6f57-4808-bcb6-8904f27c7e28",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba437d33-1a33-48db-837a-d84c68b8b4fb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a4a6d8-28c2-422a-926d-3d5bd9aa69cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Self-supervised models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4308cc-0649-4f87-b574-bd306b931b21",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745b5bb-e4a8-4886-83fd-e2ecbfbc6148",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320a250-fae9-4591-bff9-c62b46127264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    config: dict\n",
    "):\n",
    "\n",
    "    # ======================\n",
    "    # DEFINE MODELS.\n",
    "    # Setting the model and initial weights.\n",
    "    if backbone_name == 'resnet18':\n",
    "        if ini_weights == 'imagenet':\n",
    "            resnet = resnet18(\n",
    "                weights=ResNet18_Weights.DEFAULT,\n",
    "                # zero_init_residual=True\n",
    "            )\n",
    "        elif ini_weights == 'random':\n",
    "            resnet = resnet18(\n",
    "                weights=None,\n",
    "                # zero_init_residual=True\n",
    "            )\n",
    "    elif backbone_name == 'resnet50':\n",
    "        if ini_weights == 'imagenet':\n",
    "            resnet = resnet50(\n",
    "                weights=ResNet50_Weights.DEFAULT,\n",
    "                # zero_init_residual=True\n",
    "            )\n",
    "        elif ini_weights == 'random':\n",
    "            resnet = resnet50(\n",
    "                weights=None,\n",
    "                # zero_init_residual=True\n",
    "            )\n",
    "\n",
    "    # Show model's backbone structure.\n",
    "    if show and not cluster:\n",
    "        print(summary(\n",
    "            resnet,\n",
    "            input_size=(batch_size, 3, input_size, input_size),\n",
    "            device=device)\n",
    "        )\n",
    "\n",
    "    # Removing head from resnet. Embedding.\n",
    "    backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    input_dim = hidden_dim = resnet.fc.in_features\n",
    "    print(f'\\nModel name: {model_name}')\n",
    "    print(f'Backbone: {backbone_name}')\n",
    "    print(f\"Hidden layer dimension: {config['hidden_dim']}\")\n",
    "    print(f\"Output layer dimension: {config['out_dim']}\")\n",
    "\n",
    "    if model_name == 'SimSiam':\n",
    "        model = SimSiam(backbone=backbone, input_dim=input_dim, proj_hidden_dim=config['out_dim'],\n",
    "                        pred_hidden_dim=config['hidden_dim'], output_dim=config['out_dim'])\n",
    "    elif model_name == 'SimCLR':\n",
    "        model = SimCLR(backbone=backbone, input_dim=input_dim,\n",
    "                       hidden_dim=config['hidden_dim'], output_dim=config['out_dim'],\n",
    "                       num_layers=2, memory_bank_size=0)\n",
    "    elif model_name == 'SimCLRv2':\n",
    "        model = SimCLR(backbone=backbone, input_dim=input_dim,\n",
    "                       hidden_dim=config['hidden_dim'], output_dim=config['out_dim'],\n",
    "                       num_layers=3, memory_bank_size=65536)\n",
    "    elif model_name == 'BarlowTwins':\n",
    "        model = BarlowTwins(backbone=backbone, input_dim=input_dim,\n",
    "                            hidden_dim=config['hidden_dim'], output_dim=config['out_dim'])\n",
    "    elif model_name == 'MoCov2':\n",
    "        model = MoCov2(backbone=backbone, input_dim=input_dim,\n",
    "                       hidden_dim=config['hidden_dim'], output_dim=config['out_dim'])\n",
    "\n",
    "    # ======================\n",
    "    # ADDING GPU SUPPORT.\n",
    "    # Compile model (only for PT2.0).\n",
    "    if torch_compile:\n",
    "        model = torch.compile(model)\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    # Device used for training.\n",
    "    print(f'Device: {device}')\n",
    "    model.to(device)\n",
    "\n",
    "    # ======================\n",
    "    # CONFIGURE OPTIMIZER AND SCHEDULERS.\n",
    "    # Set the initial learning rate.\n",
    "    lr_init = config[\"lr\"]\n",
    "    print(f'Init lr: {lr_init}')\n",
    "\n",
    "    # Use SGD with momentum and weight decay.\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=lr_init,\n",
    "        momentum=0.9,\n",
    "        weight_decay=5e-4\n",
    "    )\n",
    "\n",
    "    # Define the warmup duration.\n",
    "    warmup_epochs = config['warmup_epochs']\n",
    "    print(f'Warmup epochs: {warmup_epochs}')\n",
    "    print(f'Total epochs:  {epochs}\\n')\n",
    "\n",
    "    # Linear warmup for the first defined epochs.\n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lambda epoch: min(1, epoch / warmup_epochs),\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Cosine decay afterwards.\n",
    "    cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=epochs-warmup_epochs,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # ======================\n",
    "    # LOAD CHECKPOINTS (IF ENABLED).\n",
    "    if resume_training:\n",
    "\n",
    "        # List of checkpoints.\n",
    "        ckpt_list = []\n",
    "        print()\n",
    "        for root, dirs, files in os.walk(paths['checkpoints']):\n",
    "            for i, filename in enumerate(sorted(files, reverse=True)):\n",
    "                if filename[:4] == 'ckpt':\n",
    "                    ckpt_list.append(os.path.join(root, filename))\n",
    "                    print(f'{i:02} --> {filename}')\n",
    "\n",
    "        # Load the best checkpoint.\n",
    "        print(f'\\nLoaded: {ckpt_list[0]}')\n",
    "        ckpt = torch.load(ckpt_list[0])\n",
    "\n",
    "        # Load from dict.\n",
    "        epoch = ckpt['epoch'] + 1\n",
    "        model.backbone.load_state_dict(ckpt['model_state_dict'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        warmup_scheduler.load_state_dict(ckpt['warmup_scheduler_state_dict'])\n",
    "        cosine_scheduler.load_state_dict(ckpt['cosine_scheduler_state_dict'])\n",
    "\n",
    "    else:\n",
    "        epoch = 0  # start training from scratch.\n",
    "\n",
    "    # ======================\n",
    "    # INITIAL PARAMETERS AND INFO.\n",
    "    print(f'\\nOptimizer:\\n{optimizer}')\n",
    "    print(f'Warmup scheduler: {warmup_scheduler}')\n",
    "    print(f'Cosine scheduler: {cosine_scheduler}')\n",
    "    save_interval = 5\n",
    "    total_train_batches = len(dataloader['train'])\n",
    "    total_val_batches = len(dataloader['val'])\n",
    "    collapse_level = 0.\n",
    "    # momentum_val = None  # only for moco.\n",
    "    print(f'\\nBatches in (train, val) datasets: '\n",
    "          f'({total_train_batches}, {total_val_batches})\\n')\n",
    "\n",
    "    # ======================\n",
    "    # TRAINING LOOP.\n",
    "    # Iterating over the epochs.\n",
    "    for epoch in range(epoch, epochs):\n",
    "\n",
    "        # Set momentum for moco.\n",
    "        # if model_name == 'MoCov2':\n",
    "        #     momentum_val = cosine_schedule(epoch, epochs, 0.996, 1)\n",
    "        #     print(f\"Momentum value: {momentum_val}\")\n",
    "\n",
    "        # Timer added.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # ======================\n",
    "        # TRAINING COMPUTATION.\n",
    "        # Iterating through the dataloader (lightly dataset is different).\n",
    "        model.train()\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "        running_train_loss = 0.\n",
    "        for b, ((x0, x1), _, _) in enumerate(dataloader['train']):\n",
    "\n",
    "            # Move images to the GPU (same batch two transformations).\n",
    "            x0 = x0.to(device)\n",
    "            x1 = x1.to(device)\n",
    "\n",
    "            # Zero the parameter gradients.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize: Compute the loss, run\n",
    "            # backpropagation, and update the parameters of the model.\n",
    "            loss = model.training_step((x0, x1), momentum_val=0.999)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if model_name == 'SimSiam':\n",
    "                model.check_collapse(loss.item())\n",
    "\n",
    "            # Print statistics.\n",
    "            # Averaged loss across all training examples * batch_size.\n",
    "            running_train_loss += loss.item() * batch_size\n",
    "\n",
    "            # Show partial stats.\n",
    "            if b % (total_train_batches//4) == (total_train_batches//4-1):\n",
    "                print(f'T[{epoch}, {b + 1:5d}] | '\n",
    "                      f'Running train loss: '\n",
    "                      f'{running_train_loss/(b*batch_size):.4f}')\n",
    "\n",
    "        # The level of collapse is large if the standard deviation of\n",
    "        # the l2 normalized output is much smaller than 1 / sqrt(dim).\n",
    "        if model_name == 'SimSiam':\n",
    "            collapse_level = max(\n",
    "                0., 1 - math.sqrt(config['out_dim']) * model.avg_output_std)\n",
    "\n",
    "        # ======================\n",
    "        # TRAINING LOSS.\n",
    "        # Loss averaged across all training examples for the current epoch.\n",
    "        epoch_train_loss = (running_train_loss\n",
    "                            / len(dataloader['train'].sampler))\n",
    "\n",
    "#         # ======================\n",
    "#         # EVALUATION COMPUTATION.\n",
    "#         # The evaluation process was not okey (it's been deleted).\n",
    "#         model.eval()\n",
    "#         running_val_loss = 0.\n",
    "#         # val_loss = 0.0\n",
    "#         # val_steps = 0\n",
    "#         with torch.no_grad():\n",
    "#             for vb, ((x0, x1), _, _) in enumerate(dataloader['val']):\n",
    "\n",
    "#                 # Move images to the GPU (same batch two transformations).\n",
    "#                 x0 = x0.to(device)\n",
    "#                 x1 = x1.to(device)\n",
    "\n",
    "#                 # Compute loss.\n",
    "#                 loss = model.training_step((x0, x1),\n",
    "#                                            momentum_val=momentum_val)\n",
    "\n",
    "#                 # Averaged loss across all validation examples * batch_size.\n",
    "#                 running_val_loss += loss.item() * batch_size\n",
    "\n",
    "#                 # val_loss += loss.cpu().numpy()\n",
    "#                 # val_steps += 1\n",
    "\n",
    "#                 # Show partial stats.\n",
    "#                 if vb % (total_val_batches//4) == (total_val_batches//4-1):\n",
    "#                     print(f'V[{epoch}, {vb + 1:5d}] | '\n",
    "#                           f'Running val loss:   '\n",
    "#                           f'{running_val_loss/(vb*batch_size):.4f}')\n",
    "\n",
    "#         # ======================\n",
    "#         # VALIDATION LOSS.\n",
    "#         # Loss averaged across all training examples for the current epoch.\n",
    "#         epoch_val_loss = (running_val_loss\n",
    "#                           / len(dataloader['val'].sampler))\n",
    "\n",
    "        # ======================\n",
    "        # UPDATE LEARNING RATE SCHEDULER.\n",
    "        # scheduler.step()\n",
    "        if (epoch < warmup_epochs):\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            cosine_scheduler.step()\n",
    "\n",
    "        # ======================\n",
    "        # SAVING CHECKPOINT.\n",
    "        # Move the model to CPU before saving it\n",
    "        # to make it more platform-independent.\n",
    "        # Problems with resuming training.\n",
    "        # model.to('cpu')\n",
    "        if epoch % save_interval == 0:\n",
    "\n",
    "            model.save(\n",
    "                backbone_name,\n",
    "                epoch,\n",
    "                epoch_train_loss,\n",
    "                dataset_ratio,\n",
    "                balanced_dataset,\n",
    "                paths['checkpoints'],\n",
    "                collapse_level=collapse_level if model_name == 'SimSiam' else 0.\n",
    "            )\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.backbone.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'warmup_scheduler_state_dict': warmup_scheduler.state_dict(),\n",
    "                'cosine_scheduler_state_dict': cosine_scheduler.state_dict(),\n",
    "                'loss': epoch_train_loss\n",
    "            }, os.path.join(paths['checkpoints'], 'ckpt_' + str(model)))\n",
    "        # model.to(device)\n",
    "\n",
    "        # ======================\n",
    "        # EPOCH STATISTICS.\n",
    "        # Show some stats per epoch completed.\n",
    "        print(f'[Epoch {epoch:3d}] | '\n",
    "              f'Train loss: {epoch_train_loss:.4f} | '\n",
    "              # f'Val loss: {epoch_val_loss:.4f} | '\n",
    "              f'Duration: {(time.time()-t0):.2f} s | '\n",
    "              f'Collapse Level (SimSiam only): {collapse_level:.4f}/1.0\\n')\n",
    "\n",
    "        # ======================\n",
    "        # RAY TUNE.\n",
    "        if ray_tune:\n",
    "            tune.report(loss=epoch_train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af191fd4-de4a-4ff8-9231-bbb6cc2636c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training (also with hyperparameter tuning using Ray Tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243d3a7-2030-46b4-97ce-a095d5d8133b",
   "metadata": {},
   "source": [
    "Collapse level: the closer to zero the better\n",
    "\n",
    "A value close to 0 indicates that the representations have collapsed. A value close to 1/sqrt(dimensions), where dimensions are the number of representation dimensions, indicates that the representations are stable.\n",
    "\n",
    "https://docs.ray.io/en/latest/tune/api/search_space.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ae8ed-504b-44d3-90e0-a77014208536",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray_tune:\n",
    "\n",
    "    max_num_epochs = epochs\n",
    "    gpus_per_trial = 1\n",
    "    paths['ray_tune'] = os.path.join(paths['output'], 'ray_results')\n",
    "    print(f'\\nMax. number of epochs: {max_num_epochs}')\n",
    "    print(f'Number of samples:     {num_samples_trials}')\n",
    "\n",
    "    # Configuration.\n",
    "    if ray_tune == 'loguniform':\n",
    "\n",
    "        # Build the filename.\n",
    "        filename = f'ray_tune_results_{backbone_name}_{model_name}.csv'\n",
    "\n",
    "        # Load the CSV file into a pandas dataframe.\n",
    "        df = pd.read_csv(os.path.join(paths['ray_tune'], filename),\n",
    "                         usecols=lambda col: col.startswith('loss')\n",
    "                         or col.startswith('config/'))\n",
    "\n",
    "        # Configuration.\n",
    "        print(f'Setting the configuration from {filename} and tuning the lr')\n",
    "        config = {\n",
    "            'hidden_dim': df.loc[0, 'config/hidden_dim'],\n",
    "            'out_dim': df.loc[0, 'config/out_dim'],\n",
    "            'lr': tune.loguniform(1e-4, 1e-1),\n",
    "            'warmup_epochs': max(1, int(0.1 * max_num_epochs)),\n",
    "        }\n",
    "\n",
    "    elif ray_tune == 'gridsearch':\n",
    "\n",
    "        print(f'Setting a new configuration using tune.grid_search')\n",
    "        config = {\n",
    "            \"hidden_dim\": tune.grid_search([128, 256, 512]),\n",
    "            \"out_dim\": tune.grid_search([128, 256, 512]),\n",
    "            \"lr\": tune.grid_search([1e-4, 1e-3, 1e-2, 1e-1]),\n",
    "            \"warmup_epochs\": max(1, int(0.1 * max_num_epochs)),\n",
    "            # \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "        }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=grace_period)\n",
    "\n",
    "    reporter = CLIReporter(\n",
    "        # ``parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"]``,\n",
    "        # metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])\n",
    "\n",
    "    result = tune.run(\n",
    "        partial(train),\n",
    "        resources_per_trial={\"cpu\": 12, \"gpu\": gpus_per_trial},\n",
    "        name=model_name,\n",
    "        config=config,\n",
    "        num_samples=num_samples_trials,\n",
    "        local_dir=paths['ray_tune'],\n",
    "        scheduler=scheduler,\n",
    "        verbose=1,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    # Sorted dataframe for the last reported results of all of the trials.\n",
    "    df = result.results_df\n",
    "    df = df.sort_values(by=['loss'], ascending=True)\n",
    "\n",
    "    # Create the name of the file.\n",
    "    if ray_tune == 'loguniform':\n",
    "        filename = f'ray_tune_results_lr_{backbone_name}_{model_name}.csv'\n",
    "    elif ray_tune == 'gridsearch':\n",
    "        filename = f'ray_tune_results_{backbone_name}_{model_name}.csv'\n",
    "\n",
    "    # Write the results to a CSV file.\n",
    "    df.to_csv(os.path.join(paths['ray_tune'], filename))\n",
    "\n",
    "    # Get best results.\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial final val loss: {best_trial.last_result['loss']}\")\n",
    "\n",
    "else:\n",
    "\n",
    "    paths['ray_tune'] = os.path.join(paths['input'], 'best_configs')\n",
    "\n",
    "    # Build the filename.\n",
    "    filename_lr = f'ray_tune_results_lr_{backbone_name}_{model_name}.csv'\n",
    "\n",
    "    # Load the CSV file into a pandas dataframe.\n",
    "    df_lr = pd.read_csv(os.path.join(paths['ray_tune'], filename_lr),\n",
    "                        usecols=lambda col: col.startswith('loss')\n",
    "                        or col.startswith('config/'))\n",
    "\n",
    "    # Configuration.\n",
    "    print(f'\\nSetting the best configuration for the model from file: {filename_lr}')\n",
    "    config = {\n",
    "        'hidden_dim': df_lr.loc[0, 'config/hidden_dim'],\n",
    "        'out_dim': df_lr.loc[0, 'config/out_dim'],\n",
    "        'lr': df_lr.loc[0, 'config/lr'],\n",
    "        'warmup_epochs': max(1, int(0.1 * epochs)),\n",
    "    }\n",
    "\n",
    "    # model_hyperparameters = {\n",
    "    #     'SimSiam': {'hidden_dim': 512,\n",
    "    #                 'out_dim': 512,\n",
    "    #                 'lr': 0.0054,\n",
    "    #                 'warmup_epochs': max(1, int(0.1 * epochs))},\n",
    "    #     'SimCLR': {'hidden_dim': 512,\n",
    "    #                'out_dim': 256,\n",
    "    #                'lr': 0.0049,\n",
    "    #                'warmup_epochs': max(1, int(0.1 * epochs))},\n",
    "    #     'SimCLRv2': {'hidden_dim': 128,\n",
    "    #                  'out_dim': 256,\n",
    "    #                  'lr': 0.0049,\n",
    "    #                  'warmup_epochs': max(1, int(0.1 * epochs))},\n",
    "    #     'BarlowTwins': {'hidden_dim': 128,\n",
    "    #                     'out_dim': 128,\n",
    "    #                     'lr': 0.0001,\n",
    "    #                     'warmup_epochs': max(1, int(0.1 * epochs))},\n",
    "    #     'MoCov2': {'hidden_dim': 512,\n",
    "    #                'out_dim': 128,\n",
    "    #                'lr': 0.0049,\n",
    "    #                'warmup_epochs': max(1, int(0.1 * epochs))}\n",
    "    # }\n",
    "\n",
    "    # Configuration.\n",
    "    # print(f'\\nSetting a custom configuration for each model')\n",
    "    # config = model_hyperparameters[str(model_name)]\n",
    "\n",
    "    train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0905a-37c4-46b3-ab93-0be315854e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load notebook extensions.\n",
    "if is_notebook():\n",
    "    %tensorboard --port 6006 --logdir ./output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab1e779-9c34-4942-9e58-664f15fc3271",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"><b>-----------------------------------------------------------------</b></p>\n",
    "<p style=\"color:red\"><b>----------> REVISED UP TO THIS POINT -----------</b></p>\n",
    "<p style=\"color:red\"><b>-----------------------------------------------------------------</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6818fcd-c195-413f-b940-d03b83799dd3",
   "metadata": {},
   "source": [
    "### Checking the weights of the last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f997d9-11ab-4b6e-a7fc-5e17e56e7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convolutional layer weights.\n",
    "# print(model.backbone[0])\n",
    "# print(model.backbone[0].weight[63])\n",
    "# print(model.backbone.conv1)\n",
    "# print(model.backbone.conv1.weight[63])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab33b97-aa53-4cf3-9ab7-6ce9341febbb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844b415-d517-418b-a261-0014089668c1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a5643-193b-4da7-b5e5-b88165318907",
   "metadata": {},
   "source": [
    "# Reduce dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847c99d-aeea-4118-a7f8-be436b93365c",
   "metadata": {},
   "source": [
    "## Calculate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea12b2bd-bcdb-4737-9d3f-82c579d35ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Empty lists.\n",
    "# embeddings = []\n",
    "# labels = []\n",
    "\n",
    "# # Disable gradients for faster calculations.\n",
    "# # Put the model in evaluation mode.\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     # for i, (x, y, fnames) in enumerate(dataloader_val):\n",
    "#     # Now taking only the first transformed batch.\n",
    "#     for i, ((x, _), y, fnames) in enumerate(dataloader['val']):\n",
    "\n",
    "#         # Move the images to the GPU.\n",
    "#         x = x.to(device)\n",
    "#         y = y.to(device)\n",
    "\n",
    "#         # Embed the images with the pre-trained backbone.\n",
    "#         emb = model.backbone(x).flatten(start_dim=1)\n",
    "\n",
    "#         # Store the embeddings and filenames in lists.\n",
    "#         embeddings.append(emb)\n",
    "#         labels.append(y)\n",
    "\n",
    "# # Concatenate the embeddings and convert to numpy.\n",
    "# embeddings = torch.cat(embeddings, dim=0).to('cpu').numpy()\n",
    "# labels = torch.cat(labels, dim=0).to('cpu').numpy()\n",
    "\n",
    "# # Show shapes.\n",
    "# print(np.shape(embeddings))\n",
    "# print(np.shape(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb69705-d01b-4f14-857e-90ee7e328ebd",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246eae4f-9f95-4c6c-9f44-0f35a37c3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18363d12-5b48-4f98-8cb1-feb3eae44aff",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1886ba-804b-413a-b167-848c396de753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA computation.\n",
    "# df = pca_computation(embeddings, labels, SEED)\n",
    "\n",
    "# # 2-D plot.\n",
    "# if plot == '2d' or plot == \"23d\" or plot == 'all':\n",
    "#     fig = plt.figure(figsize=(10, 10))\n",
    "#     sns.scatterplot(\n",
    "#         x='pca_x',\n",
    "#         y='pca_y',\n",
    "#         hue='labels',\n",
    "#         palette=sns.color_palette('hls', 29),\n",
    "#         data=df,\n",
    "#         legend='full',\n",
    "#         alpha=0.9\n",
    "#     )\n",
    "#     fig_name_save = (f'pca_2d-{model}')\n",
    "#     fig.savefig(os.path.join(paths['images'], fig_name_save+fig_format),\n",
    "#                 bbox_inches='tight')\n",
    "#     plt.show() if show else plt.close()\n",
    "\n",
    "# # 3-D plot with matplotlib.\n",
    "# if plot == '3d' or plot == \"23d\" or plot == 'all':\n",
    "#     fig = plt.figure(figsize=(10, 10))\n",
    "#     ax = fig.add_subplot(projection='3d')\n",
    "#     ax.scatter(\n",
    "#         xs=df['pca_x'],\n",
    "#         ys=df['pca_y'],\n",
    "#         zs=df['pca_z'],\n",
    "#         c=df['labels'],\n",
    "#         cmap='tab10'\n",
    "#     )\n",
    "#     ax.set_xlabel('pca_x')\n",
    "#     ax.set_ylabel('pca_y')\n",
    "#     ax.set_zlabel('pca_z')\n",
    "#     fig_name_save = (f'pca_3d-{model}')\n",
    "#     fig.savefig(os.path.join(paths['images'], fig_name_save+fig_format),\n",
    "#                 bbox_inches='tight')\n",
    "#     plt.show() if show else plt.close()\n",
    "\n",
    "# # 3-D plot with pyplot.\n",
    "# if (plot == '3d-plotly' or plot == 'all') and show:\n",
    "#     fig = px.scatter_3d(df, x='pca_x',\n",
    "#                         y='pca_y', z='pca_z',\n",
    "#                         color='labels',\n",
    "#                         width=1000, height=800)  # symbol='labels'\n",
    "\n",
    "#     fig.update_traces(marker=dict(size=3))\n",
    "\n",
    "#     # Move colorbar.\n",
    "#     # fig.update_layout(coloraxis_colorbar=dict(yanchor=\"top\", y=1, x=0,\n",
    "#     #                                           ticks=\"outside\",\n",
    "#     #                                           ticksuffix=\"\"))\n",
    "\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e076936-68bb-4709-b888-7749ae985796",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d92516-071e-4830-b4db-eff7c93e0618",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6889458-b714-47e3-832c-ed0d55ea1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # t-SNE computation for 2-D.\n",
    "# df = tsne_computation(embeddings, labels, SEED, n_components=2)\n",
    "\n",
    "# # 2-D plot.\n",
    "# if plot == '2d' or plot == \"23d\" or plot == 'all':\n",
    "#     fig = plt.figure(figsize=(10, 10))\n",
    "#     sns.scatterplot(\n",
    "#         x='tsne_x',\n",
    "#         y='tsne_y',\n",
    "#         hue='labels',\n",
    "#         palette=sns.color_palette('hls', 29),\n",
    "#         data=df,\n",
    "#         legend='full',\n",
    "#         alpha=0.9\n",
    "#     )\n",
    "#     fig_name_save = (f'tsne_2d-{model}')\n",
    "#     fig.savefig(os.path.join(paths['images'], fig_name_save+fig_format),\n",
    "#                 bbox_inches='tight')\n",
    "#     plt.show() if show else plt.close()\n",
    "\n",
    "# # t-SNE computation for 3-D.\n",
    "# df = tsne_computation(embeddings, labels, SEED, n_components=3)\n",
    "\n",
    "# # 3-D plot with matplotlib.\n",
    "# if plot == '3d' or plot == \"23d\" or plot == 'all':\n",
    "#     fig = plt.figure(figsize=(10, 10))\n",
    "#     ax = fig.add_subplot(projection='3d')\n",
    "#     ax.scatter(\n",
    "#         xs=df['tsne_x'],\n",
    "#         ys=df['tsne_y'],\n",
    "#         zs=df['tsne_z'],\n",
    "#         c=df['labels'],\n",
    "#         cmap='tab10'\n",
    "#     )\n",
    "#     ax.set_xlabel('tsne_x')\n",
    "#     ax.set_ylabel('tsne_y')\n",
    "#     ax.set_zlabel('tsne_z')\n",
    "#     fig_name_save = (f'tsne_3d-{model}')\n",
    "#     fig.savefig(os.path.join(paths['images'], fig_name_save+fig_format),\n",
    "#                 bbox_inches='tight')\n",
    "#     plt.show() if show else plt.close()\n",
    "\n",
    "# # 3-D plot with pyplot.\n",
    "# if (plot == '3d-plotly' or plot == 'all') and show:\n",
    "#     fig = px.scatter_3d(df, x='tsne_x',\n",
    "#                         y='tsne_y', z='tsne_z',\n",
    "#                         color='labels',\n",
    "#                         width=1000, height=800)  # symbol='labels'\n",
    "\n",
    "#     fig.update_traces(marker=dict(size=3))\n",
    "\n",
    "#     # Move colorbar.\n",
    "#     # fig.update_layout(coloraxis_colorbar=dict(yanchor=\"top\", y=1, x=0,\n",
    "#     #                                           ticks=\"outside\",\n",
    "#     #                                           ticksuffix=\"\"))\n",
    "\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf66499-e3f5-477f-b158-840e9c041c63",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a48bc-7ed0-430b-8328-632b93901014",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42baa2ae-266c-4d94-88aa-25f9408f8339",
   "metadata": {},
   "source": [
    "# Check each model's performance/collapse on val data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff757050-e205-4e7f-be09-ee4b088a20fd",
   "metadata": {},
   "source": [
    "ADD SOFTMAX IF NECESSARY!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5cb1c-437d-4e7c-adbd-a9c57418fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_scatter_plot_with_thumbnails_axes(ax, title=''):\n",
    "#     \"\"\"\n",
    "#     Creates a scatter plot with image overlays\n",
    "#     that are plotted in a particular ax position.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Shuffle images and find out which images to show.\n",
    "#     shown_images_idx = []\n",
    "#     shown_images = np.array([[1., 1.]])\n",
    "#     iterator = [i for i in range(embeddings_2d.shape[0])]\n",
    "#     np.random.shuffle(iterator)\n",
    "#     for i in iterator:\n",
    "\n",
    "#         # Only show image if it is sufficiently far away from the others.\n",
    "#         dist = np.sum((embeddings_2d[i] - shown_images) ** 2, 1)\n",
    "#         if np.min(dist) < 2e-3:\n",
    "#             continue\n",
    "#         shown_images = np.r_[shown_images, [embeddings_2d[i]]]\n",
    "#         shown_images_idx.append(i)\n",
    "\n",
    "#     # Plot image overlays.\n",
    "#     for idx in shown_images_idx:\n",
    "#         thumbnail_size = int(rcp['figure.figsize'][0] * 2.5)  # 2.\n",
    "#         path = os.path.join(data_path_test, filenames[idx])\n",
    "#         img = Image.open(path)\n",
    "#         img = functional.resize(img, thumbnail_size)\n",
    "#         img = np.array(img)\n",
    "#         img_box = osb.AnnotationBbox(\n",
    "#             osb.OffsetImage(img, cmap=plt.cm.gray_r),\n",
    "#             embeddings_2d[idx],\n",
    "#             pad=0.2,\n",
    "#         )\n",
    "#         ax.add_artist(img_box)\n",
    "\n",
    "#     # Set aspect ratio.\n",
    "#     ratio = 1. / ax.get_data_ratio()\n",
    "#     ax.set_aspect(ratio, adjustable='box')\n",
    "#     ax.title.set_text(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03d0fe-d736-4c71-a070-f77739c1d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validation dataset.\n",
    "# print(f'\\nValidating...')\n",
    "# data_path_test = os.path.join(data_path_target, 'val')\n",
    "# print(f'Path to dataset: {data_path_test}')\n",
    "\n",
    "# # List of trained models.\n",
    "# print('List of model checkpoints:')\n",
    "# model_list = []\n",
    "# for root, dirs, files in os.walk(output_path_models):\n",
    "#     for i, filename in enumerate(sorted(files, reverse=False)):\n",
    "#         model_list.append(os.path.join(root, filename))\n",
    "#         print(f'{i:02}: {filename}')\n",
    "\n",
    "# # Plot setup.\n",
    "# ncols = 5\n",
    "# nrows = int(math.ceil(len(model_list) / ncols))\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=nrows,\n",
    "#                          ncols=ncols,\n",
    "#                          figsize=(12*ncols, 12*nrows))\n",
    "\n",
    "# # Convert the array to 1 dimension.\n",
    "# axes = axes.ravel()\n",
    "\n",
    "# # Main loop over the models.\n",
    "# for model_id, model_name_local in enumerate(model_list):\n",
    "\n",
    "#     # Load model weights.\n",
    "#     model.backbone.load_state_dict(torch.load(model_name_local))\n",
    "\n",
    "#     # Empty lists.\n",
    "#     embeddings = []\n",
    "#     filenames = []\n",
    "\n",
    "#     # Disable gradients for faster calculations.\n",
    "#     # Put the model in evaluation mode.\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # for i, (x, _, fnames) in enumerate(dataloader_val):\n",
    "#         for i, ((x, _), _, fnames) in enumerate(dataloader_val_lightly):\n",
    "\n",
    "#             # Move the images to the GPU.\n",
    "#             x = x.to(device)\n",
    "\n",
    "#             # Embed the images with the pre-trained backbone.\n",
    "#             y = model.backbone(x).flatten(start_dim=1)\n",
    "\n",
    "#             # Store the embeddings and filenames in lists.\n",
    "#             embeddings.append(y)\n",
    "#             filenames = filenames + list(fnames)\n",
    "\n",
    "#     # Concatenate the embeddings and convert to numpy.\n",
    "#     embeddings = torch.cat(embeddings, dim=0)\n",
    "#     embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "#     # For the scatter plot we want to transform the images to a\n",
    "#     # 2-D vector space using a random Gaussian projection.\n",
    "#     projection = random_projection.GaussianRandomProjection(\n",
    "#         n_components=2,\n",
    "#         random_state=seed\n",
    "#     )\n",
    "#     embeddings_2d = projection.fit_transform(embeddings)\n",
    "\n",
    "#     # Normalize the embeddings to fit in the [0, 1] square.\n",
    "#     M = np.max(embeddings_2d, axis=0)\n",
    "#     m = np.min(embeddings_2d, axis=0)\n",
    "#     embeddings_2d = (embeddings_2d - m) / (M - m)\n",
    "\n",
    "#     # Get a scatter plot with thumbnail overlays.\n",
    "#     start_chr_epoch = model_name_local.find('-epoch') + 1\n",
    "#     start_chr_time = model_name_local.find('-time')\n",
    "#     get_scatter_plot_with_thumbnails_axes(\n",
    "#         axes[model_id],\n",
    "#         title=model_name_local[start_chr_epoch:start_chr_time]\n",
    "#     )\n",
    "\n",
    "#     # Show progress.\n",
    "#     print(f'Subplot of model-{model_id} done!',\n",
    "#           end='\\r',\n",
    "#           flush=True)\n",
    "\n",
    "# # Save figure.\n",
    "# fig.suptitle(f'{model_name}')\n",
    "# fig_name_save = (f'knn-{model}')\n",
    "# fig.savefig(os.path.join(output_path_figs, fig_name_save+fig_format),\n",
    "#             bbox_inches='tight')\n",
    "# if show:\n",
    "#     plt.show()\n",
    "# else:\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c41974-eaad-4263-9e74-6fb3fb66b2f8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d5f3d-aa82-48ef-916c-0c50dbd3636f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91de858-a1f9-4f98-90c4-586d9dee8ac1",
   "metadata": {},
   "source": [
    "# Embeddings for the samples of the test dataset (WARNING: custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabfddb-57d8-46ed-8e2d-43c73e435d8a",
   "metadata": {},
   "source": [
    "## Setup (NOT WORKING PROPERLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb1456-1752-406e-8963-5e1c2203c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test dataset.\n",
    "# print('\\nTesting...')\n",
    "# data_path_test = os.path.join(data_path_target, 'test')\n",
    "# print(f'Path to dataset: {data_path_test}')\n",
    "\n",
    "# # Load best model weights.\n",
    "# idx = -1\n",
    "\n",
    "# # Print model.\n",
    "# print(f'Target model checkpoint: {model_list[idx]}')\n",
    "# model.backbone.load_state_dict(torch.load(model_list[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b175a024-3d6e-4ccc-a3f0-8035e3e8390c",
   "metadata": {},
   "source": [
    "## Compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db33812-0e52-4ba7-82ca-11da3895677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Empty lists.\n",
    "# embeddings = []\n",
    "# filenames = []\n",
    "\n",
    "# # Disable gradients for faster calculations.\n",
    "# # Put the model in evaluation mode.\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for i, (x, _, fnames) in enumerate(dataloader_test):\n",
    "\n",
    "#         # Move the images to the GPU.\n",
    "#         x = x.to(device)\n",
    "\n",
    "#         # Embed the images with the pre-trained backbone.\n",
    "#         y = model.backbone(x).flatten(start_dim=1)\n",
    "\n",
    "#         # Store the embeddings and filenames in lists.\n",
    "#         embeddings.append(y)\n",
    "#         filenames = filenames + list(fnames)\n",
    "\n",
    "# # Concatenate the embeddings and convert to numpy.\n",
    "# embeddings = torch.cat(embeddings, dim=0)\n",
    "# embeddings = embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e672d5e-2d3c-479d-ba94-8f19e9543f4d",
   "metadata": {},
   "source": [
    "## Projection to 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb003c-70f0-46be-ade6-37e35e453aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For the scatter plot we want to transform the images to a two-dimensional\n",
    "# # vector space using a random Gaussian projection.\n",
    "# projection = random_projection.GaussianRandomProjection(\n",
    "#     n_components=2,\n",
    "#     random_state=seed\n",
    "# )\n",
    "# embeddings_2d = projection.fit_transform(embeddings)\n",
    "\n",
    "# # Normalize the embeddings to fit in the [0, 1] square.\n",
    "# M = np.max(embeddings_2d, axis=0)\n",
    "# m = np.min(embeddings_2d, axis=0)\n",
    "# embeddings_2d = (embeddings_2d - m) / (M - m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e176a-477a-4732-bd6a-960e98170a13",
   "metadata": {},
   "source": [
    "## Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f970f1-9506-43b5-a0c1-a408b1ca3705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize empty figure and add subplot.\n",
    "# fig = plt.figure(figsize=(15, 15))\n",
    "# ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# # Get a scatter plot with thumbnail overlays.\n",
    "# get_scatter_plot_with_thumbnails_axes(\n",
    "#     ax,\n",
    "#     title='Scatter plot with samples'\n",
    "# )\n",
    "\n",
    "# # Save figure.\n",
    "# fig_name_save = (f'scatter_samples-{model}')\n",
    "# fig.savefig(os.path.join(output_path_figs, fig_name_save+fig_format),\n",
    "#             bbox_inches='tight')\n",
    "# if show:\n",
    "#     plt.show()\n",
    "# else:\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc28d1-c569-4163-a3a7-f84ad17884eb",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1423fa1-653f-474e-ba2e-404cd7e37868",
   "metadata": {},
   "source": [
    "### Pick up one random sample per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998cde14-fb59-4330-b070-467a54537e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of subdirectories (classes).\n",
    "# directory_list = []\n",
    "# for root, dirs, files in os.walk(data_path_test):\n",
    "#     for dirname in sorted(dirs):\n",
    "#         directory_list.append(os.path.join(root, dirname))\n",
    "#         # print(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae54d2-c622-493a-96fa-f98fa931c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of files (samples).\n",
    "# example_images = []\n",
    "# for classes in directory_list:\n",
    "\n",
    "#     # Random samples.\n",
    "#     random_file = np.random.choice(os.listdir(classes))\n",
    "#     path_to_random_file = classes + '/' + random_file\n",
    "\n",
    "#     # Only class and filename.\n",
    "#     start_chr = path_to_random_file.index('test/') + 5\n",
    "\n",
    "#     # Append filename.\n",
    "#     example_images.append(path_to_random_file[start_chr:])\n",
    "#     # print(example_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb66cee-b413-4944-95ff-258740375d9a",
   "metadata": {},
   "source": [
    "### Look for similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afde3b-32bb-4bef-ae93-a3a18475a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_image_as_np_array(filename: str):\n",
    "#     \"\"\"\n",
    "#     Loads the image with filename and returns it as a numpy array.\n",
    "\n",
    "#     \"\"\"\n",
    "#     img = Image.open(filename)\n",
    "#     return np.asarray(img)\n",
    "\n",
    "\n",
    "# def get_image_as_np_array_with_frame(filename: str, w: int = 5):\n",
    "#     \"\"\"\n",
    "#     Returns an image as a numpy array with a black frame of width w.\n",
    "\n",
    "#     \"\"\"\n",
    "#     img = get_image_as_np_array(filename)\n",
    "#     ny, nx, _ = img.shape\n",
    "\n",
    "#     # Create an empty image with padding for the frame.\n",
    "#     framed_img = np.zeros((w + ny + w, w + nx + w, 3))\n",
    "#     framed_img = framed_img.astype(np.uint8)\n",
    "\n",
    "#     # Put the original image in the middle of the new one.\n",
    "#     framed_img[w:-w, w:-w] = img\n",
    "#     return framed_img\n",
    "\n",
    "\n",
    "# def plot_nearest_neighbors_nxn(example_image: str, i: int):\n",
    "#     \"\"\"\n",
    "#     Plots the example image and its eight nearest neighbors.\n",
    "\n",
    "#     \"\"\"\n",
    "#     n_subplots = 6\n",
    "\n",
    "#     # Initialize empty figure.\n",
    "#     fig = plt.figure(figsize=(10, 10))\n",
    "#     fig.suptitle(f\"Nearest Neighbor Plot Class {i}\")\n",
    "\n",
    "#     # Get indexes.\n",
    "#     example_idx = filenames.index(example_image)\n",
    "\n",
    "#     # Get distances to the cluster center.\n",
    "#     distances = embeddings - embeddings[example_idx]\n",
    "#     distances = np.power(distances, 2).sum(-1).squeeze()\n",
    "\n",
    "#     # Sort indices by distance to the center.\n",
    "#     nearest_neighbors = np.argsort(distances)[:n_subplots]\n",
    "\n",
    "#     # Show images.\n",
    "#     for plot_offset, plot_idx in enumerate(nearest_neighbors):\n",
    "#         ax = fig.add_subplot(3, 3, plot_offset + 1)\n",
    "\n",
    "#         # Get the corresponding filename.\n",
    "#         fname = os.path.join(data_path_test, filenames[plot_idx])\n",
    "#         if plot_offset == 0:\n",
    "#             ax.set_title(f\"Example Image\")\n",
    "#             plt.imshow(get_image_as_np_array_with_frame(fname))\n",
    "#         else:\n",
    "#             plt.imshow(get_image_as_np_array(fname))\n",
    "\n",
    "#         # Let's disable the axis.\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#     # Save figure.\n",
    "#     fig_name_save = (f'knn_per_class-c={i:02}-{model}')\n",
    "#     fig.savefig(os.path.join(output_path_figs, fig_name_save+fig_format),\n",
    "#                 bbox_inches='tight')\n",
    "#     if show:\n",
    "#         pass  # plt.show()\n",
    "#     else:\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0db2c2-96ba-4715-9482-ace9155f5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show example images for each cluster.\n",
    "# for i, example_image in enumerate(example_images):\n",
    "#     plot_nearest_neighbors_nxn(example_image, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f809fc8-79ab-4389-a8ed-5e6a98421fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lulc2-conda",
   "language": "python",
   "name": "lulc2-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

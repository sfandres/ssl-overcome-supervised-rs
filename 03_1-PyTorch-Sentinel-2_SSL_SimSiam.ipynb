{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924096aa-e3ec-437c-81fe-03dd73fbfce5",
   "metadata": {},
   "source": [
    "**FIRST ATTEMP TO APPLY SSL TO THE SENTINEL-2 DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49adbc03-f1d2-446f-b2aa-028c144e8a4a",
   "metadata": {},
   "source": [
    "Reference tutorial: https://docs.lightly.ai/tutorials/package/tutorial_simsiam_esa.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9065a367-9764-4b70-bde0-09e5e1f2fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033b88ac-5a92-4803-b0ce-c6af866dbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_notebook():\n",
    "    %load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65f2f9be-dc8e-4b2c-ab6e-94407466cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_notebook():\n",
    "    %pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85366f6f-d496-4b26-b311-47663d202597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_arg_parser():\n",
    "#     \"\"\"Creates and returns the ArgumentParser object.\"\"\"\n",
    "#     ...\n",
    "#     return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7fc52-07dd-41f2-9fb4-510f27a20e82",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de16ef-fd9a-45fb-b620-a514366d9db4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a342e-d7c1-4999-8ed0-0a3118f5f3bf",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c506cd5d-71b7-47d7-b157-1b58caee0936",
   "metadata": {},
   "source": [
    "## Packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb94f55-b0da-4d7e-b461-f6a478166de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main.\n",
    "import utils\n",
    "\n",
    "# OS module.\n",
    "import os\n",
    "\n",
    "# PyTorch.\n",
    "import torch\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "\n",
    "# Data management.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Lightly.\n",
    "import lightly\n",
    "\n",
    "# Training checks.\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from lightly.utils.debug import std_of_l2_normalized\n",
    "\n",
    "# Showing images in the notebook.\n",
    "import IPython\n",
    "\n",
    "# For plotting.\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.offsetbox as osb\n",
    "from matplotlib import rcParams as rcp\n",
    "import seaborn as sns\n",
    "\n",
    "# For resizing images to thumbnails.\n",
    "import torchvision.transforms.functional as functional\n",
    "\n",
    "# For clustering and 2d representations.\n",
    "from sklearn import random_projection\n",
    "\n",
    "from graphs import simple_bar_plot\n",
    "from utils import pca_computation, tsne_computation\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9b774-045a-41ba-bf50-2af289850e99",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f87cae0f-bec1-4fdf-9044-c3e6c6ca6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser (get arguments).\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    import sys\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Script for training the self-supervised learning models.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        'model',\n",
    "        type=str,\n",
    "        choices=['simsiam', 'simclr', 'barlowtwins'],\n",
    "        help=('SSL model for training: '\n",
    "              'use \"simsiam\", \"simclr\" or \"barlowtwins\".')\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--dataset',\n",
    "        type=str,\n",
    "        default='Sentinel2GlobalLULC',\n",
    "        help='Dataset name for training.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--balanced_dataset',\n",
    "        type=bool,\n",
    "        default=False,\n",
    "        help='Whether the dataset should be balanced.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--epochs',\n",
    "        type=int,\n",
    "        default=5,\n",
    "        help='Number of epochs for training.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        type=int,\n",
    "        default=64,\n",
    "        help='Number of images in a batch during training.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--ini_weights',\n",
    "        type=str,\n",
    "        default='random',\n",
    "        choices=['random', 'imagenet'],\n",
    "        help='Initial weights: use \"random\" (default) or \"imagenet\".'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--show_fig',\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help='Whether the images should appear.'\n",
    "    )\n",
    "\n",
    "if is_notebook():\n",
    "    args = parser.parse_args(args=['simsiam'])  # , '--ini_weights', 'imagenet'\n",
    "else:\n",
    "    args = parser.parse_args(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2265f6b-9854-4e2e-85bd-3e7c88792e77",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f88da-2c1a-46ad-a7d1-eabd1628a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target SSL model.\n",
    "model_name = args.model\n",
    "print(f'\\nTarget model for training: {model_name}')\n",
    "\n",
    "# Target dataset.\n",
    "dataset_name = args.dataset\n",
    "print(f'Target dataset: {dataset_name}')\n",
    "\n",
    "# Handling class imbalance.\n",
    "handle_imb_classes = args.balanced_dataset\n",
    "print(f'Balanced dataset: {handle_imb_classes}')\n",
    "\n",
    "# Setting number of epochs.\n",
    "epochs = args.epochs\n",
    "print(f'Number of epochs: {epochs}')\n",
    "\n",
    "# Setting batch size.\n",
    "batch_size = args.batch_size\n",
    "print(f'Batch size: {batch_size}')\n",
    "\n",
    "# Setting the initial weights.\n",
    "if args.ini_weights == 'imagenet':\n",
    "    weights = torchvision.models.ResNet18_Weights.DEFAULT\n",
    "else:\n",
    "    weights = None\n",
    "print(f'Initial weights: {weights}')\n",
    "\n",
    "# Show figures.\n",
    "show = args.show_fig\n",
    "print(f'Showing figures: {show}')\n",
    "\n",
    "# Avoiding the runtimeError: Too many open files.\n",
    "# Communication with the workers is no longer possible.\n",
    "if not is_notebook():\n",
    "    print(f'\\nExecution in shell: Torch sharing strategy set to file_system')\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "else:\n",
    "    print(f'\\nExecution in jupyter: Torch sharing '\n",
    "          f'strategy set to file_system (default)')\n",
    "    torch.multiprocessing.set_sharing_strategy('file_descriptor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3576cff2-906a-4897-ab1a-578ffebaa5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device: cuda\n",
      "\n",
      "Working directory: /home/sfandres/Documents/Git/lulc\n",
      "Directory to store model checkpoints: /home/sfandres/Documents/Git/lulc/pytorch_models/simsiam\n",
      "Directory to save figures: /home/sfandres/Documents/Git/lulc/figures/simsiam\n",
      "\n",
      "Input directory of datasets: /home/sfandres/Documents/Git/lulc/datasets/Sentinel2GlobalLULC\n"
     ]
    }
   ],
   "source": [
    "# Hyperparamenters.\n",
    "exp = utils.Experiment(epochs=epochs,\n",
    "                       batch_size=batch_size)\n",
    "print(f'\\nDevice: {exp.device}\\n')\n",
    "\n",
    "# Get current directory.\n",
    "cwd = os.getcwd()\n",
    "print(f'Working directory: {cwd}')\n",
    "\n",
    "# Directory to save the model checkpoint.\n",
    "output_dir_model = os.path.join(os.path.join(cwd, 'pytorch_models'),\n",
    "                                model_name)\n",
    "print(f'Directory to store model checkpoints: {output_dir_model}')\n",
    "\n",
    "# Folder to save the figures.\n",
    "output_dir_fig = os.path.join(os.path.join(cwd, 'figures'), model_name)\n",
    "print(f'Directory to save figures: {output_dir_fig}')\n",
    "\n",
    "# Directory where the datasets are stored.\n",
    "datasets_dir = os.path.join(os.path.join(cwd, 'datasets'), dataset_name)\n",
    "print(f'\\nInput directory of datasets: {datasets_dir}')\n",
    "\n",
    "# Figure format.\n",
    "fig_format = '.png'  # .pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a17bd-cd1f-40ea-a333-569ed975c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of the embeddings.\n",
    "num_ftrs = 512\n",
    "\n",
    "# Dimension of the output of the prediction and projection heads.\n",
    "out_dim = proj_hidden_dim = 512\n",
    "\n",
    "# The prediction head uses a bottleneck architecture.\n",
    "pred_hidden_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897cdadd-b624-406c-8c2d-d4f57d796a21",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b728bc3-9c98-474f-ba43-c1175071fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.reproducibility()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce08d1-0293-4830-a366-c149e95f0515",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6745ac7c-9497-4bf7-90b8-18a3dae2c27b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e2c6c-4e75-4708-88d2-1ef7da79a9f0",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62b9c8-daed-4a6d-b140-695cac41b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the subsets with full path.\n",
    "data_dirs = utils.listdir_fullpath(datasets_dir)\n",
    "\n",
    "# Leave out unwanted subsets\n",
    "# data_dirs = data_dirs[2:]\n",
    "for dirs in data_dirs:\n",
    "    print(dirs)\n",
    "\n",
    "# Select the target dataset.\n",
    "data_dir_target = data_dirs[2]\n",
    "print('\\nSelected: ' + data_dir_target)\n",
    "\n",
    "# Ratio.\n",
    "ratio = data_dir_target[\n",
    "    data_dir_target.index(\"(\"):data_dir_target.index(\")\")+1\n",
    "]\n",
    "print(f'Ratio: {ratio}')\n",
    "\n",
    "# Load mean and std from file.\n",
    "mean, std = utils.load_mean_std_values(data_dir_target)\n",
    "print(f'Mean loaded from .txt: {mean}')\n",
    "print(f'Std loaded from .txt: {std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ef555-daa7-48d6-be77-9d069ce7404d",
   "metadata": {},
   "source": [
    "## Custom tranforms (w/o normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a8172-7c2c-49a0-99de-223d3b0ebbca",
   "metadata": {},
   "source": [
    "Define the augmentations for self-supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cadd4e8-eeec-47d4-a1b5-eef180d9c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/facebookresearch/simsiam/blob/main/main_simsiam.py\n",
    "import random\n",
    "from PIL import ImageFilter\n",
    "\n",
    "\n",
    "class GaussianBlur(object):\n",
    "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
    "\n",
    "    def __init__(self, sigma=[.1, 2.]):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
    "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
    "        return x\n",
    "\n",
    "\n",
    "# MoCo v2's aug: similar to SimCLR https://arxiv.org/abs/2002.05709\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((exp.input_size, exp.input_size)),\n",
    "    torchvision.transforms.RandomResizedCrop(224, scale=(0.2, 1.)),\n",
    "    torchvision.transforms.RandomApply([\n",
    "        torchvision.transforms.ColorJitter(.4, .4, .4, .1)  # not strengthened\n",
    "    ], p=0.8),\n",
    "    torchvision.transforms.RandomGrayscale(p=0.2),\n",
    "    torchvision.transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean['train'], std['train'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b451709-864e-42fe-841e-b30157619385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data augmentations for the train dataset.\n",
    "# train_transform = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.Resize((exp.input_size, exp.input_size)),\n",
    "#     torchvision.transforms.RandomResizedCrop(224, scale=(0.2, 1.)),\n",
    "#     torchvision.transforms.RandomApply([\n",
    "#             torchvision.transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "#         ], p=0.8),  # not strengthened\n",
    "#     torchvision.transforms.RandomGrayscale(p=0.2),\n",
    "#     # torchvision.transforms.RandomApply([\n",
    "#     #     simsiam.loader.GaussianBlur([.1, 2.])\n",
    "#     # ], p=0.5),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(),\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.Normalize(mean['train'], std['train'])\n",
    "# ])\n",
    "\n",
    "# Data augmentations for the val and test datasets.\n",
    "val_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((exp.input_size, exp.input_size)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean['val'], std['val'])\n",
    "])\n",
    "\n",
    "# Data augmentations for the val and test datasets.\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((exp.input_size, exp.input_size)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean['test'], std['test'])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff192ed-3aa6-4ee3-aab1-76d8a737d6c6",
   "metadata": {},
   "source": [
    "## ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e5c36-ceac-4e95-b132-e60207e5ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the three datasets.\n",
    "train_data = torchvision.datasets.ImageFolder(data_dir_target + '/train/')\n",
    "\n",
    "val_data = torchvision.datasets.ImageFolder(data_dir_target + '/val/')\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(data_dir_target + '/test/')\n",
    "\n",
    "# Building the lightly datasets from the PyTorch datasets.\n",
    "train_data_lightly = lightly.data.LightlyDataset.from_torch_dataset(train_data)\n",
    "\n",
    "val_data_lightly = lightly.data.LightlyDataset.from_torch_dataset(val_data)\n",
    "# val_data_lightly = lightly.data.LightlyDataset.from_torch_dataset(\n",
    "#     val_data,\n",
    "#     transform=val_transform\n",
    "# )\n",
    "\n",
    "test_data_lightly = lightly.data.LightlyDataset.from_torch_dataset(\n",
    "    test_data,\n",
    "    transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1aa1d-3350-4786-8bd1-7e45ca32a1da",
   "metadata": {},
   "source": [
    "## Dealing with imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf7498-f773-4b39-b236-86d8957a691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if handle_imb_classes:\n",
    "\n",
    "    # Creating a list of labels of samples.\n",
    "    train_sample_labels = train_data.targets\n",
    "\n",
    "    # Calculating the number of samples per label/class.\n",
    "    class_sample_count = np.unique(train_sample_labels,\n",
    "                                   return_counts=True)[1]\n",
    "    print(class_sample_count)\n",
    "\n",
    "    # Weight per sample not per class.\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in train_sample_labels])\n",
    "\n",
    "    # Casting.\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    samples_weigth = samples_weight.double()\n",
    "\n",
    "    # Sampler, imbalanced data.\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        samples_weight,\n",
    "        len(samples_weight)\n",
    "    )\n",
    "    shuffle = False\n",
    "\n",
    "else:\n",
    "    sampler = None\n",
    "    shuffle = True\n",
    "\n",
    "print(f'\\nSampler: {sampler}')\n",
    "print(f'Shuffle: {shuffle}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c1996-9ed9-4597-bb3a-3819f70bb22d",
   "metadata": {},
   "source": [
    "## Collate functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6088d6-ddbd-4829-a1ee-dbbc60b5d522",
   "metadata": {},
   "source": [
    "PyTorch uses a Collate Function to combine the data in your batches together.\n",
    "\n",
    "BaseCollateFunction (base class) takes a batch of images as input and <b>transforms each image into two different augmentations</b> with the help of random transforms. The images are then concatenated such that the output batch is exactly twice the length of the input batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33670d-1bed-4d12-a117-279d43b98089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class for other collate implementations.\n",
    "# This allows training.\n",
    "collate_fn_train = lightly.data.collate.BaseCollateFunction(train_transform)\n",
    "collate_fn_val = lightly.data.collate.BaseCollateFunction(val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba89a4-1ae0-4411-a958-43c0ffe1fc8c",
   "metadata": {},
   "source": [
    "These functions could be removed if I implement a custom load dataset function with a get_item that gets and tranforms two batches of images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c7a8d-f598-417d-92f2-3b411a68dcc0",
   "metadata": {},
   "source": [
    "## PyTorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a496a2-2e9f-4e66-81bb-9a6f475cacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader for training.\n",
    "dataloader_train_lightly = torch.utils.data.DataLoader(\n",
    "    train_data_lightly,\n",
    "    batch_size=exp.batch_size,\n",
    "    shuffle=shuffle,\n",
    "    collate_fn=collate_fn_train,\n",
    "    drop_last=True,\n",
    "    num_workers=exp.num_workers,\n",
    "    worker_init_fn=exp.seed_worker,\n",
    "    generator=exp.g,\n",
    "    sampler=sampler\n",
    ")\n",
    "\n",
    "# Dataloader for embedding (val).\n",
    "dataloader_val_lightly = torch.utils.data.DataLoader(\n",
    "    val_data_lightly,\n",
    "    batch_size=exp.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn_val,\n",
    "    drop_last=False,\n",
    "    num_workers=exp.num_workers,\n",
    "    worker_init_fn=exp.seed_worker,\n",
    "    generator=exp.g\n",
    ")\n",
    "\n",
    "# # Dataloader for embedding (val).\n",
    "# dataloader_val = torch.utils.data.DataLoader(\n",
    "#     val_data_lightly,\n",
    "#     batch_size=exp.batch_size,\n",
    "#     shuffle=False,\n",
    "#     drop_last=False,\n",
    "#     num_workers=exp.num_workers,\n",
    "#     worker_init_fn=exp.seed_worker,\n",
    "#     generator=exp.g\n",
    "# )\n",
    "\n",
    "# Dataloader for embedding (test).\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    test_data_lightly,\n",
    "    batch_size=exp.batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=exp.num_workers,\n",
    "    worker_init_fn=exp.seed_worker,\n",
    "    generator=exp.g\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967c24a-aa41-4dd3-b183-2232799c16a4",
   "metadata": {},
   "source": [
    "## Check the balance and size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa6845-e34c-4016-983d-688c1c0928cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check samples per class in train dataset.\n",
    "samples = np.unique(train_data.targets, return_counts=True)[1]\n",
    "print(f'\\nSamples/class train: {samples}')\n",
    "print(f'Samples train: {len(train_data.targets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e1f177-016e-4f0e-96f9-a06ce945882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check samples per class in train dataset.\n",
    "samples = np.unique(val_data.targets, return_counts=True)[1]\n",
    "print(f'\\nSamples/class val: {samples}')\n",
    "print(f'Samples val: {len(val_data.targets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b964a0-5d99-48aa-8108-fad4a1fdf2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check samples per class in test dataset.\n",
    "samples = np.unique(test_data.targets, return_counts=True)[1]\n",
    "print(f'\\nSamples/class test: {samples}')\n",
    "print(f'Samples test: {len(test_data.targets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52070963-a543-484f-a134-b5a1dde41fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nBatches in train dataset: {len(dataloader_train_lightly)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc5860-964d-4847-bc93-11327f9a2bd2",
   "metadata": {},
   "source": [
    "## Check the distribution of samples in the dataloader (lightly dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc5051f-c0fc-49a7-bb4c-b00e146d9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to save the labels.\n",
    "labels_list = []\n",
    "\n",
    "# Accessing Data and Targets in a PyTorch DataLoader.\n",
    "t0 = time.time()\n",
    "for i, (images, labels, names) in enumerate(dataloader_train_lightly):\n",
    "    labels_list.append(labels)\n",
    "\n",
    "# Concatenate list of lists (batches).\n",
    "labels_list = torch.cat(labels_list, dim=0).numpy()\n",
    "print(f'Sample distribution computation in train dataset (s): '\n",
    "      f'{(time.time()-t0):.2f}')\n",
    "\n",
    "# Count number of unique values.\n",
    "data_x, data_y = np.unique(labels_list, return_counts=True)\n",
    "\n",
    "# Old function to plot.\n",
    "# utils.simple_bar_plot(data_x, data_y,\n",
    "#                       x_axis_label=r'Class',\n",
    "#                       y_axis_label=r'N samples (dataloader)',\n",
    "#                       plt_name=f'imbalance_classes_{handle_imb_classes}',\n",
    "#                       fig_size=(15, 5), save=True)\n",
    "\n",
    "# New function to plot (suitable for execution in shell).\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "simple_bar_plot(ax,\n",
    "                data_x,\n",
    "                'Class',\n",
    "                data_y,\n",
    "                'N samples (dataloader)')\n",
    "\n",
    "plt.gcf().subplots_adjust(bottom=0.15)\n",
    "plt.gcf().subplots_adjust(left=0.15)\n",
    "fig_name_save = (f'sample_distribution'\n",
    "                 f'-ratio={ratio}'\n",
    "                 f'-balanced={handle_imb_classes}')\n",
    "fig.savefig(os.path.join(output_dir_fig, fig_name_save+fig_format),\n",
    "            bbox_inches='tight')\n",
    "if show:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366bf8d4-6053-4115-8fd7-b628a5b9a284",
   "metadata": {},
   "source": [
    "## Look at some samples (lightly dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4a8eb8-0133-4f1d-954a-6228f05cb6cd",
   "metadata": {},
   "source": [
    "### Only one sample from the training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d64513-dda3-4e75-853c-2290bcf40452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing Data and Targets in a PyTorch DataLoader.\n",
    "for i, (images, labels, names) in enumerate(dataloader_train_lightly):\n",
    "    img = images[0][0]\n",
    "    label = labels[0]\n",
    "    print(images[0].shape)\n",
    "    print(labels.shape)\n",
    "    plt.title(\"Label: \" + str(int(label)))\n",
    "    plt.imshow(torch.permute(img, (1, 2, 0)))\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    if i == 0:\n",
    "        break  # Only a few batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeaf97d-4427-4093-9b8d-85e8ea574ab7",
   "metadata": {},
   "source": [
    "### Two batches (almost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a80b14-6499-47dc-8aa4-d612aba00d14",
   "metadata": {},
   "source": [
    "Note: Comment out the normalization augmentation first to view the images below properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1bec3-ccfd-4947-b06d-abd7ab3a4cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(batch, batch_id):\n",
    "    \"\"\"\n",
    "    Shows the images in the batch.\n",
    "\n",
    "    Attributes:\n",
    "        batch: Batch of images.\n",
    "        batch_id: Batch identification number.\n",
    "    \"\"\"\n",
    "\n",
    "    columns = 8\n",
    "    rows = 2\n",
    "    width = 30\n",
    "    height = 5\n",
    "\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    fig.suptitle(f'Batch {batch_id}')\n",
    "    for i in range(1, columns * rows + 1):\n",
    "        if i < exp.batch_size:\n",
    "            img = batch[i]\n",
    "            fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(torch.permute(img, (1, 2, 0)))\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Train loop.\n",
    "for b, ((x0, x1), _, _) in enumerate(dataloader_train_lightly):\n",
    "\n",
    "    # Show the images within each batch.\n",
    "    show_batch(x0, 0)\n",
    "    show_batch(x1, 1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3352095f-6c8e-4b28-88cf-ed2625b7673e",
   "metadata": {},
   "source": [
    "Each image is augmented differently in the two batches that are loaded at the same time during training. The dataloader from lightly is capable of providing two batches in one iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a4a6d8-28c2-422a-926d-3d5bd9aa69cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Self-supervised models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c24a8-5d44-4fa9-ade5-a51c2517e991",
   "metadata": {},
   "source": [
    "## Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60885a39-131d-4def-86f0-2548aadfe7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import SimSiam, SimCLRModel, BarlowTwins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced40a42-bccb-4c16-9434-b3a0da6db8ca",
   "metadata": {},
   "source": [
    "Reference: Lightly tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4e7b1-3bd7-48f6-8e44-983ebdee7b64",
   "metadata": {},
   "source": [
    "## Backbone net (w/ ResNet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591e6ba-a646-430a-ad14-0f21db32827d",
   "metadata": {},
   "source": [
    "This is different from the tutorial: resnet without pretrained weights (not now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb30195-100c-428e-97c5-6941d5bcdd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet trained from scratch.\n",
    "resnet = torchvision.models.resnet18(\n",
    "    weights=weights\n",
    ")\n",
    "\n",
    "# Removing head from resnet. Embedding.\n",
    "backbone = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "# Model creation.\n",
    "if model_name == 'simsiam':\n",
    "    model = SimSiam(backbone, num_ftrs, proj_hidden_dim,\n",
    "                    pred_hidden_dim, out_dim)\n",
    "elif model_name == 'simclr':\n",
    "    hidden_dim = resnet.fc.in_features\n",
    "    model = SimCLRModel(backbone, hidden_dim)\n",
    "elif model_name == 'barlowtwins':\n",
    "    model = BarlowTwins(backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27a0c0-75d0-473e-9d6a-44075eaa6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's backbone structure.\n",
    "summary(\n",
    "    model.backbone,\n",
    "    input_size=(exp.batch_size, 3, exp.input_size, exp.input_size),\n",
    "    device=exp.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4308cc-0649-4f87-b574-bd306b931b21",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529c018-fe2b-40f5-91d0-1708f4925e5f",
   "metadata": {},
   "source": [
    "SimSiam uses a symmetric negative cosine similarity loss and does therefore not require any negative samples. We build a criterion and an optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c498b42-1e1d-4740-b3ad-2daaecc0908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the learning rate.\n",
    "# lr = 0.05 * exp.batch_size / 256\n",
    "lr = 0.2\n",
    "\n",
    "# Use SGD with momentum and weight decay.\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665beb28-df9a-4010-8781-50423d27dda7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745b5bb-e4a8-4886-83fd-e2ecbfbc6148",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320a250-fae9-4591-bff9-c62b46127264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device used for training.\n",
    "print(f'\\nUsing {exp.device} device')\n",
    "model.to(exp.device)\n",
    "\n",
    "# Saving best model's weights.\n",
    "# best_model_wts = copy.deepcopy(model.state_dict())\n",
    "collapse_level = 0.\n",
    "lowest_train_loss = 10000\n",
    "lowest_val_loss = 10000\n",
    "total_train_batches = len(dataloader_train_lightly)\n",
    "total_val_batches = len(dataloader_val_lightly)\n",
    "print(f'\\nBatches in (train, val) datasets: ({total_train_batches}, '\n",
    "      f'{total_val_batches})\\n')\n",
    "\n",
    "# ======================\n",
    "# TRAINING LOOP.\n",
    "# Iterating over the epochs.\n",
    "for e in range(exp.epochs):\n",
    "\n",
    "    # Timer added.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Training enabled.\n",
    "    model.train()\n",
    "\n",
    "    # ======================\n",
    "    # TRAINING COMPUTATION.\n",
    "    # Iterating through the dataloader (lightly dataset is different).\n",
    "    running_train_loss = 0.\n",
    "    for b, ((x0, x1), _, _) in enumerate(dataloader_train_lightly):\n",
    "\n",
    "        # Move images to the GPU (same batch two transformations).\n",
    "        x0 = x0.to(exp.device)\n",
    "        x1 = x1.to(exp.device)\n",
    "\n",
    "        # Run the model on both transforms of the images:\n",
    "        # We get projections (z0 and z1) and\n",
    "        # predictions (p0 and p1) as output.\n",
    "        if model_name == 'simsiam':\n",
    "            z0, p0 = model(x0)\n",
    "            z1, p1 = model(x1)\n",
    "            loss = 0.5 * (model.criterion(z0, p1) + model.criterion(z1, p0))\n",
    "        else:\n",
    "            loss = model.training_step(x0, x1)\n",
    "\n",
    "        # Averaged loss across all training examples * batch_size.\n",
    "        running_train_loss += loss.item() * exp.batch_size\n",
    "\n",
    "        # Run backpropagation.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if model_name == 'simsiam':\n",
    "            model.check_collapse(p0, loss)\n",
    "\n",
    "        # Show partial stats.\n",
    "        if b % (total_train_batches//4) == (total_train_batches//4-1):\n",
    "            print(f'T[{e}, {b + 1:5d}] | '\n",
    "                  f'Running train loss: '\n",
    "                  f'{running_train_loss/(b*exp.batch_size):.4f}')\n",
    "\n",
    "    # The level of collapse is large if the standard deviation of\n",
    "    # the l2 normalized output is much smaller than 1 / sqrt(dim).\n",
    "    if model_name == 'simsiam':\n",
    "        collapse_level = max(0., 1 - math.sqrt(out_dim) * model.avg_output_std)\n",
    "\n",
    "    # ======================\n",
    "    # TRAINING LOSS.\n",
    "    # Loss averaged across all training examples for the current epoch.\n",
    "    epoch_train_loss = (running_train_loss\n",
    "                        / len(dataloader_train_lightly.sampler))\n",
    "\n",
    "    # ======================\n",
    "    # EVALUATION COMPUTATION.\n",
    "    # The evaluation process was not okey (it's been deleted).\n",
    "    model.eval()\n",
    "    running_val_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for vb, ((x0, x1), y, _) in enumerate(dataloader_val_lightly):\n",
    "\n",
    "            # Move images to the GPU (same batch two transformations).\n",
    "            x0 = x0.to(exp.device)\n",
    "            x1 = x1.to(exp.device)\n",
    "\n",
    "            # Compute loss\n",
    "            if model_name == 'simsiam':\n",
    "                z0, p0 = model(x0)\n",
    "                z1, p1 = model(x1)\n",
    "                loss = 0.5 * (model.criterion(z0, p1) + model.criterion(z1, p0))\n",
    "            else:\n",
    "                loss = model.training_step(x0, x1)\n",
    "\n",
    "            # Averaged loss across all validation examples * batch_size.\n",
    "            running_val_loss += loss.item() * exp.batch_size\n",
    "\n",
    "            # Show partial stats.\n",
    "            if vb % (total_val_batches//4) == (total_val_batches//4-1):\n",
    "                print(f'V[{e}, {vb + 1:5d}] | '\n",
    "                      f'Running val loss: '\n",
    "                      f'{running_val_loss/(vb*exp.batch_size):.4f}')\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # ======================\n",
    "    # VALIDATION LOSS.\n",
    "    # Loss averaged across all training examples for the current epoch.\n",
    "    epoch_val_loss = (running_val_loss\n",
    "                      / len(dataloader_val_lightly.sampler))\n",
    "\n",
    "    # ======================\n",
    "    # SAVING CHECKPOINT.\n",
    "    # Save model.\n",
    "    save_model = ((epoch_train_loss < lowest_train_loss)\n",
    "                  or (epoch_val_loss < lowest_val_loss)\n",
    "                  or (e == exp.epochs - 1))\n",
    "    if save_model:\n",
    "\n",
    "        # Update new lowest losses\n",
    "        if epoch_train_loss < lowest_train_loss:\n",
    "            lowest_train_loss = epoch_train_loss\n",
    "        elif epoch_val_loss < lowest_val_loss:\n",
    "            lowest_val_loss = epoch_val_loss\n",
    "\n",
    "        # Move the model to CPU before saving\n",
    "        # it and then back to the GPU.\n",
    "        model.to('cpu')\n",
    "        model.save(e,\n",
    "                   epoch_train_loss,\n",
    "                   epoch_val_loss,\n",
    "                   handle_imb_classes,\n",
    "                   ratio,\n",
    "                   output_dir_model,\n",
    "                   collapse_level=collapse_level)\n",
    "        model.to(exp.device)\n",
    "\n",
    "    # ======================\n",
    "    # EPOCH STATISTICS.\n",
    "    # Show some stats per epoch completed.\n",
    "    print(f'[Epoch {e:3d}] | '\n",
    "          f'Train loss: {epoch_train_loss:.4f} | '\n",
    "          f'Val loss: {epoch_val_loss:.4f} | '\n",
    "          f'Duration: {(time.time()-t0):.2f} s | '\n",
    "          f'Saved: {save_model} | '\n",
    "          f'Collapse Level (SimSiam only): {collapse_level:.4f}/1.0\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8940983e-2196-4a15-8539-e55cba047ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5576290-ee98-4e46-994c-9f760be29a47",
   "metadata": {},
   "source": [
    "Collapse level: the closer to zero the better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243d3a7-2030-46b4-97ce-a095d5d8133b",
   "metadata": {},
   "source": [
    "A value close to 0 indicates that the representations have collapsed. A value close to 1/sqrt(dimensions), where dimensions are the number of representation dimensions, indicates that the representations are stable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6818fcd-c195-413f-b940-d03b83799dd3",
   "metadata": {},
   "source": [
    "### Checking the weights of the last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f997d9-11ab-4b6e-a7fc-5e17e56e7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convolutional layer weights.\n",
    "print(model.backbone[0])\n",
    "print(model.backbone[0].weight[63])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab33b97-aa53-4cf3-9ab7-6ce9341febbb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844b415-d517-418b-a261-0014089668c1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a5643-193b-4da7-b5e5-b88165318907",
   "metadata": {},
   "source": [
    "# Reduce dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847c99d-aeea-4118-a7f8-be436b93365c",
   "metadata": {},
   "source": [
    "## Calculate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea12b2bd-bcdb-4737-9d3f-82c579d35ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists.\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "# Disable gradients for faster calculations.\n",
    "# Put the model in evaluation mode.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # for i, (x, y, fnames) in enumerate(dataloader_val):\n",
    "    # Now taking only the first transformed batch.\n",
    "    for i, ((x, _), y, fnames) in enumerate(dataloader_val_lightly):\n",
    "\n",
    "        # Move the images to the GPU.\n",
    "        x = x.to(exp.device)\n",
    "        y = y.to(exp.device)\n",
    "\n",
    "        # Embed the images with the pre-trained backbone.\n",
    "        emb = model.backbone(x).flatten(start_dim=1)\n",
    "\n",
    "        # Store the embeddings and filenames in lists.\n",
    "        embeddings.append(emb)\n",
    "        labels.append(y)\n",
    "\n",
    "# Concatenate the embeddings and convert to numpy.\n",
    "embeddings = torch.cat(embeddings, dim=0).to('cpu').numpy()\n",
    "labels = torch.cat(labels, dim=0).to('cpu').numpy()\n",
    "\n",
    "# Show shapes.\n",
    "print(np.shape(embeddings))\n",
    "print(np.shape(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb69705-d01b-4f14-857e-90ee7e328ebd",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246eae4f-9f95-4c6c-9f44-0f35a37c3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18363d12-5b48-4f98-8cb1-feb3eae44aff",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1886ba-804b-413a-b167-848c396de753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA computation.\n",
    "df = pca_computation(embeddings, labels, exp.seed)\n",
    "\n",
    "# 2-D plot.\n",
    "if plot == '2d' or plot == \"23d\" or plot == 'all':\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    sns.scatterplot(\n",
    "        x='pca_x',\n",
    "        y='pca_y',\n",
    "        hue='labels',\n",
    "        palette=sns.color_palette('hls', 29),\n",
    "        data=df,\n",
    "        legend='full',\n",
    "        alpha=0.9\n",
    "    )\n",
    "    fig_name_save = (f'pca_2d-{model}')\n",
    "    fig.savefig(os.path.join(output_dir_fig, fig_name_save+fig_format),\n",
    "                bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "# 3-D plot with matplotlib.\n",
    "if plot == '3d' or plot == \"23d\" or plot == 'all':\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(\n",
    "        xs=df['pca_x'],\n",
    "        ys=df['pca_y'],\n",
    "        zs=df['pca_z'],\n",
    "        c=df['labels'],\n",
    "        cmap='tab10'\n",
    "    )\n",
    "    ax.set_xlabel('pca_x')\n",
    "    ax.set_ylabel('pca_y')\n",
    "    ax.set_zlabel('pca_z')\n",
    "    fig_name_save = (f'pca_3d-{model}')\n",
    "    fig.savefig(os.path.join(output_dir_fig, fig_name_save+fig_format),\n",
    "                bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "# 3-D plot with pyplot.\n",
    "if (plot == '3d-plotly' or plot == 'all') and show:\n",
    "    fig = px.scatter_3d(df, x='pca_x',\n",
    "                        y='pca_y', z='pca_z',\n",
    "                        color='labels',\n",
    "                        width=1000, height=800)  # symbol='labels'\n",
    "\n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "\n",
    "    # Move colorbar.\n",
    "    # fig.update_layout(coloraxis_colorbar=dict(yanchor=\"top\", y=1, x=0,\n",
    "    #                                           ticks=\"outside\",\n",
    "    #                                           ticksuffix=\"\"))\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e076936-68bb-4709-b888-7749ae985796",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d92516-071e-4830-b4db-eff7c93e0618",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6889458-b714-47e3-832c-ed0d55ea1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE computation for 2-D.\n",
    "df = tsne_computation(embeddings, labels, exp.seed, n_components=2)\n",
    "\n",
    "# 2-D plot.\n",
    "if plot == '2d' or plot == \"23d\" or plot == 'all':\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    sns.scatterplot(\n",
    "        x='tsne_x',\n",
    "        y='tsne_y',\n",
    "        hue='labels',\n",
    "        palette=sns.color_palette('hls', 29),\n",
    "        data=df,\n",
    "        legend='full',\n",
    "        alpha=0.9\n",
    "    )\n",
    "    fig_name_save = (f'tsne_2d-{model}')\n",
    "    fig.savefig(os.path.join(output_dir_fig, fig_name_save+fig_format),\n",
    "                bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "# t-SNE computation for 3-D.\n",
    "df = tsne_computation(embeddings, labels, exp.seed, n_components=3)\n",
    "\n",
    "# 3-D plot with matplotlib.\n",
    "if plot == '3d' or plot == \"23d\" or plot == 'all':\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(\n",
    "        xs=df['tsne_x'],\n",
    "        ys=df['tsne_y'],\n",
    "        zs=df['tsne_z'],\n",
    "        c=df['labels'],\n",
    "        cmap='tab10'\n",
    "    )\n",
    "    ax.set_xlabel('tsne_x')\n",
    "    ax.set_ylabel('tsne_y')\n",
    "    ax.set_zlabel('tsne_z')\n",
    "    fig_name_save = (f'tsne_3d-{model}')\n",
    "    fig.savefig(os.path.join(output_dir_fig, fig_name_save+fig_format),\n",
    "                bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "# 3-D plot with pyplot.\n",
    "if (plot == '3d-plotly' or plot == 'all') and show:\n",
    "    fig = px.scatter_3d(df, x='tsne_x',\n",
    "                        y='tsne_y', z='tsne_z',\n",
    "                        color='labels',\n",
    "                        width=1000, height=800)  # symbol='labels'\n",
    "\n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "\n",
    "    # Move colorbar.\n",
    "    # fig.update_layout(coloraxis_colorbar=dict(yanchor=\"top\", y=1, x=0,\n",
    "    #                                           ticks=\"outside\",\n",
    "    #                                           ticksuffix=\"\"))\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf66499-e3f5-477f-b158-840e9c041c63",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a48bc-7ed0-430b-8328-632b93901014",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42baa2ae-266c-4d94-88aa-25f9408f8339",
   "metadata": {},
   "source": [
    "# Check each model's performance/collapse on val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5cb1c-437d-4e7c-adbd-a9c57418fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scatter_plot_with_thumbnails_axes(ax, title=''):\n",
    "    \"\"\"\n",
    "    Creates a scatter plot with image overlays\n",
    "    that are plotted in a particular ax position.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Shuffle images and find out which images to show.\n",
    "    shown_images_idx = []\n",
    "    shown_images = np.array([[1., 1.]])\n",
    "    iterator = [i for i in range(embeddings_2d.shape[0])]\n",
    "    np.random.shuffle(iterator)\n",
    "    for i in iterator:\n",
    "\n",
    "        # Only show image if it is sufficiently far away from the others.\n",
    "        dist = np.sum((embeddings_2d[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < 2e-3:\n",
    "            continue\n",
    "        shown_images = np.r_[shown_images, [embeddings_2d[i]]]\n",
    "        shown_images_idx.append(i)\n",
    "\n",
    "    # Plot image overlays.\n",
    "    for idx in shown_images_idx:\n",
    "        thumbnail_size = int(rcp['figure.figsize'][0] * 2.5)  # 2.\n",
    "        path = os.path.join(data_dir_test, filenames[idx])\n",
    "        img = Image.open(path)\n",
    "        img = functional.resize(img, thumbnail_size)\n",
    "        img = np.array(img)\n",
    "        img_box = osb.AnnotationBbox(\n",
    "            osb.OffsetImage(img, cmap=plt.cm.gray_r),\n",
    "            embeddings_2d[idx],\n",
    "            pad=0.2,\n",
    "        )\n",
    "        ax.add_artist(img_box)\n",
    "\n",
    "    # Set aspect ratio.\n",
    "    ratio = 1. / ax.get_data_ratio()\n",
    "    ax.set_aspect(ratio, adjustable='box')\n",
    "    ax.title.set_text(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03d0fe-d736-4c71-a070-f77739c1d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation dataset.\n",
    "print(f'\\nValidating...')\n",
    "data_dir_test = os.path.join(data_dir_target, 'val')\n",
    "print(f'Dataset directory: {data_dir_test}')\n",
    "\n",
    "# List of trained models.\n",
    "print('List of model checkpoints:')\n",
    "model_list = []\n",
    "for root, dirs, files in os.walk(output_dir_model):\n",
    "    for i, filename in enumerate(sorted(files, reverse=False)):\n",
    "        model_list.append(os.path.join(root, filename))\n",
    "        print(f'{i:02}: {filename}')\n",
    "\n",
    "# Plot setup.\n",
    "ncols = 5\n",
    "nrows = int(math.ceil(len(model_list) / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows,\n",
    "                         ncols=ncols,\n",
    "                         figsize=(12*ncols, 12*nrows))\n",
    "\n",
    "# Convert the array to 1 dimension.\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Main loop over the models.\n",
    "for model_id, model_name_local in enumerate(model_list):\n",
    "\n",
    "    # Load model weights.\n",
    "    model.backbone.load_state_dict(torch.load(model_name_local))\n",
    "\n",
    "    # Empty lists.\n",
    "    embeddings = []\n",
    "    filenames = []\n",
    "\n",
    "    # Disable gradients for faster calculations.\n",
    "    # Put the model in evaluation mode.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # for i, (x, _, fnames) in enumerate(dataloader_val):\n",
    "        for i, ((x, _), _, fnames) in enumerate(dataloader_val_lightly):\n",
    "\n",
    "            # Move the images to the GPU.\n",
    "            x = x.to(exp.device)\n",
    "\n",
    "            # Embed the images with the pre-trained backbone.\n",
    "            y = model.backbone(x).flatten(start_dim=1)\n",
    "\n",
    "            # Store the embeddings and filenames in lists.\n",
    "            embeddings.append(y)\n",
    "            filenames = filenames + list(fnames)\n",
    "\n",
    "    # Concatenate the embeddings and convert to numpy.\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "    # For the scatter plot we want to transform the images to a\n",
    "    # 2-D vector space using a random Gaussian projection.\n",
    "    projection = random_projection.GaussianRandomProjection(\n",
    "        n_components=2,\n",
    "        random_state=exp.seed\n",
    "    )\n",
    "    embeddings_2d = projection.fit_transform(embeddings)\n",
    "\n",
    "    # Normalize the embeddings to fit in the [0, 1] square.\n",
    "    M = np.max(embeddings_2d, axis=0)\n",
    "    m = np.min(embeddings_2d, axis=0)\n",
    "    embeddings_2d = (embeddings_2d - m) / (M - m)\n",
    "\n",
    "    # Get a scatter plot with thumbnail overlays.\n",
    "    start_chr_epoch = model_name_local.find('-epoch') + 1\n",
    "    start_chr_time = model_name_local.find('-time')\n",
    "    get_scatter_plot_with_thumbnails_axes(\n",
    "        axes[model_id],\n",
    "        title=model_name_local[start_chr_epoch:start_chr_time]\n",
    "    )\n",
    "\n",
    "    # Show progress.\n",
    "    print(f'Subplot of model-{model_id} done!',\n",
    "          end='\\r',\n",
    "          flush=True)\n",
    "\n",
    "# Save figure.\n",
    "fig.suptitle(f'{model_name}')\n",
    "fig_name_save = (f'knn-{model}')\n",
    "fig.savefig(os.path.join(output_dir_fig, fig_name_save+fig_format),\n",
    "            bbox_inches='tight')\n",
    "if show:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c41974-eaad-4263-9e74-6fb3fb66b2f8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d5f3d-aa82-48ef-916c-0c50dbd3636f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91de858-a1f9-4f98-90c4-586d9dee8ac1",
   "metadata": {},
   "source": [
    "# Embeddings for the samples of the test dataset (WARNING: custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabfddb-57d8-46ed-8e2d-43c73e435d8a",
   "metadata": {},
   "source": [
    "## Setup (NOT WORKING PROPERLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb1456-1752-406e-8963-5e1c2203c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset.\n",
    "print('\\nTesting...')\n",
    "data_dir_test = os.path.join(data_dir_target, 'test')\n",
    "print(f'Dataset directory: {data_dir_test}')\n",
    "\n",
    "# Load best model weights.\n",
    "idx = -1\n",
    "\n",
    "# Print model.\n",
    "print(f'Target model checkpoint: {model_list[idx]}')\n",
    "model.backbone.load_state_dict(torch.load(model_list[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b175a024-3d6e-4ccc-a3f0-8035e3e8390c",
   "metadata": {},
   "source": [
    "## Compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db33812-0e52-4ba7-82ca-11da3895677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists.\n",
    "embeddings = []\n",
    "filenames = []\n",
    "\n",
    "# Disable gradients for faster calculations.\n",
    "# Put the model in evaluation mode.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (x, _, fnames) in enumerate(dataloader_test):\n",
    "\n",
    "        # Move the images to the GPU.\n",
    "        x = x.to(exp.device)\n",
    "\n",
    "        # Embed the images with the pre-trained backbone.\n",
    "        y = model.backbone(x).flatten(start_dim=1)\n",
    "\n",
    "        # Store the embeddings and filenames in lists.\n",
    "        embeddings.append(y)\n",
    "        filenames = filenames + list(fnames)\n",
    "\n",
    "# Concatenate the embeddings and convert to numpy.\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "embeddings = embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e672d5e-2d3c-479d-ba94-8f19e9543f4d",
   "metadata": {},
   "source": [
    "## Projection to 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb003c-70f0-46be-ade6-37e35e453aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the scatter plot we want to transform the images to a two-dimensional\n",
    "# vector space using a random Gaussian projection.\n",
    "projection = random_projection.GaussianRandomProjection(\n",
    "    n_components=2,\n",
    "    random_state=exp.seed\n",
    ")\n",
    "embeddings_2d = projection.fit_transform(embeddings)\n",
    "\n",
    "# Normalize the embeddings to fit in the [0, 1] square.\n",
    "M = np.max(embeddings_2d, axis=0)\n",
    "m = np.min(embeddings_2d, axis=0)\n",
    "embeddings_2d = (embeddings_2d - m) / (M - m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e176a-477a-4732-bd6a-960e98170a13",
   "metadata": {},
   "source": [
    "## Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f970f1-9506-43b5-a0c1-a408b1ca3705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty figure and add subplot.\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Get a scatter plot with thumbnail overlays.\n",
    "get_scatter_plot_with_thumbnails_axes(\n",
    "    ax,\n",
    "    title='Scatter plot with samples'\n",
    ")\n",
    "\n",
    "# Save figure.\n",
    "fig_name_save = (f'scatter_samples-{model}')\n",
    "fig.savefig(os.path.join(output_dir_fig, fig_name_save+fig_format),\n",
    "            bbox_inches='tight')\n",
    "if show:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc28d1-c569-4163-a3a7-f84ad17884eb",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1423fa1-653f-474e-ba2e-404cd7e37868",
   "metadata": {},
   "source": [
    "### Pick up one random sample per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998cde14-fb59-4330-b070-467a54537e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subdirectories (classes).\n",
    "directory_list = []\n",
    "for root, dirs, files in os.walk(data_dir_test):\n",
    "    for dirname in sorted(dirs):\n",
    "        directory_list.append(os.path.join(root, dirname))\n",
    "        # print(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae54d2-c622-493a-96fa-f98fa931c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files (samples).\n",
    "example_images = []\n",
    "for classes in directory_list:\n",
    "\n",
    "    # Random samples.\n",
    "    random_file = np.random.choice(os.listdir(classes))\n",
    "    path_to_random_file = classes + '/' + random_file\n",
    "\n",
    "    # Only class and filename.\n",
    "    start_chr = path_to_random_file.index('test/') + 5\n",
    "\n",
    "    # Append filename.\n",
    "    example_images.append(path_to_random_file[start_chr:])\n",
    "    # print(example_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb66cee-b413-4944-95ff-258740375d9a",
   "metadata": {},
   "source": [
    "### Look for similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afde3b-32bb-4bef-ae93-a3a18475a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_as_np_array(filename: str):\n",
    "    \"\"\"\n",
    "    Loads the image with filename and returns it as a numpy array.\n",
    "\n",
    "    \"\"\"\n",
    "    img = Image.open(filename)\n",
    "    return np.asarray(img)\n",
    "\n",
    "\n",
    "def get_image_as_np_array_with_frame(filename: str, w: int = 5):\n",
    "    \"\"\"\n",
    "    Returns an image as a numpy array with a black frame of width w.\n",
    "\n",
    "    \"\"\"\n",
    "    img = get_image_as_np_array(filename)\n",
    "    ny, nx, _ = img.shape\n",
    "\n",
    "    # Create an empty image with padding for the frame.\n",
    "    framed_img = np.zeros((w + ny + w, w + nx + w, 3))\n",
    "    framed_img = framed_img.astype(np.uint8)\n",
    "\n",
    "    # Put the original image in the middle of the new one.\n",
    "    framed_img[w:-w, w:-w] = img\n",
    "    return framed_img\n",
    "\n",
    "\n",
    "def plot_nearest_neighbors_nxn(example_image: str, i: int):\n",
    "    \"\"\"\n",
    "    Plots the example image and its eight nearest neighbors.\n",
    "\n",
    "    \"\"\"\n",
    "    n_subplots = 6\n",
    "\n",
    "    # Initialize empty figure.\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    fig.suptitle(f\"Nearest Neighbor Plot Class {i}\")\n",
    "\n",
    "    # Get indexes.\n",
    "    example_idx = filenames.index(example_image)\n",
    "\n",
    "    # Get distances to the cluster center.\n",
    "    distances = embeddings - embeddings[example_idx]\n",
    "    distances = np.power(distances, 2).sum(-1).squeeze()\n",
    "\n",
    "    # Sort indices by distance to the center.\n",
    "    nearest_neighbors = np.argsort(distances)[:n_subplots]\n",
    "\n",
    "    # Show images.\n",
    "    for plot_offset, plot_idx in enumerate(nearest_neighbors):\n",
    "        ax = fig.add_subplot(3, 3, plot_offset + 1)\n",
    "\n",
    "        # Get the corresponding filename.\n",
    "        fname = os.path.join(data_dir_test, filenames[plot_idx])\n",
    "        if plot_offset == 0:\n",
    "            ax.set_title(f\"Example Image\")\n",
    "            plt.imshow(get_image_as_np_array_with_frame(fname))\n",
    "        else:\n",
    "            plt.imshow(get_image_as_np_array(fname))\n",
    "\n",
    "        # Let's disable the axis.\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    # Save figure.\n",
    "    fig_name_save = (f'knn_per_class-c={i:02}-{model}')\n",
    "    fig.savefig(os.path.join(output_dir_fig, fig_name_save+fig_format),\n",
    "                bbox_inches='tight')\n",
    "    if show:\n",
    "        pass  # plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0db2c2-96ba-4715-9482-ace9155f5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example images for each cluster.\n",
    "for i, example_image in enumerate(example_images):\n",
    "    plot_nearest_neighbors_nxn(example_image, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f809fc8-79ab-4389-a8ed-5e6a98421fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lulc-venv",
   "language": "python",
   "name": "lulc-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

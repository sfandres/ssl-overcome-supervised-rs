{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924096aa-e3ec-437c-81fe-03dd73fbfce5",
   "metadata": {},
   "source": [
    "**FIRST ATTEMP TO APPLY SSL TO THE SENTINEL-2 DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49adbc03-f1d2-446f-b2aa-028c144e8a4a",
   "metadata": {},
   "source": [
    "Reference tutorial: https://docs.lightly.ai/tutorials/package/tutorial_simsiam_esa.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f2f9be-dc8e-4b2c-ab6e-94407466cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c01ea34-c899-40a9-8a7f-e6cc346cee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7fc52-07dd-41f2-9fb4-510f27a20e82",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de16ef-fd9a-45fb-b620-a514366d9db4",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a342e-d7c1-4999-8ed0-0a3118f5f3bf",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c506cd5d-71b7-47d7-b157-1b58caee0936",
   "metadata": {},
   "source": [
    "## Packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb94f55-b0da-4d7e-b461-f6a478166de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main.\n",
    "import utils\n",
    "\n",
    "# OS module.\n",
    "import os\n",
    "\n",
    "# PyTorch.\n",
    "import torch\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "\n",
    "# Data management.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Lightly.\n",
    "import lightly\n",
    "from lightly.models.modules.heads import SimSiamPredictionHead\n",
    "from lightly.models.modules.heads import SimSiamProjectionHead\n",
    "from lightly.utils.debug import std_of_l2_normalized\n",
    "from lightly.models.modules.heads import SimCLRProjectionHead\n",
    "from lightly.loss import NTXentLoss\n",
    "\n",
    "# Training checks.\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# Showing images in the notebook.\n",
    "import IPython\n",
    "\n",
    "# For plotting.\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.offsetbox as osb\n",
    "from matplotlib import rcParams as rcp\n",
    "import seaborn as sns\n",
    "\n",
    "# For resizing images to thumbnails.\n",
    "import torchvision.transforms.functional as functional\n",
    "\n",
    "# For clustering and 2d representations.\n",
    "from sklearn import random_projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2265f6b-9854-4e2e-85bd-3e7c88792e77",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3576cff2-906a-4897-ab1a-578ffebaa5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Hyperparamenters.\n",
    "exp = utils.Experiment(epochs=1,\n",
    "                       batch_size=128)\n",
    "output_dir_fig = 'figures/'\n",
    "output_dir_model = 'pytorch_models/'\n",
    "print(exp.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379a17bd-cd1f-40ea-a333-569ed975c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of the embeddings.\n",
    "num_ftrs = 512\n",
    "\n",
    "# Dimension of the output of the prediction and projection heads.\n",
    "out_dim = proj_hidden_dim = 512\n",
    "\n",
    "# The prediction head uses a bottleneck architecture.\n",
    "pred_hidden_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897cdadd-b624-406c-8c2d-d4f57d796a21",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b728bc3-9c98-474f-ba43-c1175071fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.reproducibility()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce08d1-0293-4830-a366-c149e95f0515",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6745ac7c-9497-4bf7-90b8-18a3dae2c27b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e2c6c-4e75-4708-88d2-1ef7da79a9f0",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e327f67-c943-482a-8294-2c90685a1579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/Sentinel2GlobalLULC_full-ratio=(0.7, 0.1, 0.2)-seed=42\n"
     ]
    }
   ],
   "source": [
    "# ratio = (.01, .01, .98)\n",
    "ratio = (.7, .1, .2)\n",
    "data_dir_target = f'datasets/Sentinel2GlobalLULC_full' \\\n",
    "                  f'-ratio={ratio}' \\\n",
    "                  f'-seed={exp.seed}'\n",
    "\n",
    "print(data_dir_target)\n",
    "\n",
    "if ratio == (.01, .01, .98):\n",
    "\n",
    "    mean_train = [0.3341, 0.3395, 0.3636]\n",
    "    std_train = [0.2904, 0.2328, 0.2091]\n",
    "\n",
    "    mean_val = [0.3357, 0.3382, 0.3616]\n",
    "    std_val = [0.2942, 0.2330, 0.2069]\n",
    "\n",
    "    mean_test = [0.3327, 0.3372, 0.3603]\n",
    "    std_test = [0.2976, 0.2376, 0.2115]\n",
    "\n",
    "elif ratio == (.7, .1, .2):\n",
    "\n",
    "    mean_train = [0.3329, 0.3373, 0.3603]\n",
    "    std_train = [0.2978, 0.2377, 0.2115]\n",
    "\n",
    "    mean_val = [0.3318, 0.3363, 0.3597]\n",
    "    std_val = [0.2962, 0.2364, 0.2104]\n",
    "\n",
    "    mean_test = [0.3328, 0.3375, 0.3605]\n",
    "    std_test = [0.2974, 0.2378, 0.2118]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ef555-daa7-48d6-be77-9d069ce7404d",
   "metadata": {},
   "source": [
    "## Custom tranforms (w/o normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a8172-7c2c-49a0-99de-223d3b0ebbca",
   "metadata": {},
   "source": [
    "Define the augmentations for self-supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b451709-864e-42fe-841e-b30157619385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentations for the train dataset.\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((exp.input_size, exp.input_size)),\n",
    "    torchvision.transforms.RandomResizedCrop(224, scale=(0.2, 1.)),\n",
    "    torchvision.transforms.RandomApply([\n",
    "            torchvision.transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "        ], p=0.8),  # not strengthened\n",
    "    torchvision.transforms.RandomGrayscale(p=0.2),\n",
    "    # torchvision.transforms.RandomApply([\n",
    "    #     simsiam.loader.GaussianBlur([.1, 2.])\n",
    "    # ], p=0.5),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean_train, std_train)\n",
    "])\n",
    "\n",
    "# Data augmentations for the val and test datasets.\n",
    "val_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((exp.input_size, exp.input_size)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean_val, std_val)\n",
    "])\n",
    "\n",
    "# Data augmentations for the val and test datasets.\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((exp.input_size, exp.input_size)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean_test, std_test)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff192ed-3aa6-4ee3-aab1-76d8a737d6c6",
   "metadata": {},
   "source": [
    "## ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e50bcc5-7e50-4d9e-962f-9a21343618a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the three datasets.\n",
    "train_data = torchvision.datasets.ImageFolder(data_dir_target + '/train/')\n",
    "\n",
    "val_data = torchvision.datasets.ImageFolder(data_dir_target + '/val/')\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(data_dir_target + '/test/')\n",
    "\n",
    "# Building the lightly datasets from the PyTorch datasets.\n",
    "train_data_lightly = lightly.data.LightlyDataset.from_torch_dataset(train_data)\n",
    "\n",
    "val_data_lightly = lightly.data.LightlyDataset.from_torch_dataset(\n",
    "    val_data,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "test_data_lightly = lightly.data.LightlyDataset.from_torch_dataset(\n",
    "    test_data,\n",
    "    transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c1996-9ed9-4597-bb3a-3819f70bb22d",
   "metadata": {},
   "source": [
    "## Collate functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6088d6-ddbd-4829-a1ee-dbbc60b5d522",
   "metadata": {},
   "source": [
    "PyTorch uses a Collate Function to combine the data in your batches together.\n",
    "\n",
    "BaseCollateFunction (base class) takes a batch of images as input and transforms each image into two different augmentations with the help of random transforms. The images are then concatenated such that the output batch is exactly twice the length of the input batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af33670d-1bed-4d12-a117-279d43b98089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class for other collate implementations.\n",
    "# This allows training.\n",
    "collate_fn_train = lightly.data.collate.BaseCollateFunction(train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c7a8d-f598-417d-92f2-3b411a68dcc0",
   "metadata": {},
   "source": [
    "## PyTorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57a496a2-2e9f-4e66-81bb-9a6f475cacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader for training.\n",
    "dataloader_train_simsiam = torch.utils.data.DataLoader(\n",
    "    train_data_lightly,\n",
    "    batch_size=exp.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_train,\n",
    "    drop_last=True,\n",
    "    num_workers=exp.num_workers,\n",
    "    worker_init_fn=exp.seed_worker,\n",
    "    generator=exp.g\n",
    ")\n",
    "\n",
    "# Dataloader for embedding (val).\n",
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "    val_data_lightly,\n",
    "    batch_size=exp.batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=exp.num_workers,\n",
    "    worker_init_fn=exp.seed_worker,\n",
    "    generator=exp.g\n",
    ")\n",
    "\n",
    "# Dataloader for embedding (test).\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    test_data_lightly,\n",
    "    batch_size=exp.batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=exp.num_workers,\n",
    "    worker_init_fn=exp.seed_worker,\n",
    "    generator=exp.g\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967c24a-aa41-4dd3-b183-2232799c16a4",
   "metadata": {},
   "source": [
    "## Check the balance and size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80aa6845-e34c-4016-983d-688c1c0928cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]), array([9800, 3259, 6208, 9800, 8355, 3105,  943, 9800, 7306, 4466, 2015,\n",
      "        396,  880, 9800, 2739, 2710, 9793,  291,  340, 2943, 9800, 9800,\n",
      "       9800, 1402,  589,  714,  247,  289, 8813]))\n"
     ]
    }
   ],
   "source": [
    "# Check samples per class in train dataset.\n",
    "print(np.unique(train_data.targets, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01e1f177-016e-4f0e-96f9-a06ce945882a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]), array([1400,  465,  886, 1400, 1193,  443,  134, 1400, 1043,  638,  288,\n",
      "         56,  125, 1400,  391,  387, 1399,   41,   48,  420, 1400, 1400,\n",
      "       1400,  200,   84,  102,   35,   41, 1259]))\n"
     ]
    }
   ],
   "source": [
    "# Check samples per class in train dataset.\n",
    "print(np.unique(val_data.targets, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97b964a0-5d99-48aa-8108-fad4a1fdf2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]), array([2800,  932, 1775, 2800, 2389,  889,  271, 2800, 2089, 1276,  577,\n",
      "        115,  253, 2800,  784,  775, 2799,   84,   99,  842, 2800, 2800,\n",
      "       2800,  402,  169,  204,   71,   83, 2518]))\n"
     ]
    }
   ],
   "source": [
    "# Check samples per class in test dataset.\n",
    "print(np.unique(test_data.targets, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52070963-a543-484f-a134-b5a1dde41fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N batches in train dataset: 1065\n",
      "136403\n",
      "19478\n",
      "38996\n"
     ]
    }
   ],
   "source": [
    "print('N batches in train dataset: ' + str(len(dataloader_train_simsiam)))\n",
    "\n",
    "# Check the size of each dataset.\n",
    "print(len(train_data.targets))\n",
    "print(len(val_data.targets))\n",
    "print(len(test_data.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366bf8d4-6053-4115-8fd7-b628a5b9a284",
   "metadata": {},
   "source": [
    "## See some samples (lightly dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40d64513-dda3-4e75-853c-2290bcf40452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 224, 224])\n",
      "torch.Size([128])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accessing Data and Targets in a PyTorch DataLoader.\n",
    "for images, labels, names in dataloader_train_simsiam:\n",
    "    img = images[0][0]\n",
    "    label = labels[0]\n",
    "    print(images[0].shape)\n",
    "    print(labels.shape)\n",
    "    plt.title(\"Label: \" + str(int(label)))\n",
    "    plt.imshow(torch.permute(img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    break  # Only one batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a4a6d8-28c2-422a-926d-3d5bd9aa69cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SimSiam model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebc3f5ec-a5f0-43fd-ae76-f2861008ef61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://user-images.githubusercontent.com/2420753/118343499-4c410100-b4de-11eb-9313-d49e65440a7e.png\" width=\"700\" height=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Image(\n",
    "    url=\"https://user-images.githubusercontent.com/\"\n",
    "    \"2420753/118343499-4c410100-b4de-11eb-9313-d49e65440a7e.png\",\n",
    "    width=700,\n",
    "    height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325b910-69ef-48c8-8777-11189b1e0920",
   "metadata": {},
   "source": [
    "Given an image, we create two augmented views and process these two versions with the same encoder network (a backbone plus a projection MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da7b00a-48c4-4e71-8b4c-a4d54d633df4",
   "metadata": {},
   "source": [
    "The predictor is another MLP and on the other side a stop-gradient operation is applied. And we maximize the similarity at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c24a8-5d44-4fa9-ade5-a51c2517e991",
   "metadata": {},
   "source": [
    "## Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60885a39-131d-4def-86f0-2548aadfe7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from the reference tutorial.\n",
    "class SimSiam(torch.nn.Module):\n",
    "\n",
    "    # Constructor.\n",
    "    def __init__(self, backbone, num_ftrs, proj_hidden_dim,\n",
    "                 pred_hidden_dim, out_dim):\n",
    "\n",
    "        # Inheritance.\n",
    "        super().__init__()\n",
    "\n",
    "        # Blackbone model.\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # Projection head (lightly).\n",
    "        self.projection_head = SimSiamProjectionHead(\n",
    "            num_ftrs, proj_hidden_dim, out_dim)\n",
    "\n",
    "        # Prediction head (lightly).\n",
    "        self.prediction_head = SimSiamPredictionHead(\n",
    "            out_dim, pred_hidden_dim, out_dim)\n",
    "\n",
    "    # In the forward function, you define how your model\n",
    "    # is going to be run, from input to output.\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Get representations.\n",
    "        f = self.backbone(x).flatten(start_dim=1)\n",
    "\n",
    "        # Get projections.\n",
    "        z = self.projection_head(f)\n",
    "\n",
    "        # Get predictions.\n",
    "        p = self.prediction_head(z)\n",
    "\n",
    "        # Stop gradient.\n",
    "        z = z.detach()\n",
    "\n",
    "        return z, p\n",
    "\n",
    "\n",
    "class SimCLRModel(torch.nn.Module):\n",
    "\n",
    "    # Constructor.\n",
    "    def __init__(self, backbone, hidden_dim):\n",
    "\n",
    "        # Inheritance.\n",
    "        super().__init__()\n",
    "\n",
    "        # Blackbone model.\n",
    "        self.backbone = backbone\n",
    "\n",
    "        self.projection_head = SimCLRProjectionHead(hidden_dim,\n",
    "                                                    hidden_dim,\n",
    "                                                    128)\n",
    "\n",
    "        self.criterion = NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h = self.backbone(x).flatten(start_dim=1)\n",
    "\n",
    "        z = self.projection_head(h)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4e7b1-3bd7-48f6-8e44-983ebdee7b64",
   "metadata": {},
   "source": [
    "## Backbone net (w/ ResNet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591e6ba-a646-430a-ad14-0f21db32827d",
   "metadata": {},
   "source": [
    "This is different from the tutorial: resnet without pretrained weights (not now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bb30195-100c-428e-97c5-6941d5bcdd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet trained from scratch.\n",
    "resnet = torchvision.models.resnet18(\n",
    "    weights=torchvision.models.ResNet18_Weights.DEFAULT\n",
    "    # weights=None\n",
    ")\n",
    "\n",
    "# Removing head from resnet. Embedding.\n",
    "backbone = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "# Model creation.\n",
    "# model = SimSiam(backbone, num_ftrs, proj_hidden_dim,\n",
    "#                 pred_hidden_dim, out_dim)\n",
    "hidden_dim = resnet.fc.in_features\n",
    "model = SimCLRModel(backbone, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc27a0c0-75d0-473e-9d6a-44075eaa6306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [128, 512, 1, 1]          --\n",
       "├─Conv2d: 1-1                            [128, 64, 112, 112]       9,408\n",
       "├─BatchNorm2d: 1-2                       [128, 64, 112, 112]       128\n",
       "├─ReLU: 1-3                              [128, 64, 112, 112]       --\n",
       "├─MaxPool2d: 1-4                         [128, 64, 56, 56]         --\n",
       "├─Sequential: 1-5                        [128, 64, 56, 56]         --\n",
       "│    └─BasicBlock: 2-1                   [128, 64, 56, 56]         --\n",
       "│    │    └─Conv2d: 3-1                  [128, 64, 56, 56]         36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [128, 64, 56, 56]         128\n",
       "│    │    └─ReLU: 3-3                    [128, 64, 56, 56]         --\n",
       "│    │    └─Conv2d: 3-4                  [128, 64, 56, 56]         36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [128, 64, 56, 56]         128\n",
       "│    │    └─ReLU: 3-6                    [128, 64, 56, 56]         --\n",
       "│    └─BasicBlock: 2-2                   [128, 64, 56, 56]         --\n",
       "│    │    └─Conv2d: 3-7                  [128, 64, 56, 56]         36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [128, 64, 56, 56]         128\n",
       "│    │    └─ReLU: 3-9                    [128, 64, 56, 56]         --\n",
       "│    │    └─Conv2d: 3-10                 [128, 64, 56, 56]         36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [128, 64, 56, 56]         128\n",
       "│    │    └─ReLU: 3-12                   [128, 64, 56, 56]         --\n",
       "├─Sequential: 1-6                        [128, 128, 28, 28]        --\n",
       "│    └─BasicBlock: 2-3                   [128, 128, 28, 28]        --\n",
       "│    │    └─Conv2d: 3-13                 [128, 128, 28, 28]        73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [128, 128, 28, 28]        256\n",
       "│    │    └─ReLU: 3-15                   [128, 128, 28, 28]        --\n",
       "│    │    └─Conv2d: 3-16                 [128, 128, 28, 28]        147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [128, 128, 28, 28]        256\n",
       "│    │    └─Sequential: 3-18             [128, 128, 28, 28]        8,448\n",
       "│    │    └─ReLU: 3-19                   [128, 128, 28, 28]        --\n",
       "│    └─BasicBlock: 2-4                   [128, 128, 28, 28]        --\n",
       "│    │    └─Conv2d: 3-20                 [128, 128, 28, 28]        147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [128, 128, 28, 28]        256\n",
       "│    │    └─ReLU: 3-22                   [128, 128, 28, 28]        --\n",
       "│    │    └─Conv2d: 3-23                 [128, 128, 28, 28]        147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [128, 128, 28, 28]        256\n",
       "│    │    └─ReLU: 3-25                   [128, 128, 28, 28]        --\n",
       "├─Sequential: 1-7                        [128, 256, 14, 14]        --\n",
       "│    └─BasicBlock: 2-5                   [128, 256, 14, 14]        --\n",
       "│    │    └─Conv2d: 3-26                 [128, 256, 14, 14]        294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [128, 256, 14, 14]        512\n",
       "│    │    └─ReLU: 3-28                   [128, 256, 14, 14]        --\n",
       "│    │    └─Conv2d: 3-29                 [128, 256, 14, 14]        589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [128, 256, 14, 14]        512\n",
       "│    │    └─Sequential: 3-31             [128, 256, 14, 14]        33,280\n",
       "│    │    └─ReLU: 3-32                   [128, 256, 14, 14]        --\n",
       "│    └─BasicBlock: 2-6                   [128, 256, 14, 14]        --\n",
       "│    │    └─Conv2d: 3-33                 [128, 256, 14, 14]        589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [128, 256, 14, 14]        512\n",
       "│    │    └─ReLU: 3-35                   [128, 256, 14, 14]        --\n",
       "│    │    └─Conv2d: 3-36                 [128, 256, 14, 14]        589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [128, 256, 14, 14]        512\n",
       "│    │    └─ReLU: 3-38                   [128, 256, 14, 14]        --\n",
       "├─Sequential: 1-8                        [128, 512, 7, 7]          --\n",
       "│    └─BasicBlock: 2-7                   [128, 512, 7, 7]          --\n",
       "│    │    └─Conv2d: 3-39                 [128, 512, 7, 7]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [128, 512, 7, 7]          1,024\n",
       "│    │    └─ReLU: 3-41                   [128, 512, 7, 7]          --\n",
       "│    │    └─Conv2d: 3-42                 [128, 512, 7, 7]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [128, 512, 7, 7]          1,024\n",
       "│    │    └─Sequential: 3-44             [128, 512, 7, 7]          132,096\n",
       "│    │    └─ReLU: 3-45                   [128, 512, 7, 7]          --\n",
       "│    └─BasicBlock: 2-8                   [128, 512, 7, 7]          --\n",
       "│    │    └─Conv2d: 3-46                 [128, 512, 7, 7]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [128, 512, 7, 7]          1,024\n",
       "│    │    └─ReLU: 3-48                   [128, 512, 7, 7]          --\n",
       "│    │    └─Conv2d: 3-49                 [128, 512, 7, 7]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [128, 512, 7, 7]          1,024\n",
       "│    │    └─ReLU: 3-51                   [128, 512, 7, 7]          --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [128, 512, 1, 1]          --\n",
       "==========================================================================================\n",
       "Total params: 11,176,512\n",
       "Trainable params: 11,176,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 232.14\n",
       "==========================================================================================\n",
       "Input size (MB): 77.07\n",
       "Forward/backward pass size (MB): 5086.64\n",
       "Params size (MB): 44.71\n",
       "Estimated Total Size (MB): 5208.42\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model's backbone structure.\n",
    "summary(\n",
    "    model.backbone,\n",
    "    input_size=(exp.batch_size, 3, exp.input_size, exp.input_size),\n",
    "    device=exp.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4308cc-0649-4f87-b574-bd306b931b21",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529c018-fe2b-40f5-91d0-1708f4925e5f",
   "metadata": {},
   "source": [
    "SimSiam uses a symmetric negative cosine similarity loss and does therefore not require any negative samples. We build a criterion and an optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "464482d9-3c11-4581-8ea4-acb7933c2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'simclr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c498b42-1e1d-4740-b3ad-2daaecc0908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimSiam uses a symmetric negative cosine similarity loss.\n",
    "if model_name == 'simsiam':\n",
    "    criterion = lightly.loss.NegativeCosineSimilarity()\n",
    "elif model_name == 'simclr':\n",
    "    criterion = model.criterion\n",
    "\n",
    "# Scale the learning rate.\n",
    "# lr = 0.05 * exp.batch_size / 256\n",
    "lr = 0.2\n",
    "\n",
    "# Use SGD with momentum and weight decay.\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665beb28-df9a-4010-8781-50423d27dda7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745b5bb-e4a8-4886-83fd-e2ecbfbc6148",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c320a250-fae9-4591-bff9-c62b46127264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to find a valid cuDNN algorithm to run convolution",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimclr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     41\u001b[0m     z0 \u001b[38;5;241m=\u001b[39m model(x0)\n\u001b[0;32m---> 42\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(z0, z1)\n\u001b[1;32m     45\u001b[0m batch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Documents/Git/lulc/lulc-venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mSimCLRModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 60\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     62\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection_head(h)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m~/Documents/Git/lulc/lulc-venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Git/lulc/lulc-venv/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Git/lulc/lulc-venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Git/lulc/lulc-venv/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Git/lulc/lulc-venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Git/lulc/lulc-venv/lib/python3.8/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/Documents/Git/lulc/lulc-venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Git/lulc/lulc-venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Git/lulc/lulc-venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to find a valid cuDNN algorithm to run convolution"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56:80: E501 line too long (83 > 79 characters)\n",
      "70:80: E501 line too long (80 > 79 characters)\n",
      "130:80: E501 line too long (92 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using {exp.device} device\")\n",
    "model.to(exp.device)\n",
    "\n",
    "# Setup.\n",
    "avg_loss = 0.\n",
    "avg_output_std = 0.\n",
    "avg_rep_collapse = 0.\n",
    "\n",
    "# Saving best model's weights.\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "lowest_loss = 10000\n",
    "lowest_collapse_level = 10000\n",
    "lowest_vbatch_loss = 10000\n",
    "\n",
    "# Main training loop.\n",
    "for e in range(exp.epochs):\n",
    "\n",
    "    # Timer added.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Training enabled.\n",
    "    model.train()\n",
    "\n",
    "    # Iterating through the dataloader (lightly dataset is different).\n",
    "    batch_id = 0\n",
    "    batch_loss = 0.\n",
    "    for (x0, x1), _, _ in dataloader_train_simsiam:\n",
    "\n",
    "        # Move images to the GPU.\n",
    "        x0 = x0.to(exp.device)\n",
    "        x1 = x1.to(exp.device)\n",
    "\n",
    "        # Run the model on both transforms of the images:\n",
    "        # We get projections (z0 and z1) and\n",
    "        # predictions (p0 and p1) as output.\n",
    "        if model_name == 'simsiam':\n",
    "            z0, p0 = model(x0)\n",
    "            z1, p1 = model(x1)\n",
    "            loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))\n",
    "        elif model_name == 'simclr':\n",
    "            z0 = model(x0)\n",
    "            z1 = model(x1)\n",
    "            loss = criterion(z0, z1)\n",
    "\n",
    "        batch_loss += loss.item()\n",
    "\n",
    "        # Run backpropagation.\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if model_name == 'simsiam':\n",
    "\n",
    "            # Calculate the per-dimension standard deviation of the outputs.\n",
    "            # We can use this later to check whether the embeddings are collapsing.\n",
    "            output = p0.detach()\n",
    "            output = torch.nn.functional.normalize(output, dim=1)\n",
    "\n",
    "            output_std = torch.std(output, 0)\n",
    "            output_std = output_std.mean()\n",
    "\n",
    "            # Use moving averages to track the loss and standard deviation.\n",
    "            w = 0.9\n",
    "            avg_loss = w * avg_loss + (1 - w) * loss.item()\n",
    "            avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\n",
    "\n",
    "            # Use moving averages to track representation collapse.\n",
    "            avg_rep_collapse = (w * avg_rep_collapse\n",
    "                                + (1 - w) * std_of_l2_normalized(output).item())\n",
    "\n",
    "        if batch_id % 250 == 249:\n",
    "\n",
    "            if model_name == 'simsiam':\n",
    "                print(f'T[{e + 1}, {batch_id + 1:5d}] | '\n",
    "                      f'Loss: {avg_loss:.4f} | '\n",
    "                      f'Representation std: {avg_rep_collapse:.4f}')\n",
    "\n",
    "            elif model_name == 'simclr':\n",
    "                print(f'T[{e + 1}, {batch_id + 1:5d}] | Loss: {batch_loss}')\n",
    "\n",
    "        batch_id += 1\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vbatch_loss = 0\n",
    "        for i, (x, _, fnames) in enumerate(dataloader_val):\n",
    "            if model_name == 'simsiam':\n",
    "                z0, p0 = model(x0)\n",
    "                z1, p1 = model(x1)\n",
    "                vloss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))\n",
    "            elif model_name == 'simclr':\n",
    "                z0 = model(x0)\n",
    "                z1 = model(x1)\n",
    "                vloss = criterion(z0, z1)\n",
    "\n",
    "            vbatch_loss += vloss.item()\n",
    "\n",
    "    # Save model.\n",
    "    save_model = vbatch_loss < lowest_vbatch_loss\n",
    "    if save_model:\n",
    "\n",
    "        # Update new lowest validation batch loss\n",
    "        lowest_vbatch_loss = vbatch_loss\n",
    "#         lowest_loss = avg_loss\n",
    "#         lowest_collapse_level = collapse_level\n",
    "#         lowest_rep_collapse = avg_rep_collapse\n",
    "#         best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Move to CPU before saving it.\n",
    "        model.to('cpu')\n",
    "\n",
    "#         # Filename with stats.\n",
    "#         filename = f'pytorch_models/simsiam/simsiam_backbone_resnet18' \\\n",
    "#                    f'-epoch={e:03}' \\\n",
    "#                    f'-loss={lowest_loss:.4f}' \\\n",
    "#                    f'-coll={lowest_collapse_level:.4f}(0)' \\\n",
    "#                    f'_{avg_rep_collapse:.4f}({rep_ideal:4f})' \\\n",
    "#                    f'-time={datetime.now():%Y_%m_%d_%H_%M_%S}'\n",
    "\n",
    "        # Save this pretrained model (recommended approach).\n",
    "        # torch.save(model.backbone.state_dict(), filename)\n",
    "        if model_name == 'simclr':\n",
    "            filename = f'simclr_bb_resnet18' \\\n",
    "                       f'-epoch={e:03}' \\\n",
    "                       f'-train_loss={batch_loss:.4f}' \\\n",
    "                       f'-val_loss={vbatch_loss:.4f}' \\\n",
    "                       f'-time={datetime.now():%Y_%m_%d_%H_%M_%S}'\n",
    "            torch.save(model.backbone.state_dict(), output_dir_model + 'simclr/' + filename)\n",
    "\n",
    "        # Move back to the GPU.\n",
    "        model.to(exp.device)\n",
    "\n",
    "    print(f'[Epoch {e:3d}] | '\n",
    "          f'Train loss: {batch_loss:.4f} | '\n",
    "          f'Val loss: {vbatch_loss:.4f} | '\n",
    "          f'Duration: {(time.time()-t0):.2f} s | '\n",
    "          f'Saved: {save_model}')\n",
    "\n",
    "#     # Print intermediate results (timing added).\n",
    "#     print(f'[Epoch {e:3d}] '\n",
    "#           f'Loss: {avg_loss:.4f} | '\n",
    "#           f'Collapse Level: {collapse_level:.4f} / 1.00 | '\n",
    "#           f'Representation std: {avg_rep_collapse:.4f} / {rep_ideal:4f} | '\n",
    "#           f'Duration: {(time.time()-t0):.2f} s | '\n",
    "#           f'Saved: {save_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5576290-ee98-4e46-994c-9f760be29a47",
   "metadata": {},
   "source": [
    "Collapse level: the closer to zero the better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243d3a7-2030-46b4-97ce-a095d5d8133b",
   "metadata": {},
   "source": [
    "A value close to 0 indicates that the representations have collapsed. A value close to 1/sqrt(dimensions), where dimensions are the number of representation dimensions, indicates that the representations are stable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6818fcd-c195-413f-b940-d03b83799dd3",
   "metadata": {},
   "source": [
    "### Checking the weights of the last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f997d9-11ab-4b6e-a7fc-5e17e56e7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convolutional layer weights.\n",
    "print(model.backbone[0])\n",
    "print(model.backbone[0].weight[63])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab33b97-aa53-4cf3-9ab7-6ce9341febbb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844b415-d517-418b-a261-0014089668c1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a5643-193b-4da7-b5e5-b88165318907",
   "metadata": {},
   "source": [
    "# Reduce dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847c99d-aeea-4118-a7f8-be436b93365c",
   "metadata": {},
   "source": [
    "## Calculate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea12b2bd-bcdb-4737-9d3f-82c579d35ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists.\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "# Disable gradients for faster calculations.\n",
    "# Put the model in evaluation mode.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (x, y, fnames) in enumerate(dataloader_val):\n",
    "\n",
    "        # Move the images to the GPU.\n",
    "        x = x.to(exp.device)\n",
    "        y = y.to(exp.device)\n",
    "\n",
    "        # Embed the images with the pre-trained backbone.\n",
    "        emb = model.backbone(x).flatten(start_dim=1)\n",
    "\n",
    "        # Store the embeddings and filenames in lists.\n",
    "        embeddings.append(emb)\n",
    "        labels.append(y)\n",
    "\n",
    "# Concatenate the embeddings and convert to numpy.\n",
    "embeddings = torch.cat(embeddings, dim=0).to('cpu').numpy()\n",
    "labels = torch.cat(labels, dim=0).to('cpu').numpy()\n",
    "\n",
    "# Show shapes.\n",
    "print(np.shape(embeddings))\n",
    "print(np.shape(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb69705-d01b-4f14-857e-90ee7e328ebd",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18363d12-5b48-4f98-8cb1-feb3eae44aff",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a0cc2-4271-4f33-8014-af6691783b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.pca_computation_and_plot(embeddings, labels, exp.seed, plot='all', filename=filename, save_2d=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e076936-68bb-4709-b888-7749ae985796",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d92516-071e-4830-b4db-eff7c93e0618",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604892e-df36-4a24-80d6-61ba53d1ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.tsne_computation_and_plot(embeddings, labels, exp.seed, plot='all', filename=filename, save_2d=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf66499-e3f5-477f-b158-840e9c041c63",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a48bc-7ed0-430b-8328-632b93901014",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42baa2ae-266c-4d94-88aa-25f9408f8339",
   "metadata": {},
   "source": [
    "# Check each model's performance/collapse on val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5cb1c-437d-4e7c-adbd-a9c57418fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scatter_plot_with_thumbnails_axes(ax, title=''):\n",
    "    \"\"\"\n",
    "    Creates a scatter plot with image overlays\n",
    "    that are plotted in a particular ax position.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Shuffle images and find out which images to show.\n",
    "    shown_images_idx = []\n",
    "    shown_images = np.array([[1., 1.]])\n",
    "    iterator = [i for i in range(embeddings_2d.shape[0])]\n",
    "    np.random.shuffle(iterator)\n",
    "    for i in iterator:\n",
    "\n",
    "        # Only show image if it is sufficiently far away from the others.\n",
    "        dist = np.sum((embeddings_2d[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < 2e-3:\n",
    "            continue\n",
    "        shown_images = np.r_[shown_images, [embeddings_2d[i]]]\n",
    "        shown_images_idx.append(i)\n",
    "\n",
    "    # Plot image overlays.\n",
    "    for idx in shown_images_idx:\n",
    "        thumbnail_size = int(rcp['figure.figsize'][0] * 2.5)  # 2.\n",
    "        path = os.path.join(data_dir_test, filenames[idx])\n",
    "        img = Image.open(path)\n",
    "        img = functional.resize(img, thumbnail_size)\n",
    "        img = np.array(img)\n",
    "        img_box = osb.AnnotationBbox(\n",
    "            osb.OffsetImage(img, cmap=plt.cm.gray_r),\n",
    "            embeddings_2d[idx],\n",
    "            pad=0.2,\n",
    "        )\n",
    "        ax.add_artist(img_box)\n",
    "\n",
    "    # Set aspect ratio.\n",
    "    ratio = 1. / ax.get_data_ratio()\n",
    "    ax.set_aspect(ratio, adjustable='box')\n",
    "    ax.title.set_text(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03d0fe-d736-4c71-a070-f77739c1d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation dataset.\n",
    "data_dir_test = data_dir_target + '/val/'\n",
    "print(data_dir_test)\n",
    "\n",
    "# List of trained models.\n",
    "model_list = []\n",
    "# root_dir = 'pytorch_models/simsiam/'\n",
    "root_dir = 'pytorch_models/simclr/'\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    for i, filename in enumerate(sorted(files, reverse=False)):\n",
    "        model_list.append(root + filename)\n",
    "        print(f'{i:02}: {filename}')\n",
    "\n",
    "# Plot setup.\n",
    "ncols = 5\n",
    "nrows = int(math.ceil(len(model_list) / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows,\n",
    "                         ncols=ncols,\n",
    "                         figsize=(12*ncols, 12*nrows))\n",
    "\n",
    "# Convert the array to 1 dimension.\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Main loop over the models.\n",
    "for model_id, model_name in enumerate(model_list):\n",
    "\n",
    "    # Load model weights.\n",
    "    model.backbone.load_state_dict(torch.load(model_name))\n",
    "\n",
    "    # Empty lists.\n",
    "    embeddings = []\n",
    "    filenames = []\n",
    "\n",
    "    # Disable gradients for faster calculations.\n",
    "    # Put the model in evaluation mode.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _, fnames) in enumerate(dataloader_val):\n",
    "\n",
    "            # Move the images to the GPU.\n",
    "            x = x.to(exp.device)\n",
    "\n",
    "            # Embed the images with the pre-trained backbone.\n",
    "            y = model.backbone(x).flatten(start_dim=1)\n",
    "\n",
    "            # Store the embeddings and filenames in lists.\n",
    "            embeddings.append(y)\n",
    "            filenames = filenames + list(fnames)\n",
    "\n",
    "    # Concatenate the embeddings and convert to numpy.\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "    # For the scatter plot we want to transform the images to a two-dimensional\n",
    "    # vector space using a random Gaussian projection.\n",
    "    projection = random_projection.GaussianRandomProjection(\n",
    "        n_components=2,\n",
    "        random_state=exp.seed\n",
    "    )\n",
    "    embeddings_2d = projection.fit_transform(embeddings)\n",
    "\n",
    "    # Normalize the embeddings to fit in the [0, 1] square.\n",
    "    M = np.max(embeddings_2d, axis=0)\n",
    "    m = np.min(embeddings_2d, axis=0)\n",
    "    embeddings_2d = (embeddings_2d - m) / (M - m)\n",
    "\n",
    "    # Get a scatter plot with thumbnail overlays.\n",
    "    get_scatter_plot_with_thumbnails_axes(axes[model_id],\n",
    "                                          title=model_name[49:104])\n",
    "\n",
    "    # Show progress.\n",
    "    print(f'Subplot of model-{model_id} done!',\n",
    "          end='\\r',\n",
    "          flush=True)\n",
    "\n",
    "# Save figure.\n",
    "fig.savefig(f'{output_dir_fig}models_knn_{datetime.now():%Y_%m_%d_%H_%M_%S}.pdf',\n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c41974-eaad-4263-9e74-6fb3fb66b2f8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d5f3d-aa82-48ef-916c-0c50dbd3636f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91de858-a1f9-4f98-90c4-586d9dee8ac1",
   "metadata": {},
   "source": [
    "# Embeddings for the samples of the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabfddb-57d8-46ed-8e2d-43c73e435d8a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb1456-1752-406e-8963-5e1c2203c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset.\n",
    "data_dir_test = data_dir_target + '/test/'\n",
    "print(data_dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb16e0a-9190-4413-aa76-6fd557a66e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model weights\n",
    "idx = 0\n",
    "print(model_list[idx])\n",
    "model.backbone.load_state_dict(torch.load(model_list[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b175a024-3d6e-4ccc-a3f0-8035e3e8390c",
   "metadata": {},
   "source": [
    "## Compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db33812-0e52-4ba7-82ca-11da3895677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists.\n",
    "embeddings = []\n",
    "filenames = []\n",
    "\n",
    "# Disable gradients for faster calculations.\n",
    "# Put the model in evaluation mode.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (x, _, fnames) in enumerate(dataloader_test):\n",
    "\n",
    "        # Move the images to the GPU.\n",
    "        x = x.to(exp.device)\n",
    "\n",
    "        # Embed the images with the pre-trained backbone.\n",
    "        y = model.backbone(x).flatten(start_dim=1)\n",
    "\n",
    "        # Store the embeddings and filenames in lists.\n",
    "        embeddings.append(y)\n",
    "        filenames = filenames + list(fnames)\n",
    "\n",
    "# Concatenate the embeddings and convert to numpy.\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "embeddings = embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e672d5e-2d3c-479d-ba94-8f19e9543f4d",
   "metadata": {},
   "source": [
    "## Projection to 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb003c-70f0-46be-ade6-37e35e453aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the scatter plot we want to transform the images to a two-dimensional\n",
    "# vector space using a random Gaussian projection.\n",
    "projection = random_projection.GaussianRandomProjection(\n",
    "    n_components=2,\n",
    "    random_state=exp.seed\n",
    ")\n",
    "embeddings_2d = projection.fit_transform(embeddings)\n",
    "\n",
    "# Normalize the embeddings to fit in the [0, 1] square.\n",
    "M = np.max(embeddings_2d, axis=0)\n",
    "m = np.min(embeddings_2d, axis=0)\n",
    "embeddings_2d = (embeddings_2d - m) / (M - m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e176a-477a-4732-bd6a-960e98170a13",
   "metadata": {},
   "source": [
    "## Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f970f1-9506-43b5-a0c1-a408b1ca3705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty figure and add subplot.\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Get a scatter plot with thumbnail overlays.\n",
    "get_scatter_plot_with_thumbnails_axes(\n",
    "    ax,\n",
    "    title='Scatter Plot of the Sentinel-2 Dataset'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc28d1-c569-4163-a3a7-f84ad17884eb",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1423fa1-653f-474e-ba2e-404cd7e37868",
   "metadata": {},
   "source": [
    "### Pick up one random sample per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998cde14-fb59-4330-b070-467a54537e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subdirectories (classes).\n",
    "directory_list = []\n",
    "for root, dirs, files in os.walk(data_dir_test):\n",
    "    for dirname in sorted(dirs):\n",
    "        directory_list.append(os.path.join(root, dirname))\n",
    "        # print(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae54d2-c622-493a-96fa-f98fa931c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ratio == (.01, .01, .98):\n",
    "    start_chr = 72\n",
    "elif ratio == (.7, .1, .2):\n",
    "    start_chr = 69\n",
    "\n",
    "# List of files (samples).\n",
    "example_images = []\n",
    "for classes in directory_list:\n",
    "    random_file = np.random.choice(os.listdir(classes))\n",
    "    path_to_random_file = classes + '/' + random_file\n",
    "    example_images.append(path_to_random_file[start_chr:])\n",
    "    # print(example_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb66cee-b413-4944-95ff-258740375d9a",
   "metadata": {},
   "source": [
    "### Look for similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afde3b-32bb-4bef-ae93-a3a18475a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_as_np_array(filename: str):\n",
    "    \"\"\"\n",
    "    Loads the image with filename and returns it as a numpy array.\n",
    "\n",
    "    \"\"\"\n",
    "    img = Image.open(filename)\n",
    "    return np.asarray(img)\n",
    "\n",
    "\n",
    "def get_image_as_np_array_with_frame(filename: str, w: int = 5):\n",
    "    \"\"\"\n",
    "    Returns an image as a numpy array with a black frame of width w.\n",
    "\n",
    "    \"\"\"\n",
    "    img = get_image_as_np_array(filename)\n",
    "    ny, nx, _ = img.shape\n",
    "\n",
    "    # Create an empty image with padding for the frame.\n",
    "    framed_img = np.zeros((w + ny + w, w + nx + w, 3))\n",
    "    framed_img = framed_img.astype(np.uint8)\n",
    "\n",
    "    # Put the original image in the middle of the new one.\n",
    "    framed_img[w:-w, w:-w] = img\n",
    "    return framed_img\n",
    "\n",
    "\n",
    "def plot_nearest_neighbors_nxn(example_image: str, i: int):\n",
    "    \"\"\"\n",
    "    Plots the example image and its eight nearest neighbors.\n",
    "\n",
    "    \"\"\"\n",
    "    n_subplots = 6\n",
    "\n",
    "    # Initialize empty figure.\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    fig.suptitle(f\"Nearest Neighbor Plot {i + 1}\")\n",
    "\n",
    "    # Get indexes.\n",
    "    example_idx = filenames.index(example_image)\n",
    "\n",
    "    # Get distances to the cluster center.\n",
    "    distances = embeddings - embeddings[example_idx]\n",
    "    distances = np.power(distances, 2).sum(-1).squeeze()\n",
    "\n",
    "    # Sort indices by distance to the center.\n",
    "    nearest_neighbors = np.argsort(distances)[:n_subplots]\n",
    "\n",
    "    # Show images.\n",
    "    for plot_offset, plot_idx in enumerate(nearest_neighbors):\n",
    "        ax = fig.add_subplot(3, 3, plot_offset + 1)\n",
    "\n",
    "        # Get the corresponding filename.\n",
    "        fname = os.path.join(data_dir_test, filenames[plot_idx])\n",
    "        if plot_offset == 0:\n",
    "            ax.set_title(f\"Example Image\")\n",
    "            plt.imshow(get_image_as_np_array_with_frame(fname))\n",
    "        else:\n",
    "            plt.imshow(get_image_as_np_array(fname))\n",
    "\n",
    "        # Let's disable the axis.\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0db2c2-96ba-4715-9482-ace9155f5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example images for each cluster.\n",
    "for i, example_image in enumerate(example_images):\n",
    "    plot_nearest_neighbors_nxn(example_image, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f809fc8-79ab-4389-a8ed-5e6a98421fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lulc-venv",
   "language": "python",
   "name": "lulc-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 06:50:40,896	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 06:50:40,897	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 06:50:43,332	SUCC scripts.py:747 -- --------------------
2024-01-07 06:50:43,332	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 06:50:43,332	SUCC scripts.py:749 -- --------------------
2024-01-07 06:50:43,333	INFO scripts.py:751 -- Next steps
2024-01-07 06:50:43,333	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 06:50:43,333	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 06:50:43,333	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 06:50:43,333	INFO scripts.py:773 -- import ray
2024-01-07 06:50:43,333	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 06:50:43,333	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 06:50:43,333	INFO scripts.py:791 --   ray status
2024-01-07 06:50:43,333	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 06:50:43,333	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 06:50:43,333	INFO scripts.py:810 --   ray stop
2024-01-07 06:50:43,334	INFO scripts.py:891 -- --block
2024-01-07 06:50:43,334	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 06:50:43,334	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              7286596253068510324
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7f5e7ca6b100>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          BarlowTwins
task_name:           multilabel
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          5
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         random
seed:                42
dropout:             None
transfer_learning:   LP
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1


Model resnet18 with pretrained weights using BarlowTwins SSL
Model loaded from snapshot_BarlowTwins_resnet18_bd=False_iw=random.pt
Model name:        BarlowTwins
Backbone name:     resnet18
Hidden layer dim.: 256
Output layer dim.: 128
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Linear probing adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 06:51:27,290	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 06:51:27,310	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 06:51:50,027	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-07 06:51:50 (running for 00:00:21.75)
Memory usage on this node: 13.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |
| train_ccef6_00001 | PENDING  |                    | 0.001  |       0.99 |         0      |
| train_ccef6_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_ccef6_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=66954)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=66954)[0m Configuration completed!
[2m[36m(func pid=66954)[0m New optimizer parameters:
[2m[36m(func pid=66954)[0m SGD (
[2m[36m(func pid=66954)[0m Parameter Group 0
[2m[36m(func pid=66954)[0m     dampening: 0
[2m[36m(func pid=66954)[0m     differentiable: False
[2m[36m(func pid=66954)[0m     foreach: None
[2m[36m(func pid=66954)[0m     lr: 0.0001
[2m[36m(func pid=66954)[0m     maximize: False
[2m[36m(func pid=66954)[0m     momentum: 0.99
[2m[36m(func pid=66954)[0m     nesterov: False
[2m[36m(func pid=66954)[0m     weight_decay: 0
[2m[36m(func pid=66954)[0m )
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8974 | Steps: 2 | Val loss: 0.7097 | Batch size: 32 | lr: 0.0001 | Duration: 4.63s
[2m[36m(func pid=66954)[0m rmse: 0.1827496588230133
[2m[36m(func pid=66954)[0m mae:  0.13448189198970795
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
== Status ==
Current time: 2024-01-07 06:51:59 (running for 00:00:31.02)
Memory usage on this node: 15.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |
| train_ccef6_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_ccef6_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67327)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=67327)[0m Configuration completed!
[2m[36m(func pid=67327)[0m New optimizer parameters:
[2m[36m(func pid=67327)[0m SGD (
[2m[36m(func pid=67327)[0m Parameter Group 0
[2m[36m(func pid=67327)[0m     dampening: 0
[2m[36m(func pid=67327)[0m     differentiable: False
[2m[36m(func pid=67327)[0m     foreach: None
[2m[36m(func pid=67327)[0m     lr: 0.001
[2m[36m(func pid=67327)[0m     maximize: False
[2m[36m(func pid=67327)[0m     momentum: 0.99
[2m[36m(func pid=67327)[0m     nesterov: False
[2m[36m(func pid=67327)[0m     weight_decay: 0
[2m[36m(func pid=67327)[0m )
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8956 | Steps: 2 | Val loss: 0.7071 | Batch size: 32 | lr: 0.001 | Duration: 4.38s
[2m[36m(func pid=67327)[0m rmse: 0.18271969258785248
[2m[36m(func pid=67327)[0m mae:  0.1344553530216217
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
== Status ==
Current time: 2024-01-07 06:52:07 (running for 00:00:38.75)
Memory usage on this node: 17.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |
| train_ccef6_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67743)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=67743)[0m Configuration completed!
[2m[36m(func pid=67743)[0m New optimizer parameters:
[2m[36m(func pid=67743)[0m SGD (
[2m[36m(func pid=67743)[0m Parameter Group 0
[2m[36m(func pid=67743)[0m     dampening: 0
[2m[36m(func pid=67743)[0m     differentiable: False
[2m[36m(func pid=67743)[0m     foreach: None
[2m[36m(func pid=67743)[0m     lr: 0.01
[2m[36m(func pid=67743)[0m     maximize: False
[2m[36m(func pid=67743)[0m     momentum: 0.99
[2m[36m(func pid=67743)[0m     nesterov: False
[2m[36m(func pid=67743)[0m     weight_decay: 0
[2m[36m(func pid=67743)[0m )
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8924 | Steps: 2 | Val loss: 0.6861 | Batch size: 32 | lr: 0.01 | Duration: 4.64s
[2m[36m(func pid=67743)[0m rmse: 0.18276025354862213
[2m[36m(func pid=67743)[0m mae:  0.13448591530323029
[2m[36m(func pid=67743)[0m rmse_per_class: [0.116, 0.267, 0.109, 0.339, 0.11, 0.191, 0.294, 0.145, 0.144, 0.113]
== Status ==
Current time: 2024-01-07 06:52:15 (running for 00:00:46.69)
Memory usage on this node: 20.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 06:52:23 (running for 00:00:54.81)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |        |        |                      |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |        |        |                      |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.892 |  0.183 |                    1 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |        |        |                      |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=68170)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=68170)[0m Configuration completed!
[2m[36m(func pid=68170)[0m New optimizer parameters:
[2m[36m(func pid=68170)[0m SGD (
[2m[36m(func pid=68170)[0m Parameter Group 0
[2m[36m(func pid=68170)[0m     dampening: 0
[2m[36m(func pid=68170)[0m     differentiable: False
[2m[36m(func pid=68170)[0m     foreach: None
[2m[36m(func pid=68170)[0m     lr: 0.1
[2m[36m(func pid=68170)[0m     maximize: False
[2m[36m(func pid=68170)[0m     momentum: 0.99
[2m[36m(func pid=68170)[0m     nesterov: False
[2m[36m(func pid=68170)[0m     weight_decay: 0
[2m[36m(func pid=68170)[0m )
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8412 | Steps: 2 | Val loss: 0.6286 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8964 | Steps: 2 | Val loss: 0.7054 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8918 | Steps: 2 | Val loss: 0.6985 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8373 | Steps: 2 | Val loss: 0.5171 | Batch size: 32 | lr: 0.1 | Duration: 4.80s
== Status ==
Current time: 2024-01-07 06:52:28 (running for 00:00:59.84)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.897 |  0.183 |                    1 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.896 |  0.183 |                    1 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.892 |  0.183 |                    1 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |        |        |                      |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.18250194191932678
[2m[36m(func pid=67743)[0m mae:  0.13433736562728882
[2m[36m(func pid=67743)[0m rmse_per_class: [0.117, 0.267, 0.107, 0.339, 0.111, 0.19, 0.294, 0.144, 0.144, 0.113]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.18252979218959808
[2m[36m(func pid=66954)[0m mae:  0.13436461985111237
[2m[36m(func pid=66954)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.113]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.1825285255908966
[2m[36m(func pid=67327)[0m mae:  0.13435098528862
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.113]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.18266086280345917
[2m[36m(func pid=68170)[0m mae:  0.13443602621555328
[2m[36m(func pid=68170)[0m rmse_per_class: [0.118, 0.267, 0.108, 0.338, 0.106, 0.191, 0.293, 0.146, 0.144, 0.114]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7562 | Steps: 2 | Val loss: 0.5521 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8815 | Steps: 2 | Val loss: 0.6866 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8987 | Steps: 2 | Val loss: 0.7021 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5365 | Steps: 2 | Val loss: 0.3429 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 06:52:33 (running for 00:01:05.23)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.896 |  0.183 |                    2 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.892 |  0.183 |                    2 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.756 |  0.182 |                    3 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.837 |  0.183 |                    1 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.18188031017780304
[2m[36m(func pid=67743)[0m mae:  0.13387717306613922
[2m[36m(func pid=67743)[0m rmse_per_class: [0.118, 0.267, 0.105, 0.339, 0.11, 0.19, 0.293, 0.143, 0.143, 0.112]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.18221089243888855
[2m[36m(func pid=66954)[0m mae:  0.1341329962015152
[2m[36m(func pid=66954)[0m rmse_per_class: [0.117, 0.266, 0.105, 0.339, 0.113, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=67327)[0m rmse: 0.18219707906246185
[2m[36m(func pid=67327)[0m mae:  0.13410606980323792
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.18074733018875122
[2m[36m(func pid=68170)[0m mae:  0.1330273449420929
[2m[36m(func pid=68170)[0m rmse_per_class: [0.123, 0.268, 0.102, 0.336, 0.092, 0.191, 0.288, 0.147, 0.146, 0.114]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6525 | Steps: 2 | Val loss: 0.4715 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8974 | Steps: 2 | Val loss: 0.6985 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8657 | Steps: 2 | Val loss: 0.6706 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4096 | Steps: 2 | Val loss: 0.3322 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=67327)[0m rmse: 0.18177571892738342
[2m[36m(func pid=67327)[0m mae:  0.13377687335014343
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.266, 0.105, 0.338, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=67327)[0m 
== Status ==
Current time: 2024-01-07 06:52:38 (running for 00:01:10.28)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.897 |  0.182 |                    4 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.866 |  0.182 |                    4 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.756 |  0.182 |                    3 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.537 |  0.181 |                    2 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.18180561065673828
[2m[36m(func pid=66954)[0m mae:  0.13381171226501465
[2m[36m(func pid=66954)[0m rmse_per_class: [0.117, 0.266, 0.105, 0.339, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.18107694387435913
[2m[36m(func pid=67743)[0m mae:  0.13326308131217957
[2m[36m(func pid=67743)[0m rmse_per_class: [0.119, 0.266, 0.102, 0.338, 0.106, 0.189, 0.292, 0.143, 0.143, 0.112]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.17551949620246887
[2m[36m(func pid=68170)[0m mae:  0.12892432510852814
[2m[36m(func pid=68170)[0m rmse_per_class: [0.125, 0.267, 0.088, 0.327, 0.075, 0.19, 0.278, 0.145, 0.15, 0.11]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8465 | Steps: 2 | Val loss: 0.6534 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8922 | Steps: 2 | Val loss: 0.6953 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5546 | Steps: 2 | Val loss: 0.4020 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 06:52:43 (running for 00:01:15.38)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.897 |  0.182 |                    4 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.847 |  0.181 |                    5 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.653 |  0.181 |                    4 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.41  |  0.176 |                    3 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.18137605488300323
[2m[36m(func pid=66954)[0m mae:  0.13346107304096222
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.112, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.18132594227790833
[2m[36m(func pid=67327)[0m mae:  0.13340803980827332
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.265, 0.104, 0.338, 0.112, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.1798209547996521
[2m[36m(func pid=67743)[0m mae:  0.13229425251483917
[2m[36m(func pid=67743)[0m rmse_per_class: [0.12, 0.266, 0.099, 0.336, 0.101, 0.189, 0.291, 0.143, 0.143, 0.111]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4666 | Steps: 2 | Val loss: 0.3895 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=68170)[0m rmse: 0.16810235381126404
[2m[36m(func pid=68170)[0m mae:  0.12255730479955673
[2m[36m(func pid=68170)[0m rmse_per_class: [0.124, 0.264, 0.07, 0.318, 0.062, 0.187, 0.262, 0.141, 0.152, 0.102]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8892 | Steps: 2 | Val loss: 0.6931 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8268 | Steps: 2 | Val loss: 0.6333 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4787 | Steps: 2 | Val loss: 0.3521 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 06:52:48 (running for 00:01:20.54)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.889 |  0.181 |                    6 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.847 |  0.181 |                    5 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.555 |  0.18  |                    5 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.467 |  0.168 |                    4 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.1810423880815506
[2m[36m(func pid=66954)[0m mae:  0.133164182305336
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.112, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.1809430867433548
[2m[36m(func pid=67327)[0m mae:  0.1330830454826355
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.111, 0.19, 0.293, 0.142, 0.142, 0.11]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.17813655734062195
[2m[36m(func pid=67743)[0m mae:  0.13096414506435394
[2m[36m(func pid=67743)[0m rmse_per_class: [0.12, 0.264, 0.095, 0.333, 0.094, 0.189, 0.289, 0.143, 0.143, 0.111]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5526 | Steps: 2 | Val loss: 0.4484 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8862 | Steps: 2 | Val loss: 0.6908 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=68170)[0m rmse: 0.15968894958496094
[2m[36m(func pid=68170)[0m mae:  0.11448570340871811
[2m[36m(func pid=68170)[0m rmse_per_class: [0.119, 0.259, 0.053, 0.304, 0.055, 0.185, 0.243, 0.134, 0.15, 0.095]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8005 | Steps: 2 | Val loss: 0.6114 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4292 | Steps: 2 | Val loss: 0.3239 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 06:52:54 (running for 00:01:25.56)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.886 |  0.181 |                    7 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.827 |  0.181 |                    6 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.479 |  0.178 |                    6 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.553 |  0.16  |                    5 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.18081331253051758
[2m[36m(func pid=66954)[0m mae:  0.1329498291015625
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.264, 0.103, 0.338, 0.111, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.1805432140827179
[2m[36m(func pid=67327)[0m mae:  0.13274219632148743
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.11]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.17621800303459167
[2m[36m(func pid=67743)[0m mae:  0.12941935658454895
[2m[36m(func pid=67743)[0m rmse_per_class: [0.121, 0.263, 0.091, 0.33, 0.087, 0.189, 0.286, 0.142, 0.143, 0.11]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.6410 | Steps: 2 | Val loss: 0.4873 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8847 | Steps: 2 | Val loss: 0.6882 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7716 | Steps: 2 | Val loss: 0.5881 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4088 | Steps: 2 | Val loss: 0.3149 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=68170)[0m rmse: 0.1522514820098877
[2m[36m(func pid=68170)[0m mae:  0.10585862398147583
[2m[36m(func pid=68170)[0m rmse_per_class: [0.108, 0.252, 0.044, 0.29, 0.054, 0.183, 0.232, 0.128, 0.145, 0.086]
[2m[36m(func pid=68170)[0m 
== Status ==
Current time: 2024-01-07 06:52:59 (running for 00:01:30.72)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.885 |  0.181 |                    8 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.8   |  0.181 |                    7 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.429 |  0.176 |                    7 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.641 |  0.152 |                    6 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.18053777515888214
[2m[36m(func pid=66954)[0m mae:  0.13271096348762512
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.264, 0.103, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.1801529824733734
[2m[36m(func pid=67327)[0m mae:  0.13241279125213623
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.263, 0.101, 0.337, 0.109, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.17436417937278748
[2m[36m(func pid=67743)[0m mae:  0.12791547179222107
[2m[36m(func pid=67743)[0m rmse_per_class: [0.122, 0.261, 0.086, 0.327, 0.081, 0.188, 0.284, 0.142, 0.144, 0.109]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.6886 | Steps: 2 | Val loss: 0.5013 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8777 | Steps: 2 | Val loss: 0.6855 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7404 | Steps: 2 | Val loss: 0.5635 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4034 | Steps: 2 | Val loss: 0.3190 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=68170)[0m rmse: 0.15013091266155243
[2m[36m(func pid=68170)[0m mae:  0.09961742162704468
[2m[36m(func pid=68170)[0m rmse_per_class: [0.095, 0.247, 0.041, 0.284, 0.055, 0.183, 0.251, 0.124, 0.139, 0.082]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.18033039569854736
[2m[36m(func pid=66954)[0m mae:  0.13251589238643646
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.263, 0.102, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
== Status ==
Current time: 2024-01-07 06:53:04 (running for 00:01:35.86)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.878 |  0.18  |                    9 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.772 |  0.18  |                    8 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.409 |  0.174 |                    8 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.689 |  0.15  |                    7 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.17975649237632751
[2m[36m(func pid=67327)[0m mae:  0.13207745552062988
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.263, 0.1, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.17215187847614288
[2m[36m(func pid=67743)[0m mae:  0.12604793906211853
[2m[36m(func pid=67743)[0m rmse_per_class: [0.121, 0.259, 0.081, 0.323, 0.074, 0.188, 0.281, 0.141, 0.144, 0.108]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.6806 | Steps: 2 | Val loss: 0.4871 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8773 | Steps: 2 | Val loss: 0.6830 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7100 | Steps: 2 | Val loss: 0.5384 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4174 | Steps: 2 | Val loss: 0.3323 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=68170)[0m rmse: 0.15430152416229248
[2m[36m(func pid=68170)[0m mae:  0.09788922965526581
[2m[36m(func pid=68170)[0m rmse_per_class: [0.08, 0.246, 0.043, 0.282, 0.056, 0.186, 0.312, 0.123, 0.134, 0.081]
[2m[36m(func pid=68170)[0m 
== Status ==
Current time: 2024-01-07 06:53:09 (running for 00:01:40.88)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.877 |  0.18  |                   10 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.74  |  0.18  |                    9 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.403 |  0.172 |                    9 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.681 |  0.154 |                    8 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.18010938167572021
[2m[36m(func pid=66954)[0m mae:  0.13232335448265076
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.263, 0.101, 0.337, 0.111, 0.19, 0.293, 0.141, 0.141, 0.109]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.17938242852687836
[2m[36m(func pid=67327)[0m mae:  0.1317584067583084
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.262, 0.099, 0.336, 0.107, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.16999921202659607
[2m[36m(func pid=67743)[0m mae:  0.12420885264873505
[2m[36m(func pid=67743)[0m rmse_per_class: [0.121, 0.257, 0.077, 0.319, 0.069, 0.188, 0.278, 0.14, 0.144, 0.107]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6390 | Steps: 2 | Val loss: 0.4483 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8722 | Steps: 2 | Val loss: 0.6797 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6776 | Steps: 2 | Val loss: 0.5125 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4356 | Steps: 2 | Val loss: 0.3511 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=68170)[0m rmse: 0.16127493977546692
[2m[36m(func pid=68170)[0m mae:  0.10034944117069244
[2m[36m(func pid=68170)[0m rmse_per_class: [0.073, 0.244, 0.045, 0.286, 0.056, 0.19, 0.377, 0.128, 0.132, 0.082]
[2m[36m(func pid=68170)[0m 
== Status ==
Current time: 2024-01-07 06:53:14 (running for 00:01:46.03)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.872 |  0.18  |                   11 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.71  |  0.179 |                   10 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.417 |  0.17  |                   10 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.639 |  0.161 |                    9 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17994947731494904
[2m[36m(func pid=66954)[0m mae:  0.13220231235027313
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.11, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.17906221747398376
[2m[36m(func pid=67327)[0m mae:  0.13149355351924896
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.262, 0.098, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.16776886582374573
[2m[36m(func pid=67743)[0m mae:  0.12226873636245728
[2m[36m(func pid=67743)[0m rmse_per_class: [0.121, 0.255, 0.072, 0.315, 0.065, 0.187, 0.275, 0.139, 0.144, 0.105]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5704 | Steps: 2 | Val loss: 0.3935 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8668 | Steps: 2 | Val loss: 0.6755 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.6473 | Steps: 2 | Val loss: 0.4879 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4611 | Steps: 2 | Val loss: 0.3718 | Batch size: 32 | lr: 0.01 | Duration: 2.63s
[2m[36m(func pid=68170)[0m rmse: 0.16709034144878387
[2m[36m(func pid=68170)[0m mae:  0.10303697735071182
[2m[36m(func pid=68170)[0m rmse_per_class: [0.069, 0.245, 0.047, 0.295, 0.056, 0.192, 0.423, 0.131, 0.131, 0.082]
[2m[36m(func pid=68170)[0m 
== Status ==
Current time: 2024-01-07 06:53:19 (running for 00:01:51.13)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.872 |  0.18  |                   11 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.647 |  0.179 |                   12 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.436 |  0.168 |                   11 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.57  |  0.167 |                   10 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.1798180341720581
[2m[36m(func pid=66954)[0m mae:  0.13209643959999084
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.336, 0.11, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.17870989441871643
[2m[36m(func pid=67327)[0m mae:  0.13118384778499603
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.261, 0.097, 0.335, 0.105, 0.19, 0.292, 0.141, 0.141, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.1655220240354538
[2m[36m(func pid=67743)[0m mae:  0.1202651709318161
[2m[36m(func pid=67743)[0m rmse_per_class: [0.12, 0.252, 0.067, 0.311, 0.061, 0.187, 0.271, 0.137, 0.144, 0.104]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.6183 | Steps: 2 | Val loss: 0.4636 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5256 | Steps: 2 | Val loss: 0.3355 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8604 | Steps: 2 | Val loss: 0.6716 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4825 | Steps: 2 | Val loss: 0.3921 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 06:53:24 (running for 00:01:56.22)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.867 |  0.18  |                   12 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.618 |  0.178 |                   13 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.461 |  0.166 |                   12 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.57  |  0.167 |                   10 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.1783508062362671
[2m[36m(func pid=67327)[0m mae:  0.1308945119380951
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.261, 0.096, 0.335, 0.103, 0.19, 0.291, 0.141, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17973384261131287
[2m[36m(func pid=66954)[0m mae:  0.13201656937599182
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.336, 0.11, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.1646656095981598
[2m[36m(func pid=68170)[0m mae:  0.10051985085010529
[2m[36m(func pid=68170)[0m rmse_per_class: [0.07, 0.247, 0.047, 0.295, 0.056, 0.185, 0.404, 0.129, 0.13, 0.082]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.16328510642051697
[2m[36m(func pid=67743)[0m mae:  0.1182364970445633
[2m[36m(func pid=67743)[0m rmse_per_class: [0.119, 0.25, 0.063, 0.307, 0.059, 0.187, 0.267, 0.136, 0.144, 0.102]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5897 | Steps: 2 | Val loss: 0.4407 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8580 | Steps: 2 | Val loss: 0.6674 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4605 | Steps: 2 | Val loss: 0.2999 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5060 | Steps: 2 | Val loss: 0.4118 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 06:53:29 (running for 00:02:01.38)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.86  |  0.18  |                   13 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.59  |  0.178 |                   14 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.482 |  0.163 |                   13 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.526 |  0.165 |                   11 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.17797446250915527
[2m[36m(func pid=67327)[0m mae:  0.13057547807693481
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.334, 0.102, 0.19, 0.291, 0.141, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17960456013679504
[2m[36m(func pid=66954)[0m mae:  0.13190411031246185
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.16109104454517365
[2m[36m(func pid=67743)[0m mae:  0.11618117243051529
[2m[36m(func pid=67743)[0m rmse_per_class: [0.117, 0.247, 0.059, 0.303, 0.057, 0.186, 0.263, 0.134, 0.143, 0.101]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.15301375091075897
[2m[36m(func pid=68170)[0m mae:  0.09262766689062119
[2m[36m(func pid=68170)[0m rmse_per_class: [0.075, 0.246, 0.047, 0.283, 0.056, 0.169, 0.32, 0.122, 0.131, 0.08]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8488 | Steps: 2 | Val loss: 0.6617 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5592 | Steps: 2 | Val loss: 0.4196 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5297 | Steps: 2 | Val loss: 0.4291 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3989 | Steps: 2 | Val loss: 0.3034 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 06:53:35 (running for 00:02:06.58)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.849 |  0.18  |                   15 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.59  |  0.178 |                   14 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.506 |  0.161 |                   14 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.461 |  0.153 |                   12 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.1795485019683838
[2m[36m(func pid=66954)[0m mae:  0.1318524181842804
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.17771008610725403
[2m[36m(func pid=67327)[0m mae:  0.13036860525608063
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.334, 0.1, 0.19, 0.291, 0.141, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.15884409844875336
[2m[36m(func pid=67743)[0m mae:  0.11401806026697159
[2m[36m(func pid=67743)[0m rmse_per_class: [0.116, 0.245, 0.055, 0.299, 0.056, 0.186, 0.258, 0.132, 0.143, 0.098]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.14171744883060455
[2m[36m(func pid=68170)[0m mae:  0.08771426975727081
[2m[36m(func pid=68170)[0m rmse_per_class: [0.086, 0.241, 0.045, 0.273, 0.056, 0.154, 0.227, 0.116, 0.136, 0.082]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8424 | Steps: 2 | Val loss: 0.6571 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5398 | Steps: 2 | Val loss: 0.4000 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5463 | Steps: 2 | Val loss: 0.4435 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4014 | Steps: 2 | Val loss: 0.3511 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 06:53:40 (running for 00:02:11.81)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.849 |  0.18  |                   15 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.54  |  0.177 |                   16 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.53  |  0.159 |                   15 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.399 |  0.142 |                   13 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17947573959827423
[2m[36m(func pid=66954)[0m mae:  0.13178476691246033
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.17726744711399078
[2m[36m(func pid=67327)[0m mae:  0.13000541925430298
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.259, 0.093, 0.333, 0.099, 0.19, 0.29, 0.141, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.15660452842712402
[2m[36m(func pid=67743)[0m mae:  0.11181329190731049
[2m[36m(func pid=67743)[0m rmse_per_class: [0.114, 0.243, 0.052, 0.296, 0.055, 0.185, 0.253, 0.13, 0.142, 0.096]
[2m[36m(func pid=68170)[0m rmse: 0.14302369952201843
[2m[36m(func pid=68170)[0m mae:  0.09192570298910141
[2m[36m(func pid=68170)[0m rmse_per_class: [0.103, 0.237, 0.043, 0.269, 0.056, 0.151, 0.217, 0.113, 0.152, 0.089]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8354 | Steps: 2 | Val loss: 0.6518 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5103 | Steps: 2 | Val loss: 0.3825 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4684 | Steps: 2 | Val loss: 0.4159 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5614 | Steps: 2 | Val loss: 0.4563 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 06:53:45 (running for 00:02:16.87)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.835 |  0.179 |                   17 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.54  |  0.177 |                   16 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.546 |  0.157 |                   16 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.401 |  0.143 |                   14 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.1794150173664093
[2m[36m(func pid=66954)[0m mae:  0.13172194361686707
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.1767956018447876
[2m[36m(func pid=67327)[0m mae:  0.12962402403354645
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.333, 0.097, 0.19, 0.29, 0.141, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.15438759326934814
[2m[36m(func pid=67743)[0m mae:  0.10949263721704483
[2m[36m(func pid=67743)[0m rmse_per_class: [0.111, 0.241, 0.049, 0.291, 0.055, 0.185, 0.248, 0.129, 0.141, 0.094]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.1513480842113495
[2m[36m(func pid=68170)[0m mae:  0.0996100902557373
[2m[36m(func pid=68170)[0m rmse_per_class: [0.117, 0.237, 0.038, 0.271, 0.056, 0.154, 0.263, 0.115, 0.166, 0.097]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4898 | Steps: 2 | Val loss: 0.3672 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8321 | Steps: 2 | Val loss: 0.6457 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.5771 | Steps: 2 | Val loss: 0.4661 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5102 | Steps: 2 | Val loss: 0.4420 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 06:53:50 (running for 00:02:22.08)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.832 |  0.179 |                   18 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.51  |  0.177 |                   17 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.561 |  0.154 |                   17 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.468 |  0.151 |                   15 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17938454449176788
[2m[36m(func pid=66954)[0m mae:  0.13169702887535095
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.1764102578163147
[2m[36m(func pid=67327)[0m mae:  0.12931019067764282
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.258, 0.091, 0.332, 0.095, 0.19, 0.29, 0.141, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.15231990814208984
[2m[36m(func pid=67743)[0m mae:  0.10720013082027435
[2m[36m(func pid=67743)[0m rmse_per_class: [0.109, 0.239, 0.047, 0.288, 0.055, 0.184, 0.243, 0.127, 0.139, 0.092]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.15740276873111725
[2m[36m(func pid=68170)[0m mae:  0.10377577692270279
[2m[36m(func pid=68170)[0m rmse_per_class: [0.12, 0.24, 0.033, 0.273, 0.056, 0.159, 0.299, 0.121, 0.162, 0.11]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4748 | Steps: 2 | Val loss: 0.3534 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8233 | Steps: 2 | Val loss: 0.6403 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.5891 | Steps: 2 | Val loss: 0.4733 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5049 | Steps: 2 | Val loss: 0.4243 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 06:53:55 (running for 00:02:27.24)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.832 |  0.179 |                   18 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.475 |  0.176 |                   19 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.577 |  0.152 |                   18 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.51  |  0.157 |                   16 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.1759250909090042
[2m[36m(func pid=67327)[0m mae:  0.1289229691028595
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.258, 0.09, 0.332, 0.093, 0.189, 0.289, 0.141, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17934919893741608
[2m[36m(func pid=66954)[0m mae:  0.13167616724967957
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.15058884024620056
[2m[36m(func pid=67743)[0m mae:  0.10517841577529907
[2m[36m(func pid=67743)[0m rmse_per_class: [0.106, 0.236, 0.044, 0.286, 0.055, 0.185, 0.239, 0.126, 0.138, 0.091]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.16355061531066895
[2m[36m(func pid=68170)[0m mae:  0.10514938831329346
[2m[36m(func pid=68170)[0m rmse_per_class: [0.109, 0.252, 0.027, 0.276, 0.056, 0.167, 0.32, 0.133, 0.15, 0.146]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4568 | Steps: 2 | Val loss: 0.3426 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8145 | Steps: 2 | Val loss: 0.6342 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6009 | Steps: 2 | Val loss: 0.4771 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4791 | Steps: 2 | Val loss: 0.3993 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 06:54:00 (running for 00:02:32.25)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.823 |  0.179 |                   19 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.457 |  0.176 |                   20 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.589 |  0.151 |                   19 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.505 |  0.164 |                   17 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.1755913943052292
[2m[36m(func pid=67327)[0m mae:  0.12865303456783295
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.257, 0.089, 0.331, 0.091, 0.189, 0.289, 0.141, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17936381697654724
[2m[36m(func pid=66954)[0m mae:  0.1316855102777481
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.14883732795715332
[2m[36m(func pid=67743)[0m mae:  0.10291286557912827
[2m[36m(func pid=67743)[0m rmse_per_class: [0.103, 0.234, 0.043, 0.283, 0.055, 0.185, 0.236, 0.124, 0.137, 0.089]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.1745545119047165
[2m[36m(func pid=68170)[0m mae:  0.10756120830774307
[2m[36m(func pid=68170)[0m rmse_per_class: [0.098, 0.274, 0.025, 0.289, 0.056, 0.173, 0.328, 0.148, 0.137, 0.217]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4457 | Steps: 2 | Val loss: 0.3339 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8093 | Steps: 2 | Val loss: 0.6278 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6018 | Steps: 2 | Val loss: 0.4786 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4393 | Steps: 2 | Val loss: 0.3973 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 06:54:05 (running for 00:02:37.32)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.814 |  0.179 |                   20 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.446 |  0.175 |                   21 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.601 |  0.149 |                   20 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.479 |  0.175 |                   18 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.17526480555534363
[2m[36m(func pid=67327)[0m mae:  0.1283722221851349
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.257, 0.088, 0.33, 0.09, 0.189, 0.288, 0.141, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.1792997419834137
[2m[36m(func pid=66954)[0m mae:  0.13163244724273682
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.14727309346199036
[2m[36m(func pid=67743)[0m mae:  0.10078974068164825
[2m[36m(func pid=67743)[0m rmse_per_class: [0.1, 0.232, 0.042, 0.28, 0.055, 0.185, 0.233, 0.123, 0.135, 0.087]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.18973110616207123
[2m[36m(func pid=68170)[0m mae:  0.1129191666841507
[2m[36m(func pid=68170)[0m rmse_per_class: [0.097, 0.299, 0.029, 0.309, 0.056, 0.175, 0.325, 0.168, 0.131, 0.306]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4315 | Steps: 2 | Val loss: 0.3266 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8001 | Steps: 2 | Val loss: 0.6218 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6075 | Steps: 2 | Val loss: 0.4762 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4120 | Steps: 2 | Val loss: 0.4178 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=67327)[0m rmse: 0.17478780448436737
[2m[36m(func pid=67327)[0m mae:  0.12798085808753967
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.257, 0.088, 0.33, 0.088, 0.189, 0.288, 0.141, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
== Status ==
Current time: 2024-01-07 06:54:11 (running for 00:02:42.84)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.8   |  0.179 |                   22 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.432 |  0.175 |                   22 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.602 |  0.147 |                   21 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.439 |  0.19  |                   19 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.1792704463005066
[2m[36m(func pid=66954)[0m mae:  0.13160905241966248
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.14615410566329956
[2m[36m(func pid=67743)[0m mae:  0.09888913482427597
[2m[36m(func pid=67743)[0m rmse_per_class: [0.097, 0.231, 0.041, 0.277, 0.055, 0.185, 0.231, 0.122, 0.134, 0.086]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.19992952048778534
[2m[36m(func pid=68170)[0m mae:  0.11713337898254395
[2m[36m(func pid=68170)[0m rmse_per_class: [0.095, 0.317, 0.035, 0.326, 0.056, 0.176, 0.308, 0.175, 0.131, 0.379]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4259 | Steps: 2 | Val loss: 0.3211 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.7913 | Steps: 2 | Val loss: 0.6151 | Batch size: 32 | lr: 0.0001 | Duration: 2.59s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5982 | Steps: 2 | Val loss: 0.4721 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3998 | Steps: 2 | Val loss: 0.4466 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=67327)[0m rmse: 0.17437967658042908
[2m[36m(func pid=67327)[0m mae:  0.12766213715076447
[2m[36m(func pid=67327)[0m rmse_per_class: [0.118, 0.256, 0.087, 0.329, 0.086, 0.189, 0.287, 0.14, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.1791871041059494
[2m[36m(func pid=66954)[0m mae:  0.1315380036830902
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.14535662531852722
[2m[36m(func pid=67743)[0m mae:  0.09722118079662323
[2m[36m(func pid=67743)[0m rmse_per_class: [0.094, 0.23, 0.041, 0.276, 0.056, 0.186, 0.232, 0.122, 0.133, 0.085]
== Status ==
Current time: 2024-01-07 06:54:16 (running for 00:02:48.44)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.791 |  0.179 |                   23 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.426 |  0.174 |                   23 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.598 |  0.145 |                   23 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.412 |  0.2   |                   20 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.2005082070827484
[2m[36m(func pid=68170)[0m mae:  0.11564779281616211
[2m[36m(func pid=68170)[0m rmse_per_class: [0.093, 0.328, 0.041, 0.331, 0.057, 0.176, 0.27, 0.165, 0.132, 0.41]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4147 | Steps: 2 | Val loss: 0.3169 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7830 | Steps: 2 | Val loss: 0.6088 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5880 | Steps: 2 | Val loss: 0.4644 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=66954)[0m rmse: 0.17916342616081238
[2m[36m(func pid=66954)[0m mae:  0.13152287900447845
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.17390312254428864
[2m[36m(func pid=67327)[0m mae:  0.12727780640125275
[2m[36m(func pid=67327)[0m rmse_per_class: [0.118, 0.256, 0.085, 0.328, 0.085, 0.189, 0.287, 0.14, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4308 | Steps: 2 | Val loss: 0.4742 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 06:54:22 (running for 00:02:53.94)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.783 |  0.179 |                   24 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.415 |  0.174 |                   24 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.588 |  0.145 |                   24 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.4   |  0.201 |                   21 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.14479395747184753
[2m[36m(func pid=67743)[0m mae:  0.09566138684749603
[2m[36m(func pid=67743)[0m rmse_per_class: [0.091, 0.229, 0.041, 0.273, 0.056, 0.186, 0.234, 0.122, 0.132, 0.084]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4075 | Steps: 2 | Val loss: 0.3138 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7745 | Steps: 2 | Val loss: 0.6021 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=68170)[0m rmse: 0.1951012909412384
[2m[36m(func pid=68170)[0m mae:  0.11004717648029327
[2m[36m(func pid=68170)[0m rmse_per_class: [0.09, 0.331, 0.045, 0.328, 0.065, 0.178, 0.234, 0.137, 0.135, 0.409]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5766 | Steps: 2 | Val loss: 0.4535 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=67327)[0m rmse: 0.17339065670967102
[2m[36m(func pid=67327)[0m mae:  0.12686887383460999
[2m[36m(func pid=67327)[0m rmse_per_class: [0.118, 0.255, 0.084, 0.327, 0.083, 0.189, 0.286, 0.14, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17909598350524902
[2m[36m(func pid=66954)[0m mae:  0.13145892322063446
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4374 | Steps: 2 | Val loss: 0.4907 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 06:54:27 (running for 00:02:59.12)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.775 |  0.179 |                   25 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.408 |  0.173 |                   25 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.577 |  0.145 |                   25 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.431 |  0.195 |                   22 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.144504576921463
[2m[36m(func pid=67743)[0m mae:  0.09434331953525543
[2m[36m(func pid=67743)[0m rmse_per_class: [0.088, 0.228, 0.041, 0.271, 0.056, 0.187, 0.237, 0.122, 0.131, 0.084]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7699 | Steps: 2 | Val loss: 0.5957 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4055 | Steps: 2 | Val loss: 0.3123 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=68170)[0m rmse: 0.19796466827392578
[2m[36m(func pid=68170)[0m mae:  0.10942582786083221
[2m[36m(func pid=68170)[0m rmse_per_class: [0.086, 0.328, 0.045, 0.326, 0.093, 0.19, 0.289, 0.122, 0.138, 0.364]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5719 | Steps: 2 | Val loss: 0.4412 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=67327)[0m rmse: 0.17300480604171753
[2m[36m(func pid=67327)[0m mae:  0.12655755877494812
[2m[36m(func pid=67327)[0m rmse_per_class: [0.118, 0.255, 0.083, 0.327, 0.081, 0.189, 0.286, 0.14, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17910239100456238
[2m[36m(func pid=66954)[0m mae:  0.13146312534809113
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
== Status ==
Current time: 2024-01-07 06:54:32 (running for 00:03:04.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.77  |  0.179 |                   26 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.406 |  0.173 |                   26 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.572 |  0.145 |                   26 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.437 |  0.198 |                   23 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.1446608304977417
[2m[36m(func pid=67743)[0m mae:  0.09329893440008163
[2m[36m(func pid=67743)[0m rmse_per_class: [0.085, 0.228, 0.041, 0.27, 0.056, 0.189, 0.242, 0.122, 0.131, 0.083]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4462 | Steps: 2 | Val loss: 0.5017 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7601 | Steps: 2 | Val loss: 0.5885 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4027 | Steps: 2 | Val loss: 0.3116 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=68170)[0m rmse: 0.2080352008342743
[2m[36m(func pid=68170)[0m mae:  0.11583338677883148
[2m[36m(func pid=68170)[0m rmse_per_class: [0.086, 0.319, 0.041, 0.334, 0.119, 0.206, 0.404, 0.133, 0.139, 0.299]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5459 | Steps: 2 | Val loss: 0.4275 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=66954)[0m rmse: 0.1790492832660675
[2m[36m(func pid=66954)[0m mae:  0.13142839074134827
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.17254363000392914
[2m[36m(func pid=67327)[0m mae:  0.12617240846157074
[2m[36m(func pid=67327)[0m rmse_per_class: [0.118, 0.255, 0.083, 0.326, 0.08, 0.188, 0.285, 0.14, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
== Status ==
Current time: 2024-01-07 06:54:37 (running for 00:03:09.33)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.76  |  0.179 |                   27 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.403 |  0.173 |                   27 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.546 |  0.145 |                   27 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.446 |  0.208 |                   24 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.14518091082572937
[2m[36m(func pid=67743)[0m mae:  0.09269638359546661
[2m[36m(func pid=67743)[0m rmse_per_class: [0.083, 0.229, 0.042, 0.27, 0.056, 0.19, 0.247, 0.124, 0.13, 0.082]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4688 | Steps: 2 | Val loss: 0.5100 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.7545 | Steps: 2 | Val loss: 0.5821 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4057 | Steps: 2 | Val loss: 0.3120 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=68170)[0m rmse: 0.21376121044158936
[2m[36m(func pid=68170)[0m mae:  0.12177477777004242
[2m[36m(func pid=68170)[0m rmse_per_class: [0.084, 0.312, 0.035, 0.343, 0.137, 0.216, 0.488, 0.143, 0.139, 0.241]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5292 | Steps: 2 | Val loss: 0.4117 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=66954)[0m rmse: 0.1790471374988556
[2m[36m(func pid=66954)[0m mae:  0.13142773509025574
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.17210735380649567
[2m[36m(func pid=67327)[0m mae:  0.125820592045784
[2m[36m(func pid=67327)[0m rmse_per_class: [0.118, 0.254, 0.082, 0.325, 0.078, 0.188, 0.284, 0.14, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
== Status ==
Current time: 2024-01-07 06:54:42 (running for 00:03:14.36)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.755 |  0.179 |                   28 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.406 |  0.172 |                   28 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.529 |  0.146 |                   28 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.469 |  0.214 |                   25 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.14600622653961182
[2m[36m(func pid=67743)[0m mae:  0.09230908751487732
[2m[36m(func pid=67743)[0m rmse_per_class: [0.081, 0.23, 0.042, 0.27, 0.056, 0.19, 0.254, 0.125, 0.13, 0.082]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4785 | Steps: 2 | Val loss: 0.5149 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7432 | Steps: 2 | Val loss: 0.5750 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4038 | Steps: 2 | Val loss: 0.3128 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5094 | Steps: 2 | Val loss: 0.3936 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=68170)[0m rmse: 0.21496471762657166
[2m[36m(func pid=68170)[0m mae:  0.12465300410985947
[2m[36m(func pid=68170)[0m rmse_per_class: [0.082, 0.306, 0.03, 0.347, 0.153, 0.22, 0.525, 0.147, 0.139, 0.201]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17897407710552216
[2m[36m(func pid=66954)[0m mae:  0.1313621997833252
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.17166249454021454
[2m[36m(func pid=67327)[0m mae:  0.12547074258327484
[2m[36m(func pid=67327)[0m rmse_per_class: [0.118, 0.254, 0.081, 0.325, 0.077, 0.188, 0.284, 0.14, 0.142, 0.109]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.14679846167564392
[2m[36m(func pid=67743)[0m mae:  0.09199146926403046
[2m[36m(func pid=67743)[0m rmse_per_class: [0.079, 0.231, 0.042, 0.27, 0.056, 0.191, 0.262, 0.125, 0.13, 0.082]
== Status ==
Current time: 2024-01-07 06:54:47 (running for 00:03:19.50)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.743 |  0.179 |                   29 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.404 |  0.172 |                   29 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.509 |  0.147 |                   29 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.479 |  0.215 |                   26 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4555 | Steps: 2 | Val loss: 0.5078 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.7357 | Steps: 2 | Val loss: 0.5679 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4040 | Steps: 2 | Val loss: 0.3147 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4981 | Steps: 2 | Val loss: 0.3736 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=68170)[0m rmse: 0.21245649456977844
[2m[36m(func pid=68170)[0m mae:  0.12393133342266083
[2m[36m(func pid=68170)[0m rmse_per_class: [0.081, 0.303, 0.027, 0.346, 0.171, 0.218, 0.52, 0.148, 0.14, 0.17]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.17111077904701233
[2m[36m(func pid=67327)[0m mae:  0.12501171231269836
[2m[36m(func pid=67327)[0m rmse_per_class: [0.118, 0.253, 0.079, 0.324, 0.075, 0.188, 0.283, 0.14, 0.142, 0.108]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17896147072315216
[2m[36m(func pid=66954)[0m mae:  0.13134858012199402
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
== Status ==
Current time: 2024-01-07 06:54:53 (running for 00:03:24.64)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.736 |  0.179 |                   30 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.404 |  0.171 |                   30 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.498 |  0.147 |                   30 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.456 |  0.212 |                   27 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.14731334149837494
[2m[36m(func pid=67743)[0m mae:  0.09160147607326508
[2m[36m(func pid=67743)[0m rmse_per_class: [0.077, 0.233, 0.043, 0.269, 0.056, 0.191, 0.266, 0.126, 0.13, 0.082]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4835 | Steps: 2 | Val loss: 0.4862 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.7224 | Steps: 2 | Val loss: 0.5612 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4072 | Steps: 2 | Val loss: 0.3166 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4817 | Steps: 2 | Val loss: 0.3549 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=68170)[0m rmse: 0.20726923644542694
[2m[36m(func pid=68170)[0m mae:  0.1191600114107132
[2m[36m(func pid=68170)[0m rmse_per_class: [0.081, 0.303, 0.027, 0.341, 0.183, 0.212, 0.471, 0.147, 0.15, 0.158]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17892087996006012
[2m[36m(func pid=66954)[0m mae:  0.13131511211395264
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.17052647471427917
[2m[36m(func pid=67327)[0m mae:  0.12452977895736694
[2m[36m(func pid=67327)[0m rmse_per_class: [0.118, 0.253, 0.078, 0.323, 0.074, 0.188, 0.282, 0.14, 0.142, 0.108]
[2m[36m(func pid=67327)[0m 
== Status ==
Current time: 2024-01-07 06:54:58 (running for 00:03:29.76)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.722 |  0.179 |                   31 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.407 |  0.171 |                   31 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.482 |  0.148 |                   31 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.484 |  0.207 |                   28 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.1479869782924652
[2m[36m(func pid=67743)[0m mae:  0.09162306785583496
[2m[36m(func pid=67743)[0m rmse_per_class: [0.077, 0.235, 0.042, 0.271, 0.056, 0.191, 0.268, 0.127, 0.13, 0.082]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4159 | Steps: 2 | Val loss: 0.4575 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4075 | Steps: 2 | Val loss: 0.3195 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7193 | Steps: 2 | Val loss: 0.5551 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4546 | Steps: 2 | Val loss: 0.3364 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=68170)[0m rmse: 0.20013058185577393
[2m[36m(func pid=68170)[0m mae:  0.11231458187103271
[2m[36m(func pid=68170)[0m rmse_per_class: [0.08, 0.311, 0.028, 0.337, 0.2, 0.197, 0.389, 0.144, 0.163, 0.154]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.16998228430747986
[2m[36m(func pid=67327)[0m mae:  0.1240822821855545
[2m[36m(func pid=67327)[0m rmse_per_class: [0.118, 0.252, 0.077, 0.322, 0.072, 0.187, 0.281, 0.139, 0.142, 0.108]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.1788691133260727
[2m[36m(func pid=66954)[0m mae:  0.13127264380455017
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.107, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
== Status ==
Current time: 2024-01-07 06:55:03 (running for 00:03:35.00)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.719 |  0.179 |                   32 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.407 |  0.17  |                   32 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.455 |  0.148 |                   32 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.416 |  0.2   |                   29 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.1484556645154953
[2m[36m(func pid=67743)[0m mae:  0.09141390025615692
[2m[36m(func pid=67743)[0m rmse_per_class: [0.077, 0.238, 0.043, 0.272, 0.056, 0.19, 0.27, 0.127, 0.13, 0.082]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4104 | Steps: 2 | Val loss: 0.3227 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4102 | Steps: 2 | Val loss: 0.4366 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7063 | Steps: 2 | Val loss: 0.5479 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4391 | Steps: 2 | Val loss: 0.3191 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=67327)[0m rmse: 0.16949598491191864
[2m[36m(func pid=67327)[0m mae:  0.12368641793727875
[2m[36m(func pid=67327)[0m rmse_per_class: [0.118, 0.251, 0.076, 0.322, 0.071, 0.187, 0.281, 0.139, 0.142, 0.108]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17880398035049438
[2m[36m(func pid=66954)[0m mae:  0.1312156617641449
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.335, 0.107, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.19198240339756012
[2m[36m(func pid=68170)[0m mae:  0.1061616912484169
[2m[36m(func pid=68170)[0m rmse_per_class: [0.081, 0.327, 0.03, 0.336, 0.204, 0.18, 0.295, 0.136, 0.186, 0.146]
[2m[36m(func pid=68170)[0m 
== Status ==
Current time: 2024-01-07 06:55:08 (running for 00:03:40.20)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.706 |  0.179 |                   33 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.41  |  0.169 |                   33 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.439 |  0.149 |                   33 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.41  |  0.192 |                   30 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.14856229722499847
[2m[36m(func pid=67743)[0m mae:  0.09116813540458679
[2m[36m(func pid=67743)[0m rmse_per_class: [0.076, 0.24, 0.042, 0.272, 0.056, 0.189, 0.269, 0.127, 0.13, 0.083]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4111 | Steps: 2 | Val loss: 0.3259 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.7023 | Steps: 2 | Val loss: 0.5414 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3853 | Steps: 2 | Val loss: 0.4326 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4243 | Steps: 2 | Val loss: 0.3042 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=67327)[0m rmse: 0.16899268329143524
[2m[36m(func pid=67327)[0m mae:  0.12327925860881805
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.251, 0.075, 0.321, 0.07, 0.187, 0.28, 0.138, 0.142, 0.108]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17875675857067108
[2m[36m(func pid=66954)[0m mae:  0.13117843866348267
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.335, 0.107, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.18770883977413177
[2m[36m(func pid=68170)[0m mae:  0.10519661754369736
[2m[36m(func pid=68170)[0m rmse_per_class: [0.082, 0.339, 0.031, 0.343, 0.185, 0.183, 0.243, 0.131, 0.204, 0.135]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.1482752114534378
[2m[36m(func pid=67743)[0m mae:  0.09089082479476929
[2m[36m(func pid=67743)[0m rmse_per_class: [0.076, 0.242, 0.042, 0.273, 0.056, 0.187, 0.267, 0.126, 0.129, 0.083]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4164 | Steps: 2 | Val loss: 0.3293 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3921 | Steps: 2 | Val loss: 0.4430 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.6901 | Steps: 2 | Val loss: 0.5343 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3824 | Steps: 2 | Val loss: 0.2921 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 06:55:17 (running for 00:03:48.97)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.702 |  0.179 |                   34 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.416 |  0.168 |                   35 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.424 |  0.148 |                   34 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.385 |  0.188 |                   31 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.1684575378894806
[2m[36m(func pid=67327)[0m mae:  0.12282492965459824
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.25, 0.074, 0.321, 0.069, 0.187, 0.279, 0.138, 0.142, 0.108]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.1787024438381195
[2m[36m(func pid=66954)[0m mae:  0.1311427801847458
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.336, 0.107, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.1892862617969513
[2m[36m(func pid=68170)[0m mae:  0.10901707410812378
[2m[36m(func pid=68170)[0m rmse_per_class: [0.081, 0.343, 0.028, 0.346, 0.157, 0.203, 0.265, 0.143, 0.206, 0.121]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.147523432970047
[2m[36m(func pid=67743)[0m mae:  0.09048304706811905
[2m[36m(func pid=67743)[0m rmse_per_class: [0.078, 0.245, 0.042, 0.274, 0.056, 0.183, 0.259, 0.126, 0.129, 0.083]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4161 | Steps: 2 | Val loss: 0.3330 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6840 | Steps: 2 | Val loss: 0.5271 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3949 | Steps: 2 | Val loss: 0.4632 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3768 | Steps: 2 | Val loss: 0.2832 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 06:55:22 (running for 00:03:54.22)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.69  |  0.179 |                   35 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.416 |  0.168 |                   36 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.382 |  0.148 |                   35 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.392 |  0.189 |                   32 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.16797176003456116
[2m[36m(func pid=67327)[0m mae:  0.12241210788488388
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.25, 0.073, 0.32, 0.068, 0.187, 0.278, 0.138, 0.142, 0.107]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.1786612570285797
[2m[36m(func pid=66954)[0m mae:  0.13109815120697021
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.335, 0.107, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.18874052166938782
[2m[36m(func pid=68170)[0m mae:  0.11117682605981827
[2m[36m(func pid=68170)[0m rmse_per_class: [0.081, 0.334, 0.026, 0.339, 0.128, 0.214, 0.297, 0.162, 0.196, 0.111]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.1460854709148407
[2m[36m(func pid=67743)[0m mae:  0.08980688452720642
[2m[36m(func pid=67743)[0m rmse_per_class: [0.079, 0.245, 0.042, 0.273, 0.056, 0.179, 0.249, 0.124, 0.129, 0.083]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4248 | Steps: 2 | Val loss: 0.3362 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6759 | Steps: 2 | Val loss: 0.5201 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4392 | Steps: 2 | Val loss: 0.4847 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3587 | Steps: 2 | Val loss: 0.2789 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 06:55:27 (running for 00:03:59.45)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.684 |  0.179 |                   36 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.425 |  0.167 |                   37 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.377 |  0.146 |                   36 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.395 |  0.189 |                   33 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.16748854517936707
[2m[36m(func pid=67327)[0m mae:  0.12199579179286957
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.249, 0.072, 0.319, 0.067, 0.187, 0.278, 0.138, 0.142, 0.107]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17855137586593628
[2m[36m(func pid=66954)[0m mae:  0.13100910186767578
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.18671979010105133
[2m[36m(func pid=68170)[0m mae:  0.11180680990219116
[2m[36m(func pid=68170)[0m rmse_per_class: [0.084, 0.32, 0.029, 0.332, 0.105, 0.214, 0.313, 0.18, 0.186, 0.105]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.1446196287870407
[2m[36m(func pid=67743)[0m mae:  0.08921881020069122
[2m[36m(func pid=67743)[0m rmse_per_class: [0.081, 0.244, 0.042, 0.272, 0.056, 0.174, 0.238, 0.123, 0.129, 0.085]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4269 | Steps: 2 | Val loss: 0.3395 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6651 | Steps: 2 | Val loss: 0.5142 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4502 | Steps: 2 | Val loss: 0.4969 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3646 | Steps: 2 | Val loss: 0.2789 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 06:55:32 (running for 00:04:04.47)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.676 |  0.179 |                   37 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.427 |  0.167 |                   38 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.359 |  0.145 |                   37 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.439 |  0.187 |                   34 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.16702833771705627
[2m[36m(func pid=67327)[0m mae:  0.12161072343587875
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.249, 0.071, 0.318, 0.066, 0.186, 0.277, 0.137, 0.142, 0.107]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17853885889053345
[2m[36m(func pid=66954)[0m mae:  0.13099613785743713
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.26, 0.096, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.14320547878742218
[2m[36m(func pid=67743)[0m mae:  0.08889299631118774
[2m[36m(func pid=67743)[0m rmse_per_class: [0.084, 0.245, 0.042, 0.272, 0.056, 0.169, 0.227, 0.121, 0.129, 0.086]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.1855413317680359
[2m[36m(func pid=68170)[0m mae:  0.1117527037858963
[2m[36m(func pid=68170)[0m rmse_per_class: [0.094, 0.309, 0.033, 0.327, 0.091, 0.211, 0.318, 0.199, 0.173, 0.101]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4295 | Steps: 2 | Val loss: 0.3431 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6596 | Steps: 2 | Val loss: 0.5076 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3648 | Steps: 2 | Val loss: 0.2822 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4970 | Steps: 2 | Val loss: 0.4889 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 06:55:38 (running for 00:04:09.66)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.665 |  0.179 |                   38 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.429 |  0.166 |                   39 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.365 |  0.143 |                   38 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.45  |  0.186 |                   35 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.16642004251480103
[2m[36m(func pid=67327)[0m mae:  0.1210893839597702
[2m[36m(func pid=67327)[0m rmse_per_class: [0.117, 0.249, 0.069, 0.318, 0.065, 0.186, 0.275, 0.137, 0.142, 0.106]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17847350239753723
[2m[36m(func pid=66954)[0m mae:  0.13094627857208252
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.14182749390602112
[2m[36m(func pid=67743)[0m mae:  0.08867917954921722
[2m[36m(func pid=67743)[0m rmse_per_class: [0.086, 0.243, 0.041, 0.271, 0.056, 0.164, 0.218, 0.119, 0.129, 0.089]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.18592897057533264
[2m[36m(func pid=68170)[0m mae:  0.11124573647975922
[2m[36m(func pid=68170)[0m rmse_per_class: [0.11, 0.302, 0.038, 0.323, 0.081, 0.204, 0.311, 0.221, 0.166, 0.104]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4326 | Steps: 2 | Val loss: 0.3466 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6509 | Steps: 2 | Val loss: 0.5008 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3809 | Steps: 2 | Val loss: 0.2891 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4917 | Steps: 2 | Val loss: 0.4679 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 06:55:43 (running for 00:04:14.79)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.66  |  0.178 |                   39 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.433 |  0.166 |                   40 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.365 |  0.142 |                   39 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.497 |  0.186 |                   36 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.16587989032268524
[2m[36m(func pid=67327)[0m mae:  0.12062231451272964
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.248, 0.068, 0.317, 0.064, 0.186, 0.275, 0.137, 0.142, 0.106]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17840442061424255
[2m[36m(func pid=66954)[0m mae:  0.13088728487491608
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.14112719893455505
[2m[36m(func pid=67743)[0m mae:  0.08911280333995819
[2m[36m(func pid=67743)[0m rmse_per_class: [0.092, 0.243, 0.04, 0.271, 0.056, 0.16, 0.212, 0.118, 0.13, 0.09]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.1863899677991867
[2m[36m(func pid=68170)[0m mae:  0.11003459990024567
[2m[36m(func pid=68170)[0m rmse_per_class: [0.126, 0.302, 0.04, 0.324, 0.075, 0.192, 0.293, 0.235, 0.166, 0.11]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4430 | Steps: 2 | Val loss: 0.3500 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6431 | Steps: 2 | Val loss: 0.4942 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3668 | Steps: 2 | Val loss: 0.2978 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=67327)[0m rmse: 0.16537262499332428
[2m[36m(func pid=67327)[0m mae:  0.12018822133541107
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.247, 0.067, 0.316, 0.063, 0.186, 0.274, 0.136, 0.142, 0.106]
== Status ==
Current time: 2024-01-07 06:55:48 (running for 00:04:19.84)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.651 |  0.178 |                   40 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.443 |  0.165 |                   41 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.381 |  0.141 |                   40 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.492 |  0.186 |                   37 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4226 | Steps: 2 | Val loss: 0.4470 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=66954)[0m rmse: 0.17835767567157745
[2m[36m(func pid=66954)[0m mae:  0.13085073232650757
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.105, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.14066405594348907
[2m[36m(func pid=67743)[0m mae:  0.08960702270269394
[2m[36m(func pid=67743)[0m rmse_per_class: [0.095, 0.241, 0.039, 0.27, 0.056, 0.156, 0.21, 0.117, 0.131, 0.092]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4432 | Steps: 2 | Val loss: 0.3533 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=68170)[0m rmse: 0.18509432673454285
[2m[36m(func pid=68170)[0m mae:  0.10715289413928986
[2m[36m(func pid=68170)[0m rmse_per_class: [0.144, 0.306, 0.039, 0.329, 0.074, 0.179, 0.263, 0.232, 0.161, 0.124]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6328 | Steps: 2 | Val loss: 0.4875 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3883 | Steps: 2 | Val loss: 0.3082 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 06:55:53 (running for 00:04:24.84)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.643 |  0.178 |                   41 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.443 |  0.165 |                   42 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.367 |  0.141 |                   41 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.423 |  0.185 |                   38 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.16488811373710632
[2m[36m(func pid=67327)[0m mae:  0.11976955831050873
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.247, 0.066, 0.315, 0.063, 0.186, 0.273, 0.136, 0.142, 0.106]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17830190062522888
[2m[36m(func pid=66954)[0m mae:  0.1307978481054306
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.105, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3933 | Steps: 2 | Val loss: 0.4381 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=67743)[0m rmse: 0.14091745018959045
[2m[36m(func pid=67743)[0m mae:  0.09047248959541321
[2m[36m(func pid=67743)[0m rmse_per_class: [0.098, 0.241, 0.038, 0.269, 0.056, 0.153, 0.211, 0.116, 0.133, 0.094]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4526 | Steps: 2 | Val loss: 0.3571 | Batch size: 32 | lr: 0.001 | Duration: 2.60s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6263 | Steps: 2 | Val loss: 0.4810 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=68170)[0m rmse: 0.1823735237121582
[2m[36m(func pid=68170)[0m mae:  0.10357803106307983
[2m[36m(func pid=68170)[0m rmse_per_class: [0.149, 0.307, 0.038, 0.331, 0.072, 0.176, 0.245, 0.206, 0.156, 0.143]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.16429613530635834
[2m[36m(func pid=67327)[0m mae:  0.1192355751991272
[2m[36m(func pid=67327)[0m rmse_per_class: [0.116, 0.246, 0.065, 0.314, 0.062, 0.185, 0.272, 0.136, 0.142, 0.105]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3980 | Steps: 2 | Val loss: 0.3194 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 06:55:59 (running for 00:04:30.83)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.626 |  0.178 |                   43 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.453 |  0.164 |                   43 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.388 |  0.141 |                   42 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.393 |  0.182 |                   39 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.1781657636165619
[2m[36m(func pid=66954)[0m mae:  0.13068167865276337
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.334, 0.105, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3788 | Steps: 2 | Val loss: 0.4505 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=67743)[0m rmse: 0.1418531835079193
[2m[36m(func pid=67743)[0m mae:  0.09175120294094086
[2m[36m(func pid=67743)[0m rmse_per_class: [0.101, 0.24, 0.038, 0.269, 0.056, 0.151, 0.216, 0.115, 0.136, 0.096]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4512 | Steps: 2 | Val loss: 0.3600 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.6188 | Steps: 2 | Val loss: 0.4748 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=68170)[0m rmse: 0.18491852283477783
[2m[36m(func pid=68170)[0m mae:  0.10420189797878265
[2m[36m(func pid=68170)[0m rmse_per_class: [0.139, 0.305, 0.038, 0.331, 0.068, 0.188, 0.289, 0.173, 0.153, 0.166]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.16373729705810547
[2m[36m(func pid=67327)[0m mae:  0.11873308569192886
[2m[36m(func pid=67327)[0m rmse_per_class: [0.115, 0.246, 0.064, 0.313, 0.061, 0.185, 0.271, 0.135, 0.142, 0.105]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3928 | Steps: 2 | Val loss: 0.3290 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 06:56:04 (running for 00:04:36.01)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.619 |  0.178 |                   44 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.451 |  0.164 |                   44 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.398 |  0.142 |                   43 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.379 |  0.185 |                   40 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17805726826190948
[2m[36m(func pid=66954)[0m mae:  0.13059905171394348
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.334, 0.104, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3685 | Steps: 2 | Val loss: 0.4754 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=67743)[0m rmse: 0.14346514642238617
[2m[36m(func pid=67743)[0m mae:  0.0934118777513504
[2m[36m(func pid=67743)[0m rmse_per_class: [0.104, 0.241, 0.036, 0.271, 0.056, 0.15, 0.224, 0.114, 0.14, 0.099]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4525 | Steps: 2 | Val loss: 0.3626 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6091 | Steps: 2 | Val loss: 0.4691 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=68170)[0m rmse: 0.19141678512096405
[2m[36m(func pid=68170)[0m mae:  0.10849766433238983
[2m[36m(func pid=68170)[0m rmse_per_class: [0.125, 0.305, 0.035, 0.333, 0.063, 0.205, 0.362, 0.149, 0.149, 0.189]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4041 | Steps: 2 | Val loss: 0.3364 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=67327)[0m rmse: 0.16319727897644043
[2m[36m(func pid=67327)[0m mae:  0.11824498325586319
[2m[36m(func pid=67327)[0m rmse_per_class: [0.115, 0.246, 0.063, 0.312, 0.06, 0.185, 0.27, 0.135, 0.141, 0.105]
[2m[36m(func pid=67327)[0m 
== Status ==
Current time: 2024-01-07 06:56:09 (running for 00:04:41.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.609 |  0.178 |                   45 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.453 |  0.163 |                   45 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.393 |  0.143 |                   44 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.368 |  0.191 |                   41 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17802581191062927
[2m[36m(func pid=66954)[0m mae:  0.13057026267051697
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.334, 0.104, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3811 | Steps: 2 | Val loss: 0.5014 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=67743)[0m rmse: 0.14510416984558105
[2m[36m(func pid=67743)[0m mae:  0.09479950368404388
[2m[36m(func pid=67743)[0m rmse_per_class: [0.105, 0.241, 0.034, 0.272, 0.056, 0.15, 0.231, 0.113, 0.144, 0.104]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4584 | Steps: 2 | Val loss: 0.3659 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.6030 | Steps: 2 | Val loss: 0.4629 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=68170)[0m rmse: 0.1977628469467163
[2m[36m(func pid=68170)[0m mae:  0.11301498115062714
[2m[36m(func pid=68170)[0m rmse_per_class: [0.11, 0.307, 0.031, 0.336, 0.059, 0.214, 0.415, 0.14, 0.147, 0.219]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4235 | Steps: 2 | Val loss: 0.3427 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=67327)[0m rmse: 0.16266769170761108
[2m[36m(func pid=67327)[0m mae:  0.1177380234003067
[2m[36m(func pid=67327)[0m rmse_per_class: [0.115, 0.245, 0.062, 0.312, 0.06, 0.185, 0.269, 0.134, 0.141, 0.104]
[2m[36m(func pid=67327)[0m 
== Status ==
Current time: 2024-01-07 06:56:14 (running for 00:04:46.27)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.603 |  0.178 |                   46 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.458 |  0.163 |                   46 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.404 |  0.145 |                   45 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.381 |  0.198 |                   42 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17794470489025116
[2m[36m(func pid=66954)[0m mae:  0.13049939274787903
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.334, 0.104, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.1468413770198822
[2m[36m(func pid=67743)[0m mae:  0.09628443419933319
[2m[36m(func pid=67743)[0m rmse_per_class: [0.104, 0.242, 0.033, 0.273, 0.056, 0.15, 0.24, 0.113, 0.148, 0.108]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3947 | Steps: 2 | Val loss: 0.5168 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4613 | Steps: 2 | Val loss: 0.3687 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5986 | Steps: 2 | Val loss: 0.4567 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=68170)[0m rmse: 0.20273327827453613
[2m[36m(func pid=68170)[0m mae:  0.11569364368915558
[2m[36m(func pid=68170)[0m rmse_per_class: [0.099, 0.311, 0.03, 0.343, 0.059, 0.217, 0.432, 0.14, 0.147, 0.248]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4246 | Steps: 2 | Val loss: 0.3467 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=67327)[0m rmse: 0.16209645569324493
[2m[36m(func pid=67327)[0m mae:  0.11721982061862946
[2m[36m(func pid=67327)[0m rmse_per_class: [0.115, 0.245, 0.061, 0.311, 0.059, 0.184, 0.268, 0.134, 0.141, 0.104]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=66954)[0m rmse: 0.17785587906837463
[2m[36m(func pid=66954)[0m mae:  0.1304343342781067
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.334, 0.103, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
== Status ==
Current time: 2024-01-07 06:56:19 (running for 00:04:51.48)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.599 |  0.178 |                   47 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.461 |  0.162 |                   47 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.423 |  0.147 |                   46 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.395 |  0.203 |                   43 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.14882196485996246
[2m[36m(func pid=67743)[0m mae:  0.09768597036600113
[2m[36m(func pid=67743)[0m rmse_per_class: [0.104, 0.244, 0.032, 0.274, 0.056, 0.15, 0.25, 0.114, 0.152, 0.113]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3751 | Steps: 2 | Val loss: 0.5381 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4649 | Steps: 2 | Val loss: 0.3704 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5928 | Steps: 2 | Val loss: 0.4511 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=68170)[0m rmse: 0.20711569488048553
[2m[36m(func pid=68170)[0m mae:  0.1172441691160202
[2m[36m(func pid=68170)[0m rmse_per_class: [0.094, 0.319, 0.031, 0.352, 0.06, 0.215, 0.416, 0.141, 0.147, 0.296]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.16153483092784882
[2m[36m(func pid=67327)[0m mae:  0.11672015488147736
[2m[36m(func pid=67327)[0m rmse_per_class: [0.114, 0.244, 0.06, 0.31, 0.059, 0.184, 0.267, 0.134, 0.141, 0.103]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4255 | Steps: 2 | Val loss: 0.3473 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 06:56:25 (running for 00:04:56.67)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.593 |  0.178 |                   48 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.465 |  0.162 |                   48 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.425 |  0.149 |                   47 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.375 |  0.207 |                   44 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17774879932403564
[2m[36m(func pid=66954)[0m mae:  0.1303444653749466
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.334, 0.103, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.15110093355178833
[2m[36m(func pid=67743)[0m mae:  0.09926004707813263
[2m[36m(func pid=67743)[0m rmse_per_class: [0.102, 0.245, 0.03, 0.275, 0.056, 0.152, 0.26, 0.115, 0.155, 0.121]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4657 | Steps: 2 | Val loss: 0.3721 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3973 | Steps: 2 | Val loss: 0.5622 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5825 | Steps: 2 | Val loss: 0.4458 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=67327)[0m rmse: 0.16111643612384796
[2m[36m(func pid=67327)[0m mae:  0.1163429468870163
[2m[36m(func pid=67327)[0m rmse_per_class: [0.114, 0.244, 0.059, 0.309, 0.058, 0.184, 0.266, 0.133, 0.141, 0.103]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4110 | Steps: 2 | Val loss: 0.3458 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=68170)[0m rmse: 0.20990629494190216
[2m[36m(func pid=68170)[0m mae:  0.11840198189020157
[2m[36m(func pid=68170)[0m rmse_per_class: [0.092, 0.324, 0.03, 0.359, 0.06, 0.21, 0.385, 0.142, 0.149, 0.348]
[2m[36m(func pid=68170)[0m 
== Status ==
Current time: 2024-01-07 06:56:30 (running for 00:05:01.75)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.583 |  0.178 |                   49 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.466 |  0.161 |                   49 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.425 |  0.151 |                   48 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.397 |  0.21  |                   45 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.1776609718799591
[2m[36m(func pid=66954)[0m mae:  0.13026756048202515
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.334, 0.103, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.153033047914505
[2m[36m(func pid=67743)[0m mae:  0.1002902165055275
[2m[36m(func pid=67743)[0m rmse_per_class: [0.098, 0.246, 0.029, 0.275, 0.056, 0.154, 0.268, 0.117, 0.158, 0.13]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4701 | Steps: 2 | Val loss: 0.3741 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4015 | Steps: 2 | Val loss: 0.5792 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5784 | Steps: 2 | Val loss: 0.4406 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=67327)[0m rmse: 0.16055122017860413
[2m[36m(func pid=67327)[0m mae:  0.11582501232624054
[2m[36m(func pid=67327)[0m rmse_per_class: [0.113, 0.244, 0.058, 0.308, 0.058, 0.184, 0.264, 0.133, 0.141, 0.103]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4055 | Steps: 2 | Val loss: 0.3432 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=68170)[0m rmse: 0.21068596839904785
[2m[36m(func pid=68170)[0m mae:  0.11919011920690536
[2m[36m(func pid=68170)[0m rmse_per_class: [0.091, 0.333, 0.03, 0.371, 0.06, 0.199, 0.348, 0.142, 0.15, 0.384]
[2m[36m(func pid=68170)[0m 
== Status ==
Current time: 2024-01-07 06:56:35 (running for 00:05:06.90)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.578 |  0.177 |                   50 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.47  |  0.161 |                   50 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.411 |  0.153 |                   49 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.402 |  0.211 |                   46 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17748770117759705
[2m[36m(func pid=66954)[0m mae:  0.13012942671775818
[2m[36m(func pid=66954)[0m rmse_per_class: [0.115, 0.259, 0.094, 0.334, 0.102, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.15536217391490936
[2m[36m(func pid=67743)[0m mae:  0.10152442753314972
[2m[36m(func pid=67743)[0m rmse_per_class: [0.094, 0.249, 0.028, 0.276, 0.056, 0.155, 0.276, 0.119, 0.16, 0.139]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4676 | Steps: 2 | Val loss: 0.3757 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4142 | Steps: 2 | Val loss: 0.5933 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5707 | Steps: 2 | Val loss: 0.4359 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=67327)[0m rmse: 0.1600407212972641
[2m[36m(func pid=67327)[0m mae:  0.11534996330738068
[2m[36m(func pid=67327)[0m rmse_per_class: [0.113, 0.243, 0.058, 0.307, 0.058, 0.184, 0.263, 0.133, 0.141, 0.102]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3999 | Steps: 2 | Val loss: 0.3400 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=68170)[0m rmse: 0.21066877245903015
[2m[36m(func pid=68170)[0m mae:  0.1202317625284195
[2m[36m(func pid=68170)[0m rmse_per_class: [0.09, 0.336, 0.029, 0.374, 0.06, 0.188, 0.319, 0.144, 0.149, 0.419]
[2m[36m(func pid=68170)[0m 
== Status ==
Current time: 2024-01-07 06:56:40 (running for 00:05:11.99)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.571 |  0.177 |                   51 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.468 |  0.16  |                   51 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.406 |  0.155 |                   50 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.414 |  0.211 |                   47 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17748576402664185
[2m[36m(func pid=66954)[0m mae:  0.13013485074043274
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.259, 0.093, 0.334, 0.102, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.1580176055431366
[2m[36m(func pid=67743)[0m mae:  0.10273178666830063
[2m[36m(func pid=67743)[0m rmse_per_class: [0.092, 0.252, 0.027, 0.277, 0.056, 0.157, 0.283, 0.122, 0.161, 0.152]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4741 | Steps: 2 | Val loss: 0.3771 | Batch size: 32 | lr: 0.001 | Duration: 2.58s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4128 | Steps: 2 | Val loss: 0.5903 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5667 | Steps: 2 | Val loss: 0.4307 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=67327)[0m rmse: 0.159502774477005
[2m[36m(func pid=67327)[0m mae:  0.11485190689563751
[2m[36m(func pid=67327)[0m rmse_per_class: [0.113, 0.243, 0.057, 0.306, 0.057, 0.183, 0.262, 0.132, 0.141, 0.101]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3926 | Steps: 2 | Val loss: 0.3369 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 06:56:45 (running for 00:05:17.00)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.567 |  0.177 |                   52 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.474 |  0.16  |                   52 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.4   |  0.158 |                   51 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.413 |  0.21  |                   48 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17745693027973175
[2m[36m(func pid=66954)[0m mae:  0.13010936975479126
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.333, 0.102, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.20956861972808838
[2m[36m(func pid=68170)[0m mae:  0.12014354765415192
[2m[36m(func pid=68170)[0m rmse_per_class: [0.089, 0.344, 0.029, 0.376, 0.06, 0.183, 0.3, 0.146, 0.149, 0.419]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.16081218421459198
[2m[36m(func pid=67743)[0m mae:  0.10385549068450928
[2m[36m(func pid=67743)[0m rmse_per_class: [0.089, 0.257, 0.026, 0.279, 0.056, 0.159, 0.29, 0.125, 0.16, 0.167]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4757 | Steps: 2 | Val loss: 0.3780 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5579 | Steps: 2 | Val loss: 0.4256 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4019 | Steps: 2 | Val loss: 0.5798 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=67327)[0m rmse: 0.1588800698518753
[2m[36m(func pid=67327)[0m mae:  0.11426948010921478
[2m[36m(func pid=67327)[0m rmse_per_class: [0.112, 0.242, 0.056, 0.305, 0.057, 0.183, 0.261, 0.132, 0.141, 0.101]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3729 | Steps: 2 | Val loss: 0.3329 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 06:56:50 (running for 00:05:22.08)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.567 |  0.177 |                   52 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.476 |  0.159 |                   53 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.393 |  0.161 |                   52 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.402 |  0.207 |                   49 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.1774047315120697
[2m[36m(func pid=66954)[0m mae:  0.1300526112318039
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.333, 0.101, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.20732302963733673
[2m[36m(func pid=68170)[0m mae:  0.11923810094594955
[2m[36m(func pid=68170)[0m rmse_per_class: [0.089, 0.346, 0.029, 0.372, 0.059, 0.191, 0.29, 0.148, 0.147, 0.404]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4750 | Steps: 2 | Val loss: 0.3783 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=67743)[0m rmse: 0.16382542252540588
[2m[36m(func pid=67743)[0m mae:  0.10513123124837875
[2m[36m(func pid=67743)[0m rmse_per_class: [0.086, 0.262, 0.025, 0.282, 0.056, 0.16, 0.294, 0.128, 0.161, 0.184]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3762 | Steps: 2 | Val loss: 0.5606 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5531 | Steps: 2 | Val loss: 0.4207 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=67327)[0m rmse: 0.15842100977897644
[2m[36m(func pid=67327)[0m mae:  0.1138429194688797
[2m[36m(func pid=67327)[0m rmse_per_class: [0.112, 0.242, 0.055, 0.305, 0.057, 0.183, 0.26, 0.131, 0.141, 0.1]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3785 | Steps: 2 | Val loss: 0.3321 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 06:56:55 (running for 00:05:27.22)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.558 |  0.177 |                   53 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.475 |  0.158 |                   54 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.373 |  0.164 |                   53 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.376 |  0.203 |                   50 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17731624841690063
[2m[36m(func pid=66954)[0m mae:  0.12999437749385834
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.333, 0.101, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.20325055718421936
[2m[36m(func pid=68170)[0m mae:  0.11704181134700775
[2m[36m(func pid=68170)[0m rmse_per_class: [0.089, 0.344, 0.029, 0.366, 0.057, 0.212, 0.283, 0.15, 0.145, 0.358]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4804 | Steps: 2 | Val loss: 0.3797 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=67743)[0m rmse: 0.16740182042121887
[2m[36m(func pid=67743)[0m mae:  0.10643064975738525
[2m[36m(func pid=67743)[0m rmse_per_class: [0.085, 0.267, 0.025, 0.285, 0.056, 0.161, 0.297, 0.132, 0.16, 0.207]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3829 | Steps: 2 | Val loss: 0.5463 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5421 | Steps: 2 | Val loss: 0.4163 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=67327)[0m rmse: 0.15796279907226562
[2m[36m(func pid=67327)[0m mae:  0.11341605335474014
[2m[36m(func pid=67327)[0m rmse_per_class: [0.111, 0.241, 0.054, 0.304, 0.056, 0.182, 0.259, 0.131, 0.141, 0.1]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3596 | Steps: 2 | Val loss: 0.3316 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 06:57:00 (running for 00:05:32.44)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.542 |  0.177 |                   55 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.48  |  0.158 |                   55 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.378 |  0.167 |                   54 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.376 |  0.203 |                   50 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17725415527820587
[2m[36m(func pid=66954)[0m mae:  0.12995538115501404
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.333, 0.1, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.19829212129116058
[2m[36m(func pid=68170)[0m mae:  0.11489462852478027
[2m[36m(func pid=68170)[0m rmse_per_class: [0.089, 0.342, 0.028, 0.361, 0.057, 0.236, 0.279, 0.15, 0.142, 0.298]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.17072008550167084
[2m[36m(func pid=67743)[0m mae:  0.10768206417560577
[2m[36m(func pid=67743)[0m rmse_per_class: [0.085, 0.272, 0.025, 0.289, 0.056, 0.161, 0.299, 0.135, 0.158, 0.226]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4734 | Steps: 2 | Val loss: 0.3809 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5389 | Steps: 2 | Val loss: 0.4114 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3701 | Steps: 2 | Val loss: 0.5336 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=67327)[0m rmse: 0.15744677186012268
[2m[36m(func pid=67327)[0m mae:  0.11290256679058075
[2m[36m(func pid=67327)[0m rmse_per_class: [0.111, 0.24, 0.053, 0.303, 0.056, 0.182, 0.258, 0.131, 0.14, 0.099]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3512 | Steps: 2 | Val loss: 0.3323 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 06:57:05 (running for 00:05:37.55)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.539 |  0.177 |                   56 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.473 |  0.157 |                   56 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.36  |  0.171 |                   55 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.383 |  0.198 |                   51 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17715594172477722
[2m[36m(func pid=66954)[0m mae:  0.12986913323402405
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.1, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.19122470915317535
[2m[36m(func pid=68170)[0m mae:  0.11143050342798233
[2m[36m(func pid=68170)[0m rmse_per_class: [0.088, 0.334, 0.027, 0.355, 0.056, 0.244, 0.271, 0.155, 0.141, 0.241]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4775 | Steps: 2 | Val loss: 0.3810 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=67743)[0m rmse: 0.1734149307012558
[2m[36m(func pid=67743)[0m mae:  0.1083189845085144
[2m[36m(func pid=67743)[0m rmse_per_class: [0.083, 0.277, 0.026, 0.291, 0.056, 0.161, 0.299, 0.14, 0.156, 0.246]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5321 | Steps: 2 | Val loss: 0.4066 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3651 | Steps: 2 | Val loss: 0.5335 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=67327)[0m rmse: 0.15679219365119934
[2m[36m(func pid=67327)[0m mae:  0.11228953301906586
[2m[36m(func pid=67327)[0m rmse_per_class: [0.11, 0.239, 0.053, 0.303, 0.056, 0.182, 0.257, 0.13, 0.14, 0.099]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3538 | Steps: 2 | Val loss: 0.3352 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 06:57:11 (running for 00:05:42.75)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.532 |  0.177 |                   57 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.478 |  0.157 |                   57 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.351 |  0.173 |                   56 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.37  |  0.191 |                   52 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17707793414592743
[2m[36m(func pid=66954)[0m mae:  0.12980963289737701
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.1, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.18441398441791534
[2m[36m(func pid=68170)[0m mae:  0.10840468108654022
[2m[36m(func pid=68170)[0m rmse_per_class: [0.086, 0.323, 0.026, 0.352, 0.056, 0.231, 0.266, 0.164, 0.14, 0.2]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4886 | Steps: 2 | Val loss: 0.3810 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=67743)[0m rmse: 0.176629900932312
[2m[36m(func pid=67743)[0m mae:  0.10930927842855453
[2m[36m(func pid=67743)[0m rmse_per_class: [0.084, 0.281, 0.028, 0.294, 0.056, 0.161, 0.297, 0.147, 0.154, 0.265]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5281 | Steps: 2 | Val loss: 0.4018 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3872 | Steps: 2 | Val loss: 0.5322 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=67327)[0m rmse: 0.15620698034763336
[2m[36m(func pid=67327)[0m mae:  0.11168275773525238
[2m[36m(func pid=67327)[0m rmse_per_class: [0.109, 0.239, 0.052, 0.302, 0.055, 0.181, 0.256, 0.13, 0.139, 0.098]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3360 | Steps: 2 | Val loss: 0.3365 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 06:57:16 (running for 00:05:47.99)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.528 |  0.177 |                   58 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.489 |  0.156 |                   58 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.354 |  0.177 |                   57 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.365 |  0.184 |                   53 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=66954)[0m rmse: 0.17698656022548676
[2m[36m(func pid=66954)[0m mae:  0.12974263727664948
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.099, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.18029677867889404
[2m[36m(func pid=68170)[0m mae:  0.1067269816994667
[2m[36m(func pid=68170)[0m rmse_per_class: [0.089, 0.315, 0.026, 0.349, 0.056, 0.209, 0.265, 0.178, 0.141, 0.176]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4802 | Steps: 2 | Val loss: 0.3803 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=67743)[0m rmse: 0.1780545562505722
[2m[36m(func pid=67743)[0m mae:  0.10917775332927704
[2m[36m(func pid=67743)[0m rmse_per_class: [0.082, 0.286, 0.029, 0.295, 0.055, 0.162, 0.294, 0.149, 0.15, 0.277]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5224 | Steps: 2 | Val loss: 0.3976 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=67327)[0m rmse: 0.15566302835941315
[2m[36m(func pid=67327)[0m mae:  0.11117855459451675
[2m[36m(func pid=67327)[0m rmse_per_class: [0.109, 0.238, 0.051, 0.301, 0.055, 0.181, 0.254, 0.129, 0.139, 0.098]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3905 | Steps: 2 | Val loss: 0.5238 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3398 | Steps: 2 | Val loss: 0.3405 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=66954)[0m rmse: 0.17688515782356262
[2m[36m(func pid=66954)[0m mae:  0.1296669840812683
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.099, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
== Status ==
Current time: 2024-01-07 06:57:22 (running for 00:05:53.94)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.522 |  0.177 |                   59 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.48  |  0.156 |                   59 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.336 |  0.178 |                   58 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.39  |  0.18  |                   55 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68170)[0m rmse: 0.18015149235725403
[2m[36m(func pid=68170)[0m mae:  0.1066160798072815
[2m[36m(func pid=68170)[0m rmse_per_class: [0.105, 0.311, 0.027, 0.346, 0.056, 0.187, 0.265, 0.198, 0.144, 0.164]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4760 | Steps: 2 | Val loss: 0.3806 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=67743)[0m rmse: 0.17961719632148743
[2m[36m(func pid=67743)[0m mae:  0.10934604704380035
[2m[36m(func pid=67743)[0m rmse_per_class: [0.082, 0.29, 0.031, 0.298, 0.055, 0.161, 0.291, 0.149, 0.147, 0.293]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5168 | Steps: 2 | Val loss: 0.3934 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=67327)[0m rmse: 0.15507718920707703
[2m[36m(func pid=67327)[0m mae:  0.11057998239994049
[2m[36m(func pid=67327)[0m rmse_per_class: [0.108, 0.238, 0.05, 0.301, 0.055, 0.181, 0.253, 0.129, 0.139, 0.097]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3854 | Steps: 2 | Val loss: 0.5092 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3419 | Steps: 2 | Val loss: 0.3459 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=66954)[0m rmse: 0.17675496637821198
[2m[36m(func pid=66954)[0m mae:  0.12956593930721283
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.258, 0.091, 0.333, 0.098, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
== Status ==
Current time: 2024-01-07 06:57:27 (running for 00:05:59.27)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.517 |  0.177 |                   60 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.476 |  0.155 |                   60 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.34  |  0.18  |                   59 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.385 |  0.184 |                   56 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=68170)[0m rmse: 0.184156134724617
[2m[36m(func pid=68170)[0m mae:  0.10879217088222504
[2m[36m(func pid=68170)[0m rmse_per_class: [0.135, 0.312, 0.028, 0.342, 0.056, 0.18, 0.262, 0.222, 0.148, 0.156]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m rmse: 0.18092918395996094
[2m[36m(func pid=67743)[0m mae:  0.10942278802394867
[2m[36m(func pid=67743)[0m rmse_per_class: [0.08, 0.292, 0.033, 0.3, 0.055, 0.16, 0.285, 0.151, 0.145, 0.309]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4775 | Steps: 2 | Val loss: 0.3801 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.5124 | Steps: 2 | Val loss: 0.3895 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=67327)[0m rmse: 0.1545226126909256
[2m[36m(func pid=67327)[0m mae:  0.11003343015909195
[2m[36m(func pid=67327)[0m rmse_per_class: [0.108, 0.237, 0.05, 0.299, 0.055, 0.18, 0.252, 0.129, 0.139, 0.097]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3762 | Steps: 2 | Val loss: 0.5017 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3509 | Steps: 2 | Val loss: 0.3512 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=66954)[0m rmse: 0.17670640349388123
[2m[36m(func pid=66954)[0m mae:  0.1295194774866104
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.333, 0.098, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
== Status ==
Current time: 2024-01-07 06:57:33 (running for 00:06:04.70)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.512 |  0.177 |                   61 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.477 |  0.155 |                   61 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.351 |  0.182 |                   61 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.385 |  0.184 |                   56 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.1817115694284439
[2m[36m(func pid=67743)[0m mae:  0.10913257300853729
[2m[36m(func pid=67743)[0m rmse_per_class: [0.079, 0.296, 0.035, 0.302, 0.055, 0.159, 0.277, 0.154, 0.144, 0.317]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4767 | Steps: 2 | Val loss: 0.3784 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=68170)[0m rmse: 0.19140562415122986
[2m[36m(func pid=68170)[0m mae:  0.11226556450128555
[2m[36m(func pid=68170)[0m rmse_per_class: [0.171, 0.314, 0.029, 0.34, 0.057, 0.184, 0.265, 0.239, 0.153, 0.162]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5059 | Steps: 2 | Val loss: 0.3864 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=67327)[0m rmse: 0.15401466190814972
[2m[36m(func pid=67327)[0m mae:  0.10953352600336075
[2m[36m(func pid=67327)[0m rmse_per_class: [0.107, 0.237, 0.049, 0.299, 0.055, 0.18, 0.251, 0.128, 0.139, 0.096]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3406 | Steps: 2 | Val loss: 0.3554 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3684 | Steps: 2 | Val loss: 0.4963 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=66954)[0m rmse: 0.17666225135326385
[2m[36m(func pid=66954)[0m mae:  0.12947222590446472
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.258, 0.091, 0.333, 0.098, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
== Status ==
Current time: 2024-01-07 06:57:38 (running for 00:06:09.88)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.506 |  0.177 |                   62 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.477 |  0.154 |                   62 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.341 |  0.181 |                   62 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.376 |  0.191 |                   57 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.18138977885246277
[2m[36m(func pid=67743)[0m mae:  0.10804007947444916
[2m[36m(func pid=67743)[0m rmse_per_class: [0.08, 0.298, 0.036, 0.303, 0.055, 0.16, 0.266, 0.153, 0.143, 0.319]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4815 | Steps: 2 | Val loss: 0.3772 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=68170)[0m rmse: 0.19927366077899933
[2m[36m(func pid=68170)[0m mae:  0.11566241830587387
[2m[36m(func pid=68170)[0m rmse_per_class: [0.197, 0.318, 0.029, 0.346, 0.06, 0.193, 0.272, 0.243, 0.163, 0.172]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5024 | Steps: 2 | Val loss: 0.3824 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=67327)[0m rmse: 0.15351170301437378
[2m[36m(func pid=67327)[0m mae:  0.10901093482971191
[2m[36m(func pid=67327)[0m rmse_per_class: [0.106, 0.236, 0.049, 0.298, 0.055, 0.179, 0.25, 0.128, 0.139, 0.095]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3324 | Steps: 2 | Val loss: 0.3607 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3560 | Steps: 2 | Val loss: 0.4977 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=66954)[0m rmse: 0.1765112280845642
[2m[36m(func pid=66954)[0m mae:  0.12934181094169617
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.332, 0.098, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
== Status ==
Current time: 2024-01-07 06:57:43 (running for 00:06:14.90)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.502 |  0.177 |                   63 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.482 |  0.154 |                   63 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.332 |  0.181 |                   63 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.368 |  0.199 |                   58 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.1809425950050354
[2m[36m(func pid=67743)[0m mae:  0.10712845623493195
[2m[36m(func pid=67743)[0m rmse_per_class: [0.079, 0.302, 0.039, 0.305, 0.057, 0.159, 0.255, 0.148, 0.143, 0.323]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4835 | Steps: 2 | Val loss: 0.3768 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=68170)[0m rmse: 0.20540328323841095
[2m[36m(func pid=68170)[0m mae:  0.11736191809177399
[2m[36m(func pid=68170)[0m rmse_per_class: [0.203, 0.325, 0.029, 0.36, 0.067, 0.199, 0.279, 0.229, 0.178, 0.187]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4972 | Steps: 2 | Val loss: 0.3792 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=67327)[0m rmse: 0.15313497185707092
[2m[36m(func pid=67327)[0m mae:  0.1085876077413559
[2m[36m(func pid=67327)[0m rmse_per_class: [0.106, 0.236, 0.048, 0.298, 0.055, 0.179, 0.249, 0.128, 0.138, 0.095]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3389 | Steps: 2 | Val loss: 0.3644 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=66954)[0m rmse: 0.17644430696964264
[2m[36m(func pid=66954)[0m mae:  0.12929213047027588
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.332, 0.097, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3501 | Steps: 2 | Val loss: 0.5039 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 06:57:48 (running for 00:06:20.24)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.497 |  0.176 |                   64 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.483 |  0.153 |                   64 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.339 |  0.18  |                   64 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.356 |  0.205 |                   59 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4816 | Steps: 2 | Val loss: 0.3759 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=67743)[0m rmse: 0.17989687621593475
[2m[36m(func pid=67743)[0m mae:  0.10561875998973846
[2m[36m(func pid=67743)[0m rmse_per_class: [0.08, 0.303, 0.041, 0.306, 0.06, 0.159, 0.244, 0.143, 0.142, 0.32]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.21002602577209473
[2m[36m(func pid=68170)[0m mae:  0.11805602163076401
[2m[36m(func pid=68170)[0m rmse_per_class: [0.196, 0.33, 0.029, 0.38, 0.081, 0.202, 0.285, 0.202, 0.192, 0.203]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4935 | Steps: 2 | Val loss: 0.3757 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=67327)[0m rmse: 0.15260478854179382
[2m[36m(func pid=67327)[0m mae:  0.10802136361598969
[2m[36m(func pid=67327)[0m rmse_per_class: [0.106, 0.235, 0.047, 0.297, 0.055, 0.179, 0.247, 0.127, 0.138, 0.094]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3482 | Steps: 2 | Val loss: 0.3684 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=66954)[0m rmse: 0.17636466026306152
[2m[36m(func pid=66954)[0m mae:  0.12923528254032135
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.332, 0.096, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3508 | Steps: 2 | Val loss: 0.5122 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4698 | Steps: 2 | Val loss: 0.3743 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 06:57:53 (running for 00:06:25.26)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.494 |  0.176 |                   65 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.482 |  0.153 |                   65 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.348 |  0.179 |                   65 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.35  |  0.21  |                   60 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.17896568775177002
[2m[36m(func pid=67743)[0m mae:  0.10412492603063583
[2m[36m(func pid=67743)[0m rmse_per_class: [0.079, 0.306, 0.042, 0.307, 0.067, 0.16, 0.232, 0.138, 0.14, 0.318]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.21222762763500214
[2m[36m(func pid=68170)[0m mae:  0.11765719950199127
[2m[36m(func pid=68170)[0m rmse_per_class: [0.178, 0.333, 0.03, 0.392, 0.096, 0.203, 0.29, 0.179, 0.202, 0.219]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4881 | Steps: 2 | Val loss: 0.3722 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=67327)[0m rmse: 0.15218092501163483
[2m[36m(func pid=67327)[0m mae:  0.10755299031734467
[2m[36m(func pid=67327)[0m rmse_per_class: [0.105, 0.234, 0.047, 0.297, 0.055, 0.179, 0.247, 0.127, 0.138, 0.094]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3311 | Steps: 2 | Val loss: 0.3707 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=66954)[0m rmse: 0.1762278527021408
[2m[36m(func pid=66954)[0m mae:  0.1291319578886032
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.332, 0.096, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3455 | Steps: 2 | Val loss: 0.5200 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4702 | Steps: 2 | Val loss: 0.3729 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 06:57:58 (running for 00:06:30.54)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.488 |  0.176 |                   66 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.47  |  0.152 |                   66 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.331 |  0.178 |                   66 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.351 |  0.212 |                   61 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.17773298919200897
[2m[36m(func pid=67743)[0m mae:  0.1023872122168541
[2m[36m(func pid=67743)[0m rmse_per_class: [0.08, 0.307, 0.043, 0.308, 0.075, 0.162, 0.222, 0.13, 0.14, 0.31]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.21261374652385712
[2m[36m(func pid=68170)[0m mae:  0.11664871871471405
[2m[36m(func pid=68170)[0m rmse_per_class: [0.161, 0.331, 0.032, 0.392, 0.108, 0.202, 0.299, 0.161, 0.206, 0.234]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4833 | Steps: 2 | Val loss: 0.3694 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=67327)[0m rmse: 0.15169422328472137
[2m[36m(func pid=67327)[0m mae:  0.10700175911188126
[2m[36m(func pid=67327)[0m rmse_per_class: [0.104, 0.234, 0.046, 0.297, 0.055, 0.179, 0.245, 0.126, 0.138, 0.093]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3303 | Steps: 2 | Val loss: 0.3712 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=66954)[0m rmse: 0.17622676491737366
[2m[36m(func pid=66954)[0m mae:  0.12913016974925995
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.332, 0.096, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3665 | Steps: 2 | Val loss: 0.5224 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4724 | Steps: 2 | Val loss: 0.3714 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 06:58:04 (running for 00:06:35.65)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.483 |  0.176 |                   67 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.47  |  0.152 |                   67 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.33  |  0.177 |                   67 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.345 |  0.213 |                   62 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.17674598097801208
[2m[36m(func pid=67743)[0m mae:  0.10080976784229279
[2m[36m(func pid=67743)[0m rmse_per_class: [0.08, 0.307, 0.042, 0.308, 0.086, 0.164, 0.214, 0.125, 0.14, 0.3]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4823 | Steps: 2 | Val loss: 0.3665 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=68170)[0m rmse: 0.21186070144176483
[2m[36m(func pid=68170)[0m mae:  0.11528663337230682
[2m[36m(func pid=68170)[0m rmse_per_class: [0.146, 0.327, 0.032, 0.385, 0.117, 0.201, 0.314, 0.151, 0.204, 0.242]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.15125630795955658
[2m[36m(func pid=67327)[0m mae:  0.10658812522888184
[2m[36m(func pid=67327)[0m rmse_per_class: [0.104, 0.234, 0.046, 0.296, 0.055, 0.178, 0.244, 0.126, 0.138, 0.093]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3427 | Steps: 2 | Val loss: 0.3720 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=66954)[0m rmse: 0.1761186122894287
[2m[36m(func pid=66954)[0m mae:  0.1290416270494461
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.258, 0.09, 0.332, 0.095, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3685 | Steps: 2 | Val loss: 0.5190 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4648 | Steps: 2 | Val loss: 0.3696 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 06:58:09 (running for 00:06:40.78)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.482 |  0.176 |                   68 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.472 |  0.151 |                   68 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.176 |                   68 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.367 |  0.212 |                   63 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.17646881937980652
[2m[36m(func pid=67743)[0m mae:  0.0996876209974289
[2m[36m(func pid=67743)[0m rmse_per_class: [0.08, 0.306, 0.041, 0.308, 0.097, 0.168, 0.211, 0.121, 0.139, 0.293]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4771 | Steps: 2 | Val loss: 0.3631 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=68170)[0m rmse: 0.21036243438720703
[2m[36m(func pid=68170)[0m mae:  0.11375457048416138
[2m[36m(func pid=68170)[0m rmse_per_class: [0.131, 0.323, 0.033, 0.37, 0.127, 0.199, 0.338, 0.148, 0.195, 0.241]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.15077468752861023
[2m[36m(func pid=67327)[0m mae:  0.10605502128601074
[2m[36m(func pid=67327)[0m rmse_per_class: [0.103, 0.233, 0.045, 0.295, 0.054, 0.178, 0.243, 0.126, 0.137, 0.092]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3436 | Steps: 2 | Val loss: 0.3724 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=66954)[0m rmse: 0.1760009229183197
[2m[36m(func pid=66954)[0m mae:  0.12894555926322937
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.331, 0.095, 0.19, 0.289, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3729 | Steps: 2 | Val loss: 0.5126 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4619 | Steps: 2 | Val loss: 0.3678 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 06:58:14 (running for 00:06:45.98)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.477 |  0.176 |                   69 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.465 |  0.151 |                   69 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.344 |  0.177 |                   69 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.369 |  0.21  |                   64 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.1772463172674179
[2m[36m(func pid=67743)[0m mae:  0.09924308955669403
[2m[36m(func pid=67743)[0m rmse_per_class: [0.08, 0.306, 0.04, 0.309, 0.114, 0.171, 0.215, 0.116, 0.138, 0.282]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4717 | Steps: 2 | Val loss: 0.3596 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=68170)[0m rmse: 0.20864005386829376
[2m[36m(func pid=68170)[0m mae:  0.11303316056728363
[2m[36m(func pid=68170)[0m rmse_per_class: [0.125, 0.319, 0.033, 0.357, 0.128, 0.196, 0.362, 0.146, 0.185, 0.235]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.1503218561410904
[2m[36m(func pid=67327)[0m mae:  0.10554621368646622
[2m[36m(func pid=67327)[0m rmse_per_class: [0.102, 0.233, 0.045, 0.294, 0.054, 0.178, 0.242, 0.126, 0.137, 0.092]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3528 | Steps: 2 | Val loss: 0.3722 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=66954)[0m rmse: 0.1759181022644043
[2m[36m(func pid=66954)[0m mae:  0.12887991964817047
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.331, 0.095, 0.19, 0.289, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4607 | Steps: 2 | Val loss: 0.3654 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3631 | Steps: 2 | Val loss: 0.5061 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 06:58:19 (running for 00:06:51.23)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.472 |  0.176 |                   70 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.462 |  0.15  |                   70 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.353 |  0.178 |                   70 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.373 |  0.209 |                   65 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.17837640643119812
[2m[36m(func pid=67743)[0m mae:  0.09938991069793701
[2m[36m(func pid=67743)[0m rmse_per_class: [0.08, 0.305, 0.04, 0.31, 0.127, 0.175, 0.224, 0.114, 0.137, 0.271]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4694 | Steps: 2 | Val loss: 0.3569 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=68170)[0m rmse: 0.20603469014167786
[2m[36m(func pid=68170)[0m mae:  0.11271733045578003
[2m[36m(func pid=68170)[0m rmse_per_class: [0.119, 0.315, 0.033, 0.349, 0.126, 0.195, 0.381, 0.146, 0.173, 0.222]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.14984485507011414
[2m[36m(func pid=67327)[0m mae:  0.10499860346317291
[2m[36m(func pid=67327)[0m rmse_per_class: [0.102, 0.232, 0.045, 0.294, 0.054, 0.177, 0.241, 0.125, 0.137, 0.091]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3506 | Steps: 2 | Val loss: 0.3700 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=66954)[0m rmse: 0.1758359670639038
[2m[36m(func pid=66954)[0m mae:  0.1288139820098877
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.257, 0.089, 0.331, 0.094, 0.19, 0.289, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4682 | Steps: 2 | Val loss: 0.3626 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3540 | Steps: 2 | Val loss: 0.5046 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 06:58:24 (running for 00:06:56.23)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.469 |  0.176 |                   71 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.461 |  0.15  |                   71 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.351 |  0.179 |                   71 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.363 |  0.206 |                   66 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.17935049533843994
[2m[36m(func pid=67743)[0m mae:  0.09968329221010208
[2m[36m(func pid=67743)[0m rmse_per_class: [0.078, 0.303, 0.038, 0.31, 0.139, 0.178, 0.237, 0.115, 0.136, 0.261]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4653 | Steps: 2 | Val loss: 0.3547 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=67327)[0m rmse: 0.14938683807849884
[2m[36m(func pid=67327)[0m mae:  0.10444784164428711
[2m[36m(func pid=67327)[0m rmse_per_class: [0.101, 0.232, 0.044, 0.293, 0.054, 0.177, 0.241, 0.125, 0.136, 0.091]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.20341551303863525
[2m[36m(func pid=68170)[0m mae:  0.11253567785024643
[2m[36m(func pid=68170)[0m rmse_per_class: [0.119, 0.312, 0.034, 0.348, 0.117, 0.203, 0.383, 0.145, 0.166, 0.209]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3343 | Steps: 2 | Val loss: 0.3683 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=66954)[0m rmse: 0.1757916510105133
[2m[36m(func pid=66954)[0m mae:  0.12877744436264038
[2m[36m(func pid=66954)[0m rmse_per_class: [0.116, 0.257, 0.089, 0.331, 0.094, 0.19, 0.289, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4585 | Steps: 2 | Val loss: 0.3598 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3494 | Steps: 2 | Val loss: 0.5051 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 06:58:29 (running for 00:07:01.42)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.465 |  0.176 |                   72 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.468 |  0.149 |                   72 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.334 |  0.181 |                   72 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.354 |  0.203 |                   67 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.18128609657287598
[2m[36m(func pid=67743)[0m mae:  0.10075117647647858
[2m[36m(func pid=67743)[0m rmse_per_class: [0.079, 0.3, 0.037, 0.312, 0.153, 0.182, 0.255, 0.116, 0.136, 0.243]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4634 | Steps: 2 | Val loss: 0.3523 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=67327)[0m rmse: 0.14903312921524048
[2m[36m(func pid=67327)[0m mae:  0.10403939336538315
[2m[36m(func pid=67327)[0m rmse_per_class: [0.1, 0.231, 0.044, 0.293, 0.054, 0.177, 0.24, 0.125, 0.136, 0.091]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.20131559669971466
[2m[36m(func pid=68170)[0m mae:  0.1120925322175026
[2m[36m(func pid=68170)[0m rmse_per_class: [0.126, 0.31, 0.034, 0.349, 0.11, 0.223, 0.357, 0.143, 0.159, 0.202]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3369 | Steps: 2 | Val loss: 0.3662 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=66954)[0m rmse: 0.1757119745016098
[2m[36m(func pid=66954)[0m mae:  0.12870152294635773
[2m[36m(func pid=66954)[0m rmse_per_class: [0.117, 0.257, 0.089, 0.331, 0.094, 0.19, 0.289, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4490 | Steps: 2 | Val loss: 0.3573 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3505 | Steps: 2 | Val loss: 0.5052 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 06:58:34 (running for 00:07:06.45)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.463 |  0.176 |                   73 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.458 |  0.149 |                   73 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.337 |  0.183 |                   73 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.349 |  0.201 |                   68 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.18297575414180756
[2m[36m(func pid=67743)[0m mae:  0.10191609710454941
[2m[36m(func pid=67743)[0m rmse_per_class: [0.077, 0.299, 0.036, 0.314, 0.165, 0.185, 0.271, 0.119, 0.135, 0.23]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4617 | Steps: 2 | Val loss: 0.3501 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=67327)[0m rmse: 0.14866693317890167
[2m[36m(func pid=67327)[0m mae:  0.10358945280313492
[2m[36m(func pid=67327)[0m rmse_per_class: [0.1, 0.231, 0.044, 0.292, 0.054, 0.177, 0.239, 0.124, 0.136, 0.09]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.20002901554107666
[2m[36m(func pid=68170)[0m mae:  0.1121407300233841
[2m[36m(func pid=68170)[0m rmse_per_class: [0.143, 0.311, 0.034, 0.349, 0.104, 0.251, 0.319, 0.138, 0.155, 0.196]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3552 | Steps: 2 | Val loss: 0.3642 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=66954)[0m rmse: 0.17560823261737823
[2m[36m(func pid=66954)[0m mae:  0.12860922515392303
[2m[36m(func pid=66954)[0m rmse_per_class: [0.117, 0.257, 0.089, 0.331, 0.093, 0.19, 0.289, 0.141, 0.142, 0.108]
[2m[36m(func pid=66954)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4537 | Steps: 2 | Val loss: 0.3539 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3575 | Steps: 2 | Val loss: 0.5104 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 06:58:39 (running for 00:07:11.48)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | RUNNING  | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.462 |  0.176 |                   74 |
| train_ccef6_00001 | RUNNING  | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.449 |  0.149 |                   74 |
| train_ccef6_00002 | RUNNING  | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.355 |  0.184 |                   74 |
| train_ccef6_00003 | RUNNING  | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.35  |  0.2   |                   69 |
| train_ccef6_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67743)[0m rmse: 0.1843489110469818
[2m[36m(func pid=67743)[0m mae:  0.10305733978748322
[2m[36m(func pid=67743)[0m rmse_per_class: [0.079, 0.297, 0.034, 0.315, 0.171, 0.189, 0.285, 0.122, 0.135, 0.217]
[2m[36m(func pid=67743)[0m 
[2m[36m(func pid=66954)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4555 | Steps: 2 | Val loss: 0.3481 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=67327)[0m rmse: 0.14831256866455078
[2m[36m(func pid=67327)[0m mae:  0.10313892364501953
[2m[36m(func pid=67327)[0m rmse_per_class: [0.099, 0.231, 0.044, 0.292, 0.054, 0.176, 0.238, 0.124, 0.136, 0.09]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.20096655189990997
[2m[36m(func pid=68170)[0m mae:  0.11366154998540878
[2m[36m(func pid=68170)[0m rmse_per_class: [0.181, 0.314, 0.033, 0.351, 0.097, 0.271, 0.283, 0.132, 0.152, 0.195]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=67743)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3426 | Steps: 2 | Val loss: 0.3620 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=66954)[0m rmse: 0.17560246586799622
[2m[36m(func pid=66954)[0m mae:  0.1286049485206604
[2m[36m(func pid=66954)[0m rmse_per_class: [0.117, 0.257, 0.089, 0.331, 0.093, 0.19, 0.289, 0.141, 0.142, 0.108]
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.4449 | Steps: 2 | Val loss: 0.3508 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=67743)[0m rmse: 0.18538250029087067
[2m[36m(func pid=67743)[0m mae:  0.10414369404315948
[2m[36m(func pid=67743)[0m rmse_per_class: [0.078, 0.294, 0.032, 0.316, 0.175, 0.192, 0.301, 0.124, 0.134, 0.207]
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3237 | Steps: 2 | Val loss: 0.5089 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=67327)[0m rmse: 0.14794757962226868
[2m[36m(func pid=67327)[0m mae:  0.10272232443094254
[2m[36m(func pid=67327)[0m rmse_per_class: [0.098, 0.23, 0.043, 0.291, 0.055, 0.176, 0.237, 0.124, 0.136, 0.09]
[2m[36m(func pid=68170)[0m rmse: 0.20331907272338867
[2m[36m(func pid=68170)[0m mae:  0.11585891246795654
[2m[36m(func pid=68170)[0m rmse_per_class: [0.234, 0.315, 0.033, 0.35, 0.093, 0.273, 0.267, 0.132, 0.15, 0.186]
== Status ==
Current time: 2024-01-07 06:58:45 (running for 00:07:16.71)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16200000047683716
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 3 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.454 |  0.148 |                   75 |
| train_ccef6_00003 | RUNNING    | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.357 |  0.201 |                   70 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 06:58:51 (running for 00:07:23.53)
Memory usage on this node: 20.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16200000047683716
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 3 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.454 |  0.148 |                   75 |
| train_ccef6_00003 | RUNNING    | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.324 |  0.203 |                   71 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=84353)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84353)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=84353)[0m Configuration completed!
[2m[36m(func pid=84353)[0m New optimizer parameters:
[2m[36m(func pid=84353)[0m SGD (
[2m[36m(func pid=84353)[0m Parameter Group 0
[2m[36m(func pid=84353)[0m     dampening: 0
[2m[36m(func pid=84353)[0m     differentiable: False
[2m[36m(func pid=84353)[0m     foreach: None
[2m[36m(func pid=84353)[0m     lr: 0.0001
[2m[36m(func pid=84353)[0m     maximize: False
[2m[36m(func pid=84353)[0m     momentum: 0.9
[2m[36m(func pid=84353)[0m     nesterov: False
[2m[36m(func pid=84353)[0m     weight_decay: 0
[2m[36m(func pid=84353)[0m )
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.4443 | Steps: 2 | Val loss: 0.3482 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3305 | Steps: 2 | Val loss: 0.5129 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8982 | Steps: 2 | Val loss: 0.7089 | Batch size: 32 | lr: 0.0001 | Duration: 4.40s
[2m[36m(func pid=67327)[0m rmse: 0.14738982915878296
[2m[36m(func pid=67327)[0m mae:  0.10207287222146988
[2m[36m(func pid=67327)[0m rmse_per_class: [0.098, 0.23, 0.043, 0.291, 0.055, 0.175, 0.236, 0.124, 0.135, 0.089]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.20651738345623016
[2m[36m(func pid=68170)[0m mae:  0.11819437891244888
[2m[36m(func pid=68170)[0m rmse_per_class: [0.283, 0.314, 0.033, 0.349, 0.088, 0.244, 0.269, 0.145, 0.149, 0.192]
[2m[36m(func pid=84353)[0m rmse: 0.18272939324378967
[2m[36m(func pid=84353)[0m mae:  0.134456068277359
[2m[36m(func pid=84353)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
== Status ==
Current time: 2024-01-07 06:58:57 (running for 00:07:28.82)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16200000047683716
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.444 |  0.147 |                   77 |
| train_ccef6_00003 | RUNNING    | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.324 |  0.203 |                   71 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |        |        |                      |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.4386 | Steps: 2 | Val loss: 0.3449 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84788)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=84788)[0m Configuration completed!
[2m[36m(func pid=84788)[0m New optimizer parameters:
[2m[36m(func pid=84788)[0m SGD (
[2m[36m(func pid=84788)[0m Parameter Group 0
[2m[36m(func pid=84788)[0m     dampening: 0
[2m[36m(func pid=84788)[0m     differentiable: False
[2m[36m(func pid=84788)[0m     foreach: None
[2m[36m(func pid=84788)[0m     lr: 0.001
[2m[36m(func pid=84788)[0m     maximize: False
[2m[36m(func pid=84788)[0m     momentum: 0.9
[2m[36m(func pid=84788)[0m     nesterov: False
[2m[36m(func pid=84788)[0m     weight_decay: 0
[2m[36m(func pid=84788)[0m )
[2m[36m(func pid=84788)[0m 
== Status ==
Current time: 2024-01-07 06:59:02 (running for 00:07:34.00)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16200000047683716
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.439 |  0.147 |                   78 |
| train_ccef6_00003 | RUNNING    | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.331 |  0.207 |                   72 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.898 |  0.183 |                    1 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |        |        |                      |
| train_ccef6_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.147013321518898
[2m[36m(func pid=67327)[0m mae:  0.10167185217142105
[2m[36m(func pid=67327)[0m rmse_per_class: [0.097, 0.229, 0.043, 0.29, 0.055, 0.175, 0.235, 0.124, 0.135, 0.088]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8943 | Steps: 2 | Val loss: 0.7069 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3665 | Steps: 2 | Val loss: 0.5177 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8963 | Steps: 2 | Val loss: 0.7075 | Batch size: 32 | lr: 0.001 | Duration: 4.29s
[2m[36m(func pid=68170)[0m rmse: 0.20953400433063507
[2m[36m(func pid=68170)[0m mae:  0.12049072980880737
[2m[36m(func pid=68170)[0m rmse_per_class: [0.309, 0.317, 0.032, 0.348, 0.085, 0.208, 0.276, 0.173, 0.15, 0.196]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.18261191248893738
[2m[36m(func pid=84353)[0m mae:  0.1344067007303238
[2m[36m(func pid=84353)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.113]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.4392 | Steps: 2 | Val loss: 0.3415 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=84788)[0m rmse: 0.1827387660741806
[2m[36m(func pid=84788)[0m mae:  0.13446076214313507
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=84788)[0m 
== Status ==
Current time: 2024-01-07 06:59:07 (running for 00:07:39.21)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16200000047683716
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.439 |  0.147 |                   79 |
| train_ccef6_00003 | RUNNING    | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.367 |  0.21  |                   73 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.894 |  0.183 |                    2 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.896 |  0.183 |                    1 |
| train_ccef6_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.1468055695295334
[2m[36m(func pid=67327)[0m mae:  0.10139147192239761
[2m[36m(func pid=67327)[0m rmse_per_class: [0.097, 0.229, 0.043, 0.29, 0.055, 0.174, 0.235, 0.123, 0.135, 0.088]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3517 | Steps: 2 | Val loss: 0.5237 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8957 | Steps: 2 | Val loss: 0.7030 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8916 | Steps: 2 | Val loss: 0.6993 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=84353)[0m rmse: 0.18223418295383453
[2m[36m(func pid=84353)[0m mae:  0.13413284718990326
[2m[36m(func pid=84353)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.113, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.4294 | Steps: 2 | Val loss: 0.3388 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=68170)[0m rmse: 0.21270298957824707
[2m[36m(func pid=68170)[0m mae:  0.12268576771020889
[2m[36m(func pid=68170)[0m rmse_per_class: [0.315, 0.319, 0.032, 0.348, 0.085, 0.186, 0.279, 0.207, 0.15, 0.207]
[2m[36m(func pid=68170)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.1825428009033203
[2m[36m(func pid=84788)[0m mae:  0.1343672126531601
[2m[36m(func pid=84788)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.113]
[2m[36m(func pid=84788)[0m 
== Status ==
Current time: 2024-01-07 06:59:12 (running for 00:07:44.38)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16200000047683716
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.429 |  0.147 |                   80 |
| train_ccef6_00003 | RUNNING    | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.352 |  0.213 |                   74 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.896 |  0.182 |                    3 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.892 |  0.183 |                    2 |
| train_ccef6_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.14653350412845612
[2m[36m(func pid=67327)[0m mae:  0.10104928910732269
[2m[36m(func pid=67327)[0m rmse_per_class: [0.096, 0.229, 0.042, 0.29, 0.055, 0.173, 0.234, 0.123, 0.134, 0.087]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8968 | Steps: 2 | Val loss: 0.6998 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=68170)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3599 | Steps: 2 | Val loss: 0.5261 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8830 | Steps: 2 | Val loss: 0.6891 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.4248 | Steps: 2 | Val loss: 0.3357 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=84353)[0m rmse: 0.1818467378616333
[2m[36m(func pid=84353)[0m mae:  0.13382165133953094
[2m[36m(func pid=84353)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.339, 0.113, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=68170)[0m rmse: 0.21437084674835205
[2m[36m(func pid=68170)[0m mae:  0.12369350343942642
[2m[36m(func pid=68170)[0m rmse_per_class: [0.304, 0.321, 0.031, 0.347, 0.084, 0.183, 0.275, 0.227, 0.15, 0.223]
[2m[36m(func pid=84788)[0m rmse: 0.18217989802360535
[2m[36m(func pid=84788)[0m mae:  0.13409669697284698
[2m[36m(func pid=84788)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.14623810350894928
[2m[36m(func pid=67327)[0m mae:  0.10073665529489517
[2m[36m(func pid=67327)[0m rmse_per_class: [0.095, 0.229, 0.042, 0.29, 0.055, 0.173, 0.233, 0.123, 0.134, 0.087]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8924 | Steps: 2 | Val loss: 0.6975 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8705 | Steps: 2 | Val loss: 0.6777 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.4160 | Steps: 2 | Val loss: 0.3324 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=84353)[0m rmse: 0.18147632479667664
[2m[36m(func pid=84353)[0m mae:  0.1335185021162033
[2m[36m(func pid=84353)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.112, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=84788)[0m rmse: 0.181829035282135
[2m[36m(func pid=84788)[0m mae:  0.1338139772415161
[2m[36m(func pid=84788)[0m rmse_per_class: [0.117, 0.266, 0.105, 0.339, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=67327)[0m rmse: 0.14604324102401733
[2m[36m(func pid=67327)[0m mae:  0.10046035051345825
[2m[36m(func pid=67327)[0m rmse_per_class: [0.095, 0.229, 0.042, 0.29, 0.055, 0.173, 0.232, 0.123, 0.134, 0.087]
== Status ==
Current time: 2024-01-07 06:59:18 (running for 00:07:49.66)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.425 |  0.146 |                   81 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.897 |  0.182 |                    4 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.883 |  0.182 |                    3 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 06:59:24 (running for 00:07:56.48)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.416 |  0.146 |                   82 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.897 |  0.182 |                    4 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.883 |  0.182 |                    3 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=86022)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=86022)[0m Configuration completed!
[2m[36m(func pid=86022)[0m New optimizer parameters:
[2m[36m(func pid=86022)[0m SGD (
[2m[36m(func pid=86022)[0m Parameter Group 0
[2m[36m(func pid=86022)[0m     dampening: 0
[2m[36m(func pid=86022)[0m     differentiable: False
[2m[36m(func pid=86022)[0m     foreach: None
[2m[36m(func pid=86022)[0m     lr: 0.01
[2m[36m(func pid=86022)[0m     maximize: False
[2m[36m(func pid=86022)[0m     momentum: 0.9
[2m[36m(func pid=86022)[0m     nesterov: False
[2m[36m(func pid=86022)[0m     weight_decay: 0
[2m[36m(func pid=86022)[0m )
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8934 | Steps: 2 | Val loss: 0.6950 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8582 | Steps: 2 | Val loss: 0.6639 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.4181 | Steps: 2 | Val loss: 0.3288 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8885 | Steps: 2 | Val loss: 0.6862 | Batch size: 32 | lr: 0.01 | Duration: 4.50s
== Status ==
Current time: 2024-01-07 06:59:29 (running for 00:08:01.51)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.416 |  0.146 |                   82 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.892 |  0.181 |                    5 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.87  |  0.182 |                    4 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |        |        |                      |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.18112926185131073
[2m[36m(func pid=84353)[0m mae:  0.1332281082868576
[2m[36m(func pid=84353)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.112, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.18136179447174072
[2m[36m(func pid=84788)[0m mae:  0.13343313336372375
[2m[36m(func pid=84788)[0m rmse_per_class: [0.117, 0.265, 0.104, 0.338, 0.112, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.14572851359844208
[2m[36m(func pid=67327)[0m mae:  0.1001087874174118
[2m[36m(func pid=67327)[0m rmse_per_class: [0.095, 0.228, 0.042, 0.289, 0.055, 0.173, 0.232, 0.123, 0.134, 0.087]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.18271994590759277
[2m[36m(func pid=86022)[0m mae:  0.13444669544696808
[2m[36m(func pid=86022)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.11, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8397 | Steps: 2 | Val loss: 0.6504 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.4182 | Steps: 2 | Val loss: 0.3255 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8887 | Steps: 2 | Val loss: 0.6936 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8456 | Steps: 2 | Val loss: 0.6348 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 06:59:35 (running for 00:08:06.87)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.418 |  0.146 |                   83 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.889 |  0.181 |                    7 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.858 |  0.181 |                    5 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.889 |  0.183 |                    1 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.18080571293830872
[2m[36m(func pid=84353)[0m mae:  0.1329486072063446
[2m[36m(func pid=84353)[0m rmse_per_class: [0.116, 0.264, 0.103, 0.338, 0.112, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.14539678394794464
[2m[36m(func pid=67327)[0m mae:  0.09965743124485016
[2m[36m(func pid=67327)[0m rmse_per_class: [0.094, 0.228, 0.042, 0.288, 0.055, 0.172, 0.231, 0.123, 0.134, 0.087]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.18097397685050964
[2m[36m(func pid=84788)[0m mae:  0.1331024467945099
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.111, 0.19, 0.293, 0.142, 0.142, 0.11]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.18247558176517487
[2m[36m(func pid=86022)[0m mae:  0.13429872691631317
[2m[36m(func pid=86022)[0m rmse_per_class: [0.117, 0.267, 0.107, 0.339, 0.111, 0.19, 0.294, 0.144, 0.143, 0.113]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8876 | Steps: 2 | Val loss: 0.6918 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.4054 | Steps: 2 | Val loss: 0.3221 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8243 | Steps: 2 | Val loss: 0.6358 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7698 | Steps: 2 | Val loss: 0.5701 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 06:59:40 (running for 00:08:12.11)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.418 |  0.145 |                   84 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.888 |  0.181 |                    8 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.84  |  0.181 |                    6 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.846 |  0.182 |                    2 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.1452249437570572
[2m[36m(func pid=67327)[0m mae:  0.09935297071933746
[2m[36m(func pid=67327)[0m rmse_per_class: [0.093, 0.229, 0.042, 0.288, 0.055, 0.172, 0.231, 0.123, 0.134, 0.086]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.18054498732089996
[2m[36m(func pid=84353)[0m mae:  0.1327243447303772
[2m[36m(func pid=84353)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.18058544397354126
[2m[36m(func pid=84788)[0m mae:  0.13276800513267517
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.111, 0.19, 0.293, 0.142, 0.142, 0.109]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.18195882439613342
[2m[36m(func pid=86022)[0m mae:  0.13392716646194458
[2m[36m(func pid=86022)[0m rmse_per_class: [0.118, 0.267, 0.105, 0.339, 0.11, 0.19, 0.293, 0.143, 0.143, 0.112]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.4211 | Steps: 2 | Val loss: 0.3183 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8844 | Steps: 2 | Val loss: 0.6906 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8077 | Steps: 2 | Val loss: 0.6218 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6827 | Steps: 2 | Val loss: 0.5046 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 06:59:45 (running for 00:08:17.40)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.405 |  0.145 |                   85 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.884 |  0.18  |                    9 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.824 |  0.181 |                    7 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.77  |  0.182 |                    3 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.1803610622882843
[2m[36m(func pid=84353)[0m mae:  0.1325577199459076
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.263, 0.102, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.18027545511722565
[2m[36m(func pid=84788)[0m mae:  0.13250470161437988
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.263, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.1449389010667801
[2m[36m(func pid=67327)[0m mae:  0.09902962297201157
[2m[36m(func pid=67327)[0m rmse_per_class: [0.092, 0.229, 0.041, 0.288, 0.055, 0.171, 0.23, 0.123, 0.134, 0.086]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.18123184144496918
[2m[36m(func pid=86022)[0m mae:  0.13338133692741394
[2m[36m(func pid=86022)[0m rmse_per_class: [0.118, 0.266, 0.103, 0.338, 0.108, 0.19, 0.292, 0.143, 0.143, 0.112]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7906 | Steps: 2 | Val loss: 0.6078 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.4076 | Steps: 2 | Val loss: 0.3151 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8841 | Steps: 2 | Val loss: 0.6899 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.6017 | Steps: 2 | Val loss: 0.4464 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=67327)[0m rmse: 0.14467528462409973
[2m[36m(func pid=67327)[0m mae:  0.09868680685758591
[2m[36m(func pid=67327)[0m rmse_per_class: [0.092, 0.229, 0.041, 0.288, 0.055, 0.171, 0.229, 0.122, 0.134, 0.086]
[2m[36m(func pid=67327)[0m 
== Status ==
Current time: 2024-01-07 06:59:51 (running for 00:08:22.73)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.408 |  0.145 |                   87 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.884 |  0.18  |                    9 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.808 |  0.18  |                    8 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.683 |  0.181 |                    4 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.18016818165779114
[2m[36m(func pid=84353)[0m mae:  0.13239134848117828
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.263, 0.101, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17994727194309235
[2m[36m(func pid=84788)[0m mae:  0.1322217583656311
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.263, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.141, 0.109]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.180296391248703
[2m[36m(func pid=86022)[0m mae:  0.13266566395759583
[2m[36m(func pid=86022)[0m rmse_per_class: [0.119, 0.266, 0.1, 0.337, 0.104, 0.189, 0.292, 0.142, 0.143, 0.111]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8822 | Steps: 2 | Val loss: 0.6881 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3985 | Steps: 2 | Val loss: 0.3119 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7725 | Steps: 2 | Val loss: 0.5938 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5377 | Steps: 2 | Val loss: 0.4002 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 06:59:56 (running for 00:08:27.92)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.408 |  0.145 |                   87 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.884 |  0.18  |                   10 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.772 |  0.18  |                   10 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.602 |  0.18  |                    5 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.14445751905441284
[2m[36m(func pid=67327)[0m mae:  0.09835074841976166
[2m[36m(func pid=67327)[0m rmse_per_class: [0.091, 0.228, 0.041, 0.288, 0.055, 0.171, 0.229, 0.122, 0.133, 0.085]
[2m[36m(func pid=84788)[0m rmse: 0.17975737154483795
[2m[36m(func pid=84788)[0m mae:  0.13206125795841217
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.262, 0.1, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.1800227165222168
[2m[36m(func pid=84353)[0m mae:  0.1322670876979828
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.17923176288604736
[2m[36m(func pid=86022)[0m mae:  0.1318262815475464
[2m[36m(func pid=86022)[0m rmse_per_class: [0.119, 0.265, 0.097, 0.335, 0.1, 0.189, 0.291, 0.142, 0.143, 0.111]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7537 | Steps: 2 | Val loss: 0.5804 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3969 | Steps: 2 | Val loss: 0.3091 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8788 | Steps: 2 | Val loss: 0.6866 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 07:00:01 (running for 00:08:33.16)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.397 |  0.144 |                   89 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.882 |  0.18  |                   11 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.772 |  0.18  |                   10 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.538 |  0.179 |                    6 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.1443023979663849
[2m[36m(func pid=67327)[0m mae:  0.09807323664426804
[2m[36m(func pid=67327)[0m rmse_per_class: [0.091, 0.228, 0.041, 0.288, 0.055, 0.171, 0.229, 0.122, 0.133, 0.085]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17955321073532104
[2m[36m(func pid=84788)[0m mae:  0.13187576830387115
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.262, 0.099, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4857 | Steps: 2 | Val loss: 0.3660 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=84353)[0m rmse: 0.1799454391002655
[2m[36m(func pid=84353)[0m mae:  0.13219991326332092
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.1779794842004776
[2m[36m(func pid=86022)[0m mae:  0.13081885874271393
[2m[36m(func pid=86022)[0m rmse_per_class: [0.119, 0.264, 0.094, 0.333, 0.096, 0.189, 0.29, 0.142, 0.143, 0.11]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3878 | Steps: 2 | Val loss: 0.3058 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7392 | Steps: 2 | Val loss: 0.5674 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8771 | Steps: 2 | Val loss: 0.6847 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 07:00:06 (running for 00:08:38.27)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.388 |  0.144 |                   90 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.879 |  0.18  |                   12 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.754 |  0.18  |                   11 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.486 |  0.178 |                    7 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.14398150146007538
[2m[36m(func pid=67327)[0m mae:  0.09774382412433624
[2m[36m(func pid=67327)[0m rmse_per_class: [0.09, 0.229, 0.041, 0.287, 0.055, 0.17, 0.228, 0.122, 0.133, 0.085]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17941609025001526
[2m[36m(func pid=84788)[0m mae:  0.13176366686820984
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.262, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4539 | Steps: 2 | Val loss: 0.3424 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=84353)[0m rmse: 0.1798747479915619
[2m[36m(func pid=84353)[0m mae:  0.1321420818567276
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.17668022215366364
[2m[36m(func pid=86022)[0m mae:  0.12975910305976868
[2m[36m(func pid=86022)[0m rmse_per_class: [0.119, 0.262, 0.091, 0.331, 0.092, 0.188, 0.288, 0.141, 0.143, 0.11]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3864 | Steps: 2 | Val loss: 0.3025 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7209 | Steps: 2 | Val loss: 0.5548 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8742 | Steps: 2 | Val loss: 0.6833 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 07:00:11 (running for 00:08:43.35)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.386 |  0.144 |                   91 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.877 |  0.18  |                   13 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.739 |  0.179 |                   12 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.454 |  0.177 |                    8 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.14372894167900085
[2m[36m(func pid=67327)[0m mae:  0.097398541867733
[2m[36m(func pid=67327)[0m rmse_per_class: [0.09, 0.229, 0.041, 0.287, 0.055, 0.17, 0.227, 0.122, 0.133, 0.085]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.1792149841785431
[2m[36m(func pid=84788)[0m mae:  0.13158568739891052
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4283 | Steps: 2 | Val loss: 0.3276 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=84353)[0m rmse: 0.17983880639076233
[2m[36m(func pid=84353)[0m mae:  0.1321088820695877
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3769 | Steps: 2 | Val loss: 0.2993 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=86022)[0m rmse: 0.1755034476518631
[2m[36m(func pid=86022)[0m mae:  0.1287941187620163
[2m[36m(func pid=86022)[0m rmse_per_class: [0.119, 0.261, 0.088, 0.33, 0.089, 0.188, 0.287, 0.141, 0.143, 0.109]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7065 | Steps: 2 | Val loss: 0.5419 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8716 | Steps: 2 | Val loss: 0.6810 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 07:00:16 (running for 00:08:48.35)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.377 |  0.144 |                   92 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.874 |  0.18  |                   14 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.721 |  0.179 |                   13 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.428 |  0.176 |                    9 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.14356914162635803
[2m[36m(func pid=67327)[0m mae:  0.09727205336093903
[2m[36m(func pid=67327)[0m rmse_per_class: [0.09, 0.229, 0.04, 0.286, 0.055, 0.169, 0.227, 0.122, 0.133, 0.085]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17903655767440796
[2m[36m(func pid=84788)[0m mae:  0.13142749667167664
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.1798008382320404
[2m[36m(func pid=84353)[0m mae:  0.13206887245178223
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4162 | Steps: 2 | Val loss: 0.3183 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3805 | Steps: 2 | Val loss: 0.2959 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=86022)[0m rmse: 0.17430207133293152
[2m[36m(func pid=86022)[0m mae:  0.127796933054924
[2m[36m(func pid=86022)[0m rmse_per_class: [0.119, 0.259, 0.085, 0.328, 0.086, 0.188, 0.286, 0.14, 0.143, 0.108]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.6902 | Steps: 2 | Val loss: 0.5301 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8692 | Steps: 2 | Val loss: 0.6787 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 07:00:21 (running for 00:08:53.46)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.38  |  0.143 |                   93 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.872 |  0.18  |                   15 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.706 |  0.179 |                   14 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.416 |  0.174 |                   10 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=67327)[0m rmse: 0.14326708018779755
[2m[36m(func pid=67327)[0m mae:  0.0968729555606842
[2m[36m(func pid=67327)[0m rmse_per_class: [0.089, 0.228, 0.041, 0.286, 0.055, 0.168, 0.226, 0.122, 0.133, 0.085]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17881594598293304
[2m[36m(func pid=84788)[0m mae:  0.1312483251094818
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.335, 0.107, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4060 | Steps: 2 | Val loss: 0.3130 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=84353)[0m rmse: 0.17972463369369507
[2m[36m(func pid=84353)[0m mae:  0.13199381530284882
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3825 | Steps: 2 | Val loss: 0.2927 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6747 | Steps: 2 | Val loss: 0.5187 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=86022)[0m rmse: 0.17303797602653503
[2m[36m(func pid=86022)[0m mae:  0.12675027549266815
[2m[36m(func pid=86022)[0m rmse_per_class: [0.118, 0.258, 0.083, 0.326, 0.083, 0.188, 0.284, 0.14, 0.143, 0.107]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8663 | Steps: 2 | Val loss: 0.6778 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=67327)[0m rmse: 0.1430496722459793
[2m[36m(func pid=67327)[0m mae:  0.09664492309093475
[2m[36m(func pid=67327)[0m rmse_per_class: [0.088, 0.229, 0.041, 0.286, 0.055, 0.168, 0.226, 0.121, 0.133, 0.085]
[2m[36m(func pid=67327)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17868728935718536
[2m[36m(func pid=84788)[0m mae:  0.13112613558769226
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.335, 0.107, 0.19, 0.292, 0.141, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 07:00:27 (running for 00:08:58.95)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.383 |  0.143 |                   94 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.869 |  0.18  |                   16 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.675 |  0.179 |                   16 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.406 |  0.173 |                   11 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=84788)[0m 

[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4007 | Steps: 2 | Val loss: 0.3103 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=84353)[0m rmse: 0.17967936396598816
[2m[36m(func pid=84353)[0m mae:  0.13194432854652405
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3836 | Steps: 2 | Val loss: 0.2897 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6622 | Steps: 2 | Val loss: 0.5078 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=86022)[0m rmse: 0.17202259600162506
[2m[36m(func pid=86022)[0m mae:  0.1259191334247589
[2m[36m(func pid=86022)[0m rmse_per_class: [0.118, 0.256, 0.081, 0.325, 0.081, 0.187, 0.283, 0.139, 0.143, 0.107]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8642 | Steps: 2 | Val loss: 0.6764 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=67327)[0m rmse: 0.14271962642669678
[2m[36m(func pid=67327)[0m mae:  0.0962212085723877
[2m[36m(func pid=67327)[0m rmse_per_class: [0.088, 0.229, 0.041, 0.285, 0.055, 0.167, 0.225, 0.121, 0.132, 0.084]
[2m[36m(func pid=67327)[0m 
== Status ==
Current time: 2024-01-07 07:00:32 (running for 00:09:04.18)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.384 |  0.143 |                   95 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.866 |  0.18  |                   17 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.662 |  0.179 |                   17 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.401 |  0.172 |                   12 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84788)[0m rmse: 0.17855258285999298
[2m[36m(func pid=84788)[0m mae:  0.1310109794139862
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.17966172099113464
[2m[36m(func pid=84353)[0m mae:  0.13192850351333618
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3979 | Steps: 2 | Val loss: 0.3085 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3689 | Steps: 2 | Val loss: 0.2871 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6520 | Steps: 2 | Val loss: 0.4974 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8635 | Steps: 2 | Val loss: 0.6744 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=86022)[0m rmse: 0.17084535956382751
[2m[36m(func pid=86022)[0m mae:  0.1249370202422142
[2m[36m(func pid=86022)[0m rmse_per_class: [0.117, 0.255, 0.079, 0.324, 0.078, 0.187, 0.282, 0.139, 0.142, 0.106]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.14270636439323425
[2m[36m(func pid=67327)[0m mae:  0.09616532921791077
[2m[36m(func pid=67327)[0m rmse_per_class: [0.088, 0.229, 0.041, 0.285, 0.055, 0.167, 0.225, 0.121, 0.132, 0.085]
[2m[36m(func pid=67327)[0m 
== Status ==
Current time: 2024-01-07 07:00:37 (running for 00:09:09.51)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.369 |  0.143 |                   96 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.864 |  0.18  |                   18 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.652 |  0.178 |                   18 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.398 |  0.171 |                   13 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84788)[0m rmse: 0.1784505695104599
[2m[36m(func pid=84788)[0m mae:  0.13092544674873352
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.17958246171474457
[2m[36m(func pid=84353)[0m mae:  0.13185830414295197
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.111, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3958 | Steps: 2 | Val loss: 0.3075 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3722 | Steps: 2 | Val loss: 0.2834 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6365 | Steps: 2 | Val loss: 0.4885 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=86022)[0m rmse: 0.16966718435287476
[2m[36m(func pid=86022)[0m mae:  0.12394125759601593
[2m[36m(func pid=86022)[0m rmse_per_class: [0.116, 0.253, 0.077, 0.322, 0.076, 0.186, 0.28, 0.139, 0.142, 0.105]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8610 | Steps: 2 | Val loss: 0.6724 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=67327)[0m rmse: 0.1423323005437851
[2m[36m(func pid=67327)[0m mae:  0.09577760845422745
[2m[36m(func pid=67327)[0m rmse_per_class: [0.087, 0.229, 0.041, 0.283, 0.055, 0.166, 0.224, 0.12, 0.132, 0.085]
[2m[36m(func pid=67327)[0m 
== Status ==
Current time: 2024-01-07 07:00:43 (running for 00:09:14.64)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.372 |  0.142 |                   97 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.863 |  0.18  |                   19 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.636 |  0.178 |                   19 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.396 |  0.17  |                   14 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84788)[0m rmse: 0.17834950983524323
[2m[36m(func pid=84788)[0m mae:  0.13083979487419128
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.259, 0.096, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.1795441061258316
[2m[36m(func pid=84353)[0m mae:  0.1318313479423523
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3964 | Steps: 2 | Val loss: 0.3066 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3570 | Steps: 2 | Val loss: 0.2810 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6243 | Steps: 2 | Val loss: 0.4789 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8576 | Steps: 2 | Val loss: 0.6706 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=86022)[0m rmse: 0.16859658062458038
[2m[36m(func pid=86022)[0m mae:  0.12303636223077774
[2m[36m(func pid=86022)[0m rmse_per_class: [0.116, 0.252, 0.074, 0.321, 0.074, 0.186, 0.279, 0.138, 0.142, 0.105]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.1421298086643219
[2m[36m(func pid=67327)[0m mae:  0.09561116248369217
[2m[36m(func pid=67327)[0m rmse_per_class: [0.087, 0.229, 0.04, 0.282, 0.055, 0.166, 0.224, 0.12, 0.132, 0.085]
[2m[36m(func pid=67327)[0m 
== Status ==
Current time: 2024-01-07 07:00:48 (running for 00:09:20.05)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.357 |  0.142 |                   98 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.861 |  0.18  |                   20 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.624 |  0.178 |                   20 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.396 |  0.169 |                   15 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84788)[0m rmse: 0.1782117336988449
[2m[36m(func pid=84788)[0m mae:  0.1307239830493927
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.105, 0.19, 0.292, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.17951378226280212
[2m[36m(func pid=84353)[0m mae:  0.13180506229400635
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.111, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3957 | Steps: 2 | Val loss: 0.3059 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3635 | Steps: 2 | Val loss: 0.2786 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6128 | Steps: 2 | Val loss: 0.4705 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8569 | Steps: 2 | Val loss: 0.6683 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=86022)[0m rmse: 0.16768178343772888
[2m[36m(func pid=86022)[0m mae:  0.12226320803165436
[2m[36m(func pid=86022)[0m rmse_per_class: [0.115, 0.25, 0.073, 0.319, 0.073, 0.185, 0.278, 0.137, 0.141, 0.104]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.1420402228832245
[2m[36m(func pid=67327)[0m mae:  0.09545017778873444
[2m[36m(func pid=67327)[0m rmse_per_class: [0.087, 0.23, 0.04, 0.282, 0.055, 0.165, 0.223, 0.12, 0.132, 0.085]
[2m[36m(func pid=67327)[0m 
== Status ==
Current time: 2024-01-07 07:00:53 (running for 00:09:25.20)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00001 | RUNNING    | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.363 |  0.142 |                   99 |
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.858 |  0.18  |                   21 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.613 |  0.178 |                   21 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.396 |  0.168 |                   16 |
| train_ccef6_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=84788)[0m rmse: 0.17811036109924316

[2m[36m(func pid=84788)[0m mae:  0.13064420223236084
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.104, 0.19, 0.292, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.17950016260147095
[2m[36m(func pid=84353)[0m mae:  0.13179178535938263
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.111, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3930 | Steps: 2 | Val loss: 0.3052 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=67327)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3504 | Steps: 2 | Val loss: 0.2766 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6048 | Steps: 2 | Val loss: 0.4623 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8527 | Steps: 2 | Val loss: 0.6667 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=86022)[0m rmse: 0.16681928932666779
[2m[36m(func pid=86022)[0m mae:  0.12153555452823639
[2m[36m(func pid=86022)[0m rmse_per_class: [0.115, 0.249, 0.071, 0.319, 0.072, 0.185, 0.276, 0.137, 0.141, 0.103]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=67327)[0m rmse: 0.14195671677589417
[2m[36m(func pid=67327)[0m mae:  0.09538561850786209
[2m[36m(func pid=67327)[0m rmse_per_class: [0.087, 0.23, 0.04, 0.283, 0.055, 0.165, 0.223, 0.12, 0.132, 0.084]
[2m[36m(func pid=84788)[0m rmse: 0.17796000838279724
[2m[36m(func pid=84788)[0m mae:  0.1305432915687561
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.335, 0.104, 0.19, 0.292, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.17948587238788605
[2m[36m(func pid=84353)[0m mae:  0.13177938759326935
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3915 | Steps: 2 | Val loss: 0.3042 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5969 | Steps: 2 | Val loss: 0.4543 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=86022)[0m rmse: 0.16589698195457458
[2m[36m(func pid=86022)[0m mae:  0.12076399475336075
[2m[36m(func pid=86022)[0m rmse_per_class: [0.114, 0.248, 0.07, 0.317, 0.07, 0.184, 0.275, 0.136, 0.142, 0.103]
[2m[36m(func pid=84788)[0m rmse: 0.17783091962337494
[2m[36m(func pid=84788)[0m mae:  0.1304529309272766
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.334, 0.103, 0.19, 0.292, 0.14, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 07:00:58 (running for 00:09:30.24)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.857 |  0.18  |                   22 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.605 |  0.178 |                   22 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.393 |  0.167 |                   17 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 07:01:06 (running for 00:09:37.77)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.853 |  0.179 |                   23 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.605 |  0.178 |                   22 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.393 |  0.167 |                   17 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=90307)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=90307)[0m Configuration completed!
[2m[36m(func pid=90307)[0m New optimizer parameters:
[2m[36m(func pid=90307)[0m SGD (
[2m[36m(func pid=90307)[0m Parameter Group 0
[2m[36m(func pid=90307)[0m     dampening: 0
[2m[36m(func pid=90307)[0m     differentiable: False
[2m[36m(func pid=90307)[0m     foreach: None
[2m[36m(func pid=90307)[0m     lr: 0.1
[2m[36m(func pid=90307)[0m     maximize: False
[2m[36m(func pid=90307)[0m     momentum: 0.9
[2m[36m(func pid=90307)[0m     nesterov: False
[2m[36m(func pid=90307)[0m     weight_decay: 0
[2m[36m(func pid=90307)[0m )
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.8525 | Steps: 2 | Val loss: 0.6656 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5853 | Steps: 2 | Val loss: 0.4473 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3902 | Steps: 2 | Val loss: 0.3032 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8380 | Steps: 2 | Val loss: 0.5211 | Batch size: 32 | lr: 0.1 | Duration: 4.58s
== Status ==
Current time: 2024-01-07 07:01:11 (running for 00:09:42.79)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.853 |  0.179 |                   23 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.597 |  0.178 |                   23 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.391 |  0.166 |                   18 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |        |        |                      |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17941302061080933
[2m[36m(func pid=84353)[0m mae:  0.1317058503627777
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17775149643421173
[2m[36m(func pid=84788)[0m mae:  0.13038142025470734
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.334, 0.103, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.1650451123714447
[2m[36m(func pid=86022)[0m mae:  0.12003503739833832
[2m[36m(func pid=86022)[0m rmse_per_class: [0.114, 0.247, 0.068, 0.316, 0.069, 0.184, 0.273, 0.135, 0.142, 0.102]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.18249017000198364
[2m[36m(func pid=90307)[0m mae:  0.1342950314283371
[2m[36m(func pid=90307)[0m rmse_per_class: [0.118, 0.267, 0.108, 0.338, 0.106, 0.191, 0.293, 0.146, 0.144, 0.114]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.8468 | Steps: 2 | Val loss: 0.6633 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3910 | Steps: 2 | Val loss: 0.3022 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5784 | Steps: 2 | Val loss: 0.4402 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5512 | Steps: 2 | Val loss: 0.3537 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 07:01:16 (running for 00:09:48.19)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.847 |  0.179 |                   25 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.585 |  0.178 |                   24 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.39  |  0.165 |                   19 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.838 |  0.182 |                    1 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17942121624946594
[2m[36m(func pid=84353)[0m mae:  0.131712406873703
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17767225205898285
[2m[36m(func pid=84788)[0m mae:  0.13031363487243652
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.258, 0.094, 0.334, 0.103, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.16442352533340454
[2m[36m(func pid=86022)[0m mae:  0.11951510608196259
[2m[36m(func pid=86022)[0m rmse_per_class: [0.113, 0.247, 0.067, 0.316, 0.067, 0.183, 0.272, 0.135, 0.142, 0.102]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.18064352869987488
[2m[36m(func pid=90307)[0m mae:  0.133002370595932
[2m[36m(func pid=90307)[0m rmse_per_class: [0.123, 0.267, 0.101, 0.335, 0.094, 0.191, 0.29, 0.145, 0.146, 0.113]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.8466 | Steps: 2 | Val loss: 0.6609 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5689 | Steps: 2 | Val loss: 0.4341 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3869 | Steps: 2 | Val loss: 0.3007 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4140 | Steps: 2 | Val loss: 0.3221 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 07:01:21 (running for 00:09:53.25)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.847 |  0.179 |                   25 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.569 |  0.178 |                   26 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.391 |  0.164 |                   20 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.551 |  0.181 |                    2 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84788)[0m rmse: 0.17753438651561737
[2m[36m(func pid=84788)[0m mae:  0.13019894063472748
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.259, 0.093, 0.334, 0.102, 0.19, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.1794380098581314
[2m[36m(func pid=84353)[0m mae:  0.1317165046930313
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.111, 0.19, 0.293, 0.141, 0.142, 0.107]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.16368071734905243
[2m[36m(func pid=86022)[0m mae:  0.11890257894992828
[2m[36m(func pid=86022)[0m rmse_per_class: [0.113, 0.246, 0.066, 0.315, 0.066, 0.183, 0.271, 0.134, 0.142, 0.101]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.17719703912734985
[2m[36m(func pid=90307)[0m mae:  0.1304638683795929
[2m[36m(func pid=90307)[0m rmse_per_class: [0.126, 0.266, 0.091, 0.331, 0.079, 0.189, 0.284, 0.143, 0.15, 0.112]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5598 | Steps: 2 | Val loss: 0.4276 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.8417 | Steps: 2 | Val loss: 0.6596 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3848 | Steps: 2 | Val loss: 0.2996 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 07:01:26 (running for 00:09:58.56)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.847 |  0.179 |                   26 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.56  |  0.177 |                   27 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.387 |  0.164 |                   21 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.414 |  0.177 |                    3 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84788)[0m rmse: 0.17738446593284607
[2m[36m(func pid=84788)[0m mae:  0.1300813853740692
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.334, 0.101, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4225 | Steps: 2 | Val loss: 0.3407 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=84353)[0m rmse: 0.17944061756134033
[2m[36m(func pid=84353)[0m mae:  0.13172180950641632
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.1631074696779251
[2m[36m(func pid=86022)[0m mae:  0.11844972521066666
[2m[36m(func pid=86022)[0m rmse_per_class: [0.112, 0.246, 0.065, 0.314, 0.066, 0.182, 0.27, 0.134, 0.142, 0.101]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.17162171006202698
[2m[36m(func pid=90307)[0m mae:  0.12598712742328644
[2m[36m(func pid=90307)[0m rmse_per_class: [0.124, 0.263, 0.076, 0.324, 0.066, 0.187, 0.274, 0.139, 0.153, 0.109]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5528 | Steps: 2 | Val loss: 0.4218 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.8398 | Steps: 2 | Val loss: 0.6581 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3838 | Steps: 2 | Val loss: 0.2977 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 07:01:32 (running for 00:10:03.74)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.842 |  0.179 |                   27 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.553 |  0.177 |                   28 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.385 |  0.163 |                   22 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.423 |  0.172 |                    4 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84788)[0m rmse: 0.17726899683475494
[2m[36m(func pid=84788)[0m mae:  0.12998977303504944
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.334, 0.101, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.17940883338451385
[2m[36m(func pid=84353)[0m mae:  0.13169653713703156
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4598 | Steps: 2 | Val loss: 0.3587 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=86022)[0m rmse: 0.16224424540996552
[2m[36m(func pid=86022)[0m mae:  0.11769868433475494
[2m[36m(func pid=86022)[0m rmse_per_class: [0.111, 0.245, 0.064, 0.312, 0.065, 0.181, 0.269, 0.133, 0.142, 0.1]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.1655855029821396
[2m[36m(func pid=90307)[0m mae:  0.12071435153484344
[2m[36m(func pid=90307)[0m rmse_per_class: [0.12, 0.259, 0.063, 0.316, 0.059, 0.183, 0.263, 0.134, 0.157, 0.102]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5465 | Steps: 2 | Val loss: 0.4167 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.8376 | Steps: 2 | Val loss: 0.6560 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3808 | Steps: 2 | Val loss: 0.2961 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 07:01:37 (running for 00:10:08.86)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.84  |  0.179 |                   28 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.547 |  0.177 |                   29 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.384 |  0.162 |                   23 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.46  |  0.166 |                    5 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84788)[0m rmse: 0.17716577649116516
[2m[36m(func pid=84788)[0m mae:  0.12990280985832214
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.334, 0.101, 0.19, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.17941240966320038
[2m[36m(func pid=84353)[0m mae:  0.13170301914215088
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4786 | Steps: 2 | Val loss: 0.3606 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=86022)[0m rmse: 0.16146354377269745
[2m[36m(func pid=86022)[0m mae:  0.11702840030193329
[2m[36m(func pid=86022)[0m rmse_per_class: [0.111, 0.244, 0.062, 0.311, 0.064, 0.181, 0.268, 0.133, 0.141, 0.1]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5395 | Steps: 2 | Val loss: 0.4119 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=90307)[0m rmse: 0.15955142676830292
[2m[36m(func pid=90307)[0m mae:  0.1149488314986229
[2m[36m(func pid=90307)[0m rmse_per_class: [0.116, 0.254, 0.052, 0.307, 0.055, 0.179, 0.251, 0.129, 0.156, 0.097]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.8358 | Steps: 2 | Val loss: 0.6540 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3770 | Steps: 2 | Val loss: 0.2947 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 07:01:42 (running for 00:10:14.03)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.838 |  0.179 |                   29 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.54  |  0.177 |                   30 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.381 |  0.161 |                   24 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.479 |  0.16  |                    6 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84788)[0m rmse: 0.17716950178146362
[2m[36m(func pid=84788)[0m mae:  0.1299160271883011
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.334, 0.1, 0.19, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.17940565943717957
[2m[36m(func pid=84353)[0m mae:  0.13171212375164032
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.16077318787574768
[2m[36m(func pid=86022)[0m mae:  0.11644190549850464
[2m[36m(func pid=86022)[0m rmse_per_class: [0.11, 0.243, 0.061, 0.31, 0.064, 0.18, 0.267, 0.132, 0.141, 0.099]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4709 | Steps: 2 | Val loss: 0.3450 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5328 | Steps: 2 | Val loss: 0.4070 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=90307)[0m rmse: 0.15384843945503235
[2m[36m(func pid=90307)[0m mae:  0.10921899229288101
[2m[36m(func pid=90307)[0m rmse_per_class: [0.11, 0.247, 0.045, 0.298, 0.054, 0.175, 0.241, 0.124, 0.151, 0.092]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.8352 | Steps: 2 | Val loss: 0.6522 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3726 | Steps: 2 | Val loss: 0.2933 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=84788)[0m rmse: 0.17702126502990723
[2m[36m(func pid=84788)[0m mae:  0.12979574501514435
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.1, 0.19, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
== Status ==
Current time: 2024-01-07 07:01:47 (running for 00:10:19.17)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.836 |  0.179 |                   30 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.533 |  0.177 |                   31 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.377 |  0.161 |                   25 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.471 |  0.154 |                    7 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17941153049468994
[2m[36m(func pid=84353)[0m mae:  0.13171540200710297
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.16017267107963562
[2m[36m(func pid=86022)[0m mae:  0.11592515558004379
[2m[36m(func pid=86022)[0m rmse_per_class: [0.11, 0.243, 0.06, 0.309, 0.063, 0.18, 0.266, 0.132, 0.141, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4478 | Steps: 2 | Val loss: 0.3179 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5253 | Steps: 2 | Val loss: 0.4031 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.8314 | Steps: 2 | Val loss: 0.6498 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=90307)[0m rmse: 0.14905013144016266
[2m[36m(func pid=90307)[0m mae:  0.10412722826004028
[2m[36m(func pid=90307)[0m rmse_per_class: [0.102, 0.24, 0.041, 0.292, 0.055, 0.171, 0.233, 0.121, 0.147, 0.088]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3732 | Steps: 2 | Val loss: 0.2920 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 07:01:52 (running for 00:10:24.46)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.835 |  0.179 |                   31 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.525 |  0.177 |                   32 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.373 |  0.16  |                   26 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.448 |  0.149 |                    8 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84788)[0m rmse: 0.1768888384103775
[2m[36m(func pid=84788)[0m mae:  0.12967929244041443
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.1, 0.19, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.17938151955604553
[2m[36m(func pid=84353)[0m mae:  0.13168618083000183
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15961617231369019
[2m[36m(func pid=86022)[0m mae:  0.11545562744140625
[2m[36m(func pid=86022)[0m rmse_per_class: [0.109, 0.242, 0.059, 0.309, 0.063, 0.179, 0.265, 0.131, 0.141, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3960 | Steps: 2 | Val loss: 0.2895 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.5219 | Steps: 2 | Val loss: 0.3991 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8292 | Steps: 2 | Val loss: 0.6475 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=90307)[0m rmse: 0.14590984582901
[2m[36m(func pid=90307)[0m mae:  0.1005905270576477
[2m[36m(func pid=90307)[0m rmse_per_class: [0.098, 0.235, 0.039, 0.287, 0.055, 0.167, 0.229, 0.12, 0.143, 0.086]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3721 | Steps: 2 | Val loss: 0.2907 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=84788)[0m rmse: 0.17680147290229797
[2m[36m(func pid=84788)[0m mae:  0.1295982450246811
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.099, 0.19, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
== Status ==
Current time: 2024-01-07 07:01:58 (running for 00:10:29.60)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.831 |  0.179 |                   32 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.522 |  0.177 |                   33 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.373 |  0.16  |                   27 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.396 |  0.146 |                    9 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.1794261783361435
[2m[36m(func pid=84353)[0m mae:  0.13172443211078644
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15900911390781403
[2m[36m(func pid=86022)[0m mae:  0.11493466794490814
[2m[36m(func pid=86022)[0m rmse_per_class: [0.108, 0.242, 0.059, 0.308, 0.062, 0.178, 0.265, 0.131, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3571 | Steps: 2 | Val loss: 0.2698 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5123 | Steps: 2 | Val loss: 0.3950 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=90307)[0m rmse: 0.14396615326404572
[2m[36m(func pid=90307)[0m mae:  0.09865836799144745
[2m[36m(func pid=90307)[0m rmse_per_class: [0.098, 0.233, 0.037, 0.282, 0.055, 0.162, 0.226, 0.118, 0.143, 0.086]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8286 | Steps: 2 | Val loss: 0.6463 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3704 | Steps: 2 | Val loss: 0.2895 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 07:02:03 (running for 00:10:34.91)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.829 |  0.179 |                   33 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.512 |  0.177 |                   34 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.372 |  0.159 |                   28 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.357 |  0.144 |                   10 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84788)[0m rmse: 0.176717609167099
[2m[36m(func pid=84788)[0m mae:  0.12953342497348785
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.099, 0.189, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.17943458259105682
[2m[36m(func pid=84353)[0m mae:  0.13173454999923706
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15845754742622375
[2m[36m(func pid=86022)[0m mae:  0.11448261886835098
[2m[36m(func pid=86022)[0m rmse_per_class: [0.108, 0.241, 0.058, 0.307, 0.062, 0.177, 0.264, 0.13, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3285 | Steps: 2 | Val loss: 0.2657 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.5143 | Steps: 2 | Val loss: 0.3914 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8281 | Steps: 2 | Val loss: 0.6448 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=90307)[0m rmse: 0.14358389377593994
[2m[36m(func pid=90307)[0m mae:  0.09844927489757538
[2m[36m(func pid=90307)[0m rmse_per_class: [0.102, 0.232, 0.036, 0.28, 0.055, 0.158, 0.228, 0.116, 0.142, 0.088]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3722 | Steps: 2 | Val loss: 0.2885 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 07:02:08 (running for 00:10:40.19)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.829 |  0.179 |                   34 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.514 |  0.177 |                   35 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.37  |  0.158 |                   29 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.328 |  0.144 |                   11 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84788)[0m rmse: 0.176620215177536
[2m[36m(func pid=84788)[0m mae:  0.1294509768486023
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.258, 0.091, 0.333, 0.098, 0.189, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.17942044138908386
[2m[36m(func pid=84353)[0m mae:  0.13173724710941315
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.1580265909433365
[2m[36m(func pid=86022)[0m mae:  0.11412739753723145
[2m[36m(func pid=86022)[0m rmse_per_class: [0.107, 0.241, 0.057, 0.305, 0.062, 0.177, 0.264, 0.13, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3324 | Steps: 2 | Val loss: 0.2725 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.5081 | Steps: 2 | Val loss: 0.3880 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.8242 | Steps: 2 | Val loss: 0.6433 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=90307)[0m rmse: 0.1445549577474594
[2m[36m(func pid=90307)[0m mae:  0.09944295138120651
[2m[36m(func pid=90307)[0m rmse_per_class: [0.105, 0.232, 0.033, 0.279, 0.055, 0.155, 0.236, 0.115, 0.142, 0.093]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3636 | Steps: 2 | Val loss: 0.2875 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=84788)[0m rmse: 0.17657044529914856
[2m[36m(func pid=84788)[0m mae:  0.1293906271457672
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.332, 0.098, 0.19, 0.29, 0.14, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 07:02:13 (running for 00:10:45.41)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.828 |  0.179 |                   35 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.508 |  0.177 |                   36 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.372 |  0.158 |                   30 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.332 |  0.145 |                   12 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.17937220633029938
[2m[36m(func pid=84353)[0m mae:  0.13169847428798676
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.157627135515213
[2m[36m(func pid=86022)[0m mae:  0.11380292475223541
[2m[36m(func pid=86022)[0m rmse_per_class: [0.107, 0.24, 0.057, 0.304, 0.062, 0.177, 0.263, 0.129, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3336 | Steps: 2 | Val loss: 0.2784 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5013 | Steps: 2 | Val loss: 0.3841 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.8212 | Steps: 2 | Val loss: 0.6420 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=90307)[0m rmse: 0.146446093916893
[2m[36m(func pid=90307)[0m mae:  0.101007379591465
[2m[36m(func pid=90307)[0m rmse_per_class: [0.108, 0.233, 0.031, 0.28, 0.055, 0.152, 0.246, 0.114, 0.145, 0.1]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3665 | Steps: 2 | Val loss: 0.2869 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=84788)[0m rmse: 0.17643152177333832
[2m[36m(func pid=84788)[0m mae:  0.12928076088428497
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.332, 0.098, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
== Status ==
Current time: 2024-01-07 07:02:20 (running for 00:10:51.79)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.821 |  0.179 |                   37 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.501 |  0.176 |                   37 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.364 |  0.158 |                   31 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.334 |  0.146 |                   13 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17941829562187195
[2m[36m(func pid=84353)[0m mae:  0.1317228078842163
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.1573859602212906
[2m[36m(func pid=86022)[0m mae:  0.11363770067691803
[2m[36m(func pid=86022)[0m rmse_per_class: [0.107, 0.24, 0.056, 0.304, 0.062, 0.176, 0.263, 0.129, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3309 | Steps: 2 | Val loss: 0.2775 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4992 | Steps: 2 | Val loss: 0.3809 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.8167 | Steps: 2 | Val loss: 0.6402 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3657 | Steps: 2 | Val loss: 0.2864 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=90307)[0m rmse: 0.14786453545093536
[2m[36m(func pid=90307)[0m mae:  0.10191944986581802
[2m[36m(func pid=90307)[0m rmse_per_class: [0.104, 0.235, 0.03, 0.281, 0.054, 0.15, 0.254, 0.114, 0.147, 0.109]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17627988755702972
[2m[36m(func pid=84788)[0m mae:  0.12916995584964752
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.332, 0.098, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
== Status ==
Current time: 2024-01-07 07:02:25 (running for 00:10:56.85)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.817 |  0.179 |                   38 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.499 |  0.176 |                   38 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.367 |  0.157 |                   32 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.331 |  0.148 |                   14 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.1793845146894455
[2m[36m(func pid=84353)[0m mae:  0.13169154524803162
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.1572457253932953
[2m[36m(func pid=86022)[0m mae:  0.11356648057699203
[2m[36m(func pid=86022)[0m rmse_per_class: [0.107, 0.24, 0.056, 0.304, 0.061, 0.176, 0.263, 0.129, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3177 | Steps: 2 | Val loss: 0.2730 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4941 | Steps: 2 | Val loss: 0.3780 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8170 | Steps: 2 | Val loss: 0.6384 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3656 | Steps: 2 | Val loss: 0.2858 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=90307)[0m rmse: 0.14807173609733582
[2m[36m(func pid=90307)[0m mae:  0.10141877084970474
[2m[36m(func pid=90307)[0m rmse_per_class: [0.093, 0.238, 0.031, 0.279, 0.053, 0.149, 0.257, 0.115, 0.147, 0.119]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17617599666118622
[2m[36m(func pid=84788)[0m mae:  0.12908677756786346
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.332, 0.098, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
== Status ==
Current time: 2024-01-07 07:02:30 (running for 00:11:02.16)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.817 |  0.179 |                   39 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.494 |  0.176 |                   39 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.366 |  0.157 |                   33 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.318 |  0.148 |                   15 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17935284972190857
[2m[36m(func pid=84353)[0m mae:  0.13166561722755432
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15697145462036133
[2m[36m(func pid=86022)[0m mae:  0.11335334926843643
[2m[36m(func pid=86022)[0m rmse_per_class: [0.106, 0.239, 0.056, 0.304, 0.061, 0.175, 0.263, 0.128, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3091 | Steps: 2 | Val loss: 0.2711 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4901 | Steps: 2 | Val loss: 0.3753 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.8129 | Steps: 2 | Val loss: 0.6367 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=90307)[0m rmse: 0.14793957769870758
[2m[36m(func pid=90307)[0m mae:  0.10030309855937958
[2m[36m(func pid=90307)[0m rmse_per_class: [0.084, 0.241, 0.034, 0.278, 0.053, 0.149, 0.254, 0.115, 0.142, 0.129]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.1760498583316803
[2m[36m(func pid=84788)[0m mae:  0.12899553775787354
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.332, 0.097, 0.189, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3594 | Steps: 2 | Val loss: 0.2854 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=84788)[0m 
== Status ==
Current time: 2024-01-07 07:02:35 (running for 00:11:07.44)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.813 |  0.179 |                   40 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.49  |  0.176 |                   40 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.366 |  0.157 |                   34 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.309 |  0.148 |                   16 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17941631376743317
[2m[36m(func pid=84353)[0m mae:  0.13171283900737762
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.1567511409521103
[2m[36m(func pid=86022)[0m mae:  0.11319722980260849
[2m[36m(func pid=86022)[0m rmse_per_class: [0.106, 0.239, 0.055, 0.303, 0.061, 0.175, 0.263, 0.128, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.2933 | Steps: 2 | Val loss: 0.2724 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4879 | Steps: 2 | Val loss: 0.3724 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.8114 | Steps: 2 | Val loss: 0.6352 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=84788)[0m rmse: 0.17595037817955017
[2m[36m(func pid=84788)[0m mae:  0.12892335653305054
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.332, 0.097, 0.189, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.14789843559265137
[2m[36m(func pid=90307)[0m mae:  0.09918658435344696
[2m[36m(func pid=90307)[0m rmse_per_class: [0.078, 0.244, 0.036, 0.279, 0.058, 0.15, 0.246, 0.115, 0.138, 0.135]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3579 | Steps: 2 | Val loss: 0.2847 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 07:02:41 (running for 00:11:12.64)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.811 |  0.179 |                   41 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.488 |  0.176 |                   41 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.359 |  0.157 |                   35 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.293 |  0.148 |                   17 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.1794026792049408
[2m[36m(func pid=84353)[0m mae:  0.13169978559017181
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15641197562217712
[2m[36m(func pid=86022)[0m mae:  0.11290927976369858
[2m[36m(func pid=86022)[0m rmse_per_class: [0.106, 0.239, 0.055, 0.303, 0.061, 0.174, 0.262, 0.128, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4848 | Steps: 2 | Val loss: 0.3701 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2903 | Steps: 2 | Val loss: 0.2741 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.8085 | Steps: 2 | Val loss: 0.6339 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=84788)[0m rmse: 0.17592307925224304
[2m[36m(func pid=84788)[0m mae:  0.12890300154685974
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.331, 0.097, 0.189, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.1477077156305313
[2m[36m(func pid=90307)[0m mae:  0.09798549115657806
[2m[36m(func pid=90307)[0m rmse_per_class: [0.074, 0.245, 0.036, 0.28, 0.071, 0.152, 0.236, 0.114, 0.135, 0.134]
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3591 | Steps: 2 | Val loss: 0.2841 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.17936278879642487
[2m[36m(func pid=84353)[0m mae:  0.13166335225105286
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
== Status ==
Current time: 2024-01-07 07:02:46 (running for 00:11:17.85)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.809 |  0.179 |                   42 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.485 |  0.176 |                   42 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.358 |  0.156 |                   36 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.29  |  0.148 |                   18 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4784 | Steps: 2 | Val loss: 0.3679 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=86022)[0m rmse: 0.15608368813991547
[2m[36m(func pid=86022)[0m mae:  0.11262186616659164
[2m[36m(func pid=86022)[0m rmse_per_class: [0.105, 0.238, 0.054, 0.302, 0.061, 0.174, 0.262, 0.127, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.2880 | Steps: 2 | Val loss: 0.2753 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8077 | Steps: 2 | Val loss: 0.6318 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=84788)[0m rmse: 0.1758427917957306
[2m[36m(func pid=84788)[0m mae:  0.12884511053562164
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.331, 0.096, 0.189, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.14787884056568146
[2m[36m(func pid=90307)[0m mae:  0.09716237336397171
[2m[36m(func pid=90307)[0m rmse_per_class: [0.072, 0.246, 0.033, 0.282, 0.088, 0.156, 0.225, 0.113, 0.134, 0.13]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3589 | Steps: 2 | Val loss: 0.2838 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 07:02:51 (running for 00:11:23.24)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.808 |  0.179 |                   43 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.478 |  0.176 |                   43 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.359 |  0.156 |                   37 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.288 |  0.148 |                   19 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.1793481409549713
[2m[36m(func pid=84353)[0m mae:  0.13164815306663513
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4766 | Steps: 2 | Val loss: 0.3655 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=86022)[0m rmse: 0.15584059059619904
[2m[36m(func pid=86022)[0m mae:  0.11242316663265228
[2m[36m(func pid=86022)[0m rmse_per_class: [0.105, 0.238, 0.054, 0.302, 0.061, 0.173, 0.262, 0.127, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.2913 | Steps: 2 | Val loss: 0.2756 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8061 | Steps: 2 | Val loss: 0.6301 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=84788)[0m rmse: 0.17578914761543274
[2m[36m(func pid=84788)[0m mae:  0.12878987193107605
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.331, 0.096, 0.189, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3518 | Steps: 2 | Val loss: 0.2834 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=90307)[0m rmse: 0.14848138391971588
[2m[36m(func pid=90307)[0m mae:  0.09682638198137283
[2m[36m(func pid=90307)[0m rmse_per_class: [0.072, 0.247, 0.029, 0.285, 0.104, 0.158, 0.219, 0.112, 0.133, 0.126]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:02:56 (running for 00:11:28.51)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.806 |  0.179 |                   44 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.477 |  0.176 |                   44 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.359 |  0.156 |                   38 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.291 |  0.148 |                   20 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=84353)[0m rmse: 0.1793312132358551

[2m[36m(func pid=84353)[0m mae:  0.13164380192756653
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4750 | Steps: 2 | Val loss: 0.3629 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=86022)[0m rmse: 0.15561482310295105
[2m[36m(func pid=86022)[0m mae:  0.11223433166742325
[2m[36m(func pid=86022)[0m rmse_per_class: [0.104, 0.238, 0.054, 0.302, 0.061, 0.172, 0.262, 0.127, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.2946 | Steps: 2 | Val loss: 0.2738 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.8040 | Steps: 2 | Val loss: 0.6289 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=84788)[0m rmse: 0.17563214898109436
[2m[36m(func pid=84788)[0m mae:  0.12866291403770447
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.331, 0.096, 0.189, 0.29, 0.14, 0.141, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3511 | Steps: 2 | Val loss: 0.2830 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=90307)[0m rmse: 0.14843782782554626
[2m[36m(func pid=90307)[0m mae:  0.09631432592868805
[2m[36m(func pid=90307)[0m rmse_per_class: [0.072, 0.247, 0.027, 0.286, 0.109, 0.16, 0.215, 0.113, 0.133, 0.122]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:03:02 (running for 00:11:33.96)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.804 |  0.179 |                   45 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.475 |  0.176 |                   45 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.352 |  0.156 |                   39 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.295 |  0.148 |                   21 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17930111289024353
[2m[36m(func pid=84353)[0m mae:  0.13162389397621155
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4701 | Steps: 2 | Val loss: 0.3603 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=86022)[0m rmse: 0.15535250306129456
[2m[36m(func pid=86022)[0m mae:  0.1120210662484169
[2m[36m(func pid=86022)[0m rmse_per_class: [0.104, 0.237, 0.054, 0.301, 0.061, 0.172, 0.261, 0.127, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.2908 | Steps: 2 | Val loss: 0.2709 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=84788)[0m rmse: 0.17553307116031647
[2m[36m(func pid=84788)[0m mae:  0.1285923421382904
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.331, 0.095, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.8024 | Steps: 2 | Val loss: 0.6267 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3511 | Steps: 2 | Val loss: 0.2827 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=90307)[0m rmse: 0.14729730784893036
[2m[36m(func pid=90307)[0m mae:  0.09568615257740021
[2m[36m(func pid=90307)[0m rmse_per_class: [0.073, 0.246, 0.026, 0.285, 0.107, 0.16, 0.214, 0.114, 0.134, 0.114]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:03:07 (running for 00:11:39.11)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.802 |  0.179 |                   46 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.47  |  0.176 |                   46 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.351 |  0.155 |                   40 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.291 |  0.147 |                   22 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17930173873901367
[2m[36m(func pid=84353)[0m mae:  0.13163481652736664
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4691 | Steps: 2 | Val loss: 0.3585 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=86022)[0m rmse: 0.15521101653575897
[2m[36m(func pid=86022)[0m mae:  0.11188812553882599
[2m[36m(func pid=86022)[0m rmse_per_class: [0.104, 0.237, 0.053, 0.302, 0.061, 0.171, 0.261, 0.126, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2926 | Steps: 2 | Val loss: 0.2696 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=84788)[0m rmse: 0.17547304928302765
[2m[36m(func pid=84788)[0m mae:  0.12853768467903137
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.331, 0.095, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.7982 | Steps: 2 | Val loss: 0.6245 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3483 | Steps: 2 | Val loss: 0.2824 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=90307)[0m rmse: 0.14684176445007324
[2m[36m(func pid=90307)[0m mae:  0.09567330032587051
[2m[36m(func pid=90307)[0m rmse_per_class: [0.076, 0.246, 0.026, 0.288, 0.097, 0.159, 0.213, 0.114, 0.137, 0.112]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:03:12 (running for 00:11:44.24)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.798 |  0.179 |                   47 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.469 |  0.175 |                   47 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.351 |  0.155 |                   41 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.293 |  0.147 |                   23 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17927972972393036
[2m[36m(func pid=84353)[0m mae:  0.13161471486091614
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4673 | Steps: 2 | Val loss: 0.3572 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
[2m[36m(func pid=86022)[0m rmse: 0.15502695739269257
[2m[36m(func pid=86022)[0m mae:  0.11169300228357315
[2m[36m(func pid=86022)[0m rmse_per_class: [0.104, 0.237, 0.053, 0.301, 0.061, 0.171, 0.26, 0.126, 0.139, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2801 | Steps: 2 | Val loss: 0.2679 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=84788)[0m rmse: 0.1753707230091095
[2m[36m(func pid=84788)[0m mae:  0.1284467875957489
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.331, 0.095, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7952 | Steps: 2 | Val loss: 0.6226 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3495 | Steps: 2 | Val loss: 0.2822 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=90307)[0m rmse: 0.14582666754722595
[2m[36m(func pid=90307)[0m mae:  0.09510578215122223
[2m[36m(func pid=90307)[0m rmse_per_class: [0.076, 0.246, 0.027, 0.288, 0.086, 0.155, 0.214, 0.114, 0.139, 0.114]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4631 | Steps: 2 | Val loss: 0.3551 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
== Status ==
Current time: 2024-01-07 07:03:17 (running for 00:11:49.35)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.795 |  0.179 |                   48 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.467 |  0.175 |                   48 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.348 |  0.155 |                   42 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.28  |  0.146 |                   24 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17926393449306488
[2m[36m(func pid=84353)[0m mae:  0.13161197304725647
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.1549351066350937
[2m[36m(func pid=86022)[0m mae:  0.11160437762737274
[2m[36m(func pid=86022)[0m rmse_per_class: [0.104, 0.236, 0.053, 0.301, 0.061, 0.171, 0.26, 0.126, 0.139, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2772 | Steps: 2 | Val loss: 0.2681 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=84788)[0m rmse: 0.17527984082698822
[2m[36m(func pid=84788)[0m mae:  0.12837359309196472
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.256, 0.088, 0.331, 0.095, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7945 | Steps: 2 | Val loss: 0.6212 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3497 | Steps: 2 | Val loss: 0.2825 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=90307)[0m rmse: 0.1458750069141388
[2m[36m(func pid=90307)[0m mae:  0.09519770741462708
[2m[36m(func pid=90307)[0m rmse_per_class: [0.076, 0.247, 0.027, 0.289, 0.077, 0.152, 0.218, 0.113, 0.141, 0.119]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4624 | Steps: 2 | Val loss: 0.3533 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 07:03:22 (running for 00:11:54.49)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.794 |  0.179 |                   49 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.463 |  0.175 |                   49 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.35  |  0.155 |                   43 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.277 |  0.146 |                   25 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17922750115394592
[2m[36m(func pid=84353)[0m mae:  0.13157936930656433
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.155034601688385
[2m[36m(func pid=86022)[0m mae:  0.11170358955860138
[2m[36m(func pid=86022)[0m rmse_per_class: [0.104, 0.236, 0.053, 0.302, 0.061, 0.171, 0.26, 0.126, 0.14, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2694 | Steps: 2 | Val loss: 0.2688 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=84788)[0m rmse: 0.17516639828681946
[2m[36m(func pid=84788)[0m mae:  0.12828317284584045
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.256, 0.088, 0.331, 0.094, 0.189, 0.289, 0.14, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7929 | Steps: 2 | Val loss: 0.6198 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3454 | Steps: 2 | Val loss: 0.2824 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=90307)[0m rmse: 0.14613096415996552
[2m[36m(func pid=90307)[0m mae:  0.09550173580646515
[2m[36m(func pid=90307)[0m rmse_per_class: [0.075, 0.249, 0.026, 0.29, 0.072, 0.15, 0.222, 0.113, 0.142, 0.122]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4608 | Steps: 2 | Val loss: 0.3519 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=84353)[0m rmse: 0.179275244474411
[2m[36m(func pid=84353)[0m mae:  0.13162103295326233
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 07:03:28 (running for 00:11:59.66)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.793 |  0.179 |                   50 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.462 |  0.175 |                   50 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.35  |  0.155 |                   44 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.269 |  0.146 |                   26 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15496748685836792
[2m[36m(func pid=86022)[0m mae:  0.11164498329162598
[2m[36m(func pid=86022)[0m rmse_per_class: [0.103, 0.236, 0.052, 0.302, 0.061, 0.171, 0.26, 0.125, 0.14, 0.099]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.1751338541507721
[2m[36m(func pid=84788)[0m mae:  0.12825416028499603
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.256, 0.088, 0.331, 0.094, 0.189, 0.289, 0.14, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2712 | Steps: 2 | Val loss: 0.2691 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7918 | Steps: 2 | Val loss: 0.6178 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3443 | Steps: 2 | Val loss: 0.2821 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4565 | Steps: 2 | Val loss: 0.3506 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=90307)[0m rmse: 0.14636071026325226
[2m[36m(func pid=90307)[0m mae:  0.09546913951635361
[2m[36m(func pid=90307)[0m rmse_per_class: [0.073, 0.25, 0.026, 0.289, 0.068, 0.149, 0.226, 0.113, 0.141, 0.128]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:03:33 (running for 00:12:04.80)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.792 |  0.179 |                   51 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.461 |  0.175 |                   51 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.345 |  0.155 |                   45 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.271 |  0.146 |                   27 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17924544215202332
[2m[36m(func pid=84353)[0m mae:  0.13159655034542084
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15475888550281525
[2m[36m(func pid=86022)[0m mae:  0.11145027726888657
[2m[36m(func pid=86022)[0m rmse_per_class: [0.103, 0.236, 0.052, 0.302, 0.061, 0.171, 0.259, 0.125, 0.14, 0.099]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17504478991031647
[2m[36m(func pid=84788)[0m mae:  0.12816616892814636
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.256, 0.088, 0.331, 0.094, 0.189, 0.288, 0.14, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2708 | Steps: 2 | Val loss: 0.2692 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7906 | Steps: 2 | Val loss: 0.6163 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3450 | Steps: 2 | Val loss: 0.2818 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4557 | Steps: 2 | Val loss: 0.3486 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 07:03:38 (running for 00:12:09.91)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.792 |  0.179 |                   51 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.457 |  0.175 |                   52 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.344 |  0.155 |                   46 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.271 |  0.146 |                   28 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17924775183200836
[2m[36m(func pid=84353)[0m mae:  0.13158978521823883
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.14633090794086456
[2m[36m(func pid=90307)[0m mae:  0.09519527852535248
[2m[36m(func pid=90307)[0m rmse_per_class: [0.071, 0.25, 0.026, 0.287, 0.065, 0.149, 0.229, 0.115, 0.14, 0.132]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15459828078746796
[2m[36m(func pid=86022)[0m mae:  0.11132047325372696
[2m[36m(func pid=86022)[0m rmse_per_class: [0.103, 0.236, 0.052, 0.302, 0.061, 0.17, 0.259, 0.125, 0.139, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17492523789405823
[2m[36m(func pid=84788)[0m mae:  0.1280651092529297
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.255, 0.088, 0.33, 0.094, 0.189, 0.288, 0.14, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.7865 | Steps: 2 | Val loss: 0.6146 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2733 | Steps: 2 | Val loss: 0.2695 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3467 | Steps: 2 | Val loss: 0.2817 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4517 | Steps: 2 | Val loss: 0.3473 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=84353)[0m rmse: 0.17929229140281677
[2m[36m(func pid=84353)[0m mae:  0.13162417709827423
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
== Status ==
Current time: 2024-01-07 07:03:43 (running for 00:12:15.18)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.787 |  0.179 |                   53 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.456 |  0.175 |                   53 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.345 |  0.155 |                   47 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.271 |  0.146 |                   28 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=90307)[0m rmse: 0.14627715945243835
[2m[36m(func pid=90307)[0m mae:  0.09479355812072754
[2m[36m(func pid=90307)[0m rmse_per_class: [0.069, 0.251, 0.025, 0.287, 0.063, 0.148, 0.227, 0.116, 0.139, 0.137]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17487111687660217
[2m[36m(func pid=84788)[0m mae:  0.12802310287952423
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.255, 0.088, 0.33, 0.094, 0.189, 0.288, 0.14, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15447571873664856
[2m[36m(func pid=86022)[0m mae:  0.11121126264333725
[2m[36m(func pid=86022)[0m rmse_per_class: [0.103, 0.235, 0.052, 0.302, 0.061, 0.171, 0.259, 0.125, 0.14, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.7853 | Steps: 2 | Val loss: 0.6129 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2669 | Steps: 2 | Val loss: 0.2705 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3416 | Steps: 2 | Val loss: 0.2813 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4523 | Steps: 2 | Val loss: 0.3458 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 07:03:48 (running for 00:12:20.38)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.785 |  0.179 |                   54 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.452 |  0.175 |                   54 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.347 |  0.154 |                   48 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.273 |  0.146 |                   29 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17922136187553406
[2m[36m(func pid=84353)[0m mae:  0.13156327605247498
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.14653202891349792
[2m[36m(func pid=90307)[0m mae:  0.09460566192865372
[2m[36m(func pid=90307)[0m rmse_per_class: [0.07, 0.253, 0.025, 0.289, 0.063, 0.148, 0.225, 0.116, 0.137, 0.14]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.1542297899723053
[2m[36m(func pid=86022)[0m mae:  0.11096616089344025
[2m[36m(func pid=86022)[0m rmse_per_class: [0.102, 0.235, 0.051, 0.302, 0.062, 0.17, 0.258, 0.124, 0.139, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.1748027503490448
[2m[36m(func pid=84788)[0m mae:  0.1279677003622055
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.255, 0.088, 0.33, 0.093, 0.189, 0.288, 0.14, 0.141, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.7821 | Steps: 2 | Val loss: 0.6110 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2709 | Steps: 2 | Val loss: 0.2716 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4515 | Steps: 2 | Val loss: 0.3445 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3445 | Steps: 2 | Val loss: 0.2813 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 07:03:54 (running for 00:12:25.71)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.782 |  0.179 |                   55 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.452 |  0.175 |                   55 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.342 |  0.154 |                   49 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.267 |  0.147 |                   30 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17919030785560608
[2m[36m(func pid=84353)[0m mae:  0.1315433233976364
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.1470346450805664
[2m[36m(func pid=90307)[0m mae:  0.09458689391613007
[2m[36m(func pid=90307)[0m rmse_per_class: [0.07, 0.254, 0.025, 0.29, 0.063, 0.149, 0.223, 0.116, 0.137, 0.144]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17473497986793518
[2m[36m(func pid=84788)[0m mae:  0.1279192417860031
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.256, 0.088, 0.33, 0.093, 0.189, 0.288, 0.14, 0.142, 0.108]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15409409999847412
[2m[36m(func pid=86022)[0m mae:  0.1108548492193222
[2m[36m(func pid=86022)[0m rmse_per_class: [0.102, 0.235, 0.051, 0.303, 0.061, 0.17, 0.257, 0.124, 0.139, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.7826 | Steps: 2 | Val loss: 0.6096 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2676 | Steps: 2 | Val loss: 0.2717 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4465 | Steps: 2 | Val loss: 0.3434 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3372 | Steps: 2 | Val loss: 0.2810 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 07:03:59 (running for 00:12:31.01)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.783 |  0.179 |                   56 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.452 |  0.175 |                   56 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.345 |  0.154 |                   50 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.271 |  0.147 |                   31 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17914089560508728
[2m[36m(func pid=84353)[0m mae:  0.1314983367919922
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.1468953937292099
[2m[36m(func pid=90307)[0m mae:  0.09427647292613983
[2m[36m(func pid=90307)[0m rmse_per_class: [0.07, 0.254, 0.025, 0.29, 0.064, 0.15, 0.221, 0.116, 0.136, 0.143]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17474445700645447
[2m[36m(func pid=84788)[0m mae:  0.1279405951499939
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.256, 0.088, 0.33, 0.092, 0.188, 0.288, 0.14, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15385723114013672
[2m[36m(func pid=86022)[0m mae:  0.11063561588525772
[2m[36m(func pid=86022)[0m rmse_per_class: [0.101, 0.235, 0.051, 0.303, 0.061, 0.169, 0.257, 0.124, 0.139, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7787 | Steps: 2 | Val loss: 0.6077 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2670 | Steps: 2 | Val loss: 0.2723 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4444 | Steps: 2 | Val loss: 0.3421 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3415 | Steps: 2 | Val loss: 0.2807 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=84353)[0m rmse: 0.1791173219680786
[2m[36m(func pid=84353)[0m mae:  0.1314759999513626
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
== Status ==
Current time: 2024-01-07 07:04:04 (running for 00:12:36.38)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.779 |  0.179 |                   57 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.447 |  0.175 |                   57 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.337 |  0.154 |                   51 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.268 |  0.147 |                   32 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=90307)[0m rmse: 0.14714278280735016
[2m[36m(func pid=90307)[0m mae:  0.09426148980855942
[2m[36m(func pid=90307)[0m rmse_per_class: [0.07, 0.254, 0.025, 0.292, 0.065, 0.15, 0.219, 0.116, 0.136, 0.144]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17460289597511292
[2m[36m(func pid=84788)[0m mae:  0.12782733142375946
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.256, 0.087, 0.33, 0.092, 0.188, 0.288, 0.14, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.1536766141653061
[2m[36m(func pid=86022)[0m mae:  0.11044569313526154
[2m[36m(func pid=86022)[0m rmse_per_class: [0.101, 0.235, 0.051, 0.303, 0.062, 0.169, 0.257, 0.124, 0.139, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7802 | Steps: 2 | Val loss: 0.6063 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2629 | Steps: 2 | Val loss: 0.2723 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4436 | Steps: 2 | Val loss: 0.3408 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3341 | Steps: 2 | Val loss: 0.2807 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 07:04:10 (running for 00:12:41.61)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.78  |  0.179 |                   58 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.444 |  0.175 |                   58 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.341 |  0.154 |                   52 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.267 |  0.147 |                   33 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17912180721759796
[2m[36m(func pid=84353)[0m mae:  0.13147518038749695
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.14714324474334717
[2m[36m(func pid=90307)[0m mae:  0.09412229061126709
[2m[36m(func pid=90307)[0m rmse_per_class: [0.071, 0.254, 0.025, 0.292, 0.065, 0.151, 0.218, 0.115, 0.136, 0.144]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17453202605247498
[2m[36m(func pid=84788)[0m mae:  0.12777861952781677
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.256, 0.087, 0.33, 0.092, 0.188, 0.288, 0.14, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15363554656505585
[2m[36m(func pid=86022)[0m mae:  0.1103852242231369
[2m[36m(func pid=86022)[0m rmse_per_class: [0.1, 0.235, 0.051, 0.303, 0.062, 0.169, 0.256, 0.124, 0.139, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7742 | Steps: 2 | Val loss: 0.6046 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2629 | Steps: 2 | Val loss: 0.2732 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4415 | Steps: 2 | Val loss: 0.3395 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3345 | Steps: 2 | Val loss: 0.2804 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 07:04:15 (running for 00:12:46.85)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.774 |  0.179 |                   59 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.444 |  0.175 |                   59 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.334 |  0.154 |                   53 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.263 |  0.147 |                   34 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.1790689080953598
[2m[36m(func pid=84353)[0m mae:  0.13142982125282288
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.14764215052127838
[2m[36m(func pid=90307)[0m mae:  0.09429154545068741
[2m[36m(func pid=90307)[0m rmse_per_class: [0.071, 0.255, 0.025, 0.294, 0.066, 0.151, 0.217, 0.115, 0.137, 0.144]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.1744224578142166
[2m[36m(func pid=84788)[0m mae:  0.1276785284280777
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.255, 0.087, 0.33, 0.092, 0.188, 0.288, 0.14, 0.141, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15339666604995728
[2m[36m(func pid=86022)[0m mae:  0.11017861217260361
[2m[36m(func pid=86022)[0m rmse_per_class: [0.1, 0.235, 0.05, 0.303, 0.062, 0.168, 0.256, 0.123, 0.139, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7746 | Steps: 2 | Val loss: 0.6030 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2680 | Steps: 2 | Val loss: 0.2735 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4412 | Steps: 2 | Val loss: 0.3385 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3372 | Steps: 2 | Val loss: 0.2801 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 07:04:20 (running for 00:12:52.07)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.775 |  0.179 |                   60 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.442 |  0.174 |                   60 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.335 |  0.153 |                   54 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.263 |  0.148 |                   35 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.1790725439786911
[2m[36m(func pid=84353)[0m mae:  0.13142512738704681
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.14785116910934448
[2m[36m(func pid=90307)[0m mae:  0.09427157044410706
[2m[36m(func pid=90307)[0m rmse_per_class: [0.073, 0.255, 0.025, 0.294, 0.067, 0.151, 0.216, 0.114, 0.139, 0.145]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.1743437647819519
[2m[36m(func pid=84788)[0m mae:  0.12761512398719788
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.255, 0.087, 0.33, 0.091, 0.188, 0.288, 0.14, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15316340327262878
[2m[36m(func pid=86022)[0m mae:  0.10997090488672256
[2m[36m(func pid=86022)[0m rmse_per_class: [0.099, 0.235, 0.05, 0.303, 0.062, 0.168, 0.255, 0.123, 0.139, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.7712 | Steps: 2 | Val loss: 0.6013 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2784 | Steps: 2 | Val loss: 0.2736 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4380 | Steps: 2 | Val loss: 0.3372 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3320 | Steps: 2 | Val loss: 0.2797 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 07:04:25 (running for 00:12:57.35)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.771 |  0.179 |                   61 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.441 |  0.174 |                   61 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.337 |  0.153 |                   55 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.268 |  0.148 |                   36 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.1790652722120285
[2m[36m(func pid=84353)[0m mae:  0.13141855597496033
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.14792586863040924
[2m[36m(func pid=90307)[0m mae:  0.09442563354969025
[2m[36m(func pid=90307)[0m rmse_per_class: [0.075, 0.255, 0.025, 0.294, 0.066, 0.15, 0.216, 0.115, 0.141, 0.141]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17425097525119781
[2m[36m(func pid=84788)[0m mae:  0.1275509148836136
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.33, 0.091, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15289092063903809
[2m[36m(func pid=86022)[0m mae:  0.1097271665930748
[2m[36m(func pid=86022)[0m rmse_per_class: [0.098, 0.235, 0.05, 0.302, 0.062, 0.167, 0.255, 0.123, 0.139, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7708 | Steps: 2 | Val loss: 0.5999 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2626 | Steps: 2 | Val loss: 0.2750 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4396 | Steps: 2 | Val loss: 0.3362 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3340 | Steps: 2 | Val loss: 0.2792 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 07:04:30 (running for 00:13:02.56)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.771 |  0.179 |                   62 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.438 |  0.174 |                   62 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.332 |  0.153 |                   56 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.278 |  0.148 |                   37 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.1790880560874939
[2m[36m(func pid=84353)[0m mae:  0.13145138323307037
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17415912449359894
[2m[36m(func pid=84788)[0m mae:  0.12748084962368011
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.329, 0.091, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.14909155666828156
[2m[36m(func pid=90307)[0m mae:  0.09504365175962448
[2m[36m(func pid=90307)[0m rmse_per_class: [0.078, 0.256, 0.025, 0.296, 0.068, 0.15, 0.219, 0.115, 0.141, 0.143]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15259841084480286
[2m[36m(func pid=86022)[0m mae:  0.10945039987564087
[2m[36m(func pid=86022)[0m rmse_per_class: [0.098, 0.235, 0.049, 0.3, 0.062, 0.167, 0.255, 0.123, 0.139, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.7666 | Steps: 2 | Val loss: 0.5989 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4352 | Steps: 2 | Val loss: 0.3354 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2592 | Steps: 2 | Val loss: 0.2758 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 07:04:36 (running for 00:13:07.79)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.767 |  0.179 |                   63 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.44  |  0.174 |                   63 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.334 |  0.153 |                   57 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.263 |  0.149 |                   38 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17911040782928467
[2m[36m(func pid=84353)[0m mae:  0.13145342469215393
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.107]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3320 | Steps: 2 | Val loss: 0.2784 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=84788)[0m rmse: 0.1740824282169342
[2m[36m(func pid=84788)[0m mae:  0.12742361426353455
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.329, 0.091, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.14980903267860413
[2m[36m(func pid=90307)[0m mae:  0.09542998671531677
[2m[36m(func pid=90307)[0m rmse_per_class: [0.078, 0.257, 0.025, 0.297, 0.073, 0.15, 0.22, 0.115, 0.142, 0.143]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15208521485328674
[2m[36m(func pid=86022)[0m mae:  0.10898822546005249
[2m[36m(func pid=86022)[0m rmse_per_class: [0.097, 0.235, 0.049, 0.298, 0.062, 0.167, 0.254, 0.123, 0.139, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.7659 | Steps: 2 | Val loss: 0.5980 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4342 | Steps: 2 | Val loss: 0.3345 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2610 | Steps: 2 | Val loss: 0.2763 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 07:04:41 (running for 00:13:13.13)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.766 |  0.179 |                   64 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.435 |  0.174 |                   64 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.332 |  0.152 |                   58 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.259 |  0.15  |                   39 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17912176251411438
[2m[36m(func pid=84353)[0m mae:  0.13145995140075684
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3374 | Steps: 2 | Val loss: 0.2782 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=84788)[0m rmse: 0.17401829361915588
[2m[36m(func pid=84788)[0m mae:  0.12738843262195587
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.329, 0.09, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.15006399154663086
[2m[36m(func pid=90307)[0m mae:  0.09550363570451736
[2m[36m(func pid=90307)[0m rmse_per_class: [0.077, 0.258, 0.026, 0.297, 0.072, 0.15, 0.22, 0.115, 0.142, 0.143]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15190908312797546
[2m[36m(func pid=86022)[0m mae:  0.10879448801279068
[2m[36m(func pid=86022)[0m rmse_per_class: [0.097, 0.235, 0.049, 0.298, 0.063, 0.166, 0.254, 0.122, 0.139, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7665 | Steps: 2 | Val loss: 0.5967 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4346 | Steps: 2 | Val loss: 0.3333 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2642 | Steps: 2 | Val loss: 0.2752 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 07:04:46 (running for 00:13:18.42)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.766 |  0.179 |                   65 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.434 |  0.174 |                   65 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.337 |  0.152 |                   59 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.261 |  0.15  |                   40 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.1791413128376007
[2m[36m(func pid=84353)[0m mae:  0.13146604597568512
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3308 | Steps: 2 | Val loss: 0.2782 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=84788)[0m rmse: 0.17389896512031555
[2m[36m(func pid=84788)[0m mae:  0.12730050086975098
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.255, 0.085, 0.329, 0.09, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.1495700180530548
[2m[36m(func pid=90307)[0m mae:  0.09492022544145584
[2m[36m(func pid=90307)[0m rmse_per_class: [0.075, 0.258, 0.026, 0.295, 0.073, 0.151, 0.22, 0.116, 0.14, 0.144]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.1518835425376892
[2m[36m(func pid=86022)[0m mae:  0.10874947160482407
[2m[36m(func pid=86022)[0m rmse_per_class: [0.096, 0.235, 0.048, 0.298, 0.063, 0.167, 0.254, 0.122, 0.139, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7622 | Steps: 2 | Val loss: 0.5948 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4333 | Steps: 2 | Val loss: 0.3323 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2648 | Steps: 2 | Val loss: 0.2754 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 07:04:52 (running for 00:13:23.79)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.762 |  0.179 |                   66 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.435 |  0.174 |                   66 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.331 |  0.152 |                   60 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.264 |  0.15  |                   41 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.179121732711792
[2m[36m(func pid=84353)[0m mae:  0.13145890831947327
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3321 | Steps: 2 | Val loss: 0.2779 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=84788)[0m rmse: 0.17378611862659454
[2m[36m(func pid=84788)[0m mae:  0.12720167636871338
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.255, 0.085, 0.329, 0.09, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.14965161681175232
[2m[36m(func pid=90307)[0m mae:  0.0947623997926712
[2m[36m(func pid=90307)[0m rmse_per_class: [0.073, 0.258, 0.026, 0.294, 0.072, 0.151, 0.219, 0.117, 0.139, 0.147]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.7595 | Steps: 2 | Val loss: 0.5937 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=86022)[0m rmse: 0.15168634057044983
[2m[36m(func pid=86022)[0m mae:  0.10858981311321259
[2m[36m(func pid=86022)[0m rmse_per_class: [0.096, 0.235, 0.048, 0.298, 0.063, 0.167, 0.253, 0.122, 0.139, 0.096]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4305 | Steps: 2 | Val loss: 0.3317 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2560 | Steps: 2 | Val loss: 0.2763 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=84353)[0m rmse: 0.17906546592712402
[2m[36m(func pid=84353)[0m mae:  0.1314195692539215
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 07:04:57 (running for 00:13:28.82)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.76  |  0.179 |                   67 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.433 |  0.174 |                   67 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.332 |  0.152 |                   61 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.265 |  0.15  |                   42 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3302 | Steps: 2 | Val loss: 0.2777 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=84788)[0m rmse: 0.17376956343650818
[2m[36m(func pid=84788)[0m mae:  0.12719759345054626
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.255, 0.085, 0.329, 0.09, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.15007485449314117
[2m[36m(func pid=90307)[0m mae:  0.09474380314350128
[2m[36m(func pid=90307)[0m rmse_per_class: [0.073, 0.258, 0.026, 0.295, 0.072, 0.15, 0.219, 0.118, 0.138, 0.151]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7597 | Steps: 2 | Val loss: 0.5918 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=86022)[0m rmse: 0.1515677273273468
[2m[36m(func pid=86022)[0m mae:  0.10840588808059692
[2m[36m(func pid=86022)[0m rmse_per_class: [0.096, 0.235, 0.048, 0.297, 0.063, 0.166, 0.253, 0.122, 0.138, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4319 | Steps: 2 | Val loss: 0.3312 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2579 | Steps: 2 | Val loss: 0.2766 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 07:05:02 (running for 00:13:33.95)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.76  |  0.179 |                   68 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.43  |  0.174 |                   68 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.33  |  0.152 |                   62 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.256 |  0.15  |                   43 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17905327677726746
[2m[36m(func pid=84353)[0m mae:  0.1314001977443695
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3287 | Steps: 2 | Val loss: 0.2776 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=84788)[0m rmse: 0.17367979884147644
[2m[36m(func pid=84788)[0m mae:  0.12713657319545746
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.254, 0.085, 0.329, 0.09, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.15005254745483398
[2m[36m(func pid=90307)[0m mae:  0.09466864168643951
[2m[36m(func pid=90307)[0m rmse_per_class: [0.074, 0.259, 0.025, 0.295, 0.072, 0.15, 0.219, 0.119, 0.137, 0.15]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7581 | Steps: 2 | Val loss: 0.5900 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=86022)[0m rmse: 0.1514841914176941
[2m[36m(func pid=86022)[0m mae:  0.10827948898077011
[2m[36m(func pid=86022)[0m rmse_per_class: [0.095, 0.235, 0.048, 0.297, 0.063, 0.166, 0.253, 0.122, 0.138, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4317 | Steps: 2 | Val loss: 0.3304 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=84353)[0m rmse: 0.1791277676820755
[2m[36m(func pid=84353)[0m mae:  0.1314728558063507
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
== Status ==
Current time: 2024-01-07 07:05:07 (running for 00:13:38.95)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.758 |  0.179 |                   69 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.432 |  0.174 |                   69 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.329 |  0.151 |                   63 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.258 |  0.15  |                   44 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2692 | Steps: 2 | Val loss: 0.2769 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=84788)[0m rmse: 0.17358963191509247
[2m[36m(func pid=84788)[0m mae:  0.12707003951072693
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.254, 0.085, 0.329, 0.089, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3300 | Steps: 2 | Val loss: 0.2772 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=90307)[0m rmse: 0.1500871628522873
[2m[36m(func pid=90307)[0m mae:  0.09475790709257126
[2m[36m(func pid=90307)[0m rmse_per_class: [0.074, 0.259, 0.025, 0.295, 0.069, 0.151, 0.219, 0.119, 0.138, 0.153]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.7512 | Steps: 2 | Val loss: 0.5895 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=86022)[0m rmse: 0.15125736594200134
[2m[36m(func pid=86022)[0m mae:  0.10801204293966293
[2m[36m(func pid=86022)[0m rmse_per_class: [0.094, 0.235, 0.047, 0.297, 0.063, 0.166, 0.252, 0.122, 0.138, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4324 | Steps: 2 | Val loss: 0.3296 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 07:05:12 (running for 00:13:44.10)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.751 |  0.179 |                   70 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.432 |  0.174 |                   70 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.33  |  0.151 |                   64 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.269 |  0.15  |                   45 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17916443943977356
[2m[36m(func pid=84353)[0m mae:  0.13151171803474426
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.108, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2644 | Steps: 2 | Val loss: 0.2775 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=84788)[0m rmse: 0.1735752522945404
[2m[36m(func pid=84788)[0m mae:  0.1270478069782257
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.254, 0.085, 0.329, 0.089, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3359 | Steps: 2 | Val loss: 0.2768 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.7524 | Steps: 2 | Val loss: 0.5874 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=90307)[0m rmse: 0.1503584235906601
[2m[36m(func pid=90307)[0m mae:  0.09487249702215195
[2m[36m(func pid=90307)[0m rmse_per_class: [0.074, 0.26, 0.025, 0.296, 0.068, 0.151, 0.22, 0.118, 0.138, 0.154]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15105798840522766
[2m[36m(func pid=86022)[0m mae:  0.10780705511569977
[2m[36m(func pid=86022)[0m rmse_per_class: [0.094, 0.235, 0.047, 0.296, 0.063, 0.166, 0.252, 0.122, 0.138, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4266 | Steps: 2 | Val loss: 0.3286 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 07:05:17 (running for 00:13:49.13)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.752 |  0.179 |                   71 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.432 |  0.174 |                   71 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.336 |  0.151 |                   65 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.264 |  0.15  |                   46 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17910993099212646
[2m[36m(func pid=84353)[0m mae:  0.13146711885929108
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2546 | Steps: 2 | Val loss: 0.2765 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=84788)[0m rmse: 0.17342421412467957
[2m[36m(func pid=84788)[0m mae:  0.126924529671669
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.254, 0.084, 0.328, 0.089, 0.188, 0.286, 0.139, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3257 | Steps: 2 | Val loss: 0.2767 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.7525 | Steps: 2 | Val loss: 0.5851 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=90307)[0m rmse: 0.14972802996635437
[2m[36m(func pid=90307)[0m mae:  0.09432660043239594
[2m[36m(func pid=90307)[0m rmse_per_class: [0.074, 0.26, 0.025, 0.294, 0.067, 0.15, 0.221, 0.117, 0.136, 0.153]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4253 | Steps: 2 | Val loss: 0.3281 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=86022)[0m rmse: 0.15088701248168945
[2m[36m(func pid=86022)[0m mae:  0.10761523246765137
[2m[36m(func pid=86022)[0m rmse_per_class: [0.094, 0.235, 0.047, 0.295, 0.063, 0.166, 0.252, 0.122, 0.138, 0.098]
[2m[36m(func pid=86022)[0m 
== Status ==
Current time: 2024-01-07 07:05:22 (running for 00:13:54.32)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.753 |  0.179 |                   72 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.427 |  0.173 |                   72 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.326 |  0.151 |                   66 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.255 |  0.15  |                   47 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17903806269168854
[2m[36m(func pid=84353)[0m mae:  0.13140833377838135
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.1734289526939392
[2m[36m(func pid=84788)[0m mae:  0.1269231140613556
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.254, 0.084, 0.328, 0.089, 0.188, 0.286, 0.139, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2556 | Steps: 2 | Val loss: 0.2767 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3308 | Steps: 2 | Val loss: 0.2762 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.7496 | Steps: 2 | Val loss: 0.5840 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=90307)[0m rmse: 0.14998073875904083
[2m[36m(func pid=90307)[0m mae:  0.09452829509973526
[2m[36m(func pid=90307)[0m rmse_per_class: [0.076, 0.26, 0.025, 0.295, 0.067, 0.15, 0.222, 0.116, 0.136, 0.153]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4269 | Steps: 2 | Val loss: 0.3274 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=86022)[0m rmse: 0.1505594551563263
[2m[36m(func pid=86022)[0m mae:  0.1072675809264183
[2m[36m(func pid=86022)[0m rmse_per_class: [0.093, 0.235, 0.046, 0.295, 0.063, 0.165, 0.251, 0.121, 0.138, 0.098]
[2m[36m(func pid=86022)[0m 
== Status ==
Current time: 2024-01-07 07:05:28 (running for 00:13:59.68)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.75  |  0.179 |                   73 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.425 |  0.173 |                   73 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.331 |  0.151 |                   67 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.256 |  0.15  |                   48 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17900338768959045
[2m[36m(func pid=84353)[0m mae:  0.13138338923454285
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17336422204971313
[2m[36m(func pid=84788)[0m mae:  0.12688322365283966
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.254, 0.084, 0.328, 0.088, 0.188, 0.286, 0.139, 0.142, 0.107]
[2m[36m(func pid=84788)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2611 | Steps: 2 | Val loss: 0.2771 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3283 | Steps: 2 | Val loss: 0.2763 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7491 | Steps: 2 | Val loss: 0.5830 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=90307)[0m rmse: 0.15021605789661407
[2m[36m(func pid=90307)[0m mae:  0.09464339911937714
[2m[36m(func pid=90307)[0m rmse_per_class: [0.077, 0.259, 0.025, 0.295, 0.066, 0.15, 0.221, 0.116, 0.137, 0.155]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=84788)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4264 | Steps: 2 | Val loss: 0.3263 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=86022)[0m rmse: 0.15062429010868073
[2m[36m(func pid=86022)[0m mae:  0.10731971263885498
[2m[36m(func pid=86022)[0m rmse_per_class: [0.093, 0.235, 0.046, 0.295, 0.063, 0.165, 0.251, 0.121, 0.139, 0.098]
[2m[36m(func pid=86022)[0m 
== Status ==
Current time: 2024-01-07 07:05:33 (running for 00:14:05.03)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16899999976158142
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00004 | RUNNING    | 192.168.7.53:84353 | 0.0001 |       0.9  |         0      |  0.749 |  0.179 |                   74 |
| train_ccef6_00005 | RUNNING    | 192.168.7.53:84788 | 0.001  |       0.9  |         0      |  0.427 |  0.173 |                   74 |
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022 | 0.01   |       0.9  |         0      |  0.328 |  0.151 |                   68 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307 | 0.1    |       0.9  |         0      |  0.261 |  0.15  |                   49 |
| train_ccef6_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954 | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327 | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743 | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170 | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84353)[0m rmse: 0.17897368967533112
[2m[36m(func pid=84353)[0m mae:  0.1313544064760208
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=84353)[0m 
[2m[36m(func pid=84788)[0m rmse: 0.17320522665977478
[2m[36m(func pid=84788)[0m mae:  0.12674903869628906
[2m[36m(func pid=84788)[0m rmse_per_class: [0.116, 0.254, 0.084, 0.328, 0.088, 0.188, 0.286, 0.139, 0.142, 0.107]
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2566 | Steps: 2 | Val loss: 0.2770 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3275 | Steps: 2 | Val loss: 0.2761 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=84353)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.7461 | Steps: 2 | Val loss: 0.5813 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=90307)[0m rmse: 0.14983752369880676
[2m[36m(func pid=90307)[0m mae:  0.0945567637681961
[2m[36m(func pid=90307)[0m rmse_per_class: [0.078, 0.26, 0.025, 0.297, 0.065, 0.15, 0.221, 0.116, 0.137, 0.149]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.15046550333499908
[2m[36m(func pid=86022)[0m mae:  0.10716371238231659
[2m[36m(func pid=86022)[0m rmse_per_class: [0.093, 0.235, 0.046, 0.294, 0.063, 0.165, 0.251, 0.121, 0.139, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=84353)[0m rmse: 0.1789541244506836
[2m[36m(func pid=84353)[0m mae:  0.131339892745018
[2m[36m(func pid=84353)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2567 | Steps: 2 | Val loss: 0.2770 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3237 | Steps: 2 | Val loss: 0.2759 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=90307)[0m rmse: 0.14970774948596954
[2m[36m(func pid=90307)[0m mae:  0.09431873261928558
[2m[36m(func pid=90307)[0m rmse_per_class: [0.077, 0.26, 0.025, 0.297, 0.064, 0.15, 0.22, 0.116, 0.137, 0.15]
== Status ==
Current time: 2024-01-07 07:05:38 (running for 00:14:10.34)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17374999448657036
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 3 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.328 |  0.15  |                   69 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.257 |  0.15  |                   50 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.15040867030620575
[2m[36m(func pid=86022)[0m mae:  0.10708652436733246
[2m[36m(func pid=86022)[0m rmse_per_class: [0.093, 0.235, 0.046, 0.294, 0.063, 0.165, 0.251, 0.121, 0.139, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=101638)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=101638)[0m Configuration completed!
[2m[36m(func pid=101638)[0m New optimizer parameters:
[2m[36m(func pid=101638)[0m SGD (
[2m[36m(func pid=101638)[0m Parameter Group 0
[2m[36m(func pid=101638)[0m     dampening: 0
[2m[36m(func pid=101638)[0m     differentiable: False
[2m[36m(func pid=101638)[0m     foreach: None
[2m[36m(func pid=101638)[0m     lr: 0.0001
[2m[36m(func pid=101638)[0m     maximize: False
[2m[36m(func pid=101638)[0m     momentum: 0.99
[2m[36m(func pid=101638)[0m     nesterov: False
[2m[36m(func pid=101638)[0m     weight_decay: 0.0001
[2m[36m(func pid=101638)[0m )
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3233 | Steps: 2 | Val loss: 0.2757 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2563 | Steps: 2 | Val loss: 0.2769 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8961 | Steps: 2 | Val loss: 0.7087 | Batch size: 32 | lr: 0.0001 | Duration: 4.36s
[2m[36m(func pid=86022)[0m rmse: 0.15033970773220062
[2m[36m(func pid=86022)[0m mae:  0.10698512941598892
[2m[36m(func pid=86022)[0m rmse_per_class: [0.093, 0.235, 0.046, 0.293, 0.063, 0.165, 0.25, 0.12, 0.14, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.14963912963867188
[2m[36m(func pid=90307)[0m mae:  0.09413614124059677
[2m[36m(func pid=90307)[0m rmse_per_class: [0.076, 0.26, 0.025, 0.297, 0.066, 0.15, 0.219, 0.116, 0.137, 0.15]
[2m[36m(func pid=101638)[0m rmse: 0.1827378123998642
[2m[36m(func pid=101638)[0m mae:  0.13446709513664246
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
== Status ==
Current time: 2024-01-07 07:05:48 (running for 00:14:19.72)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17374999448657036
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.323 |  0.15  |                   71 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.257 |  0.15  |                   51 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=102191)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=102191)[0m Configuration completed!
[2m[36m(func pid=102191)[0m New optimizer parameters:
[2m[36m(func pid=102191)[0m SGD (
[2m[36m(func pid=102191)[0m Parameter Group 0
[2m[36m(func pid=102191)[0m     dampening: 0
[2m[36m(func pid=102191)[0m     differentiable: False
[2m[36m(func pid=102191)[0m     foreach: None
[2m[36m(func pid=102191)[0m     lr: 0.001
[2m[36m(func pid=102191)[0m     maximize: False
[2m[36m(func pid=102191)[0m     momentum: 0.99
[2m[36m(func pid=102191)[0m     nesterov: False
[2m[36m(func pid=102191)[0m     weight_decay: 0.0001
[2m[36m(func pid=102191)[0m )
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3221 | Steps: 2 | Val loss: 0.2759 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 07:05:53 (running for 00:14:25.06)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17374999448657036
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.322 |  0.15  |                   72 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.256 |  0.15  |                   52 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.896 |  0.183 |                    1 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.15048560500144958
[2m[36m(func pid=86022)[0m mae:  0.10707143694162369
[2m[36m(func pid=86022)[0m rmse_per_class: [0.094, 0.235, 0.045, 0.294, 0.063, 0.165, 0.25, 0.12, 0.14, 0.099]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8985 | Steps: 2 | Val loss: 0.7049 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2574 | Steps: 2 | Val loss: 0.2777 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8984 | Steps: 2 | Val loss: 0.7069 | Batch size: 32 | lr: 0.001 | Duration: 4.46s
[2m[36m(func pid=101638)[0m rmse: 0.18253976106643677
[2m[36m(func pid=101638)[0m mae:  0.1343640238046646
[2m[36m(func pid=101638)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.113]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3216 | Steps: 2 | Val loss: 0.2754 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=90307)[0m rmse: 0.14987680315971375
[2m[36m(func pid=90307)[0m mae:  0.09422658383846283
[2m[36m(func pid=90307)[0m rmse_per_class: [0.075, 0.26, 0.025, 0.298, 0.066, 0.15, 0.219, 0.117, 0.137, 0.153]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1826723963022232
[2m[36m(func pid=102191)[0m mae:  0.1344081610441208
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=102191)[0m 
== Status ==
Current time: 2024-01-07 07:05:58 (running for 00:14:30.50)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17374999448657036
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.322 |  0.15  |                   73 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.257 |  0.15  |                   53 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.899 |  0.183 |                    2 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.898 |  0.183 |                    1 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.15007081627845764
[2m[36m(func pid=86022)[0m mae:  0.10670282691717148
[2m[36m(func pid=86022)[0m rmse_per_class: [0.094, 0.235, 0.045, 0.293, 0.063, 0.165, 0.249, 0.12, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8966 | Steps: 2 | Val loss: 0.7014 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2542 | Steps: 2 | Val loss: 0.2781 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8908 | Steps: 2 | Val loss: 0.6974 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=101638)[0m rmse: 0.18219569325447083
[2m[36m(func pid=101638)[0m mae:  0.1341138780117035
[2m[36m(func pid=101638)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.113, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.15010783076286316
[2m[36m(func pid=90307)[0m mae:  0.09425120055675507
[2m[36m(func pid=90307)[0m rmse_per_class: [0.074, 0.26, 0.025, 0.299, 0.066, 0.15, 0.219, 0.118, 0.138, 0.153]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3239 | Steps: 2 | Val loss: 0.2752 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=102191)[0m rmse: 0.1824740171432495
[2m[36m(func pid=102191)[0m mae:  0.13430288434028625
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.112]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8965 | Steps: 2 | Val loss: 0.6993 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 07:06:04 (running for 00:14:35.99)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17374999448657036
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.324 |  0.15  |                   74 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.254 |  0.15  |                   54 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.897 |  0.182 |                    3 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.891 |  0.182 |                    2 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.1499296873807907
[2m[36m(func pid=86022)[0m mae:  0.10657161474227905
[2m[36m(func pid=86022)[0m rmse_per_class: [0.094, 0.235, 0.045, 0.293, 0.063, 0.164, 0.249, 0.12, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2514 | Steps: 2 | Val loss: 0.2784 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8816 | Steps: 2 | Val loss: 0.6861 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=101638)[0m rmse: 0.18184413015842438
[2m[36m(func pid=101638)[0m mae:  0.1338338702917099
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.339, 0.113, 0.19, 0.294, 0.142, 0.143, 0.112]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.15013626217842102
[2m[36m(func pid=90307)[0m mae:  0.09416403621435165
[2m[36m(func pid=90307)[0m rmse_per_class: [0.073, 0.261, 0.025, 0.299, 0.066, 0.15, 0.218, 0.119, 0.138, 0.152]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3226 | Steps: 2 | Val loss: 0.2750 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=102191)[0m rmse: 0.1821633130311966
[2m[36m(func pid=102191)[0m mae:  0.13407376408576965
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8924 | Steps: 2 | Val loss: 0.6960 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 07:06:09 (running for 00:14:41.15)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.323 |  0.15  |                   75 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.15  |                   55 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.897 |  0.182 |                    4 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.882 |  0.182 |                    3 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.1497887521982193
[2m[36m(func pid=86022)[0m mae:  0.10637732595205307
[2m[36m(func pid=86022)[0m rmse_per_class: [0.093, 0.235, 0.045, 0.293, 0.063, 0.164, 0.249, 0.12, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2616 | Steps: 2 | Val loss: 0.2780 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8669 | Steps: 2 | Val loss: 0.6713 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=101638)[0m rmse: 0.18139776587486267
[2m[36m(func pid=101638)[0m mae:  0.13347169756889343
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.112, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3265 | Steps: 2 | Val loss: 0.2753 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=90307)[0m rmse: 0.15006081759929657
[2m[36m(func pid=90307)[0m mae:  0.09403063356876373
[2m[36m(func pid=90307)[0m rmse_per_class: [0.072, 0.261, 0.025, 0.297, 0.066, 0.15, 0.219, 0.12, 0.138, 0.153]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1817607879638672
[2m[36m(func pid=102191)[0m mae:  0.13375680148601532
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.266, 0.105, 0.338, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8890 | Steps: 2 | Val loss: 0.6930 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 07:06:14 (running for 00:14:46.32)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.327 |  0.15  |                   76 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.262 |  0.15  |                   56 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.892 |  0.181 |                    5 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.867 |  0.182 |                    4 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.1500283181667328
[2m[36m(func pid=86022)[0m mae:  0.10656659305095673
[2m[36m(func pid=86022)[0m rmse_per_class: [0.094, 0.235, 0.045, 0.294, 0.063, 0.164, 0.249, 0.12, 0.14, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2475 | Steps: 2 | Val loss: 0.2786 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8482 | Steps: 2 | Val loss: 0.6542 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=101638)[0m rmse: 0.18102404475212097
[2m[36m(func pid=101638)[0m mae:  0.13315001130104065
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.112, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3300 | Steps: 2 | Val loss: 0.2747 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=90307)[0m rmse: 0.1508525013923645
[2m[36m(func pid=90307)[0m mae:  0.09428863972425461
[2m[36m(func pid=90307)[0m rmse_per_class: [0.074, 0.26, 0.025, 0.296, 0.067, 0.15, 0.22, 0.12, 0.139, 0.156]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1813468635082245
[2m[36m(func pid=102191)[0m mae:  0.13341805338859558
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.265, 0.104, 0.338, 0.111, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8862 | Steps: 2 | Val loss: 0.6911 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 07:06:20 (running for 00:14:51.82)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.33  |  0.149 |                   77 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.248 |  0.151 |                   57 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.889 |  0.181 |                    6 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.848 |  0.181 |                    5 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.14949984848499298
[2m[36m(func pid=86022)[0m mae:  0.1061461940407753
[2m[36m(func pid=86022)[0m rmse_per_class: [0.093, 0.235, 0.044, 0.293, 0.063, 0.164, 0.248, 0.12, 0.14, 0.096]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2509 | Steps: 2 | Val loss: 0.2791 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8245 | Steps: 2 | Val loss: 0.6342 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=101638)[0m rmse: 0.18069617450237274
[2m[36m(func pid=101638)[0m mae:  0.1328626573085785
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.264, 0.103, 0.338, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3239 | Steps: 2 | Val loss: 0.2742 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=90307)[0m rmse: 0.1515970528125763
[2m[36m(func pid=90307)[0m mae:  0.09472234547138214
[2m[36m(func pid=90307)[0m rmse_per_class: [0.075, 0.26, 0.025, 0.296, 0.069, 0.151, 0.222, 0.119, 0.141, 0.157]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1809215247631073
[2m[36m(func pid=102191)[0m mae:  0.13305960595607758
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.265, 0.103, 0.338, 0.111, 0.19, 0.293, 0.142, 0.142, 0.11]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8796 | Steps: 2 | Val loss: 0.6890 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 07:06:25 (running for 00:14:57.20)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.324 |  0.149 |                   78 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.152 |                   58 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.886 |  0.181 |                    7 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.824 |  0.181 |                    6 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.14917317032814026
[2m[36m(func pid=86022)[0m mae:  0.1058553084731102
[2m[36m(func pid=86022)[0m rmse_per_class: [0.093, 0.235, 0.044, 0.292, 0.063, 0.163, 0.247, 0.119, 0.139, 0.095]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2463 | Steps: 2 | Val loss: 0.2803 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7992 | Steps: 2 | Val loss: 0.6118 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=101638)[0m rmse: 0.18048568069934845
[2m[36m(func pid=101638)[0m mae:  0.1326606571674347
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.264, 0.103, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.18052083253860474
[2m[36m(func pid=102191)[0m mae:  0.132722869515419
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.11]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3214 | Steps: 2 | Val loss: 0.2741 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=90307)[0m rmse: 0.1523672491312027
[2m[36m(func pid=90307)[0m mae:  0.09507935494184494
[2m[36m(func pid=90307)[0m rmse_per_class: [0.077, 0.261, 0.026, 0.298, 0.069, 0.151, 0.223, 0.119, 0.141, 0.159]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8816 | Steps: 2 | Val loss: 0.6853 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 07:06:31 (running for 00:15:02.61)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.321 |  0.149 |                   79 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.246 |  0.152 |                   59 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.88  |  0.18  |                    8 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.799 |  0.181 |                    7 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.14917810261249542
[2m[36m(func pid=86022)[0m mae:  0.1057981476187706
[2m[36m(func pid=86022)[0m rmse_per_class: [0.093, 0.235, 0.043, 0.292, 0.063, 0.163, 0.247, 0.119, 0.139, 0.096]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7699 | Steps: 2 | Val loss: 0.5881 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2492 | Steps: 2 | Val loss: 0.2814 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=101638)[0m rmse: 0.1802816092967987
[2m[36m(func pid=101638)[0m mae:  0.13248364627361298
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.263, 0.102, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.18006429076194763
[2m[36m(func pid=102191)[0m mae:  0.1323448121547699
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.263, 0.1, 0.337, 0.109, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3169 | Steps: 2 | Val loss: 0.2740 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=90307)[0m rmse: 0.15285739302635193
[2m[36m(func pid=90307)[0m mae:  0.09530138224363327
[2m[36m(func pid=90307)[0m rmse_per_class: [0.079, 0.261, 0.026, 0.299, 0.07, 0.151, 0.222, 0.119, 0.141, 0.161]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8758 | Steps: 2 | Val loss: 0.6830 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 07:06:36 (running for 00:15:08.04)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.317 |  0.149 |                   80 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.249 |  0.153 |                   60 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.882 |  0.18  |                    9 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.77  |  0.18  |                    8 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.1490430384874344
[2m[36m(func pid=86022)[0m mae:  0.10563743114471436
[2m[36m(func pid=86022)[0m rmse_per_class: [0.092, 0.235, 0.043, 0.293, 0.064, 0.163, 0.247, 0.119, 0.138, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7395 | Steps: 2 | Val loss: 0.5639 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2564 | Steps: 2 | Val loss: 0.2818 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=101638)[0m rmse: 0.18010278046131134
[2m[36m(func pid=101638)[0m mae:  0.13232530653476715
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.263, 0.101, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1797454059123993
[2m[36m(func pid=102191)[0m mae:  0.1320468932390213
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.263, 0.1, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3259 | Steps: 2 | Val loss: 0.2744 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=90307)[0m rmse: 0.15283994376659393
[2m[36m(func pid=90307)[0m mae:  0.09535543620586395
[2m[36m(func pid=90307)[0m rmse_per_class: [0.079, 0.262, 0.026, 0.3, 0.069, 0.151, 0.222, 0.118, 0.142, 0.16]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8718 | Steps: 2 | Val loss: 0.6793 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 07:06:41 (running for 00:15:13.38)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.326 |  0.149 |                   81 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.256 |  0.153 |                   61 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.876 |  0.18  |                   10 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.739 |  0.18  |                    9 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.1492987424135208
[2m[36m(func pid=86022)[0m mae:  0.10583041608333588
[2m[36m(func pid=86022)[0m rmse_per_class: [0.092, 0.235, 0.043, 0.294, 0.064, 0.163, 0.247, 0.119, 0.139, 0.097]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7089 | Steps: 2 | Val loss: 0.5386 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2479 | Steps: 2 | Val loss: 0.2810 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=101638)[0m rmse: 0.17996670305728912
[2m[36m(func pid=101638)[0m mae:  0.13220122456550598
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.17940808832645416
[2m[36m(func pid=102191)[0m mae:  0.1317659318447113
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.262, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.142, 0.109]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3163 | Steps: 2 | Val loss: 0.2741 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=90307)[0m rmse: 0.15222035348415375
[2m[36m(func pid=90307)[0m mae:  0.0949716717004776
[2m[36m(func pid=90307)[0m rmse_per_class: [0.08, 0.263, 0.025, 0.301, 0.07, 0.151, 0.22, 0.118, 0.14, 0.154]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8675 | Steps: 2 | Val loss: 0.6757 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6785 | Steps: 2 | Val loss: 0.5131 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 07:06:47 (running for 00:15:18.76)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.316 |  0.149 |                   82 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.248 |  0.152 |                   62 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.872 |  0.18  |                   11 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.709 |  0.179 |                   10 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.14905834197998047
[2m[36m(func pid=86022)[0m mae:  0.10555758327245712
[2m[36m(func pid=86022)[0m rmse_per_class: [0.091, 0.235, 0.043, 0.294, 0.064, 0.162, 0.246, 0.119, 0.138, 0.098]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2519 | Steps: 2 | Val loss: 0.2801 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=101638)[0m rmse: 0.17983265221118927
[2m[36m(func pid=101638)[0m mae:  0.13208863139152527
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.17903874814510345
[2m[36m(func pid=102191)[0m mae:  0.13144756853580475
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.262, 0.098, 0.335, 0.107, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3233 | Steps: 2 | Val loss: 0.2741 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=90307)[0m rmse: 0.15164494514465332
[2m[36m(func pid=90307)[0m mae:  0.09452293068170547
[2m[36m(func pid=90307)[0m rmse_per_class: [0.079, 0.263, 0.025, 0.3, 0.069, 0.151, 0.219, 0.118, 0.139, 0.152]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8623 | Steps: 2 | Val loss: 0.6714 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.6471 | Steps: 2 | Val loss: 0.4881 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 07:06:52 (running for 00:15:24.05)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.323 |  0.149 |                   83 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.252 |  0.152 |                   63 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.867 |  0.18  |                   12 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.678 |  0.179 |                   11 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.14911702275276184
[2m[36m(func pid=86022)[0m mae:  0.10551686584949493
[2m[36m(func pid=86022)[0m rmse_per_class: [0.09, 0.235, 0.042, 0.294, 0.064, 0.162, 0.246, 0.119, 0.139, 0.1]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2508 | Steps: 2 | Val loss: 0.2802 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=101638)[0m rmse: 0.1797541081905365
[2m[36m(func pid=101638)[0m mae:  0.13202443718910217
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.17861175537109375
[2m[36m(func pid=102191)[0m mae:  0.13109369575977325
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.261, 0.097, 0.335, 0.105, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3158 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=90307)[0m rmse: 0.15145912766456604
[2m[36m(func pid=90307)[0m mae:  0.09435364603996277
[2m[36m(func pid=90307)[0m rmse_per_class: [0.079, 0.263, 0.025, 0.301, 0.068, 0.151, 0.219, 0.118, 0.139, 0.151]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8541 | Steps: 2 | Val loss: 0.6671 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.6163 | Steps: 2 | Val loss: 0.4640 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 07:06:57 (running for 00:15:29.35)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.316 |  0.149 |                   84 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.151 |                   64 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.862 |  0.18  |                   13 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.647 |  0.179 |                   12 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.14892318844795227
[2m[36m(func pid=86022)[0m mae:  0.1051914244890213
[2m[36m(func pid=86022)[0m rmse_per_class: [0.089, 0.235, 0.042, 0.293, 0.064, 0.162, 0.246, 0.119, 0.138, 0.101]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2506 | Steps: 2 | Val loss: 0.2803 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=101638)[0m rmse: 0.17969436943531036
[2m[36m(func pid=101638)[0m mae:  0.13196629285812378
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1782296597957611
[2m[36m(func pid=102191)[0m mae:  0.1307872235774994
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.335, 0.104, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3117 | Steps: 2 | Val loss: 0.2732 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=90307)[0m rmse: 0.1514682173728943
[2m[36m(func pid=90307)[0m mae:  0.09433571249246597
[2m[36m(func pid=90307)[0m rmse_per_class: [0.078, 0.263, 0.025, 0.3, 0.069, 0.151, 0.219, 0.119, 0.139, 0.152]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8479 | Steps: 2 | Val loss: 0.6627 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5859 | Steps: 2 | Val loss: 0.4412 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 07:07:03 (running for 00:15:34.66)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.312 |  0.149 |                   85 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.151 |                   65 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.854 |  0.18  |                   14 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.616 |  0.178 |                   13 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.1485898345708847
[2m[36m(func pid=86022)[0m mae:  0.10488773882389069
[2m[36m(func pid=86022)[0m rmse_per_class: [0.088, 0.235, 0.042, 0.293, 0.065, 0.161, 0.245, 0.119, 0.138, 0.101]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17965325713157654
[2m[36m(func pid=101638)[0m mae:  0.13193804025650024
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2476 | Steps: 2 | Val loss: 0.2807 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=102191)[0m rmse: 0.1779196560382843
[2m[36m(func pid=102191)[0m mae:  0.13051822781562805
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.334, 0.103, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3164 | Steps: 2 | Val loss: 0.2731 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=90307)[0m rmse: 0.15169255435466766
[2m[36m(func pid=90307)[0m mae:  0.09431710094213486
[2m[36m(func pid=90307)[0m rmse_per_class: [0.076, 0.264, 0.025, 0.3, 0.071, 0.15, 0.22, 0.119, 0.138, 0.154]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8415 | Steps: 2 | Val loss: 0.6579 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5598 | Steps: 2 | Val loss: 0.4194 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 07:07:08 (running for 00:15:39.98)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.316 |  0.149 |                   86 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.248 |  0.152 |                   66 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.848 |  0.18  |                   15 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.586 |  0.178 |                   14 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.14850717782974243
[2m[36m(func pid=86022)[0m mae:  0.10476012527942657
[2m[36m(func pid=86022)[0m rmse_per_class: [0.088, 0.235, 0.041, 0.292, 0.064, 0.161, 0.245, 0.119, 0.138, 0.102]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17959609627723694
[2m[36m(func pid=101638)[0m mae:  0.13188983500003815
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2452 | Steps: 2 | Val loss: 0.2815 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=102191)[0m rmse: 0.1774728000164032
[2m[36m(func pid=102191)[0m mae:  0.1301577240228653
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.26, 0.093, 0.334, 0.101, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3131 | Steps: 2 | Val loss: 0.2730 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=90307)[0m rmse: 0.1520441472530365
[2m[36m(func pid=90307)[0m mae:  0.09454146772623062
[2m[36m(func pid=90307)[0m rmse_per_class: [0.077, 0.265, 0.025, 0.3, 0.071, 0.15, 0.22, 0.12, 0.137, 0.154]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8390 | Steps: 2 | Val loss: 0.6524 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5380 | Steps: 2 | Val loss: 0.4002 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 07:07:13 (running for 00:15:45.34)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.313 |  0.148 |                   87 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.245 |  0.152 |                   67 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.842 |  0.18  |                   16 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.56  |  0.177 |                   15 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.1484854817390442
[2m[36m(func pid=86022)[0m mae:  0.10466410964727402
[2m[36m(func pid=86022)[0m rmse_per_class: [0.087, 0.235, 0.041, 0.293, 0.064, 0.162, 0.244, 0.119, 0.138, 0.102]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17953571677207947
[2m[36m(func pid=101638)[0m mae:  0.13182860612869263
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2468 | Steps: 2 | Val loss: 0.2819 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=102191)[0m rmse: 0.17704737186431885
[2m[36m(func pid=102191)[0m mae:  0.12980888783931732
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.259, 0.092, 0.333, 0.099, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3143 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8282 | Steps: 2 | Val loss: 0.6464 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=90307)[0m rmse: 0.15220481157302856
[2m[36m(func pid=90307)[0m mae:  0.09449869394302368
[2m[36m(func pid=90307)[0m rmse_per_class: [0.078, 0.265, 0.025, 0.3, 0.07, 0.15, 0.221, 0.121, 0.137, 0.156]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5123 | Steps: 2 | Val loss: 0.3826 | Batch size: 32 | lr: 0.001 | Duration: 2.62s
== Status ==
Current time: 2024-01-07 07:07:19 (running for 00:15:50.69)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.313 |  0.148 |                   87 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.247 |  0.152 |                   68 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.828 |  0.18  |                   18 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.538 |  0.177 |                   16 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.1484588235616684
[2m[36m(func pid=86022)[0m mae:  0.10458259284496307
[2m[36m(func pid=86022)[0m rmse_per_class: [0.088, 0.235, 0.041, 0.293, 0.064, 0.161, 0.244, 0.119, 0.138, 0.103]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17950312793254852
[2m[36m(func pid=101638)[0m mae:  0.13180139660835266
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2461 | Steps: 2 | Val loss: 0.2833 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=102191)[0m rmse: 0.17661580443382263
[2m[36m(func pid=102191)[0m mae:  0.12944522500038147
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.259, 0.092, 0.333, 0.097, 0.19, 0.289, 0.141, 0.141, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3151 | Steps: 2 | Val loss: 0.2730 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8239 | Steps: 2 | Val loss: 0.6402 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=90307)[0m rmse: 0.15303993225097656
[2m[36m(func pid=90307)[0m mae:  0.0948534831404686
[2m[36m(func pid=90307)[0m rmse_per_class: [0.079, 0.265, 0.025, 0.3, 0.071, 0.151, 0.221, 0.123, 0.137, 0.158]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4919 | Steps: 2 | Val loss: 0.3673 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 07:07:24 (running for 00:15:56.00)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.315 |  0.148 |                   89 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.246 |  0.153 |                   69 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.828 |  0.18  |                   18 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.512 |  0.177 |                   17 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.1484953761100769
[2m[36m(func pid=86022)[0m mae:  0.10458735376596451
[2m[36m(func pid=86022)[0m rmse_per_class: [0.087, 0.236, 0.04, 0.293, 0.064, 0.161, 0.244, 0.118, 0.138, 0.104]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17943206429481506
[2m[36m(func pid=101638)[0m mae:  0.1317427009344101
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2517 | Steps: 2 | Val loss: 0.2839 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=102191)[0m rmse: 0.17617157101631165
[2m[36m(func pid=102191)[0m mae:  0.12910178303718567
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.258, 0.09, 0.332, 0.096, 0.189, 0.289, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3187 | Steps: 2 | Val loss: 0.2724 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8151 | Steps: 2 | Val loss: 0.6337 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=90307)[0m rmse: 0.15320758521556854
[2m[36m(func pid=90307)[0m mae:  0.09490062296390533
[2m[36m(func pid=90307)[0m rmse_per_class: [0.079, 0.266, 0.025, 0.301, 0.07, 0.151, 0.221, 0.123, 0.137, 0.159]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4729 | Steps: 2 | Val loss: 0.3544 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 07:07:29 (running for 00:16:01.26)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.315 |  0.148 |                   89 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.252 |  0.153 |                   70 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.815 |  0.179 |                   20 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.492 |  0.176 |                   18 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.14801116287708282
[2m[36m(func pid=86022)[0m mae:  0.10420002788305283
[2m[36m(func pid=86022)[0m rmse_per_class: [0.087, 0.235, 0.04, 0.292, 0.064, 0.161, 0.243, 0.118, 0.137, 0.101]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.1794072985649109
[2m[36m(func pid=101638)[0m mae:  0.13172796368598938
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2500 | Steps: 2 | Val loss: 0.2835 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=102191)[0m rmse: 0.17576871812343597
[2m[36m(func pid=102191)[0m mae:  0.12878181040287018
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.257, 0.089, 0.332, 0.094, 0.189, 0.289, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8069 | Steps: 2 | Val loss: 0.6276 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3148 | Steps: 2 | Val loss: 0.2724 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=90307)[0m rmse: 0.1528996080160141
[2m[36m(func pid=90307)[0m mae:  0.09474705904722214
[2m[36m(func pid=90307)[0m rmse_per_class: [0.08, 0.266, 0.025, 0.301, 0.069, 0.151, 0.22, 0.123, 0.138, 0.155]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4575 | Steps: 2 | Val loss: 0.3429 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 07:07:35 (running for 00:16:06.61)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.319 |  0.148 |                   90 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.25  |  0.153 |                   71 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.807 |  0.179 |                   21 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.473 |  0.176 |                   19 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.17938226461410522
[2m[36m(func pid=101638)[0m mae:  0.13169695436954498
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.14805707335472107
[2m[36m(func pid=86022)[0m mae:  0.10414163023233414
[2m[36m(func pid=86022)[0m rmse_per_class: [0.087, 0.236, 0.04, 0.292, 0.064, 0.161, 0.243, 0.118, 0.137, 0.102]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2430 | Steps: 2 | Val loss: 0.2837 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=102191)[0m rmse: 0.1752987802028656
[2m[36m(func pid=102191)[0m mae:  0.12839430570602417
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.257, 0.088, 0.331, 0.092, 0.189, 0.288, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.7978 | Steps: 2 | Val loss: 0.6215 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3182 | Steps: 2 | Val loss: 0.2723 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=90307)[0m rmse: 0.15294957160949707
[2m[36m(func pid=90307)[0m mae:  0.09477508813142776
[2m[36m(func pid=90307)[0m rmse_per_class: [0.079, 0.266, 0.025, 0.302, 0.068, 0.151, 0.219, 0.123, 0.141, 0.155]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4431 | Steps: 2 | Val loss: 0.3334 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 07:07:40 (running for 00:16:11.79)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.315 |  0.148 |                   91 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.243 |  0.153 |                   72 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.798 |  0.179 |                   22 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.458 |  0.175 |                   20 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.1793259084224701
[2m[36m(func pid=101638)[0m mae:  0.13165096938610077
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.337, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.14801311492919922
[2m[36m(func pid=86022)[0m mae:  0.10404962301254272
[2m[36m(func pid=86022)[0m rmse_per_class: [0.087, 0.236, 0.04, 0.292, 0.064, 0.161, 0.243, 0.118, 0.137, 0.102]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.17490032315254211
[2m[36m(func pid=102191)[0m mae:  0.12807510793209076
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.256, 0.088, 0.33, 0.091, 0.189, 0.288, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2486 | Steps: 2 | Val loss: 0.2831 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.7929 | Steps: 2 | Val loss: 0.6149 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3151 | Steps: 2 | Val loss: 0.2719 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=90307)[0m rmse: 0.15275149047374725
[2m[36m(func pid=90307)[0m mae:  0.09458156675100327
[2m[36m(func pid=90307)[0m rmse_per_class: [0.078, 0.266, 0.025, 0.3, 0.066, 0.151, 0.219, 0.122, 0.143, 0.157]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4330 | Steps: 2 | Val loss: 0.3263 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=101638)[0m rmse: 0.17933236062526703
[2m[36m(func pid=101638)[0m mae:  0.13164988160133362
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.261, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
== Status ==
Current time: 2024-01-07 07:07:45 (running for 00:16:17.10)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.318 |  0.148 |                   92 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.249 |  0.153 |                   73 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.793 |  0.179 |                   23 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.443 |  0.175 |                   21 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=86022)[0m rmse: 0.14772608876228333
[2m[36m(func pid=86022)[0m mae:  0.10377715528011322
[2m[36m(func pid=86022)[0m rmse_per_class: [0.086, 0.236, 0.04, 0.292, 0.065, 0.161, 0.243, 0.118, 0.137, 0.101]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.17455948889255524
[2m[36m(func pid=102191)[0m mae:  0.12780725955963135
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.256, 0.087, 0.33, 0.089, 0.189, 0.288, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2429 | Steps: 2 | Val loss: 0.2827 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7840 | Steps: 2 | Val loss: 0.6078 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4214 | Steps: 2 | Val loss: 0.3208 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3095 | Steps: 2 | Val loss: 0.2717 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=90307)[0m rmse: 0.15249110758304596
[2m[36m(func pid=90307)[0m mae:  0.09442323446273804
[2m[36m(func pid=90307)[0m rmse_per_class: [0.077, 0.266, 0.025, 0.3, 0.066, 0.151, 0.219, 0.122, 0.143, 0.156]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:07:50 (running for 00:16:22.14)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.315 |  0.148 |                   93 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.243 |  0.152 |                   74 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.784 |  0.179 |                   24 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.433 |  0.175 |                   22 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.17923586070537567
[2m[36m(func pid=101638)[0m mae:  0.1315702348947525
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1742037981748581
[2m[36m(func pid=102191)[0m mae:  0.12751504778862
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.256, 0.086, 0.329, 0.087, 0.189, 0.287, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.14759573340415955
[2m[36m(func pid=86022)[0m mae:  0.10355013608932495
[2m[36m(func pid=86022)[0m rmse_per_class: [0.086, 0.236, 0.039, 0.291, 0.065, 0.161, 0.243, 0.118, 0.136, 0.102]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2490 | Steps: 2 | Val loss: 0.2828 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7754 | Steps: 2 | Val loss: 0.6017 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4171 | Steps: 2 | Val loss: 0.3168 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3142 | Steps: 2 | Val loss: 0.2722 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=90307)[0m rmse: 0.15263748168945312
[2m[36m(func pid=90307)[0m mae:  0.09478209912776947
[2m[36m(func pid=90307)[0m rmse_per_class: [0.077, 0.268, 0.025, 0.301, 0.067, 0.151, 0.221, 0.121, 0.143, 0.152]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:07:56 (running for 00:16:27.64)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.31  |  0.148 |                   94 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.249 |  0.153 |                   75 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.775 |  0.179 |                   25 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.421 |  0.174 |                   23 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.17916561663150787
[2m[36m(func pid=101638)[0m mae:  0.13150310516357422
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1738249957561493
[2m[36m(func pid=102191)[0m mae:  0.1271965205669403
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.256, 0.086, 0.328, 0.086, 0.189, 0.287, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.1478632390499115
[2m[36m(func pid=86022)[0m mae:  0.10387121140956879
[2m[36m(func pid=86022)[0m rmse_per_class: [0.087, 0.236, 0.04, 0.292, 0.065, 0.161, 0.243, 0.118, 0.136, 0.101]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2549 | Steps: 2 | Val loss: 0.2821 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7650 | Steps: 2 | Val loss: 0.5951 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4101 | Steps: 2 | Val loss: 0.3137 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3191 | Steps: 2 | Val loss: 0.2721 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=90307)[0m rmse: 0.15262582898139954
[2m[36m(func pid=90307)[0m mae:  0.0946548730134964
[2m[36m(func pid=90307)[0m rmse_per_class: [0.077, 0.266, 0.025, 0.299, 0.068, 0.151, 0.221, 0.122, 0.142, 0.155]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:08:01 (running for 00:16:32.85)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.314 |  0.148 |                   95 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.255 |  0.153 |                   76 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.765 |  0.179 |                   26 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.417 |  0.174 |                   24 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.17914465069770813
[2m[36m(func pid=101638)[0m mae:  0.13148735463619232
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1733001172542572
[2m[36m(func pid=102191)[0m mae:  0.12676770985126495
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.255, 0.084, 0.327, 0.084, 0.189, 0.286, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.14794176816940308
[2m[36m(func pid=86022)[0m mae:  0.10386842489242554
[2m[36m(func pid=86022)[0m rmse_per_class: [0.087, 0.236, 0.039, 0.292, 0.065, 0.161, 0.243, 0.118, 0.136, 0.102]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2475 | Steps: 2 | Val loss: 0.2826 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4067 | Steps: 2 | Val loss: 0.3119 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7603 | Steps: 2 | Val loss: 0.5885 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3072 | Steps: 2 | Val loss: 0.2721 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=90307)[0m rmse: 0.1530495584011078
[2m[36m(func pid=90307)[0m mae:  0.0947687104344368
[2m[36m(func pid=90307)[0m rmse_per_class: [0.078, 0.265, 0.025, 0.298, 0.068, 0.151, 0.222, 0.122, 0.14, 0.16]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:08:06 (running for 00:16:38.04)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.319 |  0.148 |                   96 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.247 |  0.153 |                   77 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.765 |  0.179 |                   26 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.407 |  0.173 |                   26 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.17281028628349304
[2m[36m(func pid=102191)[0m mae:  0.12636658549308777
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.254, 0.083, 0.327, 0.082, 0.189, 0.285, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.179099440574646
[2m[36m(func pid=101638)[0m mae:  0.1314556896686554
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.14792047441005707
[2m[36m(func pid=86022)[0m mae:  0.10375197976827621
[2m[36m(func pid=86022)[0m rmse_per_class: [0.086, 0.236, 0.039, 0.292, 0.065, 0.161, 0.242, 0.118, 0.136, 0.103]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2505 | Steps: 2 | Val loss: 0.2820 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4027 | Steps: 2 | Val loss: 0.3112 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.7521 | Steps: 2 | Val loss: 0.5821 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3109 | Steps: 2 | Val loss: 0.2722 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=90307)[0m rmse: 0.1526443362236023
[2m[36m(func pid=90307)[0m mae:  0.09451089054346085
[2m[36m(func pid=90307)[0m rmse_per_class: [0.078, 0.265, 0.025, 0.297, 0.067, 0.151, 0.222, 0.123, 0.139, 0.159]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.1790740042924881
[2m[36m(func pid=101638)[0m mae:  0.13143791258335114
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 07:08:11 (running for 00:16:43.21)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.307 |  0.148 |                   97 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.25  |  0.153 |                   78 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.752 |  0.179 |                   28 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.407 |  0.173 |                   26 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.17232616245746613
[2m[36m(func pid=102191)[0m mae:  0.12596678733825684
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.254, 0.082, 0.326, 0.081, 0.189, 0.285, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.14792782068252563
[2m[36m(func pid=86022)[0m mae:  0.10372263193130493
[2m[36m(func pid=86022)[0m rmse_per_class: [0.087, 0.236, 0.039, 0.293, 0.065, 0.16, 0.242, 0.118, 0.137, 0.103]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2606 | Steps: 2 | Val loss: 0.2809 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7414 | Steps: 2 | Val loss: 0.5750 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4005 | Steps: 2 | Val loss: 0.3115 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3080 | Steps: 2 | Val loss: 0.2723 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=90307)[0m rmse: 0.15177400410175323
[2m[36m(func pid=90307)[0m mae:  0.09395436197519302
[2m[36m(func pid=90307)[0m rmse_per_class: [0.077, 0.263, 0.025, 0.295, 0.065, 0.151, 0.221, 0.123, 0.138, 0.16]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:08:16 (running for 00:16:48.28)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.311 |  0.148 |                   98 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.261 |  0.152 |                   79 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.752 |  0.179 |                   28 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.401 |  0.172 |                   28 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.17192216217517853
[2m[36m(func pid=102191)[0m mae:  0.12563030421733856
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.254, 0.081, 0.325, 0.079, 0.188, 0.284, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17898549139499664
[2m[36m(func pid=101638)[0m mae:  0.1313682347536087
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.14794404804706573
[2m[36m(func pid=86022)[0m mae:  0.10366018116474152
[2m[36m(func pid=86022)[0m rmse_per_class: [0.087, 0.236, 0.039, 0.293, 0.065, 0.16, 0.242, 0.118, 0.137, 0.104]
[2m[36m(func pid=86022)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2448 | Steps: 2 | Val loss: 0.2815 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4050 | Steps: 2 | Val loss: 0.3126 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.7323 | Steps: 2 | Val loss: 0.5683 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=86022)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3053 | Steps: 2 | Val loss: 0.2720 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=90307)[0m rmse: 0.15186700224876404
[2m[36m(func pid=90307)[0m mae:  0.094069704413414
[2m[36m(func pid=90307)[0m rmse_per_class: [0.078, 0.264, 0.025, 0.297, 0.066, 0.151, 0.221, 0.122, 0.137, 0.157]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:08:22 (running for 00:16:53.64)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00006 | RUNNING    | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.308 |  0.148 |                   99 |
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.245 |  0.152 |                   80 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.741 |  0.179 |                   29 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.405 |  0.172 |                   29 |
| train_ccef6_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.1789269894361496
[2m[36m(func pid=101638)[0m mae:  0.1313069462776184
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.17151084542274475
[2m[36m(func pid=102191)[0m mae:  0.12530282139778137
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.254, 0.08, 0.324, 0.078, 0.188, 0.284, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=86022)[0m rmse: 0.14781978726387024
[2m[36m(func pid=86022)[0m mae:  0.10347374528646469
[2m[36m(func pid=86022)[0m rmse_per_class: [0.086, 0.236, 0.039, 0.293, 0.065, 0.16, 0.241, 0.118, 0.136, 0.104]
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2449 | Steps: 2 | Val loss: 0.2821 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4015 | Steps: 2 | Val loss: 0.3141 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.7269 | Steps: 2 | Val loss: 0.5609 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=90307)[0m rmse: 0.15202496945858002
[2m[36m(func pid=90307)[0m mae:  0.0940326601266861
[2m[36m(func pid=90307)[0m rmse_per_class: [0.079, 0.265, 0.025, 0.298, 0.066, 0.151, 0.22, 0.122, 0.137, 0.156]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.17099419236183167
[2m[36m(func pid=102191)[0m mae:  0.12487368285655975
[2m[36m(func pid=102191)[0m rmse_per_class: [0.118, 0.253, 0.079, 0.324, 0.076, 0.188, 0.283, 0.14, 0.142, 0.107]
[2m[36m(func pid=101638)[0m rmse: 0.17888297140598297
[2m[36m(func pid=101638)[0m mae:  0.13127292692661285
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2480 | Steps: 2 | Val loss: 0.2836 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 07:08:27 (running for 00:16:58.66)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.245 |  0.152 |                   81 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.732 |  0.179 |                   30 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.405 |  0.172 |                   29 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 07:08:32 (running for 00:17:03.77)
Memory usage on this node: 23.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.245 |  0.152 |                   81 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.732 |  0.179 |                   30 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.402 |  0.171 |                   30 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=109115)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=109115)[0m Configuration completed!
[2m[36m(func pid=109115)[0m New optimizer parameters:
[2m[36m(func pid=109115)[0m SGD (
[2m[36m(func pid=109115)[0m Parameter Group 0
[2m[36m(func pid=109115)[0m     dampening: 0
[2m[36m(func pid=109115)[0m     differentiable: False
[2m[36m(func pid=109115)[0m     foreach: None
[2m[36m(func pid=109115)[0m     lr: 0.01
[2m[36m(func pid=109115)[0m     maximize: False
[2m[36m(func pid=109115)[0m     momentum: 0.99
[2m[36m(func pid=109115)[0m     nesterov: False
[2m[36m(func pid=109115)[0m     weight_decay: 0.0001
[2m[36m(func pid=109115)[0m )
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.15271714329719543
[2m[36m(func pid=90307)[0m mae:  0.09435925632715225
[2m[36m(func pid=90307)[0m rmse_per_class: [0.079, 0.267, 0.025, 0.301, 0.067, 0.152, 0.22, 0.122, 0.137, 0.157]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4056 | Steps: 2 | Val loss: 0.3164 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7175 | Steps: 2 | Val loss: 0.5543 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2490 | Steps: 2 | Val loss: 0.2847 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8892 | Steps: 2 | Val loss: 0.6852 | Batch size: 32 | lr: 0.01 | Duration: 4.46s
== Status ==
Current time: 2024-01-07 07:08:37 (running for 00:17:09.00)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.248 |  0.153 |                   82 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.718 |  0.179 |                   32 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.402 |  0.171 |                   30 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.17891642451286316
[2m[36m(func pid=101638)[0m mae:  0.1312899887561798
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.17043298482894897
[2m[36m(func pid=102191)[0m mae:  0.12441811710596085
[2m[36m(func pid=102191)[0m rmse_per_class: [0.118, 0.253, 0.078, 0.323, 0.075, 0.188, 0.282, 0.14, 0.142, 0.107]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.15306589007377625
[2m[36m(func pid=90307)[0m mae:  0.09449641406536102
[2m[36m(func pid=90307)[0m rmse_per_class: [0.078, 0.267, 0.025, 0.302, 0.068, 0.151, 0.22, 0.122, 0.138, 0.158]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1826978474855423
[2m[36m(func pid=109115)[0m mae:  0.13443389534950256
[2m[36m(func pid=109115)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.11, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4083 | Steps: 2 | Val loss: 0.3190 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7082 | Steps: 2 | Val loss: 0.5473 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2508 | Steps: 2 | Val loss: 0.2845 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8411 | Steps: 2 | Val loss: 0.6285 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=102191)[0m rmse: 0.16988661885261536
[2m[36m(func pid=102191)[0m mae:  0.12397631257772446
[2m[36m(func pid=102191)[0m rmse_per_class: [0.118, 0.252, 0.076, 0.322, 0.073, 0.188, 0.281, 0.14, 0.142, 0.107]
== Status ==
Current time: 2024-01-07 07:08:42 (running for 00:17:14.16)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.249 |  0.153 |                   83 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.718 |  0.179 |                   32 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.408 |  0.17  |                   32 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.889 |  0.183 |                    1 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17881162464618683
[2m[36m(func pid=101638)[0m mae:  0.13120383024215698
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.107, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.15321418642997742
[2m[36m(func pid=90307)[0m mae:  0.09448439627885818
[2m[36m(func pid=90307)[0m rmse_per_class: [0.08, 0.266, 0.025, 0.302, 0.069, 0.151, 0.221, 0.122, 0.138, 0.158]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.18246981501579285
[2m[36m(func pid=109115)[0m mae:  0.1343163549900055
[2m[36m(func pid=109115)[0m rmse_per_class: [0.117, 0.267, 0.106, 0.339, 0.111, 0.19, 0.294, 0.144, 0.144, 0.113]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4086 | Steps: 2 | Val loss: 0.3218 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.6996 | Steps: 2 | Val loss: 0.5406 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2551 | Steps: 2 | Val loss: 0.2850 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 07:08:47 (running for 00:17:19.52)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.153 |                   84 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.708 |  0.179 |                   33 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.409 |  0.169 |                   33 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.841 |  0.182 |                    2 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.16942037642002106
[2m[36m(func pid=102191)[0m mae:  0.12360300868749619
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.251, 0.075, 0.322, 0.072, 0.188, 0.281, 0.139, 0.142, 0.107]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7530 | Steps: 2 | Val loss: 0.5527 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=101638)[0m rmse: 0.1787634789943695
[2m[36m(func pid=101638)[0m mae:  0.13116469979286194
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.107, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.1533508449792862
[2m[36m(func pid=90307)[0m mae:  0.09462614357471466
[2m[36m(func pid=90307)[0m rmse_per_class: [0.08, 0.266, 0.025, 0.303, 0.068, 0.151, 0.221, 0.122, 0.139, 0.158]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1820034682750702
[2m[36m(func pid=109115)[0m mae:  0.13397669792175293
[2m[36m(func pid=109115)[0m rmse_per_class: [0.118, 0.267, 0.104, 0.338, 0.11, 0.19, 0.293, 0.143, 0.143, 0.113]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4109 | Steps: 2 | Val loss: 0.3249 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.6907 | Steps: 2 | Val loss: 0.5336 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2440 | Steps: 2 | Val loss: 0.2848 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 07:08:53 (running for 00:17:24.68)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.255 |  0.153 |                   85 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.7   |  0.179 |                   34 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.411 |  0.169 |                   34 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.753 |  0.182 |                    3 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.1786564588546753
[2m[36m(func pid=101638)[0m mae:  0.13108566403388977
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.096, 0.336, 0.107, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.16890838742256165
[2m[36m(func pid=102191)[0m mae:  0.12318253517150879
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.251, 0.074, 0.321, 0.071, 0.188, 0.28, 0.139, 0.142, 0.107]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6527 | Steps: 2 | Val loss: 0.4722 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=90307)[0m rmse: 0.15350215137004852
[2m[36m(func pid=90307)[0m mae:  0.09467203915119171
[2m[36m(func pid=90307)[0m rmse_per_class: [0.082, 0.265, 0.025, 0.301, 0.067, 0.151, 0.222, 0.122, 0.141, 0.159]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6847 | Steps: 2 | Val loss: 0.5272 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4156 | Steps: 2 | Val loss: 0.3276 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=109115)[0m rmse: 0.181116983294487
[2m[36m(func pid=109115)[0m mae:  0.13330510258674622
[2m[36m(func pid=109115)[0m rmse_per_class: [0.119, 0.267, 0.102, 0.337, 0.106, 0.189, 0.292, 0.143, 0.143, 0.112]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2531 | Steps: 2 | Val loss: 0.2854 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 07:08:58 (running for 00:17:29.88)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.244 |  0.154 |                   86 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.685 |  0.179 |                   36 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.411 |  0.169 |                   34 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.653 |  0.181 |                    4 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.17864574491977692
[2m[36m(func pid=101638)[0m mae:  0.13107101619243622
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.259, 0.096, 0.335, 0.107, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1684153527021408
[2m[36m(func pid=102191)[0m mae:  0.12277748435735703
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.25, 0.073, 0.32, 0.07, 0.188, 0.279, 0.139, 0.142, 0.107]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5549 | Steps: 2 | Val loss: 0.4023 | Batch size: 32 | lr: 0.01 | Duration: 3.26s
[2m[36m(func pid=90307)[0m rmse: 0.15403306484222412
[2m[36m(func pid=90307)[0m mae:  0.09497664868831635
[2m[36m(func pid=90307)[0m rmse_per_class: [0.085, 0.264, 0.025, 0.301, 0.067, 0.151, 0.222, 0.122, 0.142, 0.161]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6754 | Steps: 2 | Val loss: 0.5203 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4208 | Steps: 2 | Val loss: 0.3315 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=109115)[0m rmse: 0.17988504469394684
[2m[36m(func pid=109115)[0m mae:  0.13235722482204437
[2m[36m(func pid=109115)[0m rmse_per_class: [0.12, 0.266, 0.099, 0.335, 0.101, 0.189, 0.291, 0.142, 0.143, 0.112]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2538 | Steps: 2 | Val loss: 0.2857 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 07:09:03 (running for 00:17:34.92)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.253 |  0.154 |                   87 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.675 |  0.179 |                   37 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.416 |  0.168 |                   35 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.555 |  0.18  |                    5 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.1785924732685089
[2m[36m(func pid=101638)[0m mae:  0.13104034960269928
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.259, 0.096, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1678909957408905
[2m[36m(func pid=102191)[0m mae:  0.12232204526662827
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.249, 0.072, 0.319, 0.069, 0.187, 0.278, 0.138, 0.142, 0.107]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4771 | Steps: 2 | Val loss: 0.3528 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=90307)[0m rmse: 0.15412738919258118
[2m[36m(func pid=90307)[0m mae:  0.09508807957172394
[2m[36m(func pid=90307)[0m rmse_per_class: [0.084, 0.265, 0.025, 0.302, 0.068, 0.151, 0.223, 0.121, 0.142, 0.161]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6642 | Steps: 2 | Val loss: 0.5141 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4219 | Steps: 2 | Val loss: 0.3353 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=109115)[0m rmse: 0.1783389151096344
[2m[36m(func pid=109115)[0m mae:  0.1311318576335907
[2m[36m(func pid=109115)[0m rmse_per_class: [0.121, 0.265, 0.096, 0.333, 0.094, 0.189, 0.289, 0.142, 0.142, 0.112]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2438 | Steps: 2 | Val loss: 0.2862 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 07:09:08 (running for 00:17:40.15)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.254 |  0.154 |                   88 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.675 |  0.179 |                   37 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.422 |  0.167 |                   37 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.477 |  0.178 |                    6 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.16747596859931946
[2m[36m(func pid=102191)[0m mae:  0.12196600437164307
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.249, 0.071, 0.319, 0.068, 0.187, 0.278, 0.138, 0.142, 0.106]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17857035994529724
[2m[36m(func pid=101638)[0m mae:  0.1310041844844818
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.26, 0.096, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4305 | Steps: 2 | Val loss: 0.3245 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=90307)[0m rmse: 0.1542423665523529
[2m[36m(func pid=90307)[0m mae:  0.09518547356128693
[2m[36m(func pid=90307)[0m rmse_per_class: [0.085, 0.265, 0.025, 0.304, 0.067, 0.151, 0.222, 0.122, 0.141, 0.16]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4276 | Steps: 2 | Val loss: 0.3391 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6568 | Steps: 2 | Val loss: 0.5067 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=109115)[0m rmse: 0.17651648819446564
[2m[36m(func pid=109115)[0m mae:  0.12965941429138184
[2m[36m(func pid=109115)[0m rmse_per_class: [0.121, 0.264, 0.092, 0.33, 0.087, 0.188, 0.287, 0.142, 0.142, 0.112]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2565 | Steps: 2 | Val loss: 0.2860 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 07:09:13 (running for 00:17:45.33)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.244 |  0.154 |                   89 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.664 |  0.179 |                   38 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.428 |  0.167 |                   38 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.43  |  0.177 |                    7 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.17844776809215546
[2m[36m(func pid=101638)[0m mae:  0.1309148520231247
[2m[36m(func pid=101638)[0m rmse_per_class: [0.115, 0.259, 0.096, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=102191)[0m rmse: 0.16710121929645538
[2m[36m(func pid=102191)[0m mae:  0.12165595591068268
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.249, 0.071, 0.318, 0.067, 0.187, 0.277, 0.138, 0.142, 0.106]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.15391597151756287
[2m[36m(func pid=90307)[0m mae:  0.09478695690631866
[2m[36m(func pid=90307)[0m rmse_per_class: [0.083, 0.266, 0.025, 0.304, 0.067, 0.151, 0.221, 0.122, 0.142, 0.159]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4047 | Steps: 2 | Val loss: 0.3148 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4313 | Steps: 2 | Val loss: 0.3427 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6527 | Steps: 2 | Val loss: 0.5002 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 07:09:18 (running for 00:17:50.43)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.256 |  0.154 |                   90 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.657 |  0.178 |                   39 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.428 |  0.167 |                   38 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.405 |  0.174 |                    8 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.1665247678756714
[2m[36m(func pid=102191)[0m mae:  0.12116314470767975
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.248, 0.069, 0.317, 0.066, 0.187, 0.276, 0.137, 0.142, 0.106]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.17442990839481354
[2m[36m(func pid=109115)[0m mae:  0.12793761491775513
[2m[36m(func pid=109115)[0m rmse_per_class: [0.121, 0.262, 0.087, 0.325, 0.081, 0.188, 0.285, 0.141, 0.142, 0.111]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17836663126945496
[2m[36m(func pid=101638)[0m mae:  0.13085612654685974
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.105, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2506 | Steps: 2 | Val loss: 0.2865 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=90307)[0m rmse: 0.15424363315105438
[2m[36m(func pid=90307)[0m mae:  0.09495842456817627
[2m[36m(func pid=90307)[0m rmse_per_class: [0.083, 0.267, 0.025, 0.305, 0.069, 0.151, 0.222, 0.122, 0.141, 0.157]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4340 | Steps: 2 | Val loss: 0.3460 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6445 | Steps: 2 | Val loss: 0.4938 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4057 | Steps: 2 | Val loss: 0.3188 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 07:09:24 (running for 00:17:55.74)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.154 |                   91 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.653 |  0.178 |                   40 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.434 |  0.166 |                   40 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.405 |  0.174 |                    8 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.1659086048603058
[2m[36m(func pid=102191)[0m mae:  0.12062910944223404
[2m[36m(func pid=102191)[0m rmse_per_class: [0.117, 0.248, 0.068, 0.316, 0.065, 0.187, 0.275, 0.137, 0.142, 0.106]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17826560139656067
[2m[36m(func pid=101638)[0m mae:  0.13078266382217407
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.105, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1722177267074585
[2m[36m(func pid=109115)[0m mae:  0.12608033418655396
[2m[36m(func pid=109115)[0m rmse_per_class: [0.121, 0.26, 0.082, 0.321, 0.075, 0.187, 0.283, 0.14, 0.142, 0.111]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2447 | Steps: 2 | Val loss: 0.2867 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4378 | Steps: 2 | Val loss: 0.3495 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6356 | Steps: 2 | Val loss: 0.4872 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=90307)[0m rmse: 0.15421094000339508
[2m[36m(func pid=90307)[0m mae:  0.09493087232112885
[2m[36m(func pid=90307)[0m rmse_per_class: [0.082, 0.268, 0.025, 0.305, 0.071, 0.151, 0.222, 0.122, 0.139, 0.157]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4167 | Steps: 2 | Val loss: 0.3319 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 07:09:29 (running for 00:18:00.94)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.245 |  0.154 |                   92 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.645 |  0.178 |                   41 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.438 |  0.165 |                   41 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.406 |  0.172 |                    9 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.1653708517551422
[2m[36m(func pid=102191)[0m mae:  0.1201690062880516
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.247, 0.067, 0.315, 0.064, 0.186, 0.274, 0.137, 0.142, 0.105]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17824110388755798
[2m[36m(func pid=101638)[0m mae:  0.13076762855052948
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.105, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.16991743445396423
[2m[36m(func pid=109115)[0m mae:  0.12412754446268082
[2m[36m(func pid=109115)[0m rmse_per_class: [0.12, 0.258, 0.077, 0.317, 0.07, 0.187, 0.28, 0.139, 0.141, 0.11]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2530 | Steps: 2 | Val loss: 0.2873 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6255 | Steps: 2 | Val loss: 0.4811 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4468 | Steps: 2 | Val loss: 0.3526 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=90307)[0m rmse: 0.15458786487579346
[2m[36m(func pid=90307)[0m mae:  0.09519495069980621
[2m[36m(func pid=90307)[0m rmse_per_class: [0.083, 0.269, 0.025, 0.306, 0.072, 0.151, 0.223, 0.123, 0.138, 0.156]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4332 | Steps: 2 | Val loss: 0.3500 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 07:09:34 (running for 00:18:06.12)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.253 |  0.155 |                   93 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.636 |  0.178 |                   42 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.447 |  0.165 |                   42 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.417 |  0.17  |                   10 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.1648390144109726
[2m[36m(func pid=102191)[0m mae:  0.11971817165613174
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.247, 0.066, 0.314, 0.063, 0.186, 0.273, 0.136, 0.142, 0.105]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17821559309959412
[2m[36m(func pid=101638)[0m mae:  0.1307428628206253
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.105, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2437 | Steps: 2 | Val loss: 0.2862 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=109115)[0m rmse: 0.16758796572685242
[2m[36m(func pid=109115)[0m mae:  0.12209850549697876
[2m[36m(func pid=109115)[0m rmse_per_class: [0.119, 0.256, 0.072, 0.313, 0.066, 0.186, 0.277, 0.138, 0.141, 0.109]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4450 | Steps: 2 | Val loss: 0.3555 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.6177 | Steps: 2 | Val loss: 0.4754 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=90307)[0m rmse: 0.15394911170005798
[2m[36m(func pid=90307)[0m mae:  0.09494087845087051
[2m[36m(func pid=90307)[0m rmse_per_class: [0.081, 0.269, 0.025, 0.306, 0.073, 0.151, 0.223, 0.123, 0.137, 0.152]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4600 | Steps: 2 | Val loss: 0.3705 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=102191)[0m rmse: 0.16433008015155792
[2m[36m(func pid=102191)[0m mae:  0.1192808598279953
[2m[36m(func pid=102191)[0m rmse_per_class: [0.116, 0.247, 0.065, 0.313, 0.062, 0.186, 0.272, 0.136, 0.142, 0.105]
[2m[36m(func pid=102191)[0m 
== Status ==
Current time: 2024-01-07 07:09:39 (running for 00:18:11.49)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.244 |  0.154 |                   94 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.618 |  0.178 |                   44 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.445 |  0.164 |                   43 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.433 |  0.168 |                   11 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.17815615236759186
[2m[36m(func pid=101638)[0m mae:  0.13069051504135132
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.104, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2476 | Steps: 2 | Val loss: 0.2859 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=109115)[0m rmse: 0.1652754247188568
[2m[36m(func pid=109115)[0m mae:  0.120047926902771
[2m[36m(func pid=109115)[0m rmse_per_class: [0.119, 0.253, 0.067, 0.308, 0.062, 0.186, 0.273, 0.137, 0.14, 0.107]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4483 | Steps: 2 | Val loss: 0.3587 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6104 | Steps: 2 | Val loss: 0.4691 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=90307)[0m rmse: 0.15386775135993958
[2m[36m(func pid=90307)[0m mae:  0.09479941427707672
[2m[36m(func pid=90307)[0m rmse_per_class: [0.081, 0.269, 0.025, 0.305, 0.074, 0.151, 0.223, 0.124, 0.135, 0.152]
[2m[36m(func pid=90307)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4835 | Steps: 2 | Val loss: 0.3909 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=102191)[0m rmse: 0.1637212634086609
[2m[36m(func pid=102191)[0m mae:  0.11874555051326752
[2m[36m(func pid=102191)[0m rmse_per_class: [0.115, 0.246, 0.064, 0.312, 0.062, 0.186, 0.271, 0.136, 0.141, 0.104]
[2m[36m(func pid=102191)[0m 
== Status ==
Current time: 2024-01-07 07:09:44 (running for 00:18:16.51)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.248 |  0.154 |                   95 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.61  |  0.178 |                   45 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.448 |  0.164 |                   44 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.46  |  0.165 |                   12 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.1780804842710495
[2m[36m(func pid=101638)[0m mae:  0.1306261569261551
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.104, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2483 | Steps: 2 | Val loss: 0.2855 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=109115)[0m rmse: 0.1628694385290146
[2m[36m(func pid=109115)[0m mae:  0.11785759776830673
[2m[36m(func pid=109115)[0m rmse_per_class: [0.117, 0.251, 0.062, 0.304, 0.059, 0.186, 0.269, 0.135, 0.14, 0.106]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4530 | Steps: 2 | Val loss: 0.3618 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.6007 | Steps: 2 | Val loss: 0.4626 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=90307)[0m rmse: 0.1534682959318161
[2m[36m(func pid=90307)[0m mae:  0.09452317655086517
[2m[36m(func pid=90307)[0m rmse_per_class: [0.078, 0.268, 0.025, 0.304, 0.072, 0.151, 0.222, 0.124, 0.136, 0.154]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:09:49 (running for 00:18:21.52)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.248 |  0.153 |                   96 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.61  |  0.178 |                   45 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.453 |  0.163 |                   45 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.483 |  0.163 |                   13 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.16316983103752136
[2m[36m(func pid=102191)[0m mae:  0.11826317012310028
[2m[36m(func pid=102191)[0m rmse_per_class: [0.115, 0.246, 0.063, 0.311, 0.061, 0.186, 0.27, 0.135, 0.141, 0.104]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5070 | Steps: 2 | Val loss: 0.4113 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=101638)[0m rmse: 0.17796187102794647
[2m[36m(func pid=101638)[0m mae:  0.13051868975162506
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.335, 0.104, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2485 | Steps: 2 | Val loss: 0.2864 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=109115)[0m rmse: 0.16050508618354797
[2m[36m(func pid=109115)[0m mae:  0.11561224609613419
[2m[36m(func pid=109115)[0m rmse_per_class: [0.115, 0.248, 0.059, 0.3, 0.057, 0.185, 0.265, 0.133, 0.139, 0.104]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4526 | Steps: 2 | Val loss: 0.3644 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5975 | Steps: 2 | Val loss: 0.4567 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=90307)[0m rmse: 0.15412823855876923
[2m[36m(func pid=90307)[0m mae:  0.09494374692440033
[2m[36m(func pid=90307)[0m rmse_per_class: [0.08, 0.267, 0.025, 0.304, 0.071, 0.151, 0.223, 0.124, 0.138, 0.158]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:09:55 (running for 00:18:26.65)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.249 |  0.154 |                   97 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.601 |  0.178 |                   46 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.453 |  0.163 |                   46 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.507 |  0.161 |                   14 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.16276483237743378
[2m[36m(func pid=102191)[0m mae:  0.11792612075805664
[2m[36m(func pid=102191)[0m rmse_per_class: [0.115, 0.246, 0.062, 0.31, 0.06, 0.185, 0.269, 0.135, 0.141, 0.104]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.1779402792453766
[2m[36m(func pid=101638)[0m mae:  0.1305059939622879
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.335, 0.103, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5265 | Steps: 2 | Val loss: 0.4277 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2454 | Steps: 2 | Val loss: 0.2874 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4645 | Steps: 2 | Val loss: 0.3657 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5904 | Steps: 2 | Val loss: 0.4514 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=109115)[0m rmse: 0.15814602375030518
[2m[36m(func pid=109115)[0m mae:  0.11335626989603043
[2m[36m(func pid=109115)[0m rmse_per_class: [0.113, 0.246, 0.055, 0.296, 0.056, 0.185, 0.26, 0.131, 0.138, 0.102]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.15450569987297058
[2m[36m(func pid=90307)[0m mae:  0.09520720690488815
[2m[36m(func pid=90307)[0m rmse_per_class: [0.08, 0.268, 0.025, 0.305, 0.07, 0.151, 0.222, 0.124, 0.139, 0.16]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:10:00 (running for 00:18:31.70)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.245 |  0.155 |                   98 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.598 |  0.178 |                   47 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.162 |                   47 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.527 |  0.158 |                   15 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.1621701419353485
[2m[36m(func pid=102191)[0m mae:  0.11738134920597076
[2m[36m(func pid=102191)[0m rmse_per_class: [0.114, 0.246, 0.061, 0.309, 0.06, 0.185, 0.267, 0.135, 0.141, 0.103]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17789779603481293
[2m[36m(func pid=101638)[0m mae:  0.1304698884487152
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.335, 0.103, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5427 | Steps: 2 | Val loss: 0.4429 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2521 | Steps: 2 | Val loss: 0.2868 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4658 | Steps: 2 | Val loss: 0.3676 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5838 | Steps: 2 | Val loss: 0.4460 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=109115)[0m rmse: 0.1558378040790558
[2m[36m(func pid=109115)[0m mae:  0.11106248199939728
[2m[36m(func pid=109115)[0m rmse_per_class: [0.111, 0.243, 0.052, 0.292, 0.055, 0.184, 0.256, 0.13, 0.138, 0.099]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.1542644500732422
[2m[36m(func pid=90307)[0m mae:  0.09507059305906296
[2m[36m(func pid=90307)[0m rmse_per_class: [0.083, 0.266, 0.025, 0.304, 0.067, 0.151, 0.222, 0.123, 0.14, 0.16]
[2m[36m(func pid=90307)[0m 
== Status ==
Current time: 2024-01-07 07:10:05 (running for 00:18:36.78)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00007 | RUNNING    | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.252 |  0.154 |                   99 |
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.59  |  0.178 |                   48 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.466 |  0.162 |                   48 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.543 |  0.156 |                   16 |
| train_ccef6_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.16159562766551971
[2m[36m(func pid=102191)[0m mae:  0.1168590560555458
[2m[36m(func pid=102191)[0m rmse_per_class: [0.113, 0.245, 0.06, 0.309, 0.059, 0.184, 0.267, 0.134, 0.141, 0.103]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17777106165885925
[2m[36m(func pid=101638)[0m mae:  0.13036885857582092
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.334, 0.103, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5645 | Steps: 2 | Val loss: 0.4551 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=90307)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2507 | Steps: 2 | Val loss: 0.2868 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4644 | Steps: 2 | Val loss: 0.3695 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5794 | Steps: 2 | Val loss: 0.4410 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=109115)[0m rmse: 0.15375198423862457
[2m[36m(func pid=109115)[0m mae:  0.1088133454322815
[2m[36m(func pid=109115)[0m rmse_per_class: [0.109, 0.241, 0.049, 0.288, 0.055, 0.184, 0.251, 0.128, 0.137, 0.096]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=90307)[0m rmse: 0.1547568291425705
[2m[36m(func pid=90307)[0m mae:  0.09533308446407318
[2m[36m(func pid=90307)[0m rmse_per_class: [0.088, 0.264, 0.025, 0.302, 0.067, 0.152, 0.222, 0.123, 0.142, 0.161]
[2m[36m(func pid=102191)[0m rmse: 0.16092516481876373
[2m[36m(func pid=102191)[0m mae:  0.11624813079833984
[2m[36m(func pid=102191)[0m rmse_per_class: [0.113, 0.244, 0.059, 0.308, 0.059, 0.184, 0.265, 0.134, 0.141, 0.102]
[2m[36m(func pid=101638)[0m rmse: 0.17778164148330688
[2m[36m(func pid=101638)[0m mae:  0.13039179146289825
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.258, 0.094, 0.334, 0.102, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.5731 | Steps: 2 | Val loss: 0.4637 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=109115)[0m rmse: 0.15153487026691437
[2m[36m(func pid=109115)[0m mae:  0.10636495053768158
[2m[36m(func pid=109115)[0m rmse_per_class: [0.106, 0.239, 0.046, 0.283, 0.054, 0.183, 0.247, 0.127, 0.136, 0.094]
== Status ==
Current time: 2024-01-07 07:10:10 (running for 00:18:42.07)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.584 |  0.178 |                   49 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.466 |  0.162 |                   48 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.565 |  0.154 |                   17 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 07:10:18 (running for 00:18:50.17)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.579 |  0.178 |                   50 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.466 |  0.162 |                   48 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.565 |  0.154 |                   17 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=113492)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=113492)[0m Configuration completed!
[2m[36m(func pid=113492)[0m New optimizer parameters:
[2m[36m(func pid=113492)[0m SGD (
[2m[36m(func pid=113492)[0m Parameter Group 0
[2m[36m(func pid=113492)[0m     dampening: 0
[2m[36m(func pid=113492)[0m     differentiable: False
[2m[36m(func pid=113492)[0m     foreach: None
[2m[36m(func pid=113492)[0m     lr: 0.1
[2m[36m(func pid=113492)[0m     maximize: False
[2m[36m(func pid=113492)[0m     momentum: 0.99
[2m[36m(func pid=113492)[0m     nesterov: False
[2m[36m(func pid=113492)[0m     weight_decay: 0.0001
[2m[36m(func pid=113492)[0m )
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5678 | Steps: 2 | Val loss: 0.4358 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4714 | Steps: 2 | Val loss: 0.3708 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.5910 | Steps: 2 | Val loss: 0.4702 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8376 | Steps: 2 | Val loss: 0.5171 | Batch size: 32 | lr: 0.1 | Duration: 4.51s
== Status ==
Current time: 2024-01-07 07:10:23 (running for 00:18:55.20)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.579 |  0.178 |                   50 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.464 |  0.161 |                   49 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.573 |  0.152 |                   18 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.16033676266670227
[2m[36m(func pid=102191)[0m mae:  0.11569575220346451
[2m[36m(func pid=102191)[0m rmse_per_class: [0.113, 0.244, 0.059, 0.307, 0.058, 0.184, 0.264, 0.133, 0.14, 0.102]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.1777001917362213
[2m[36m(func pid=101638)[0m mae:  0.13031306862831116
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.334, 0.102, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.14954769611358643
[2m[36m(func pid=109115)[0m mae:  0.10406716167926788
[2m[36m(func pid=109115)[0m rmse_per_class: [0.103, 0.237, 0.045, 0.28, 0.054, 0.182, 0.243, 0.125, 0.135, 0.092]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.18269820511341095
[2m[36m(func pid=113492)[0m mae:  0.1344657838344574
[2m[36m(func pid=113492)[0m rmse_per_class: [0.118, 0.267, 0.109, 0.339, 0.106, 0.191, 0.293, 0.146, 0.144, 0.114]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4697 | Steps: 2 | Val loss: 0.3729 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5624 | Steps: 2 | Val loss: 0.4307 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5922 | Steps: 2 | Val loss: 0.4747 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5364 | Steps: 2 | Val loss: 0.3422 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
== Status ==
Current time: 2024-01-07 07:10:29 (running for 00:19:00.80)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.568 |  0.178 |                   51 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.47  |  0.16  |                   51 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.591 |  0.15  |                   19 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.838 |  0.183 |                    1 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.17763349413871765
[2m[36m(func pid=101638)[0m mae:  0.13025826215744019
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.258, 0.094, 0.334, 0.102, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.15980033576488495
[2m[36m(func pid=102191)[0m mae:  0.11519788205623627
[2m[36m(func pid=102191)[0m rmse_per_class: [0.112, 0.243, 0.058, 0.307, 0.058, 0.184, 0.263, 0.133, 0.14, 0.101]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1477457731962204
[2m[36m(func pid=109115)[0m mae:  0.10181976854801178
[2m[36m(func pid=109115)[0m rmse_per_class: [0.1, 0.235, 0.043, 0.277, 0.055, 0.182, 0.239, 0.124, 0.134, 0.09]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.18046033382415771
[2m[36m(func pid=113492)[0m mae:  0.1328546702861786
[2m[36m(func pid=113492)[0m rmse_per_class: [0.123, 0.267, 0.099, 0.336, 0.091, 0.191, 0.288, 0.147, 0.145, 0.117]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4701 | Steps: 2 | Val loss: 0.3751 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5569 | Steps: 2 | Val loss: 0.4255 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5972 | Steps: 2 | Val loss: 0.4757 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4134 | Steps: 2 | Val loss: 0.3310 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 07:10:34 (running for 00:19:06.06)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.562 |  0.178 |                   52 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.47  |  0.159 |                   52 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.592 |  0.148 |                   20 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.536 |  0.18  |                    2 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.15923874080181122
[2m[36m(func pid=102191)[0m mae:  0.11467331647872925
[2m[36m(func pid=102191)[0m rmse_per_class: [0.112, 0.242, 0.057, 0.305, 0.057, 0.183, 0.262, 0.132, 0.14, 0.101]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.1775427758693695
[2m[36m(func pid=101638)[0m mae:  0.130184605717659
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.258, 0.094, 0.334, 0.101, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.17492373287677765
[2m[36m(func pid=113492)[0m mae:  0.12851165235042572
[2m[36m(func pid=113492)[0m rmse_per_class: [0.126, 0.264, 0.084, 0.33, 0.073, 0.189, 0.278, 0.144, 0.144, 0.118]
[2m[36m(func pid=109115)[0m rmse: 0.1463075578212738
[2m[36m(func pid=109115)[0m mae:  0.0997428148984909
[2m[36m(func pid=109115)[0m rmse_per_class: [0.096, 0.233, 0.042, 0.274, 0.055, 0.182, 0.237, 0.123, 0.133, 0.087]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4762 | Steps: 2 | Val loss: 0.3768 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5493 | Steps: 2 | Val loss: 0.4205 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4622 | Steps: 2 | Val loss: 0.3876 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5965 | Steps: 2 | Val loss: 0.4728 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 07:10:39 (running for 00:19:11.34)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.557 |  0.178 |                   53 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.476 |  0.159 |                   53 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.597 |  0.146 |                   21 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.413 |  0.175 |                    3 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.15856745839118958
[2m[36m(func pid=102191)[0m mae:  0.1140386313199997
[2m[36m(func pid=102191)[0m rmse_per_class: [0.112, 0.241, 0.056, 0.304, 0.057, 0.183, 0.261, 0.132, 0.14, 0.1]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17748181521892548
[2m[36m(func pid=101638)[0m mae:  0.13014712929725647
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.258, 0.094, 0.334, 0.101, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.1670195609331131
[2m[36m(func pid=113492)[0m mae:  0.12181080877780914
[2m[36m(func pid=113492)[0m rmse_per_class: [0.125, 0.258, 0.066, 0.321, 0.06, 0.186, 0.262, 0.138, 0.142, 0.112]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.14498209953308105
[2m[36m(func pid=109115)[0m mae:  0.09765778481960297
[2m[36m(func pid=109115)[0m rmse_per_class: [0.093, 0.231, 0.042, 0.271, 0.055, 0.182, 0.236, 0.122, 0.132, 0.086]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5450 | Steps: 2 | Val loss: 0.4154 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4748 | Steps: 2 | Val loss: 0.3782 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5543 | Steps: 2 | Val loss: 0.4468 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5977 | Steps: 2 | Val loss: 0.4670 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=101638)[0m rmse: 0.17733293771743774
[2m[36m(func pid=101638)[0m mae:  0.13003234565258026
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.334, 0.1, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
== Status ==
Current time: 2024-01-07 07:10:45 (running for 00:19:16.85)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.545 |  0.177 |                   55 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.476 |  0.159 |                   53 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.597 |  0.145 |                   22 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.462 |  0.167 |                    4 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.1582212746143341
[2m[36m(func pid=102191)[0m mae:  0.11372586339712143
[2m[36m(func pid=102191)[0m rmse_per_class: [0.111, 0.241, 0.055, 0.304, 0.057, 0.183, 0.26, 0.131, 0.14, 0.1]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.1585245579481125
[2m[36m(func pid=113492)[0m mae:  0.11391738802194595
[2m[36m(func pid=113492)[0m rmse_per_class: [0.118, 0.248, 0.052, 0.312, 0.055, 0.182, 0.244, 0.131, 0.14, 0.103]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.14406928420066833
[2m[36m(func pid=109115)[0m mae:  0.09592489898204803
[2m[36m(func pid=109115)[0m rmse_per_class: [0.089, 0.231, 0.041, 0.268, 0.055, 0.182, 0.236, 0.122, 0.132, 0.085]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5382 | Steps: 2 | Val loss: 0.4107 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.6303 | Steps: 2 | Val loss: 0.4874 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4799 | Steps: 2 | Val loss: 0.3786 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5850 | Steps: 2 | Val loss: 0.4593 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 07:10:50 (running for 00:19:21.99)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.538 |  0.177 |                   56 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.475 |  0.158 |                   54 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.598 |  0.144 |                   23 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.554 |  0.159 |                    5 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.17717242240905762
[2m[36m(func pid=101638)[0m mae:  0.12988923490047455
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.334, 0.1, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.15754514932632446
[2m[36m(func pid=102191)[0m mae:  0.11307375133037567
[2m[36m(func pid=102191)[0m rmse_per_class: [0.111, 0.241, 0.054, 0.302, 0.056, 0.183, 0.259, 0.131, 0.139, 0.099]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.15123404562473297
[2m[36m(func pid=113492)[0m mae:  0.10574959218502045
[2m[36m(func pid=113492)[0m rmse_per_class: [0.109, 0.24, 0.043, 0.302, 0.054, 0.18, 0.229, 0.125, 0.138, 0.093]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1434546858072281
[2m[36m(func pid=109115)[0m mae:  0.09427881240844727
[2m[36m(func pid=109115)[0m rmse_per_class: [0.086, 0.229, 0.041, 0.266, 0.056, 0.182, 0.238, 0.122, 0.131, 0.084]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5352 | Steps: 2 | Val loss: 0.4063 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4792 | Steps: 2 | Val loss: 0.3798 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.6560 | Steps: 2 | Val loss: 0.5024 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=101638)[0m rmse: 0.1771264225244522
[2m[36m(func pid=101638)[0m mae:  0.12986350059509277
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.334, 0.1, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5785 | Steps: 2 | Val loss: 0.4486 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 07:10:55 (running for 00:19:27.22)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.535 |  0.177 |                   57 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.48  |  0.158 |                   55 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.585 |  0.143 |                   24 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.63  |  0.151 |                    6 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102191)[0m rmse: 0.1568981111049652
[2m[36m(func pid=102191)[0m mae:  0.11243045330047607
[2m[36m(func pid=102191)[0m rmse_per_class: [0.11, 0.24, 0.053, 0.302, 0.056, 0.182, 0.258, 0.13, 0.139, 0.099]
[2m[36m(func pid=113492)[0m rmse: 0.1474401205778122
[2m[36m(func pid=113492)[0m mae:  0.09905140101909637
[2m[36m(func pid=113492)[0m rmse_per_class: [0.099, 0.234, 0.04, 0.294, 0.055, 0.18, 0.228, 0.124, 0.135, 0.084]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.14334869384765625
[2m[36m(func pid=109115)[0m mae:  0.09312815219163895
[2m[36m(func pid=109115)[0m rmse_per_class: [0.083, 0.227, 0.042, 0.265, 0.056, 0.182, 0.243, 0.122, 0.131, 0.083]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5258 | Steps: 2 | Val loss: 0.4020 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.6720 | Steps: 2 | Val loss: 0.4886 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4761 | Steps: 2 | Val loss: 0.3800 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 07:11:00 (running for 00:19:32.53)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.535 |  0.177 |                   57 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.479 |  0.157 |                   56 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.578 |  0.143 |                   25 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.672 |  0.149 |                    8 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.1770401895046234
[2m[36m(func pid=101638)[0m mae:  0.1297989785671234
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.334, 0.099, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.14870087802410126
[2m[36m(func pid=113492)[0m mae:  0.0954829603433609
[2m[36m(func pid=113492)[0m rmse_per_class: [0.089, 0.234, 0.041, 0.29, 0.056, 0.182, 0.254, 0.127, 0.133, 0.08]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.15621238946914673
[2m[36m(func pid=102191)[0m mae:  0.1117798238992691
[2m[36m(func pid=102191)[0m rmse_per_class: [0.109, 0.239, 0.052, 0.301, 0.056, 0.182, 0.256, 0.13, 0.139, 0.098]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5731 | Steps: 2 | Val loss: 0.4347 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5233 | Steps: 2 | Val loss: 0.3982 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=109115)[0m rmse: 0.14342868328094482
[2m[36m(func pid=109115)[0m mae:  0.0920642614364624
[2m[36m(func pid=109115)[0m rmse_per_class: [0.08, 0.227, 0.042, 0.264, 0.056, 0.183, 0.248, 0.122, 0.13, 0.082]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6318 | Steps: 2 | Val loss: 0.4493 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4784 | Steps: 2 | Val loss: 0.3802 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 07:11:06 (running for 00:19:37.85)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.526 |  0.177 |                   58 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.476 |  0.156 |                   57 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.573 |  0.143 |                   26 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.632 |  0.155 |                    9 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.1545954942703247
[2m[36m(func pid=113492)[0m mae:  0.09595534950494766
[2m[36m(func pid=113492)[0m rmse_per_class: [0.081, 0.246, 0.042, 0.293, 0.056, 0.186, 0.295, 0.133, 0.131, 0.08]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17702554166316986
[2m[36m(func pid=101638)[0m mae:  0.12978966534137726
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.257, 0.093, 0.333, 0.099, 0.19, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.15574762225151062
[2m[36m(func pid=102191)[0m mae:  0.11130279302597046
[2m[36m(func pid=102191)[0m rmse_per_class: [0.109, 0.238, 0.052, 0.301, 0.056, 0.182, 0.255, 0.13, 0.138, 0.097]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5537 | Steps: 2 | Val loss: 0.4201 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5741 | Steps: 2 | Val loss: 0.3919 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5187 | Steps: 2 | Val loss: 0.3943 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4770 | Steps: 2 | Val loss: 0.3802 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=109115)[0m rmse: 0.14388494193553925
[2m[36m(func pid=109115)[0m mae:  0.09130588173866272
[2m[36m(func pid=109115)[0m rmse_per_class: [0.077, 0.226, 0.043, 0.263, 0.056, 0.183, 0.256, 0.123, 0.13, 0.082]
[2m[36m(func pid=109115)[0m 
== Status ==
Current time: 2024-01-07 07:11:11 (running for 00:19:43.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.523 |  0.177 |                   59 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.478 |  0.156 |                   58 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.554 |  0.144 |                   27 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.574 |  0.161 |                   10 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.1610458493232727
[2m[36m(func pid=113492)[0m mae:  0.09797951579093933
[2m[36m(func pid=113492)[0m rmse_per_class: [0.077, 0.261, 0.043, 0.296, 0.056, 0.191, 0.335, 0.137, 0.131, 0.081]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17701272666454315
[2m[36m(func pid=101638)[0m mae:  0.12977610528469086
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.257, 0.093, 0.333, 0.098, 0.19, 0.291, 0.14, 0.142, 0.109]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.15518124401569366
[2m[36m(func pid=102191)[0m mae:  0.11073080450296402
[2m[36m(func pid=102191)[0m rmse_per_class: [0.108, 0.237, 0.051, 0.3, 0.055, 0.181, 0.254, 0.129, 0.138, 0.097]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5336 | Steps: 2 | Val loss: 0.4034 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4905 | Steps: 2 | Val loss: 0.3356 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.5102 | Steps: 2 | Val loss: 0.3908 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4728 | Steps: 2 | Val loss: 0.3800 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=109115)[0m rmse: 0.14437493681907654
[2m[36m(func pid=109115)[0m mae:  0.09066615253686905
[2m[36m(func pid=109115)[0m rmse_per_class: [0.075, 0.226, 0.043, 0.262, 0.056, 0.183, 0.263, 0.124, 0.13, 0.082]
[2m[36m(func pid=109115)[0m 
== Status ==
Current time: 2024-01-07 07:11:16 (running for 00:19:48.31)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.519 |  0.177 |                   60 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.477 |  0.155 |                   59 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.534 |  0.144 |                   28 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.491 |  0.162 |                   11 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.16201560199260712
[2m[36m(func pid=113492)[0m mae:  0.09771058708429337
[2m[36m(func pid=113492)[0m rmse_per_class: [0.076, 0.27, 0.044, 0.296, 0.056, 0.187, 0.34, 0.137, 0.131, 0.083]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17697791755199432
[2m[36m(func pid=101638)[0m mae:  0.1297534704208374
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.257, 0.092, 0.333, 0.098, 0.19, 0.291, 0.14, 0.142, 0.109]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1546339988708496
[2m[36m(func pid=102191)[0m mae:  0.11018850654363632
[2m[36m(func pid=102191)[0m rmse_per_class: [0.108, 0.237, 0.05, 0.299, 0.055, 0.181, 0.253, 0.129, 0.138, 0.096]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5116 | Steps: 2 | Val loss: 0.3856 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4385 | Steps: 2 | Val loss: 0.3074 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5035 | Steps: 2 | Val loss: 0.3870 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4765 | Steps: 2 | Val loss: 0.3787 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=109115)[0m rmse: 0.14485938847064972
[2m[36m(func pid=109115)[0m mae:  0.09018377214670181
[2m[36m(func pid=109115)[0m rmse_per_class: [0.073, 0.225, 0.044, 0.263, 0.056, 0.182, 0.269, 0.125, 0.13, 0.083]
[2m[36m(func pid=109115)[0m 
== Status ==
Current time: 2024-01-07 07:11:21 (running for 00:19:53.45)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.51  |  0.177 |                   61 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.473 |  0.155 |                   60 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.512 |  0.145 |                   29 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.439 |  0.154 |                   12 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.15442392230033875
[2m[36m(func pid=113492)[0m mae:  0.09321435540914536
[2m[36m(func pid=113492)[0m rmse_per_class: [0.083, 0.266, 0.043, 0.286, 0.056, 0.173, 0.29, 0.132, 0.13, 0.084]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17688965797424316
[2m[36m(func pid=101638)[0m mae:  0.1296606808900833
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.257, 0.092, 0.333, 0.098, 0.19, 0.291, 0.141, 0.142, 0.109]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.15402883291244507
[2m[36m(func pid=102191)[0m mae:  0.10955774784088135
[2m[36m(func pid=102191)[0m rmse_per_class: [0.107, 0.236, 0.05, 0.299, 0.055, 0.181, 0.251, 0.129, 0.138, 0.095]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4835 | Steps: 2 | Val loss: 0.3676 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3965 | Steps: 2 | Val loss: 0.3171 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4997 | Steps: 2 | Val loss: 0.3834 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4729 | Steps: 2 | Val loss: 0.3778 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=109115)[0m rmse: 0.1455163210630417
[2m[36m(func pid=109115)[0m mae:  0.08996038883924484
[2m[36m(func pid=109115)[0m rmse_per_class: [0.072, 0.226, 0.044, 0.263, 0.056, 0.181, 0.275, 0.126, 0.13, 0.083]
[2m[36m(func pid=109115)[0m 
== Status ==
Current time: 2024-01-07 07:11:27 (running for 00:19:58.74)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.503 |  0.177 |                   62 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.476 |  0.154 |                   61 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.483 |  0.146 |                   30 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.396 |  0.145 |                   13 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.14529481530189514
[2m[36m(func pid=113492)[0m mae:  0.08955200761556625
[2m[36m(func pid=113492)[0m rmse_per_class: [0.098, 0.252, 0.041, 0.274, 0.056, 0.157, 0.226, 0.124, 0.135, 0.091]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17675670981407166
[2m[36m(func pid=101638)[0m mae:  0.12955188751220703
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.257, 0.092, 0.333, 0.098, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.15345552563667297
[2m[36m(func pid=102191)[0m mae:  0.10898350179195404
[2m[36m(func pid=102191)[0m rmse_per_class: [0.106, 0.235, 0.049, 0.298, 0.055, 0.18, 0.25, 0.128, 0.137, 0.095]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4801 | Steps: 2 | Val loss: 0.3480 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4119 | Steps: 2 | Val loss: 0.3600 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4986 | Steps: 2 | Val loss: 0.3798 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4771 | Steps: 2 | Val loss: 0.3770 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=109115)[0m rmse: 0.14606121182441711
[2m[36m(func pid=109115)[0m mae:  0.08978372812271118
[2m[36m(func pid=109115)[0m rmse_per_class: [0.071, 0.226, 0.044, 0.263, 0.056, 0.181, 0.28, 0.126, 0.13, 0.084]
[2m[36m(func pid=109115)[0m 
== Status ==
Current time: 2024-01-07 07:11:32 (running for 00:20:03.74)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.5   |  0.177 |                   63 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.473 |  0.153 |                   62 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.48  |  0.146 |                   31 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.412 |  0.145 |                   14 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.14512142539024353
[2m[36m(func pid=113492)[0m mae:  0.09250987321138382
[2m[36m(func pid=113492)[0m rmse_per_class: [0.114, 0.242, 0.036, 0.269, 0.056, 0.151, 0.213, 0.119, 0.151, 0.099]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17667464911937714
[2m[36m(func pid=101638)[0m mae:  0.1294785737991333
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.333, 0.097, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.15295009315013885
[2m[36m(func pid=102191)[0m mae:  0.10845950990915298
[2m[36m(func pid=102191)[0m rmse_per_class: [0.105, 0.235, 0.048, 0.297, 0.055, 0.18, 0.249, 0.128, 0.137, 0.094]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4809 | Steps: 2 | Val loss: 0.4117 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4621 | Steps: 2 | Val loss: 0.3295 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4913 | Steps: 2 | Val loss: 0.3763 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4736 | Steps: 2 | Val loss: 0.3755 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 07:11:37 (running for 00:20:08.86)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.499 |  0.177 |                   64 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.477 |  0.153 |                   63 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.48  |  0.146 |                   31 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.481 |  0.152 |                   15 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.15211619436740875
[2m[36m(func pid=113492)[0m mae:  0.09898890554904938
[2m[36m(func pid=113492)[0m rmse_per_class: [0.124, 0.24, 0.031, 0.27, 0.056, 0.152, 0.248, 0.115, 0.174, 0.11]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.14617639780044556
[2m[36m(func pid=109115)[0m mae:  0.08942529559135437
[2m[36m(func pid=109115)[0m rmse_per_class: [0.07, 0.227, 0.044, 0.263, 0.056, 0.18, 0.282, 0.126, 0.13, 0.084]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17656676471233368
[2m[36m(func pid=101638)[0m mae:  0.12940455973148346
[2m[36m(func pid=101638)[0m rmse_per_class: [0.117, 0.257, 0.091, 0.333, 0.097, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.15242734551429749
[2m[36m(func pid=102191)[0m mae:  0.10792028903961182
[2m[36m(func pid=102191)[0m rmse_per_class: [0.105, 0.234, 0.048, 0.296, 0.055, 0.179, 0.248, 0.127, 0.137, 0.094]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5108 | Steps: 2 | Val loss: 0.4289 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4235 | Steps: 2 | Val loss: 0.3123 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4899 | Steps: 2 | Val loss: 0.3731 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4754 | Steps: 2 | Val loss: 0.3743 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 07:11:42 (running for 00:20:14.01)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.491 |  0.177 |                   65 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.474 |  0.152 |                   64 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.462 |  0.146 |                   32 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.511 |  0.159 |                   16 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.158634752035141
[2m[36m(func pid=113492)[0m mae:  0.1031753197312355
[2m[36m(func pid=113492)[0m rmse_per_class: [0.119, 0.247, 0.028, 0.271, 0.056, 0.159, 0.283, 0.118, 0.18, 0.127]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1459421068429947
[2m[36m(func pid=109115)[0m mae:  0.08900158107280731
[2m[36m(func pid=109115)[0m rmse_per_class: [0.069, 0.228, 0.045, 0.264, 0.056, 0.177, 0.28, 0.125, 0.13, 0.085]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.1765056699514389
[2m[36m(func pid=101638)[0m mae:  0.1293439269065857
[2m[36m(func pid=101638)[0m rmse_per_class: [0.117, 0.257, 0.091, 0.332, 0.097, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1519501656293869
[2m[36m(func pid=102191)[0m mae:  0.10742833465337753
[2m[36m(func pid=102191)[0m rmse_per_class: [0.104, 0.234, 0.047, 0.296, 0.055, 0.179, 0.247, 0.127, 0.137, 0.093]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5025 | Steps: 2 | Val loss: 0.4127 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4857 | Steps: 2 | Val loss: 0.3699 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3970 | Steps: 2 | Val loss: 0.2978 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4666 | Steps: 2 | Val loss: 0.3726 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=113492)[0m rmse: 0.16663238406181335
[2m[36m(func pid=113492)[0m mae:  0.1057496890425682
[2m[36m(func pid=113492)[0m rmse_per_class: [0.107, 0.262, 0.025, 0.275, 0.056, 0.169, 0.308, 0.129, 0.17, 0.166]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:11:47 (running for 00:20:19.30)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.49  |  0.177 |                   66 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.475 |  0.152 |                   65 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.423 |  0.146 |                   33 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.503 |  0.167 |                   17 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=101638)[0m rmse: 0.17639859020709991
[2m[36m(func pid=101638)[0m mae:  0.12926985323429108
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.332, 0.096, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1514931172132492
[2m[36m(func pid=102191)[0m mae:  0.10694608837366104
[2m[36m(func pid=102191)[0m rmse_per_class: [0.104, 0.233, 0.047, 0.295, 0.055, 0.179, 0.246, 0.127, 0.137, 0.093]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1450686752796173
[2m[36m(func pid=109115)[0m mae:  0.08839789777994156
[2m[36m(func pid=109115)[0m rmse_per_class: [0.07, 0.229, 0.045, 0.265, 0.056, 0.174, 0.271, 0.125, 0.13, 0.085]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4873 | Steps: 2 | Val loss: 0.3988 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4794 | Steps: 2 | Val loss: 0.3667 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4684 | Steps: 2 | Val loss: 0.3713 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4111 | Steps: 2 | Val loss: 0.2858 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 07:11:52 (running for 00:20:24.53)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.486 |  0.176 |                   67 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.467 |  0.151 |                   66 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.397 |  0.145 |                   34 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.487 |  0.178 |                   18 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.17826209962368011
[2m[36m(func pid=113492)[0m mae:  0.1087108626961708
[2m[36m(func pid=113492)[0m rmse_per_class: [0.095, 0.283, 0.027, 0.287, 0.056, 0.177, 0.32, 0.149, 0.152, 0.235]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17630578577518463
[2m[36m(func pid=101638)[0m mae:  0.1291896253824234
[2m[36m(func pid=101638)[0m rmse_per_class: [0.117, 0.257, 0.091, 0.332, 0.096, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.15110601484775543
[2m[36m(func pid=102191)[0m mae:  0.10652722418308258
[2m[36m(func pid=102191)[0m rmse_per_class: [0.103, 0.233, 0.046, 0.294, 0.055, 0.179, 0.245, 0.127, 0.136, 0.093]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1441020667552948
[2m[36m(func pid=109115)[0m mae:  0.08788800984621048
[2m[36m(func pid=109115)[0m rmse_per_class: [0.07, 0.231, 0.044, 0.266, 0.056, 0.171, 0.263, 0.124, 0.129, 0.086]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4313 | Steps: 2 | Val loss: 0.4046 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4777 | Steps: 2 | Val loss: 0.3635 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4656 | Steps: 2 | Val loss: 0.3696 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3777 | Steps: 2 | Val loss: 0.2777 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 07:11:58 (running for 00:20:29.57)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.479 |  0.176 |                   68 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.468 |  0.151 |                   67 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.411 |  0.144 |                   35 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.431 |  0.192 |                   19 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.19201284646987915
[2m[36m(func pid=113492)[0m mae:  0.11343429237604141
[2m[36m(func pid=113492)[0m rmse_per_class: [0.09, 0.305, 0.034, 0.306, 0.056, 0.179, 0.321, 0.174, 0.138, 0.316]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.15064817667007446
[2m[36m(func pid=102191)[0m mae:  0.10599036514759064
[2m[36m(func pid=102191)[0m rmse_per_class: [0.103, 0.232, 0.046, 0.294, 0.055, 0.179, 0.244, 0.126, 0.136, 0.092]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17621256411075592
[2m[36m(func pid=101638)[0m mae:  0.12912003695964813
[2m[36m(func pid=101638)[0m rmse_per_class: [0.117, 0.257, 0.09, 0.332, 0.096, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1427648365497589
[2m[36m(func pid=109115)[0m mae:  0.08731526136398315
[2m[36m(func pid=109115)[0m rmse_per_class: [0.071, 0.233, 0.044, 0.266, 0.056, 0.168, 0.25, 0.123, 0.129, 0.086]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4268 | Steps: 2 | Val loss: 0.4260 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4654 | Steps: 2 | Val loss: 0.3674 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4720 | Steps: 2 | Val loss: 0.3609 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3659 | Steps: 2 | Val loss: 0.2732 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 07:12:03 (running for 00:20:34.77)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.478 |  0.176 |                   69 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.466 |  0.151 |                   68 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.378 |  0.143 |                   36 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.427 |  0.202 |                   20 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.20212510228157043
[2m[36m(func pid=113492)[0m mae:  0.11721052974462509
[2m[36m(func pid=113492)[0m rmse_per_class: [0.092, 0.32, 0.041, 0.322, 0.056, 0.181, 0.31, 0.187, 0.134, 0.378]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.15012145042419434
[2m[36m(func pid=102191)[0m mae:  0.10543829202651978
[2m[36m(func pid=102191)[0m rmse_per_class: [0.101, 0.232, 0.046, 0.293, 0.054, 0.178, 0.243, 0.126, 0.136, 0.091]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17610809206962585
[2m[36m(func pid=101638)[0m mae:  0.1290397346019745
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.332, 0.095, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1412176787853241
[2m[36m(func pid=109115)[0m mae:  0.08678147196769714
[2m[36m(func pid=109115)[0m rmse_per_class: [0.073, 0.234, 0.044, 0.266, 0.056, 0.163, 0.237, 0.121, 0.129, 0.087]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4324 | Steps: 2 | Val loss: 0.4552 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4662 | Steps: 2 | Val loss: 0.3652 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4687 | Steps: 2 | Val loss: 0.3582 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=113492)[0m rmse: 0.20428037643432617
[2m[36m(func pid=113492)[0m mae:  0.11687681823968887
[2m[36m(func pid=113492)[0m rmse_per_class: [0.091, 0.332, 0.047, 0.329, 0.056, 0.178, 0.28, 0.186, 0.134, 0.41]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:12:08 (running for 00:20:40.21)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.472 |  0.176 |                   70 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.15  |                   69 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.366 |  0.141 |                   37 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.432 |  0.204 |                   21 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3705 | Steps: 2 | Val loss: 0.2733 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=102191)[0m rmse: 0.14957863092422485
[2m[36m(func pid=102191)[0m mae:  0.1048312559723854
[2m[36m(func pid=102191)[0m rmse_per_class: [0.101, 0.232, 0.045, 0.293, 0.054, 0.178, 0.241, 0.126, 0.136, 0.091]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17605948448181152
[2m[36m(func pid=101638)[0m mae:  0.12900449335575104
[2m[36m(func pid=101638)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.332, 0.095, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.13994917273521423
[2m[36m(func pid=109115)[0m mae:  0.08659958839416504
[2m[36m(func pid=109115)[0m rmse_per_class: [0.075, 0.235, 0.044, 0.266, 0.056, 0.16, 0.226, 0.12, 0.129, 0.089]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4279 | Steps: 2 | Val loss: 0.4776 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4647 | Steps: 2 | Val loss: 0.3632 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4655 | Steps: 2 | Val loss: 0.3556 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 07:12:13 (running for 00:20:45.49)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.469 |  0.176 |                   71 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.466 |  0.15  |                   70 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.37  |  0.14  |                   38 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.428 |  0.198 |                   22 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.19774499535560608
[2m[36m(func pid=113492)[0m mae:  0.11061610281467438
[2m[36m(func pid=113492)[0m rmse_per_class: [0.09, 0.338, 0.049, 0.329, 0.058, 0.174, 0.239, 0.162, 0.135, 0.402]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3613 | Steps: 2 | Val loss: 0.2774 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=102191)[0m rmse: 0.14917849004268646
[2m[36m(func pid=102191)[0m mae:  0.10435284674167633
[2m[36m(func pid=102191)[0m rmse_per_class: [0.1, 0.231, 0.045, 0.292, 0.054, 0.178, 0.24, 0.125, 0.135, 0.09]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17598465085029602
[2m[36m(func pid=101638)[0m mae:  0.12894776463508606
[2m[36m(func pid=101638)[0m rmse_per_class: [0.117, 0.257, 0.09, 0.332, 0.094, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.13924990594387054
[2m[36m(func pid=109115)[0m mae:  0.0869758129119873
[2m[36m(func pid=109115)[0m rmse_per_class: [0.079, 0.236, 0.043, 0.266, 0.056, 0.156, 0.217, 0.118, 0.129, 0.091]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4708 | Steps: 2 | Val loss: 0.4899 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4569 | Steps: 2 | Val loss: 0.3612 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4628 | Steps: 2 | Val loss: 0.3532 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 07:12:19 (running for 00:20:50.88)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.466 |  0.176 |                   72 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.149 |                   71 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.361 |  0.139 |                   39 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.471 |  0.194 |                   23 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.1936904489994049
[2m[36m(func pid=113492)[0m mae:  0.10649404674768448
[2m[36m(func pid=113492)[0m rmse_per_class: [0.089, 0.337, 0.047, 0.326, 0.068, 0.18, 0.26, 0.129, 0.136, 0.366]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.14877168834209442
[2m[36m(func pid=102191)[0m mae:  0.10388640314340591
[2m[36m(func pid=102191)[0m rmse_per_class: [0.099, 0.23, 0.044, 0.292, 0.054, 0.178, 0.239, 0.125, 0.135, 0.09]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3640 | Steps: 2 | Val loss: 0.2840 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=101638)[0m rmse: 0.17590081691741943
[2m[36m(func pid=101638)[0m mae:  0.12888041138648987
[2m[36m(func pid=101638)[0m rmse_per_class: [0.117, 0.257, 0.09, 0.332, 0.094, 0.19, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4481 | Steps: 2 | Val loss: 0.4943 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4553 | Steps: 2 | Val loss: 0.3590 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=109115)[0m rmse: 0.13871771097183228
[2m[36m(func pid=109115)[0m mae:  0.08746526390314102
[2m[36m(func pid=109115)[0m rmse_per_class: [0.083, 0.235, 0.043, 0.265, 0.056, 0.154, 0.211, 0.117, 0.13, 0.092]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4582 | Steps: 2 | Val loss: 0.3510 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 07:12:24 (running for 00:20:56.09)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.463 |  0.176 |                   73 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.457 |  0.149 |                   72 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.364 |  0.139 |                   40 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.448 |  0.2   |                   24 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.20023636519908905
[2m[36m(func pid=113492)[0m mae:  0.11089120805263519
[2m[36m(func pid=113492)[0m rmse_per_class: [0.085, 0.328, 0.041, 0.331, 0.087, 0.195, 0.365, 0.125, 0.136, 0.309]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.14848585426807404
[2m[36m(func pid=102191)[0m mae:  0.1035080999135971
[2m[36m(func pid=102191)[0m rmse_per_class: [0.099, 0.23, 0.044, 0.292, 0.054, 0.178, 0.239, 0.125, 0.135, 0.089]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3737 | Steps: 2 | Val loss: 0.2934 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=101638)[0m rmse: 0.17583835124969482
[2m[36m(func pid=101638)[0m mae:  0.12883274257183075
[2m[36m(func pid=101638)[0m rmse_per_class: [0.117, 0.257, 0.089, 0.331, 0.094, 0.19, 0.29, 0.141, 0.143, 0.108]
[2m[36m(func pid=101638)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4651 | Steps: 2 | Val loss: 0.4962 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4553 | Steps: 2 | Val loss: 0.3565 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=109115)[0m rmse: 0.13919129967689514
[2m[36m(func pid=109115)[0m mae:  0.08863438665866852
[2m[36m(func pid=109115)[0m rmse_per_class: [0.088, 0.236, 0.041, 0.266, 0.056, 0.152, 0.21, 0.116, 0.132, 0.094]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=101638)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4556 | Steps: 2 | Val loss: 0.3487 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 07:12:29 (running for 00:21:01.34)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00008 | RUNNING    | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.458 |  0.176 |                   74 |
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.455 |  0.148 |                   73 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.374 |  0.139 |                   41 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.465 |  0.208 |                   25 |
| train_ccef6_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=113492)[0m rmse: 0.20785334706306458
[2m[36m(func pid=113492)[0m mae:  0.1179313063621521
[2m[36m(func pid=113492)[0m rmse_per_class: [0.083, 0.315, 0.034, 0.34, 0.104, 0.209, 0.466, 0.137, 0.136, 0.254]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.14833031594753265
[2m[36m(func pid=102191)[0m mae:  0.10333368927240372
[2m[36m(func pid=102191)[0m rmse_per_class: [0.099, 0.23, 0.044, 0.293, 0.054, 0.177, 0.238, 0.125, 0.135, 0.089]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=101638)[0m rmse: 0.17571306228637695
[2m[36m(func pid=101638)[0m mae:  0.12873128056526184
[2m[36m(func pid=101638)[0m rmse_per_class: [0.117, 0.256, 0.089, 0.331, 0.093, 0.19, 0.289, 0.14, 0.143, 0.108]
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3941 | Steps: 2 | Val loss: 0.3049 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4389 | Steps: 2 | Val loss: 0.4982 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4537 | Steps: 2 | Val loss: 0.3535 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=109115)[0m rmse: 0.14033636450767517
[2m[36m(func pid=109115)[0m mae:  0.09018892794847488
[2m[36m(func pid=109115)[0m rmse_per_class: [0.094, 0.237, 0.04, 0.267, 0.056, 0.151, 0.213, 0.115, 0.135, 0.095]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.1478988230228424
[2m[36m(func pid=102191)[0m mae:  0.10287611186504364
[2m[36m(func pid=102191)[0m rmse_per_class: [0.098, 0.23, 0.043, 0.292, 0.054, 0.177, 0.237, 0.125, 0.135, 0.088]
[2m[36m(func pid=113492)[0m rmse: 0.20980794727802277
[2m[36m(func pid=113492)[0m mae:  0.12160706520080566
[2m[36m(func pid=113492)[0m rmse_per_class: [0.082, 0.308, 0.029, 0.346, 0.12, 0.215, 0.512, 0.143, 0.135, 0.209]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3843 | Steps: 2 | Val loss: 0.3153 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4541 | Steps: 2 | Val loss: 0.4920 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=109115)[0m rmse: 0.14155647158622742
[2m[36m(func pid=109115)[0m mae:  0.0915551632642746
[2m[36m(func pid=109115)[0m rmse_per_class: [0.097, 0.236, 0.039, 0.267, 0.056, 0.15, 0.218, 0.114, 0.138, 0.098]
== Status ==
Current time: 2024-01-07 07:12:34 (running for 00:21:06.48)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.455 |  0.148 |                   74 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.394 |  0.14  |                   42 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.439 |  0.21  |                   26 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=119168)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=119168)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=119168)[0m Configuration completed!
[2m[36m(func pid=119168)[0m New optimizer parameters:
[2m[36m(func pid=119168)[0m SGD (
[2m[36m(func pid=119168)[0m Parameter Group 0
[2m[36m(func pid=119168)[0m     dampening: 0
[2m[36m(func pid=119168)[0m     differentiable: False
[2m[36m(func pid=119168)[0m     foreach: None
[2m[36m(func pid=119168)[0m     lr: 0.0001
[2m[36m(func pid=119168)[0m     maximize: False
[2m[36m(func pid=119168)[0m     momentum: 0.9
[2m[36m(func pid=119168)[0m     nesterov: False
[2m[36m(func pid=119168)[0m     weight_decay: 0.0001
[2m[36m(func pid=119168)[0m )
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.20710638165473938
[2m[36m(func pid=113492)[0m mae:  0.12075114250183105
[2m[36m(func pid=113492)[0m rmse_per_class: [0.081, 0.303, 0.025, 0.345, 0.133, 0.214, 0.507, 0.146, 0.136, 0.182]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:12:40 (running for 00:21:12.02)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.454 |  0.148 |                   75 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.384 |  0.142 |                   43 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.454 |  0.207 |                   27 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.4453 | Steps: 2 | Val loss: 0.3504 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4049 | Steps: 2 | Val loss: 0.3255 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4312 | Steps: 2 | Val loss: 0.4754 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8986 | Steps: 2 | Val loss: 0.7099 | Batch size: 32 | lr: 0.0001 | Duration: 4.42s
[2m[36m(func pid=102191)[0m rmse: 0.14743053913116455
[2m[36m(func pid=102191)[0m mae:  0.10226953029632568
[2m[36m(func pid=102191)[0m rmse_per_class: [0.097, 0.23, 0.043, 0.291, 0.055, 0.177, 0.236, 0.124, 0.134, 0.088]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.14306965470314026
[2m[36m(func pid=109115)[0m mae:  0.09306599199771881
[2m[36m(func pid=109115)[0m rmse_per_class: [0.101, 0.236, 0.038, 0.267, 0.056, 0.15, 0.226, 0.114, 0.142, 0.1]
[2m[36m(func pid=109115)[0m 
== Status ==
Current time: 2024-01-07 07:12:45 (running for 00:21:17.08)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.445 |  0.147 |                   76 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.405 |  0.143 |                   44 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.431 |  0.202 |                   28 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=113492)[0m rmse: 0.20249071717262268
[2m[36m(func pid=113492)[0m mae:  0.11643503606319427
[2m[36m(func pid=113492)[0m rmse_per_class: [0.079, 0.308, 0.024, 0.341, 0.155, 0.208, 0.457, 0.145, 0.138, 0.169]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.1827840656042099
[2m[36m(func pid=119168)[0m mae:  0.13450253009796143
[2m[36m(func pid=119168)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.4507 | Steps: 2 | Val loss: 0.3469 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4040 | Steps: 2 | Val loss: 0.3346 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4163 | Steps: 2 | Val loss: 0.4539 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8977 | Steps: 2 | Val loss: 0.7057 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=102191)[0m rmse: 0.14700467884540558
[2m[36m(func pid=102191)[0m mae:  0.10177850723266602
[2m[36m(func pid=102191)[0m rmse_per_class: [0.097, 0.23, 0.043, 0.29, 0.055, 0.176, 0.235, 0.124, 0.134, 0.087]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.14513292908668518
[2m[36m(func pid=109115)[0m mae:  0.09490140527486801
[2m[36m(func pid=109115)[0m rmse_per_class: [0.104, 0.236, 0.036, 0.268, 0.056, 0.15, 0.235, 0.114, 0.146, 0.104]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.1961047351360321
[2m[36m(func pid=113492)[0m mae:  0.11016354709863663
[2m[36m(func pid=113492)[0m rmse_per_class: [0.079, 0.325, 0.025, 0.338, 0.176, 0.194, 0.373, 0.143, 0.145, 0.162]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:12:50 (running for 00:21:22.09)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.451 |  0.147 |                   77 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.404 |  0.145 |                   45 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.416 |  0.196 |                   29 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.899 |  0.183 |                    1 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.18259373307228088
[2m[36m(func pid=119168)[0m mae:  0.1344020962715149
[2m[36m(func pid=119168)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.112]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.4373 | Steps: 2 | Val loss: 0.3446 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4255 | Steps: 2 | Val loss: 0.3412 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4028 | Steps: 2 | Val loss: 0.4379 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8946 | Steps: 2 | Val loss: 0.7019 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=102191)[0m rmse: 0.14675378799438477
[2m[36m(func pid=102191)[0m mae:  0.10138697922229767
[2m[36m(func pid=102191)[0m rmse_per_class: [0.096, 0.229, 0.042, 0.29, 0.055, 0.176, 0.234, 0.124, 0.134, 0.087]
[2m[36m(func pid=102191)[0m 
== Status ==
Current time: 2024-01-07 07:12:55 (running for 00:21:27.21)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.437 |  0.147 |                   78 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.404 |  0.145 |                   45 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.403 |  0.189 |                   30 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.898 |  0.183 |                    2 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=113492)[0m rmse: 0.1886751502752304
[2m[36m(func pid=113492)[0m mae:  0.10480532795190811
[2m[36m(func pid=113492)[0m rmse_per_class: [0.08, 0.345, 0.028, 0.338, 0.183, 0.178, 0.286, 0.136, 0.156, 0.156]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.14721235632896423
[2m[36m(func pid=109115)[0m mae:  0.09667980670928955
[2m[36m(func pid=109115)[0m rmse_per_class: [0.106, 0.238, 0.034, 0.27, 0.056, 0.151, 0.245, 0.114, 0.149, 0.108]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.18225038051605225
[2m[36m(func pid=119168)[0m mae:  0.1341506987810135
[2m[36m(func pid=119168)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.113, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.4332 | Steps: 2 | Val loss: 0.3417 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3901 | Steps: 2 | Val loss: 0.4369 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4144 | Steps: 2 | Val loss: 0.3453 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8923 | Steps: 2 | Val loss: 0.6988 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=102191)[0m rmse: 0.14638613164424896
[2m[36m(func pid=102191)[0m mae:  0.10086314380168915
[2m[36m(func pid=102191)[0m rmse_per_class: [0.095, 0.229, 0.042, 0.289, 0.055, 0.176, 0.234, 0.123, 0.134, 0.087]
[2m[36m(func pid=102191)[0m 
== Status ==
Current time: 2024-01-07 07:13:00 (running for 00:21:32.42)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.433 |  0.146 |                   79 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.425 |  0.147 |                   46 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.39  |  0.185 |                   31 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.895 |  0.182 |                    3 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=113492)[0m rmse: 0.18533171713352203
[2m[36m(func pid=113492)[0m mae:  0.10432149469852448
[2m[36m(func pid=113492)[0m rmse_per_class: [0.081, 0.357, 0.028, 0.349, 0.175, 0.178, 0.241, 0.131, 0.169, 0.145]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1491386592388153
[2m[36m(func pid=109115)[0m mae:  0.09819434583187103
[2m[36m(func pid=109115)[0m rmse_per_class: [0.106, 0.238, 0.032, 0.271, 0.056, 0.152, 0.255, 0.115, 0.154, 0.111]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.1818925142288208
[2m[36m(func pid=119168)[0m mae:  0.13386034965515137
[2m[36m(func pid=119168)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.339, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.4360 | Steps: 2 | Val loss: 0.3385 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3713 | Steps: 2 | Val loss: 0.4422 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8934 | Steps: 2 | Val loss: 0.6963 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4174 | Steps: 2 | Val loss: 0.3469 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=102191)[0m rmse: 0.1461302936077118
[2m[36m(func pid=102191)[0m mae:  0.10052094608545303
[2m[36m(func pid=102191)[0m rmse_per_class: [0.095, 0.229, 0.042, 0.289, 0.055, 0.176, 0.233, 0.123, 0.134, 0.086]
[2m[36m(func pid=102191)[0m 
== Status ==
Current time: 2024-01-07 07:13:06 (running for 00:21:37.63)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.436 |  0.146 |                   80 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.414 |  0.149 |                   47 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.371 |  0.188 |                   32 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.892 |  0.182 |                    4 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=113492)[0m rmse: 0.18788747489452362
[2m[36m(func pid=113492)[0m mae:  0.10767211765050888
[2m[36m(func pid=113492)[0m rmse_per_class: [0.081, 0.349, 0.028, 0.355, 0.155, 0.194, 0.263, 0.135, 0.18, 0.139]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.18140514194965363
[2m[36m(func pid=119168)[0m mae:  0.13346198201179504
[2m[36m(func pid=119168)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.112, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.15149368345737457
[2m[36m(func pid=109115)[0m mae:  0.09970402717590332
[2m[36m(func pid=109115)[0m rmse_per_class: [0.107, 0.239, 0.03, 0.273, 0.056, 0.153, 0.265, 0.116, 0.157, 0.119]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.4243 | Steps: 2 | Val loss: 0.3353 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4000 | Steps: 2 | Val loss: 0.4544 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8933 | Steps: 2 | Val loss: 0.6947 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=102191)[0m rmse: 0.14591506123542786
[2m[36m(func pid=102191)[0m mae:  0.10023758560419083
[2m[36m(func pid=102191)[0m rmse_per_class: [0.094, 0.229, 0.042, 0.288, 0.055, 0.176, 0.232, 0.123, 0.134, 0.086]
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4253 | Steps: 2 | Val loss: 0.3461 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.1893431693315506
[2m[36m(func pid=113492)[0m mae:  0.11028613150119781
[2m[36m(func pid=113492)[0m rmse_per_class: [0.083, 0.335, 0.031, 0.353, 0.132, 0.207, 0.294, 0.141, 0.184, 0.135]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.18107713758945465
[2m[36m(func pid=119168)[0m mae:  0.13318867981433868
[2m[36m(func pid=119168)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.112, 0.189, 0.294, 0.142, 0.142, 0.11]
== Status ==
Current time: 2024-01-07 07:13:12 (running for 00:21:43.63)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.424 |  0.146 |                   81 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.417 |  0.151 |                   48 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.4   |  0.189 |                   33 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.893 |  0.181 |                    6 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)

[2m[36m(func pid=119168)[0m 

[2m[36m(func pid=109115)[0m rmse: 0.15410341322422028
[2m[36m(func pid=109115)[0m mae:  0.10133679211139679
[2m[36m(func pid=109115)[0m rmse_per_class: [0.107, 0.241, 0.029, 0.275, 0.056, 0.154, 0.273, 0.118, 0.16, 0.127]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.4251 | Steps: 2 | Val loss: 0.3316 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4219 | Steps: 2 | Val loss: 0.4673 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8877 | Steps: 2 | Val loss: 0.6920 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=102191)[0m rmse: 0.14563366770744324
[2m[36m(func pid=102191)[0m mae:  0.09987718611955643
[2m[36m(func pid=102191)[0m rmse_per_class: [0.094, 0.229, 0.042, 0.288, 0.055, 0.175, 0.231, 0.123, 0.133, 0.086]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4165 | Steps: 2 | Val loss: 0.3436 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=113492)[0m rmse: 0.18866480886936188
[2m[36m(func pid=113492)[0m mae:  0.11107144504785538
[2m[36m(func pid=113492)[0m rmse_per_class: [0.089, 0.315, 0.036, 0.347, 0.114, 0.209, 0.31, 0.148, 0.186, 0.131]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:13:17 (running for 00:21:48.83)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.425 |  0.146 |                   82 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.425 |  0.154 |                   49 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.422 |  0.189 |                   34 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.888 |  0.181 |                    7 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.18071496486663818
[2m[36m(func pid=119168)[0m mae:  0.13288439810276031
[2m[36m(func pid=119168)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.4149 | Steps: 2 | Val loss: 0.3279 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=109115)[0m rmse: 0.15676063299179077
[2m[36m(func pid=109115)[0m mae:  0.10265852510929108
[2m[36m(func pid=109115)[0m rmse_per_class: [0.106, 0.244, 0.028, 0.276, 0.056, 0.155, 0.281, 0.12, 0.162, 0.139]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4464 | Steps: 2 | Val loss: 0.4738 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8885 | Steps: 2 | Val loss: 0.6915 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=102191)[0m rmse: 0.1451878547668457
[2m[36m(func pid=102191)[0m mae:  0.09942706674337387
[2m[36m(func pid=102191)[0m rmse_per_class: [0.093, 0.229, 0.042, 0.287, 0.055, 0.175, 0.231, 0.123, 0.133, 0.085]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.18858705461025238
[2m[36m(func pid=113492)[0m mae:  0.11127515137195587
[2m[36m(func pid=113492)[0m rmse_per_class: [0.1, 0.304, 0.042, 0.338, 0.101, 0.209, 0.315, 0.162, 0.183, 0.132]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4026 | Steps: 2 | Val loss: 0.3385 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 07:13:22 (running for 00:21:54.04)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.415 |  0.145 |                   83 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.417 |  0.157 |                   50 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.446 |  0.189 |                   35 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.889 |  0.18  |                    8 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.1804550588130951
[2m[36m(func pid=119168)[0m mae:  0.13266073167324066
[2m[36m(func pid=119168)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.4192 | Steps: 2 | Val loss: 0.3245 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4670 | Steps: 2 | Val loss: 0.4680 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=109115)[0m rmse: 0.158784419298172
[2m[36m(func pid=109115)[0m mae:  0.1035410538315773
[2m[36m(func pid=109115)[0m rmse_per_class: [0.105, 0.249, 0.027, 0.278, 0.056, 0.156, 0.287, 0.123, 0.161, 0.146]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8876 | Steps: 2 | Val loss: 0.6906 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=102191)[0m rmse: 0.1448483169078827
[2m[36m(func pid=102191)[0m mae:  0.09899594634771347
[2m[36m(func pid=102191)[0m rmse_per_class: [0.092, 0.229, 0.042, 0.286, 0.055, 0.175, 0.23, 0.123, 0.133, 0.085]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.18983006477355957
[2m[36m(func pid=113492)[0m mae:  0.1113409623503685
[2m[36m(func pid=113492)[0m rmse_per_class: [0.114, 0.302, 0.044, 0.333, 0.093, 0.202, 0.309, 0.183, 0.182, 0.136]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3898 | Steps: 2 | Val loss: 0.3339 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 07:13:27 (running for 00:21:59.22)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.419 |  0.145 |                   84 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.403 |  0.159 |                   51 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.467 |  0.19  |                   36 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.888 |  0.18  |                    9 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.18024560809135437
[2m[36m(func pid=119168)[0m mae:  0.1324724555015564
[2m[36m(func pid=119168)[0m rmse_per_class: [0.116, 0.263, 0.101, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.4153 | Steps: 2 | Val loss: 0.3208 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4353 | Steps: 2 | Val loss: 0.4552 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=109115)[0m rmse: 0.1610802859067917
[2m[36m(func pid=109115)[0m mae:  0.10433463752269745
[2m[36m(func pid=109115)[0m rmse_per_class: [0.103, 0.252, 0.025, 0.28, 0.056, 0.158, 0.292, 0.126, 0.16, 0.157]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8839 | Steps: 2 | Val loss: 0.6894 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=102191)[0m rmse: 0.14468501508235931
[2m[36m(func pid=102191)[0m mae:  0.09865938872098923
[2m[36m(func pid=102191)[0m rmse_per_class: [0.092, 0.229, 0.042, 0.286, 0.055, 0.174, 0.229, 0.123, 0.133, 0.085]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.18940480053424835
[2m[36m(func pid=113492)[0m mae:  0.10979022085666656
[2m[36m(func pid=113492)[0m rmse_per_class: [0.125, 0.306, 0.046, 0.332, 0.086, 0.188, 0.29, 0.204, 0.175, 0.141]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3876 | Steps: 2 | Val loss: 0.3306 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 07:13:32 (running for 00:22:04.33)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.415 |  0.145 |                   85 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.39  |  0.161 |                   52 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.435 |  0.189 |                   37 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.884 |  0.18  |                   10 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.180160254240036
[2m[36m(func pid=119168)[0m mae:  0.13238182663917542
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.263, 0.101, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.4011 | Steps: 2 | Val loss: 0.3173 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4681 | Steps: 2 | Val loss: 0.4457 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=109115)[0m rmse: 0.1638428121805191
[2m[36m(func pid=109115)[0m mae:  0.10521718114614487
[2m[36m(func pid=109115)[0m rmse_per_class: [0.099, 0.257, 0.025, 0.282, 0.056, 0.159, 0.296, 0.129, 0.159, 0.175]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8824 | Steps: 2 | Val loss: 0.6882 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=102191)[0m rmse: 0.14437249302864075
[2m[36m(func pid=102191)[0m mae:  0.09829595685005188
[2m[36m(func pid=102191)[0m rmse_per_class: [0.091, 0.229, 0.041, 0.285, 0.055, 0.174, 0.229, 0.122, 0.133, 0.085]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.18804144859313965
[2m[36m(func pid=113492)[0m mae:  0.10710345208644867
[2m[36m(func pid=113492)[0m rmse_per_class: [0.133, 0.309, 0.046, 0.332, 0.083, 0.176, 0.266, 0.211, 0.171, 0.153]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:13:37 (running for 00:22:09.49)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.401 |  0.144 |                   86 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.388 |  0.164 |                   53 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.468 |  0.188 |                   38 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.882 |  0.18  |                   11 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.1800273060798645
[2m[36m(func pid=119168)[0m mae:  0.13226695358753204
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3602 | Steps: 2 | Val loss: 0.3280 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.4005 | Steps: 2 | Val loss: 0.3138 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3799 | Steps: 2 | Val loss: 0.4498 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=109115)[0m rmse: 0.16677141189575195
[2m[36m(func pid=109115)[0m mae:  0.10604065656661987
[2m[36m(func pid=109115)[0m rmse_per_class: [0.095, 0.263, 0.025, 0.285, 0.056, 0.16, 0.297, 0.132, 0.157, 0.196]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8803 | Steps: 2 | Val loss: 0.6869 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=102191)[0m rmse: 0.14397403597831726
[2m[36m(func pid=102191)[0m mae:  0.0978497564792633
[2m[36m(func pid=102191)[0m rmse_per_class: [0.09, 0.229, 0.041, 0.285, 0.055, 0.173, 0.228, 0.122, 0.133, 0.084]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.186929851770401
[2m[36m(func pid=113492)[0m mae:  0.10489790141582489
[2m[36m(func pid=113492)[0m rmse_per_class: [0.133, 0.309, 0.045, 0.33, 0.079, 0.174, 0.263, 0.199, 0.167, 0.171]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:13:43 (running for 00:22:14.65)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.4   |  0.144 |                   87 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.36  |  0.167 |                   54 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.38  |  0.187 |                   39 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.88  |  0.18  |                   12 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.17989620566368103
[2m[36m(func pid=119168)[0m mae:  0.13215696811676025
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3573 | Steps: 2 | Val loss: 0.3280 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3989 | Steps: 2 | Val loss: 0.3108 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3846 | Steps: 2 | Val loss: 0.4660 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8758 | Steps: 2 | Val loss: 0.6846 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=109115)[0m rmse: 0.16981224715709686
[2m[36m(func pid=109115)[0m mae:  0.10682406276464462
[2m[36m(func pid=109115)[0m rmse_per_class: [0.092, 0.269, 0.026, 0.287, 0.056, 0.16, 0.297, 0.137, 0.154, 0.219]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.14380192756652832
[2m[36m(func pid=102191)[0m mae:  0.09767446666955948
[2m[36m(func pid=102191)[0m rmse_per_class: [0.09, 0.229, 0.041, 0.284, 0.055, 0.173, 0.228, 0.122, 0.133, 0.084]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.1894298940896988
[2m[36m(func pid=113492)[0m mae:  0.10631934553384781
[2m[36m(func pid=113492)[0m rmse_per_class: [0.12, 0.305, 0.044, 0.329, 0.072, 0.185, 0.314, 0.175, 0.165, 0.184]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:13:48 (running for 00:22:19.75)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.399 |  0.144 |                   88 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.357 |  0.17  |                   55 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.385 |  0.189 |                   40 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.876 |  0.18  |                   13 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.17980723083019257
[2m[36m(func pid=119168)[0m mae:  0.13207724690437317
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3459 | Steps: 2 | Val loss: 0.3291 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3959 | Steps: 2 | Val loss: 0.3073 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3558 | Steps: 2 | Val loss: 0.4916 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8761 | Steps: 2 | Val loss: 0.6836 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=102191)[0m rmse: 0.1435684710741043
[2m[36m(func pid=102191)[0m mae:  0.0972975343465805
[2m[36m(func pid=102191)[0m rmse_per_class: [0.089, 0.229, 0.041, 0.284, 0.055, 0.172, 0.227, 0.122, 0.133, 0.084]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.17280760407447815
[2m[36m(func pid=109115)[0m mae:  0.10765500366687775
[2m[36m(func pid=109115)[0m rmse_per_class: [0.092, 0.274, 0.026, 0.291, 0.056, 0.16, 0.297, 0.141, 0.152, 0.24]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.19515018165111542
[2m[36m(func pid=113492)[0m mae:  0.11094620078802109
[2m[36m(func pid=113492)[0m rmse_per_class: [0.112, 0.305, 0.041, 0.334, 0.064, 0.199, 0.381, 0.155, 0.161, 0.2]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:13:53 (running for 00:22:25.16)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.396 |  0.144 |                   89 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.346 |  0.173 |                   56 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.356 |  0.195 |                   41 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.876 |  0.18  |                   14 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.1797173023223877
[2m[36m(func pid=119168)[0m mae:  0.13199540972709656
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.111, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3876 | Steps: 2 | Val loss: 0.3041 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3805 | Steps: 2 | Val loss: 0.5128 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3406 | Steps: 2 | Val loss: 0.3318 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8719 | Steps: 2 | Val loss: 0.6824 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=102191)[0m rmse: 0.14343777298927307
[2m[36m(func pid=102191)[0m mae:  0.09713220596313477
[2m[36m(func pid=102191)[0m rmse_per_class: [0.089, 0.229, 0.041, 0.283, 0.055, 0.172, 0.227, 0.122, 0.133, 0.084]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.2001950442790985
[2m[36m(func pid=113492)[0m mae:  0.11506123840808868
[2m[36m(func pid=113492)[0m rmse_per_class: [0.102, 0.305, 0.035, 0.338, 0.058, 0.208, 0.427, 0.143, 0.159, 0.226]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.17537881433963776
[2m[36m(func pid=109115)[0m mae:  0.10830627381801605
[2m[36m(func pid=109115)[0m rmse_per_class: [0.089, 0.278, 0.028, 0.293, 0.056, 0.16, 0.294, 0.144, 0.151, 0.261]
[2m[36m(func pid=109115)[0m 
== Status ==
Current time: 2024-01-07 07:13:58 (running for 00:22:30.52)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.388 |  0.143 |                   90 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.341 |  0.175 |                   57 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.38  |  0.2   |                   42 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.872 |  0.18  |                   15 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.17965276539325714
[2m[36m(func pid=119168)[0m mae:  0.1319403201341629
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.11, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3902 | Steps: 2 | Val loss: 0.3004 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3832 | Steps: 2 | Val loss: 0.5281 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3402 | Steps: 2 | Val loss: 0.3366 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8687 | Steps: 2 | Val loss: 0.6809 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=102191)[0m rmse: 0.14314399659633636
[2m[36m(func pid=102191)[0m mae:  0.09673372656106949
[2m[36m(func pid=102191)[0m rmse_per_class: [0.088, 0.229, 0.041, 0.282, 0.055, 0.172, 0.226, 0.122, 0.132, 0.084]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.20471850037574768
[2m[36m(func pid=113492)[0m mae:  0.11726158857345581
[2m[36m(func pid=113492)[0m rmse_per_class: [0.095, 0.311, 0.03, 0.345, 0.059, 0.21, 0.435, 0.14, 0.16, 0.262]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.17792926728725433
[2m[36m(func pid=109115)[0m mae:  0.10891275107860565
[2m[36m(func pid=109115)[0m rmse_per_class: [0.088, 0.283, 0.03, 0.296, 0.056, 0.16, 0.29, 0.147, 0.147, 0.282]
[2m[36m(func pid=109115)[0m 
== Status ==
Current time: 2024-01-07 07:14:04 (running for 00:22:35.89)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.39  |  0.143 |                   91 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.34  |  0.178 |                   58 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.383 |  0.205 |                   43 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.869 |  0.18  |                   16 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.1796552836894989
[2m[36m(func pid=119168)[0m mae:  0.13192439079284668
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3916 | Steps: 2 | Val loss: 0.2978 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3586 | Steps: 2 | Val loss: 0.5447 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3326 | Steps: 2 | Val loss: 0.3414 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8679 | Steps: 2 | Val loss: 0.6792 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=113492)[0m rmse: 0.2080610990524292
[2m[36m(func pid=113492)[0m mae:  0.11796370893716812
[2m[36m(func pid=113492)[0m rmse_per_class: [0.094, 0.318, 0.031, 0.35, 0.059, 0.206, 0.42, 0.141, 0.159, 0.301]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.14314596354961395
[2m[36m(func pid=102191)[0m mae:  0.09681706130504608
[2m[36m(func pid=102191)[0m rmse_per_class: [0.088, 0.23, 0.04, 0.283, 0.055, 0.172, 0.226, 0.122, 0.133, 0.084]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.17953558266162872
[2m[36m(func pid=109115)[0m mae:  0.10911902040243149
[2m[36m(func pid=109115)[0m rmse_per_class: [0.086, 0.286, 0.032, 0.298, 0.055, 0.16, 0.285, 0.149, 0.146, 0.298]
[2m[36m(func pid=109115)[0m 
== Status ==
Current time: 2024-01-07 07:14:09 (running for 00:22:41.13)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.392 |  0.143 |                   92 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.333 |  0.18  |                   59 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.359 |  0.208 |                   44 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.868 |  0.18  |                   17 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.17961804568767548
[2m[36m(func pid=119168)[0m mae:  0.1318863034248352
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.111, 0.19, 0.294, 0.14, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3881 | Steps: 2 | Val loss: 0.2953 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3869 | Steps: 2 | Val loss: 0.5664 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3315 | Steps: 2 | Val loss: 0.3468 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8630 | Steps: 2 | Val loss: 0.6769 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=102191)[0m rmse: 0.14289869368076324
[2m[36m(func pid=102191)[0m mae:  0.0965377539396286
[2m[36m(func pid=102191)[0m rmse_per_class: [0.088, 0.23, 0.04, 0.282, 0.055, 0.172, 0.225, 0.122, 0.132, 0.084]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.2095613032579422
[2m[36m(func pid=113492)[0m mae:  0.1177438274025917
[2m[36m(func pid=113492)[0m rmse_per_class: [0.095, 0.327, 0.032, 0.358, 0.06, 0.197, 0.38, 0.144, 0.162, 0.341]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.18075820803642273
[2m[36m(func pid=109115)[0m mae:  0.1091117262840271
[2m[36m(func pid=109115)[0m rmse_per_class: [0.084, 0.289, 0.034, 0.3, 0.055, 0.16, 0.279, 0.15, 0.143, 0.312]
[2m[36m(func pid=109115)[0m 
== Status ==
Current time: 2024-01-07 07:14:14 (running for 00:22:46.49)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.388 |  0.143 |                   93 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.332 |  0.181 |                   60 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.387 |  0.21  |                   45 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.863 |  0.18  |                   18 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.1795947253704071
[2m[36m(func pid=119168)[0m mae:  0.13187795877456665
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4292 | Steps: 2 | Val loss: 0.5852 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3757 | Steps: 2 | Val loss: 0.2924 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3356 | Steps: 2 | Val loss: 0.3500 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=113492)[0m rmse: 0.2091566026210785
[2m[36m(func pid=113492)[0m mae:  0.11733396351337433
[2m[36m(func pid=113492)[0m rmse_per_class: [0.094, 0.337, 0.031, 0.364, 0.059, 0.186, 0.339, 0.149, 0.161, 0.373]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8627 | Steps: 2 | Val loss: 0.6745 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=102191)[0m rmse: 0.14274707436561584
[2m[36m(func pid=102191)[0m mae:  0.09620948880910873
[2m[36m(func pid=102191)[0m rmse_per_class: [0.087, 0.23, 0.04, 0.282, 0.055, 0.171, 0.225, 0.122, 0.132, 0.084]
[2m[36m(func pid=102191)[0m 
== Status ==
Current time: 2024-01-07 07:14:20 (running for 00:22:51.70)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.376 |  0.143 |                   94 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.336 |  0.181 |                   61 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.429 |  0.209 |                   46 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.863 |  0.18  |                   18 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=109115)[0m rmse: 0.18057909607887268
[2m[36m(func pid=109115)[0m mae:  0.10819081962108612
[2m[36m(func pid=109115)[0m rmse_per_class: [0.083, 0.293, 0.035, 0.302, 0.056, 0.159, 0.269, 0.15, 0.141, 0.316]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.1795678436756134
[2m[36m(func pid=119168)[0m mae:  0.1318642795085907
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3988 | Steps: 2 | Val loss: 0.5997 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3727 | Steps: 2 | Val loss: 0.2895 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=113492)[0m rmse: 0.20903420448303223
[2m[36m(func pid=113492)[0m mae:  0.11786691844463348
[2m[36m(func pid=113492)[0m rmse_per_class: [0.095, 0.343, 0.032, 0.368, 0.059, 0.185, 0.306, 0.151, 0.158, 0.394]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8617 | Steps: 2 | Val loss: 0.6728 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=102191)[0m rmse: 0.14268383383750916
[2m[36m(func pid=102191)[0m mae:  0.09601540118455887
[2m[36m(func pid=102191)[0m rmse_per_class: [0.087, 0.231, 0.04, 0.282, 0.055, 0.17, 0.224, 0.122, 0.132, 0.083]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3322 | Steps: 2 | Val loss: 0.3545 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 07:14:25 (running for 00:22:56.95)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.373 |  0.143 |                   95 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.336 |  0.181 |                   61 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.399 |  0.209 |                   47 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.862 |  0.18  |                   20 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.17954277992248535
[2m[36m(func pid=119168)[0m mae:  0.13183757662773132
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1802166998386383
[2m[36m(func pid=109115)[0m mae:  0.10715042054653168
[2m[36m(func pid=109115)[0m rmse_per_class: [0.081, 0.295, 0.038, 0.303, 0.057, 0.159, 0.258, 0.148, 0.14, 0.323]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4158 | Steps: 2 | Val loss: 0.6036 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3799 | Steps: 2 | Val loss: 0.2861 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=113492)[0m rmse: 0.20977528393268585
[2m[36m(func pid=113492)[0m mae:  0.11927898228168488
[2m[36m(func pid=113492)[0m rmse_per_class: [0.096, 0.348, 0.032, 0.368, 0.058, 0.207, 0.292, 0.151, 0.153, 0.392]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8568 | Steps: 2 | Val loss: 0.6711 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=102191)[0m rmse: 0.1424911469221115
[2m[36m(func pid=102191)[0m mae:  0.09587074816226959
[2m[36m(func pid=102191)[0m rmse_per_class: [0.087, 0.231, 0.04, 0.282, 0.055, 0.17, 0.224, 0.121, 0.132, 0.083]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3369 | Steps: 2 | Val loss: 0.3598 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 07:14:30 (running for 00:23:02.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.38  |  0.142 |                   96 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.332 |  0.18  |                   62 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.416 |  0.21  |                   48 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.857 |  0.18  |                   21 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.1795112043619156
[2m[36m(func pid=119168)[0m mae:  0.13180288672447205
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4329 | Steps: 2 | Val loss: 0.5951 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3631 | Steps: 2 | Val loss: 0.2835 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=109115)[0m rmse: 0.17981873452663422
[2m[36m(func pid=109115)[0m mae:  0.10612703859806061
[2m[36m(func pid=109115)[0m rmse_per_class: [0.08, 0.297, 0.039, 0.305, 0.059, 0.16, 0.247, 0.143, 0.138, 0.329]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.20960375666618347
[2m[36m(func pid=113492)[0m mae:  0.12014792114496231
[2m[36m(func pid=113492)[0m rmse_per_class: [0.097, 0.345, 0.032, 0.365, 0.057, 0.245, 0.286, 0.151, 0.151, 0.367]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8547 | Steps: 2 | Val loss: 0.6691 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=102191)[0m rmse: 0.14241516590118408
[2m[36m(func pid=102191)[0m mae:  0.09578023850917816
[2m[36m(func pid=102191)[0m rmse_per_class: [0.086, 0.231, 0.04, 0.282, 0.055, 0.17, 0.224, 0.121, 0.132, 0.083]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3512 | Steps: 2 | Val loss: 0.3653 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 07:14:35 (running for 00:23:07.29)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.363 |  0.142 |                   97 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.337 |  0.18  |                   63 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.433 |  0.21  |                   49 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.855 |  0.179 |                   22 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)

[2m[36m(func pid=119168)[0m rmse: 0.17946138978004456

[2m[36m(func pid=119168)[0m mae:  0.13175415992736816
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3805 | Steps: 2 | Val loss: 0.5794 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3543 | Steps: 2 | Val loss: 0.2811 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=109115)[0m rmse: 0.17933759093284607
[2m[36m(func pid=109115)[0m mae:  0.1050485372543335
[2m[36m(func pid=109115)[0m rmse_per_class: [0.08, 0.299, 0.04, 0.306, 0.064, 0.161, 0.236, 0.136, 0.138, 0.332]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8537 | Steps: 2 | Val loss: 0.6682 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=113492)[0m rmse: 0.2069101631641388
[2m[36m(func pid=113492)[0m mae:  0.11938466131687164
[2m[36m(func pid=113492)[0m rmse_per_class: [0.099, 0.339, 0.032, 0.362, 0.056, 0.275, 0.284, 0.15, 0.147, 0.325]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.14221857488155365
[2m[36m(func pid=102191)[0m mae:  0.09553412348031998
[2m[36m(func pid=102191)[0m rmse_per_class: [0.086, 0.231, 0.04, 0.281, 0.055, 0.169, 0.223, 0.121, 0.132, 0.083]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3438 | Steps: 2 | Val loss: 0.3677 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 07:14:41 (running for 00:23:12.62)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.354 |  0.142 |                   98 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.351 |  0.179 |                   64 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.381 |  0.207 |                   50 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.854 |  0.179 |                   23 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.17947174608707428
[2m[36m(func pid=119168)[0m mae:  0.13174812495708466
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4100 | Steps: 2 | Val loss: 0.5653 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3633 | Steps: 2 | Val loss: 0.2783 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=109115)[0m rmse: 0.17787966132164001
[2m[36m(func pid=109115)[0m mae:  0.10326780378818512
[2m[36m(func pid=109115)[0m rmse_per_class: [0.079, 0.299, 0.041, 0.306, 0.068, 0.163, 0.225, 0.131, 0.138, 0.329]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.8528 | Steps: 2 | Val loss: 0.6659 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=113492)[0m rmse: 0.20164553821086884
[2m[36m(func pid=113492)[0m mae:  0.11677513271570206
[2m[36m(func pid=113492)[0m rmse_per_class: [0.102, 0.333, 0.03, 0.36, 0.056, 0.275, 0.278, 0.15, 0.145, 0.286]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.14194831252098083
[2m[36m(func pid=102191)[0m mae:  0.0952252522110939
[2m[36m(func pid=102191)[0m rmse_per_class: [0.086, 0.231, 0.04, 0.28, 0.055, 0.169, 0.223, 0.121, 0.132, 0.083]
[2m[36m(func pid=102191)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3517 | Steps: 2 | Val loss: 0.3709 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 07:14:46 (running for 00:23:17.82)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00009 | RUNNING    | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.363 |  0.142 |                   99 |
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.344 |  0.178 |                   65 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.41  |  0.202 |                   51 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.853 |  0.179 |                   24 |
| train_ccef6_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.17945101857185364
[2m[36m(func pid=119168)[0m mae:  0.13173608481884003
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3921 | Steps: 2 | Val loss: 0.5503 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=102191)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3552 | Steps: 2 | Val loss: 0.2758 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=109115)[0m rmse: 0.17709963023662567
[2m[36m(func pid=109115)[0m mae:  0.10176761448383331
[2m[36m(func pid=109115)[0m rmse_per_class: [0.078, 0.3, 0.041, 0.306, 0.077, 0.165, 0.217, 0.126, 0.137, 0.324]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.8475 | Steps: 2 | Val loss: 0.6645 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=113492)[0m rmse: 0.1937168687582016
[2m[36m(func pid=113492)[0m mae:  0.11229681968688965
[2m[36m(func pid=113492)[0m rmse_per_class: [0.105, 0.33, 0.028, 0.358, 0.056, 0.245, 0.272, 0.152, 0.143, 0.249]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=102191)[0m rmse: 0.14185474812984467
[2m[36m(func pid=102191)[0m mae:  0.0951315313577652
[2m[36m(func pid=102191)[0m rmse_per_class: [0.086, 0.232, 0.04, 0.28, 0.055, 0.168, 0.222, 0.121, 0.132, 0.083]
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3427 | Steps: 2 | Val loss: 0.3724 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=119168)[0m rmse: 0.17950907349586487
[2m[36m(func pid=119168)[0m mae:  0.13179680705070496
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3776 | Steps: 2 | Val loss: 0.5415 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=109115)[0m rmse: 0.17641709744930267
[2m[36m(func pid=109115)[0m mae:  0.10035364329814911
[2m[36m(func pid=109115)[0m rmse_per_class: [0.078, 0.3, 0.041, 0.306, 0.085, 0.168, 0.213, 0.122, 0.136, 0.316]
[2m[36m(func pid=113492)[0m rmse: 0.18693986535072327
[2m[36m(func pid=113492)[0m mae:  0.10861340910196304
[2m[36m(func pid=113492)[0m rmse_per_class: [0.113, 0.325, 0.027, 0.356, 0.056, 0.2, 0.269, 0.158, 0.142, 0.224]
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.8450 | Steps: 2 | Val loss: 0.6623 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=119168)[0m rmse: 0.1794990599155426
[2m[36m(func pid=119168)[0m mae:  0.1317923367023468
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.294, 0.141, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 07:14:51 (running for 00:23:23.22)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.352 |  0.177 |                   66 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.392 |  0.194 |                   52 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.848 |  0.18  |                   25 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=124933)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=124933)[0m Configuration completed!
[2m[36m(func pid=124933)[0m New optimizer parameters:
[2m[36m(func pid=124933)[0m SGD (
[2m[36m(func pid=124933)[0m Parameter Group 0
[2m[36m(func pid=124933)[0m     dampening: 0
[2m[36m(func pid=124933)[0m     differentiable: False
[2m[36m(func pid=124933)[0m     foreach: None
[2m[36m(func pid=124933)[0m     lr: 0.001
[2m[36m(func pid=124933)[0m     maximize: False
[2m[36m(func pid=124933)[0m     momentum: 0.9
[2m[36m(func pid=124933)[0m     nesterov: False
[2m[36m(func pid=124933)[0m     weight_decay: 0.0001
[2m[36m(func pid=124933)[0m )
[2m[36m(func pid=124933)[0m 
== Status ==
Current time: 2024-01-07 07:14:58 (running for 00:23:30.25)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.352 |  0.177 |                   66 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.392 |  0.194 |                   52 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.845 |  0.179 |                   26 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3903 | Steps: 2 | Val loss: 0.5332 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.8412 | Steps: 2 | Val loss: 0.6598 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3388 | Steps: 2 | Val loss: 0.3722 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8954 | Steps: 2 | Val loss: 0.7068 | Batch size: 32 | lr: 0.001 | Duration: 4.57s
== Status ==
Current time: 2024-01-07 07:15:03 (running for 00:23:35.26)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.343 |  0.176 |                   67 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.378 |  0.187 |                   53 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.845 |  0.179 |                   26 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.17949649691581726
[2m[36m(func pid=119168)[0m mae:  0.13178899884223938
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=113492)[0m rmse: 0.18344251811504364
[2m[36m(func pid=113492)[0m mae:  0.10685257613658905
[2m[36m(func pid=113492)[0m rmse_per_class: [0.126, 0.32, 0.026, 0.351, 0.056, 0.172, 0.27, 0.167, 0.142, 0.205]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1760389655828476
[2m[36m(func pid=109115)[0m mae:  0.09939475357532501
[2m[36m(func pid=109115)[0m rmse_per_class: [0.077, 0.3, 0.042, 0.307, 0.094, 0.171, 0.214, 0.117, 0.135, 0.303]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.18268917500972748
[2m[36m(func pid=124933)[0m mae:  0.13442331552505493
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3823 | Steps: 2 | Val loss: 0.5230 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.8411 | Steps: 2 | Val loss: 0.6578 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3436 | Steps: 2 | Val loss: 0.3716 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8945 | Steps: 2 | Val loss: 0.6992 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 07:15:09 (running for 00:23:40.99)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.339 |  0.176 |                   68 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.382 |  0.185 |                   55 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.841 |  0.179 |                   27 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.895 |  0.183 |                    1 |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=113492)[0m rmse: 0.18453961610794067
[2m[36m(func pid=113492)[0m mae:  0.10769131034612656
[2m[36m(func pid=113492)[0m rmse_per_class: [0.148, 0.317, 0.026, 0.347, 0.056, 0.175, 0.271, 0.178, 0.143, 0.185]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17946508526802063
[2m[36m(func pid=119168)[0m mae:  0.1317654252052307
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1765735149383545
[2m[36m(func pid=109115)[0m mae:  0.09911038726568222
[2m[36m(func pid=109115)[0m rmse_per_class: [0.078, 0.3, 0.041, 0.307, 0.104, 0.173, 0.222, 0.116, 0.135, 0.29]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.1825171560049057
[2m[36m(func pid=124933)[0m mae:  0.13433216512203217
[2m[36m(func pid=124933)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.112]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3883 | Steps: 2 | Val loss: 0.5145 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.8381 | Steps: 2 | Val loss: 0.6559 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3362 | Steps: 2 | Val loss: 0.3697 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8846 | Steps: 2 | Val loss: 0.6890 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 07:15:14 (running for 00:23:46.13)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.344 |  0.177 |                   69 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.388 |  0.188 |                   56 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.841 |  0.179 |                   28 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.894 |  0.183 |                    2 |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=113492)[0m rmse: 0.18771584331989288
[2m[36m(func pid=113492)[0m mae:  0.10958633571863174
[2m[36m(func pid=113492)[0m rmse_per_class: [0.164, 0.318, 0.026, 0.343, 0.058, 0.188, 0.268, 0.188, 0.146, 0.179]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17944256961345673
[2m[36m(func pid=119168)[0m mae:  0.1317422091960907
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.17728903889656067
[2m[36m(func pid=109115)[0m mae:  0.09896449744701385
[2m[36m(func pid=109115)[0m rmse_per_class: [0.077, 0.297, 0.04, 0.306, 0.113, 0.177, 0.235, 0.116, 0.135, 0.278]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.18219386041164398
[2m[36m(func pid=124933)[0m mae:  0.13409534096717834
[2m[36m(func pid=124933)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3882 | Steps: 2 | Val loss: 0.4968 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.8371 | Steps: 2 | Val loss: 0.6540 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3606 | Steps: 2 | Val loss: 0.3670 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8715 | Steps: 2 | Val loss: 0.6768 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=113492)[0m rmse: 0.1912500113248825
[2m[36m(func pid=113492)[0m mae:  0.11114083230495453
[2m[36m(func pid=113492)[0m rmse_per_class: [0.165, 0.326, 0.027, 0.342, 0.059, 0.197, 0.268, 0.2, 0.152, 0.177]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:15:19 (running for 00:23:51.48)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.336 |  0.177 |                   70 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.388 |  0.191 |                   57 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.837 |  0.179 |                   30 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.885 |  0.182 |                    3 |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.17940694093704224
[2m[36m(func pid=119168)[0m mae:  0.13171103596687317
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.18181101977825165
[2m[36m(func pid=124933)[0m mae:  0.13380196690559387
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.339, 0.112, 0.19, 0.294, 0.142, 0.143, 0.112]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.1783442497253418
[2m[36m(func pid=109115)[0m mae:  0.09942367672920227
[2m[36m(func pid=109115)[0m rmse_per_class: [0.075, 0.295, 0.038, 0.306, 0.121, 0.181, 0.251, 0.117, 0.134, 0.267]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3635 | Steps: 2 | Val loss: 0.4836 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.8349 | Steps: 2 | Val loss: 0.6525 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8571 | Steps: 2 | Val loss: 0.6638 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3428 | Steps: 2 | Val loss: 0.3646 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=113492)[0m rmse: 0.19571493566036224
[2m[36m(func pid=113492)[0m mae:  0.11250296980142593
[2m[36m(func pid=113492)[0m rmse_per_class: [0.158, 0.331, 0.027, 0.344, 0.061, 0.201, 0.272, 0.214, 0.162, 0.188]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:15:25 (running for 00:23:56.78)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.361 |  0.178 |                   71 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.363 |  0.196 |                   58 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.835 |  0.179 |                   31 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.872 |  0.182 |                    4 |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.17937111854553223
[2m[36m(func pid=119168)[0m mae:  0.13167309761047363
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.1813102811574936
[2m[36m(func pid=124933)[0m mae:  0.1333901733160019
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.111, 0.19, 0.293, 0.142, 0.143, 0.111]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.18000932037830353
[2m[36m(func pid=109115)[0m mae:  0.10029897838830948
[2m[36m(func pid=109115)[0m rmse_per_class: [0.074, 0.294, 0.036, 0.306, 0.133, 0.184, 0.27, 0.119, 0.133, 0.251]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3375 | Steps: 2 | Val loss: 0.4802 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.8328 | Steps: 2 | Val loss: 0.6503 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8413 | Steps: 2 | Val loss: 0.6503 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=113492)[0m rmse: 0.2007829248905182
[2m[36m(func pid=113492)[0m mae:  0.11410434544086456
[2m[36m(func pid=113492)[0m rmse_per_class: [0.145, 0.333, 0.029, 0.356, 0.065, 0.2, 0.28, 0.218, 0.178, 0.204]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3480 | Steps: 2 | Val loss: 0.3632 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 07:15:30 (running for 00:24:01.95)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.343 |  0.18  |                   72 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.337 |  0.201 |                   59 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.833 |  0.179 |                   32 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.857 |  0.181 |                    5 |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.1793898046016693
[2m[36m(func pid=119168)[0m mae:  0.13169774413108826
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.18088363111019135
[2m[36m(func pid=124933)[0m mae:  0.1330312043428421
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.111, 0.189, 0.293, 0.141, 0.142, 0.11]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=109115)[0m rmse: 0.18171396851539612
[2m[36m(func pid=109115)[0m mae:  0.10151580721139908
[2m[36m(func pid=109115)[0m rmse_per_class: [0.073, 0.293, 0.035, 0.308, 0.142, 0.187, 0.288, 0.121, 0.132, 0.238]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3688 | Steps: 2 | Val loss: 0.4837 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8310 | Steps: 2 | Val loss: 0.6482 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8226 | Steps: 2 | Val loss: 0.6364 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=113492)[0m rmse: 0.20583109557628632
[2m[36m(func pid=113492)[0m mae:  0.11564824730157852
[2m[36m(func pid=113492)[0m rmse_per_class: [0.14, 0.331, 0.031, 0.377, 0.073, 0.197, 0.287, 0.206, 0.196, 0.219]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:15:35 (running for 00:24:07.12)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.348 |  0.182 |                   73 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.369 |  0.206 |                   60 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.831 |  0.179 |                   33 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.841 |  0.181 |                    6 |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.17939302325248718
[2m[36m(func pid=119168)[0m mae:  0.13169729709625244
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3330 | Steps: 2 | Val loss: 0.3605 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=124933)[0m rmse: 0.18052470684051514
[2m[36m(func pid=124933)[0m mae:  0.13273276388645172
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.11, 0.189, 0.293, 0.141, 0.142, 0.11]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3583 | Steps: 2 | Val loss: 0.4893 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=109115)[0m rmse: 0.18286697566509247
[2m[36m(func pid=109115)[0m mae:  0.10255346447229385
[2m[36m(func pid=109115)[0m rmse_per_class: [0.072, 0.29, 0.033, 0.309, 0.15, 0.189, 0.303, 0.124, 0.132, 0.225]
[2m[36m(func pid=109115)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8264 | Steps: 2 | Val loss: 0.6464 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8072 | Steps: 2 | Val loss: 0.6226 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=113492)[0m rmse: 0.20875847339630127
[2m[36m(func pid=113492)[0m mae:  0.11638794839382172
[2m[36m(func pid=113492)[0m rmse_per_class: [0.136, 0.329, 0.031, 0.399, 0.086, 0.193, 0.293, 0.184, 0.207, 0.228]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:15:40 (running for 00:24:12.43)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15075000375509262
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00010 | RUNNING    | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.333 |  0.183 |                   74 |
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.358 |  0.209 |                   61 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.826 |  0.179 |                   34 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.823 |  0.181 |                    7 |
| train_ccef6_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.17933407425880432
[2m[36m(func pid=119168)[0m mae:  0.13164439797401428
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=109115)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3384 | Steps: 2 | Val loss: 0.3571 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=124933)[0m rmse: 0.18025389313697815
[2m[36m(func pid=124933)[0m mae:  0.1324884593486786
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.263, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3473 | Steps: 2 | Val loss: 0.4922 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8227 | Steps: 2 | Val loss: 0.6451 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=109115)[0m rmse: 0.18322357535362244
[2m[36m(func pid=109115)[0m mae:  0.10317514091730118
[2m[36m(func pid=109115)[0m rmse_per_class: [0.072, 0.288, 0.032, 0.309, 0.155, 0.191, 0.315, 0.126, 0.132, 0.212]
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7893 | Steps: 2 | Val loss: 0.6089 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=113492)[0m rmse: 0.21014335751533508
[2m[36m(func pid=113492)[0m mae:  0.11630938947200775
[2m[36m(func pid=113492)[0m rmse_per_class: [0.135, 0.328, 0.031, 0.409, 0.101, 0.193, 0.296, 0.162, 0.212, 0.234]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.179325670003891
[2m[36m(func pid=119168)[0m mae:  0.13164344429969788
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17993147671222687
[2m[36m(func pid=124933)[0m mae:  0.13220520317554474
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.263, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3642 | Steps: 2 | Val loss: 0.4894 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.8242 | Steps: 2 | Val loss: 0.6432 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=113492)[0m rmse: 0.20956750214099884
[2m[36m(func pid=113492)[0m mae:  0.11526429653167725
[2m[36m(func pid=113492)[0m rmse_per_class: [0.136, 0.326, 0.031, 0.404, 0.115, 0.201, 0.296, 0.147, 0.21, 0.231]
[2m[36m(func pid=119168)[0m rmse: 0.17931044101715088
[2m[36m(func pid=119168)[0m mae:  0.1316252201795578
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 07:15:46 (running for 00:24:17.67)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.347 |  0.21  |                   62 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.823 |  0.179 |                   35 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.807 |  0.18  |                    8 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


== Status ==
Current time: 2024-01-07 07:15:52 (running for 00:24:23.78)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.364 |  0.21  |                   63 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.823 |  0.179 |                   35 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.807 |  0.18  |                    8 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=127238)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=127238)[0m Configuration completed!
[2m[36m(func pid=127238)[0m New optimizer parameters:
[2m[36m(func pid=127238)[0m SGD (
[2m[36m(func pid=127238)[0m Parameter Group 0
[2m[36m(func pid=127238)[0m     dampening: 0
[2m[36m(func pid=127238)[0m     differentiable: False
[2m[36m(func pid=127238)[0m     foreach: None
[2m[36m(func pid=127238)[0m     lr: 0.01
[2m[36m(func pid=127238)[0m     maximize: False
[2m[36m(func pid=127238)[0m     momentum: 0.9
[2m[36m(func pid=127238)[0m     nesterov: False
[2m[36m(func pid=127238)[0m     weight_decay: 0.0001
[2m[36m(func pid=127238)[0m )
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3504 | Steps: 2 | Val loss: 0.4856 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.8198 | Steps: 2 | Val loss: 0.6415 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7726 | Steps: 2 | Val loss: 0.5948 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8905 | Steps: 2 | Val loss: 0.6859 | Batch size: 32 | lr: 0.01 | Duration: 4.69s
== Status ==
Current time: 2024-01-07 07:15:57 (running for 00:24:28.80)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.364 |  0.21  |                   63 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.824 |  0.179 |                   36 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.789 |  0.18  |                    9 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=113492)[0m rmse: 0.20776402950286865
[2m[36m(func pid=113492)[0m mae:  0.11352843046188354
[2m[36m(func pid=113492)[0m rmse_per_class: [0.135, 0.325, 0.031, 0.384, 0.122, 0.217, 0.296, 0.138, 0.203, 0.228]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.1793028861284256
[2m[36m(func pid=119168)[0m mae:  0.13161802291870117
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.179666206240654
[2m[36m(func pid=124933)[0m mae:  0.13197745382785797
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.262, 0.1, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.1826869696378708
[2m[36m(func pid=127238)[0m mae:  0.13442237675189972
[2m[36m(func pid=127238)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.11, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3603 | Steps: 2 | Val loss: 0.4804 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.8181 | Steps: 2 | Val loss: 0.6391 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7573 | Steps: 2 | Val loss: 0.5808 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8431 | Steps: 2 | Val loss: 0.6341 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 07:16:02 (running for 00:24:34.47)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.36  |  0.205 |                   65 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.82  |  0.179 |                   37 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.773 |  0.18  |                   10 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.891 |  0.183 |                    1 |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=113492)[0m rmse: 0.20450985431671143
[2m[36m(func pid=113492)[0m mae:  0.11143603175878525
[2m[36m(func pid=113492)[0m rmse_per_class: [0.137, 0.321, 0.031, 0.359, 0.123, 0.234, 0.298, 0.135, 0.19, 0.218]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.1793104112148285
[2m[36m(func pid=119168)[0m mae:  0.13162031769752502
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17950274050235748
[2m[36m(func pid=124933)[0m mae:  0.1318328082561493
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.262, 0.099, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.18250897526741028
[2m[36m(func pid=127238)[0m mae:  0.1343364119529724
[2m[36m(func pid=127238)[0m rmse_per_class: [0.117, 0.267, 0.107, 0.339, 0.111, 0.19, 0.294, 0.144, 0.144, 0.113]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3503 | Steps: 2 | Val loss: 0.4842 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8157 | Steps: 2 | Val loss: 0.6370 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7373 | Steps: 2 | Val loss: 0.5675 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7675 | Steps: 2 | Val loss: 0.5693 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 07:16:08 (running for 00:24:39.57)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.35  |  0.203 |                   66 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.818 |  0.179 |                   38 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.757 |  0.18  |                   11 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.843 |  0.183 |                    2 |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=113492)[0m rmse: 0.20285961031913757
[2m[36m(func pid=113492)[0m mae:  0.11107897758483887
[2m[36m(func pid=113492)[0m rmse_per_class: [0.138, 0.318, 0.031, 0.346, 0.125, 0.242, 0.308, 0.134, 0.175, 0.211]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17925560474395752
[2m[36m(func pid=119168)[0m mae:  0.13157601654529572
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17931175231933594
[2m[36m(func pid=124933)[0m mae:  0.1316678822040558
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.18206259608268738
[2m[36m(func pid=127238)[0m mae:  0.13401630520820618
[2m[36m(func pid=127238)[0m rmse_per_class: [0.118, 0.267, 0.105, 0.338, 0.11, 0.19, 0.293, 0.144, 0.143, 0.112]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3303 | Steps: 2 | Val loss: 0.4916 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.8164 | Steps: 2 | Val loss: 0.6356 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7164 | Steps: 2 | Val loss: 0.5548 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6845 | Steps: 2 | Val loss: 0.5039 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=113492)[0m rmse: 0.2008412778377533
[2m[36m(func pid=113492)[0m mae:  0.11140630394220352
[2m[36m(func pid=113492)[0m rmse_per_class: [0.145, 0.313, 0.031, 0.346, 0.119, 0.238, 0.322, 0.133, 0.162, 0.2]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:16:13 (running for 00:24:44.95)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.33  |  0.201 |                   67 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.816 |  0.179 |                   40 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.737 |  0.179 |                   12 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.767 |  0.182 |                    3 |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.1792597472667694
[2m[36m(func pid=119168)[0m mae:  0.13158752024173737
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17913582921028137
[2m[36m(func pid=124933)[0m mae:  0.13151606917381287
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.336, 0.108, 0.19, 0.293, 0.141, 0.141, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.18127954006195068
[2m[36m(func pid=127238)[0m mae:  0.13341307640075684
[2m[36m(func pid=127238)[0m rmse_per_class: [0.118, 0.266, 0.103, 0.337, 0.108, 0.19, 0.293, 0.143, 0.143, 0.112]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3583 | Steps: 2 | Val loss: 0.5025 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.8138 | Steps: 2 | Val loss: 0.6344 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7063 | Steps: 2 | Val loss: 0.5423 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5990 | Steps: 2 | Val loss: 0.4456 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=113492)[0m rmse: 0.19955535233020782
[2m[36m(func pid=113492)[0m mae:  0.11186975240707397
[2m[36m(func pid=113492)[0m rmse_per_class: [0.159, 0.313, 0.031, 0.351, 0.112, 0.229, 0.324, 0.131, 0.153, 0.193]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.1792362630367279
[2m[36m(func pid=119168)[0m mae:  0.13157030940055847
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 07:16:18 (running for 00:24:50.19)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.358 |  0.2   |                   68 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.814 |  0.179 |                   41 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.716 |  0.179 |                   13 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.684 |  0.181 |                    4 |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.1790505349636078
[2m[36m(func pid=124933)[0m mae:  0.131443053483963
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.336, 0.107, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.1802879273891449
[2m[36m(func pid=127238)[0m mae:  0.13264428079128265
[2m[36m(func pid=127238)[0m rmse_per_class: [0.118, 0.266, 0.1, 0.336, 0.105, 0.189, 0.292, 0.143, 0.143, 0.112]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3492 | Steps: 2 | Val loss: 0.5143 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.8080 | Steps: 2 | Val loss: 0.6321 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.6892 | Steps: 2 | Val loss: 0.5304 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5360 | Steps: 2 | Val loss: 0.3991 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=113492)[0m rmse: 0.19977089762687683
[2m[36m(func pid=113492)[0m mae:  0.11256061494350433
[2m[36m(func pid=113492)[0m rmse_per_class: [0.191, 0.316, 0.032, 0.357, 0.111, 0.219, 0.31, 0.128, 0.147, 0.187]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.1792450100183487
[2m[36m(func pid=119168)[0m mae:  0.13157790899276733
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
== Status ==
Current time: 2024-01-07 07:16:23 (running for 00:24:55.42)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.349 |  0.2   |                   69 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.808 |  0.179 |                   42 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.706 |  0.179 |                   14 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.599 |  0.18  |                    5 |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17890046536922455
[2m[36m(func pid=124933)[0m mae:  0.13131499290466309
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.261, 0.097, 0.336, 0.107, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.17924103140830994
[2m[36m(func pid=127238)[0m mae:  0.13179966807365417
[2m[36m(func pid=127238)[0m rmse_per_class: [0.119, 0.265, 0.097, 0.335, 0.101, 0.189, 0.291, 0.143, 0.142, 0.111]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3560 | Steps: 2 | Val loss: 0.5269 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8085 | Steps: 2 | Val loss: 0.6306 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6758 | Steps: 2 | Val loss: 0.5190 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4882 | Steps: 2 | Val loss: 0.3653 | Batch size: 32 | lr: 0.01 | Duration: 2.60s
[2m[36m(func pid=113492)[0m rmse: 0.2012830227613449
[2m[36m(func pid=113492)[0m mae:  0.11391179263591766
[2m[36m(func pid=113492)[0m rmse_per_class: [0.239, 0.32, 0.032, 0.362, 0.11, 0.208, 0.284, 0.127, 0.144, 0.185]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:16:29 (running for 00:25:00.72)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.356 |  0.201 |                   70 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.808 |  0.179 |                   43 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.689 |  0.179 |                   15 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.536 |  0.179 |                    6 |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.1792535036802292
[2m[36m(func pid=119168)[0m mae:  0.1315811574459076
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17875052988529205
[2m[36m(func pid=124933)[0m mae:  0.13119421899318695
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.17804138362407684
[2m[36m(func pid=127238)[0m mae:  0.1308380514383316
[2m[36m(func pid=127238)[0m rmse_per_class: [0.119, 0.264, 0.094, 0.333, 0.097, 0.189, 0.29, 0.143, 0.142, 0.111]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3589 | Steps: 2 | Val loss: 0.5412 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8071 | Steps: 2 | Val loss: 0.6292 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6630 | Steps: 2 | Val loss: 0.5090 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4489 | Steps: 2 | Val loss: 0.3422 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=113492)[0m rmse: 0.20459571480751038
[2m[36m(func pid=113492)[0m mae:  0.11645235121250153
[2m[36m(func pid=113492)[0m rmse_per_class: [0.286, 0.328, 0.033, 0.365, 0.11, 0.195, 0.264, 0.136, 0.143, 0.187]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:16:34 (running for 00:25:06.08)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.359 |  0.205 |                   71 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.807 |  0.179 |                   44 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.676 |  0.179 |                   16 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.488 |  0.178 |                    7 |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.17922690510749817
[2m[36m(func pid=119168)[0m mae:  0.1315550059080124
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17862336337566376
[2m[36m(func pid=124933)[0m mae:  0.13109098374843597
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.17689363658428192
[2m[36m(func pid=127238)[0m mae:  0.12989048659801483
[2m[36m(func pid=127238)[0m rmse_per_class: [0.119, 0.262, 0.091, 0.331, 0.093, 0.188, 0.288, 0.143, 0.142, 0.11]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3556 | Steps: 2 | Val loss: 0.5562 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.8019 | Steps: 2 | Val loss: 0.6274 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6496 | Steps: 2 | Val loss: 0.4983 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4304 | Steps: 2 | Val loss: 0.3278 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=113492)[0m rmse: 0.20986530184745789
[2m[36m(func pid=113492)[0m mae:  0.12035946547985077
[2m[36m(func pid=113492)[0m rmse_per_class: [0.321, 0.336, 0.033, 0.366, 0.107, 0.186, 0.258, 0.157, 0.141, 0.193]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:16:39 (running for 00:25:11.26)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.356 |  0.21  |                   72 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.807 |  0.179 |                   44 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.65  |  0.178 |                   18 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.449 |  0.177 |                    8 |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17844133079051971
[2m[36m(func pid=124933)[0m mae:  0.13094419240951538
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.335, 0.105, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17922905087471008
[2m[36m(func pid=119168)[0m mae:  0.13155856728553772
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.17585858702659607
[2m[36m(func pid=127238)[0m mae:  0.12904612720012665
[2m[36m(func pid=127238)[0m rmse_per_class: [0.12, 0.261, 0.089, 0.33, 0.089, 0.188, 0.287, 0.142, 0.142, 0.11]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3425 | Steps: 2 | Val loss: 0.5631 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6363 | Steps: 2 | Val loss: 0.4880 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.8023 | Steps: 2 | Val loss: 0.6258 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4134 | Steps: 2 | Val loss: 0.3188 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=113492)[0m rmse: 0.21480384469032288
[2m[36m(func pid=113492)[0m mae:  0.12346573919057846
[2m[36m(func pid=113492)[0m rmse_per_class: [0.323, 0.345, 0.033, 0.365, 0.105, 0.181, 0.263, 0.186, 0.141, 0.206]
[2m[36m(func pid=113492)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.1782664954662323
[2m[36m(func pid=124933)[0m mae:  0.1308121383190155
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.335, 0.104, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
== Status ==
Current time: 2024-01-07 07:16:44 (running for 00:25:16.42)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.342 |  0.215 |                   73 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.802 |  0.179 |                   45 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.636 |  0.178 |                   19 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.43  |  0.176 |                    9 |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.17922455072402954
[2m[36m(func pid=119168)[0m mae:  0.13156792521476746
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.17469695210456848
[2m[36m(func pid=127238)[0m mae:  0.1280914694070816
[2m[36m(func pid=127238)[0m rmse_per_class: [0.119, 0.259, 0.086, 0.329, 0.087, 0.188, 0.286, 0.142, 0.142, 0.109]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3576 | Steps: 2 | Val loss: 0.5595 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6280 | Steps: 2 | Val loss: 0.4787 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.8006 | Steps: 2 | Val loss: 0.6240 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4066 | Steps: 2 | Val loss: 0.3139 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=113492)[0m rmse: 0.21742084622383118
[2m[36m(func pid=113492)[0m mae:  0.12482963502407074
[2m[36m(func pid=113492)[0m rmse_per_class: [0.305, 0.354, 0.033, 0.363, 0.1, 0.181, 0.268, 0.208, 0.141, 0.221]
[2m[36m(func pid=113492)[0m 
== Status ==
Current time: 2024-01-07 07:16:50 (running for 00:25:21.56)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00011 | RUNNING    | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.358 |  0.217 |                   74 |
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.802 |  0.179 |                   46 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.628 |  0.178 |                   20 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.413 |  0.175 |                   10 |
| train_ccef6_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17816360294818878
[2m[36m(func pid=124933)[0m mae:  0.13074102997779846
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.104, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17921176552772522
[2m[36m(func pid=119168)[0m mae:  0.13155336678028107
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.17359843850135803
[2m[36m(func pid=127238)[0m mae:  0.12721116840839386
[2m[36m(func pid=127238)[0m rmse_per_class: [0.118, 0.258, 0.084, 0.328, 0.084, 0.188, 0.284, 0.141, 0.142, 0.109]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=113492)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3357 | Steps: 2 | Val loss: 0.5484 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6165 | Steps: 2 | Val loss: 0.4701 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7965 | Steps: 2 | Val loss: 0.6224 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4015 | Steps: 2 | Val loss: 0.3111 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=113492)[0m rmse: 0.21741077303886414
[2m[36m(func pid=113492)[0m mae:  0.12415317445993423
[2m[36m(func pid=113492)[0m rmse_per_class: [0.272, 0.358, 0.033, 0.359, 0.097, 0.184, 0.269, 0.219, 0.141, 0.242]
[2m[36m(func pid=119168)[0m rmse: 0.1792449802160263
[2m[36m(func pid=119168)[0m mae:  0.13158035278320312
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=124933)[0m rmse: 0.17804035544395447
[2m[36m(func pid=124933)[0m mae:  0.13063876330852509
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.335, 0.104, 0.19, 0.292, 0.14, 0.142, 0.109]
[2m[36m(func pid=127238)[0m rmse: 0.17247019708156586
[2m[36m(func pid=127238)[0m mae:  0.12630042433738708
[2m[36m(func pid=127238)[0m rmse_per_class: [0.118, 0.256, 0.081, 0.327, 0.081, 0.187, 0.283, 0.14, 0.142, 0.109]
== Status ==
Current time: 2024-01-07 07:16:55 (running for 00:25:27.14)
Memory usage on this node: 21.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.801 |  0.179 |                   47 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.628 |  0.178 |                   20 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.407 |  0.174 |                   11 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 07:17:04 (running for 00:25:36.31)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.801 |  0.179 |                   47 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.628 |  0.178 |                   20 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.402 |  0.172 |                   12 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=130296)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=130296)[0m Configuration completed!
[2m[36m(func pid=130296)[0m New optimizer parameters:
[2m[36m(func pid=130296)[0m SGD (
[2m[36m(func pid=130296)[0m Parameter Group 0
[2m[36m(func pid=130296)[0m     dampening: 0
[2m[36m(func pid=130296)[0m     differentiable: False
[2m[36m(func pid=130296)[0m     foreach: None
[2m[36m(func pid=130296)[0m     lr: 0.1
[2m[36m(func pid=130296)[0m     maximize: False
[2m[36m(func pid=130296)[0m     momentum: 0.9
[2m[36m(func pid=130296)[0m     nesterov: False
[2m[36m(func pid=130296)[0m     weight_decay: 0.0001
[2m[36m(func pid=130296)[0m )
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3978 | Steps: 2 | Val loss: 0.3096 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7962 | Steps: 2 | Val loss: 0.6216 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6036 | Steps: 2 | Val loss: 0.4623 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8367 | Steps: 2 | Val loss: 0.5211 | Batch size: 32 | lr: 0.1 | Duration: 4.58s
== Status ==
Current time: 2024-01-07 07:17:09 (running for 00:25:41.34)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.797 |  0.179 |                   48 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.616 |  0.178 |                   21 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.402 |  0.172 |                   12 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.1792488843202591
[2m[36m(func pid=119168)[0m mae:  0.13157619535923004
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17797508835792542
[2m[36m(func pid=124933)[0m mae:  0.1305888593196869
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.335, 0.103, 0.19, 0.292, 0.14, 0.142, 0.109]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.17134562134742737
[2m[36m(func pid=127238)[0m mae:  0.12537668645381927
[2m[36m(func pid=127238)[0m rmse_per_class: [0.117, 0.254, 0.079, 0.326, 0.079, 0.187, 0.282, 0.14, 0.142, 0.108]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.1825278401374817
[2m[36m(func pid=130296)[0m mae:  0.1343175321817398
[2m[36m(func pid=130296)[0m rmse_per_class: [0.118, 0.267, 0.109, 0.338, 0.106, 0.191, 0.293, 0.146, 0.144, 0.114]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3966 | Steps: 2 | Val loss: 0.3087 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7939 | Steps: 2 | Val loss: 0.6198 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5943 | Steps: 2 | Val loss: 0.4548 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5489 | Steps: 2 | Val loss: 0.3532 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 07:17:15 (running for 00:25:46.67)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.796 |  0.179 |                   49 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.604 |  0.178 |                   22 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.397 |  0.17  |                   14 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.837 |  0.183 |                    1 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.17029538750648499
[2m[36m(func pid=127238)[0m mae:  0.1245122179389
[2m[36m(func pid=127238)[0m rmse_per_class: [0.116, 0.253, 0.076, 0.325, 0.077, 0.186, 0.28, 0.139, 0.142, 0.107]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17922818660736084
[2m[36m(func pid=119168)[0m mae:  0.13156965374946594
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.1778450310230255
[2m[36m(func pid=124933)[0m mae:  0.1304779350757599
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.335, 0.103, 0.19, 0.292, 0.141, 0.142, 0.109]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.18070633709430695
[2m[36m(func pid=130296)[0m mae:  0.13296493887901306
[2m[36m(func pid=130296)[0m rmse_per_class: [0.122, 0.268, 0.102, 0.336, 0.096, 0.191, 0.289, 0.146, 0.144, 0.115]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3945 | Steps: 2 | Val loss: 0.3081 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5863 | Steps: 2 | Val loss: 0.4474 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7895 | Steps: 2 | Val loss: 0.6187 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4099 | Steps: 2 | Val loss: 0.3208 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 07:17:20 (running for 00:25:51.93)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.794 |  0.179 |                   50 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.594 |  0.178 |                   23 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.394 |  0.169 |                   15 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.549 |  0.181 |                    2 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.16924366354942322
[2m[36m(func pid=127238)[0m mae:  0.12364988029003143
[2m[36m(func pid=127238)[0m rmse_per_class: [0.115, 0.252, 0.074, 0.324, 0.075, 0.186, 0.279, 0.138, 0.142, 0.107]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.1777014583349228
[2m[36m(func pid=124933)[0m mae:  0.13036073744297028
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.334, 0.102, 0.19, 0.291, 0.14, 0.142, 0.109]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17923524975776672
[2m[36m(func pid=119168)[0m mae:  0.13157275319099426
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.17636153101921082
[2m[36m(func pid=130296)[0m mae:  0.12968583405017853
[2m[36m(func pid=130296)[0m rmse_per_class: [0.124, 0.266, 0.089, 0.33, 0.082, 0.189, 0.282, 0.143, 0.146, 0.112]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3943 | Steps: 2 | Val loss: 0.3075 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5764 | Steps: 2 | Val loss: 0.4402 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7867 | Steps: 2 | Val loss: 0.6172 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4247 | Steps: 2 | Val loss: 0.3391 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 07:17:25 (running for 00:25:57.09)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.79  |  0.179 |                   51 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.586 |  0.178 |                   24 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.394 |  0.168 |                   16 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.41  |  0.176 |                    3 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.1683732271194458
[2m[36m(func pid=127238)[0m mae:  0.1229291781783104
[2m[36m(func pid=127238)[0m rmse_per_class: [0.114, 0.251, 0.073, 0.323, 0.074, 0.185, 0.278, 0.138, 0.142, 0.106]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17756828665733337
[2m[36m(func pid=124933)[0m mae:  0.13024944067001343
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.334, 0.102, 0.19, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17923253774642944
[2m[36m(func pid=119168)[0m mae:  0.1315731257200241
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.17065554857254028
[2m[36m(func pid=130296)[0m mae:  0.12506769597530365
[2m[36m(func pid=130296)[0m rmse_per_class: [0.125, 0.262, 0.075, 0.323, 0.07, 0.186, 0.272, 0.14, 0.146, 0.107]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3933 | Steps: 2 | Val loss: 0.3065 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5692 | Steps: 2 | Val loss: 0.4340 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.7853 | Steps: 2 | Val loss: 0.6159 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4564 | Steps: 2 | Val loss: 0.3563 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 07:17:30 (running for 00:26:02.13)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.787 |  0.179 |                   52 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.576 |  0.178 |                   25 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.393 |  0.167 |                   17 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.425 |  0.171 |                    4 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.16738876700401306
[2m[36m(func pid=127238)[0m mae:  0.12211152166128159
[2m[36m(func pid=127238)[0m rmse_per_class: [0.114, 0.25, 0.072, 0.322, 0.072, 0.185, 0.276, 0.137, 0.142, 0.105]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.1792421191930771
[2m[36m(func pid=119168)[0m mae:  0.1315692514181137
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17744991183280945
[2m[36m(func pid=124933)[0m mae:  0.13014347851276398
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.334, 0.101, 0.19, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.1636989414691925
[2m[36m(func pid=130296)[0m mae:  0.11903276294469833
[2m[36m(func pid=130296)[0m rmse_per_class: [0.12, 0.256, 0.062, 0.316, 0.061, 0.183, 0.258, 0.135, 0.145, 0.102]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3905 | Steps: 2 | Val loss: 0.3055 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5608 | Steps: 2 | Val loss: 0.4276 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.7852 | Steps: 2 | Val loss: 0.6144 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4787 | Steps: 2 | Val loss: 0.3567 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=127238)[0m rmse: 0.16645005345344543
[2m[36m(func pid=127238)[0m mae:  0.12130888551473618
[2m[36m(func pid=127238)[0m rmse_per_class: [0.113, 0.248, 0.07, 0.321, 0.071, 0.184, 0.275, 0.136, 0.142, 0.104]
== Status ==
Current time: 2024-01-07 07:17:35 (running for 00:26:07.39)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.785 |  0.179 |                   53 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.569 |  0.177 |                   26 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.391 |  0.166 |                   18 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.456 |  0.164 |                    5 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.177361398935318
[2m[36m(func pid=124933)[0m mae:  0.13007108867168427
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.334, 0.101, 0.19, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.1792266070842743
[2m[36m(func pid=119168)[0m mae:  0.13155880570411682
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.15686610341072083
[2m[36m(func pid=130296)[0m mae:  0.11253862082958221
[2m[36m(func pid=130296)[0m rmse_per_class: [0.114, 0.25, 0.051, 0.307, 0.056, 0.179, 0.245, 0.13, 0.143, 0.095]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3893 | Steps: 2 | Val loss: 0.3043 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5526 | Steps: 2 | Val loss: 0.4215 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.7857 | Steps: 2 | Val loss: 0.6132 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 07:17:41 (running for 00:26:12.68)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.785 |  0.179 |                   54 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.561 |  0.177 |                   27 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.389 |  0.166 |                   19 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.479 |  0.157 |                    6 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.16555938124656677
[2m[36m(func pid=127238)[0m mae:  0.12055230140686035
[2m[36m(func pid=127238)[0m rmse_per_class: [0.113, 0.247, 0.069, 0.32, 0.07, 0.183, 0.274, 0.135, 0.142, 0.104]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4716 | Steps: 2 | Val loss: 0.3409 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=124933)[0m rmse: 0.1772874891757965
[2m[36m(func pid=124933)[0m mae:  0.12999804317951202
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.334, 0.101, 0.19, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.1792028248310089
[2m[36m(func pid=119168)[0m mae:  0.1315390169620514
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.15068836510181427
[2m[36m(func pid=130296)[0m mae:  0.10624232143163681
[2m[36m(func pid=130296)[0m rmse_per_class: [0.105, 0.242, 0.044, 0.296, 0.054, 0.174, 0.235, 0.126, 0.14, 0.091]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3877 | Steps: 2 | Val loss: 0.3031 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5467 | Steps: 2 | Val loss: 0.4163 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.7832 | Steps: 2 | Val loss: 0.6116 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=127238)[0m rmse: 0.16477303206920624
== Status ==
Current time: 2024-01-07 07:17:46 (running for 00:26:17.82)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.786 |  0.179 |                   55 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.553 |  0.177 |                   28 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.388 |  0.165 |                   20 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.472 |  0.151 |                    7 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=127238)[0m mae:  0.11988844722509384

[2m[36m(func pid=127238)[0m rmse_per_class: [0.113, 0.246, 0.067, 0.319, 0.068, 0.183, 0.273, 0.135, 0.141, 0.103]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4424 | Steps: 2 | Val loss: 0.3146 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=124933)[0m rmse: 0.1772521585226059
[2m[36m(func pid=124933)[0m mae:  0.12997975945472717
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.334, 0.101, 0.19, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17917804419994354
[2m[36m(func pid=119168)[0m mae:  0.13152766227722168
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.1466357409954071
[2m[36m(func pid=130296)[0m mae:  0.10156746208667755
[2m[36m(func pid=130296)[0m rmse_per_class: [0.1, 0.235, 0.041, 0.29, 0.054, 0.171, 0.228, 0.123, 0.138, 0.087]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3848 | Steps: 2 | Val loss: 0.3018 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5393 | Steps: 2 | Val loss: 0.4115 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7800 | Steps: 2 | Val loss: 0.6088 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 07:17:51 (running for 00:26:22.90)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.783 |  0.179 |                   56 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.547 |  0.177 |                   29 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.385 |  0.164 |                   21 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.442 |  0.147 |                    8 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.1640116274356842
[2m[36m(func pid=127238)[0m mae:  0.1192401647567749
[2m[36m(func pid=127238)[0m rmse_per_class: [0.112, 0.245, 0.066, 0.318, 0.067, 0.182, 0.272, 0.134, 0.141, 0.102]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3990 | Steps: 2 | Val loss: 0.2860 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=124933)[0m rmse: 0.17718730866909027
[2m[36m(func pid=124933)[0m mae:  0.12991030514240265
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.334, 0.1, 0.19, 0.291, 0.14, 0.142, 0.109]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17920294404029846
[2m[36m(func pid=119168)[0m mae:  0.13155733048915863
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3847 | Steps: 2 | Val loss: 0.2999 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=130296)[0m rmse: 0.14349660277366638
[2m[36m(func pid=130296)[0m mae:  0.0979151725769043
[2m[36m(func pid=130296)[0m rmse_per_class: [0.095, 0.232, 0.038, 0.283, 0.055, 0.167, 0.224, 0.121, 0.136, 0.084]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5319 | Steps: 2 | Val loss: 0.4071 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7789 | Steps: 2 | Val loss: 0.6072 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 07:17:56 (running for 00:26:28.09)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.78  |  0.179 |                   57 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.539 |  0.177 |                   30 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.385 |  0.163 |                   22 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.399 |  0.143 |                    9 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.16319166123867035
[2m[36m(func pid=127238)[0m mae:  0.11851783096790314
[2m[36m(func pid=127238)[0m rmse_per_class: [0.112, 0.245, 0.065, 0.316, 0.066, 0.182, 0.27, 0.133, 0.141, 0.101]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3589 | Steps: 2 | Val loss: 0.2669 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=124933)[0m rmse: 0.17711301147937775
[2m[36m(func pid=124933)[0m mae:  0.12985293567180634
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.334, 0.1, 0.19, 0.29, 0.14, 0.142, 0.109]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17919251322746277
[2m[36m(func pid=119168)[0m mae:  0.13154703378677368
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3817 | Steps: 2 | Val loss: 0.2984 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=130296)[0m rmse: 0.14191177487373352
[2m[36m(func pid=130296)[0m mae:  0.09623592346906662
[2m[36m(func pid=130296)[0m rmse_per_class: [0.096, 0.231, 0.038, 0.278, 0.055, 0.163, 0.221, 0.12, 0.136, 0.083]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5273 | Steps: 2 | Val loss: 0.4028 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7768 | Steps: 2 | Val loss: 0.6053 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=127238)[0m rmse: 0.1624826192855835
[2m[36m(func pid=127238)[0m mae:  0.11791646480560303
[2m[36m(func pid=127238)[0m rmse_per_class: [0.111, 0.244, 0.064, 0.315, 0.065, 0.181, 0.269, 0.133, 0.141, 0.101]
[2m[36m(func pid=127238)[0m 
== Status ==
Current time: 2024-01-07 07:18:01 (running for 00:26:33.25)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.779 |  0.179 |                   58 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.532 |  0.177 |                   31 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.382 |  0.162 |                   23 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.359 |  0.142 |                   10 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3364 | Steps: 2 | Val loss: 0.2636 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=124933)[0m rmse: 0.17702487111091614
[2m[36m(func pid=124933)[0m mae:  0.12978258728981018
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.334, 0.1, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17913347482681274
[2m[36m(func pid=119168)[0m mae:  0.13149847090244293
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3788 | Steps: 2 | Val loss: 0.2968 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=130296)[0m rmse: 0.14197637140750885
[2m[36m(func pid=130296)[0m mae:  0.09648586809635162
[2m[36m(func pid=130296)[0m rmse_per_class: [0.1, 0.231, 0.036, 0.278, 0.055, 0.158, 0.223, 0.117, 0.136, 0.085]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.5217 | Steps: 2 | Val loss: 0.3983 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7726 | Steps: 2 | Val loss: 0.6037 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 07:18:06 (running for 00:26:38.33)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.777 |  0.179 |                   59 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.527 |  0.177 |                   32 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.379 |  0.162 |                   24 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.336 |  0.142 |                   11 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.16176185011863708
[2m[36m(func pid=127238)[0m mae:  0.11729855835437775
[2m[36m(func pid=127238)[0m rmse_per_class: [0.111, 0.243, 0.064, 0.314, 0.065, 0.18, 0.268, 0.133, 0.141, 0.1]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3250 | Steps: 2 | Val loss: 0.2705 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=124933)[0m rmse: 0.1768840253353119
[2m[36m(func pid=124933)[0m mae:  0.12968148291110992
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.334, 0.099, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.1791689097881317
[2m[36m(func pid=119168)[0m mae:  0.13152796030044556
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3784 | Steps: 2 | Val loss: 0.2955 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=130296)[0m rmse: 0.14344914257526398
[2m[36m(func pid=130296)[0m mae:  0.09809010475873947
[2m[36m(func pid=130296)[0m rmse_per_class: [0.106, 0.233, 0.034, 0.277, 0.055, 0.154, 0.231, 0.115, 0.139, 0.089]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5150 | Steps: 2 | Val loss: 0.3944 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.7698 | Steps: 2 | Val loss: 0.6023 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 07:18:11 (running for 00:26:43.51)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.773 |  0.179 |                   60 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.522 |  0.177 |                   33 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.378 |  0.161 |                   25 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.325 |  0.143 |                   12 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.16132280230522156
[2m[36m(func pid=127238)[0m mae:  0.11697329580783844
[2m[36m(func pid=127238)[0m rmse_per_class: [0.111, 0.243, 0.063, 0.313, 0.064, 0.18, 0.267, 0.132, 0.141, 0.099]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17675945162773132
[2m[36m(func pid=124933)[0m mae:  0.12958446145057678
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.334, 0.098, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3291 | Steps: 2 | Val loss: 0.2776 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=119168)[0m rmse: 0.17919021844863892
[2m[36m(func pid=119168)[0m mae:  0.13153161108493805
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3769 | Steps: 2 | Val loss: 0.2939 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=130296)[0m rmse: 0.14611022174358368
[2m[36m(func pid=130296)[0m mae:  0.10036323219537735
[2m[36m(func pid=130296)[0m rmse_per_class: [0.111, 0.235, 0.032, 0.278, 0.055, 0.152, 0.243, 0.115, 0.143, 0.097]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.5121 | Steps: 2 | Val loss: 0.3908 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7681 | Steps: 2 | Val loss: 0.6011 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 07:18:17 (running for 00:26:48.76)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.77  |  0.179 |                   61 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.515 |  0.177 |                   34 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.377 |  0.161 |                   26 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.329 |  0.146 |                   13 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.16056491434574127
[2m[36m(func pid=127238)[0m mae:  0.11631935834884644
[2m[36m(func pid=127238)[0m rmse_per_class: [0.11, 0.242, 0.062, 0.312, 0.064, 0.179, 0.266, 0.131, 0.14, 0.099]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17671486735343933
[2m[36m(func pid=124933)[0m mae:  0.12957532703876495
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.098, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3276 | Steps: 2 | Val loss: 0.2785 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=119168)[0m rmse: 0.1792052686214447
[2m[36m(func pid=119168)[0m mae:  0.13152845203876495
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3743 | Steps: 2 | Val loss: 0.2925 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=130296)[0m rmse: 0.14837637543678284
[2m[36m(func pid=130296)[0m mae:  0.10209232568740845
[2m[36m(func pid=130296)[0m rmse_per_class: [0.109, 0.237, 0.029, 0.279, 0.054, 0.151, 0.253, 0.116, 0.147, 0.108]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.5074 | Steps: 2 | Val loss: 0.3870 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.7704 | Steps: 2 | Val loss: 0.5996 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 07:18:22 (running for 00:26:53.78)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.768 |  0.179 |                   62 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.512 |  0.177 |                   35 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.374 |  0.16  |                   27 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.328 |  0.148 |                   14 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.15998031198978424
[2m[36m(func pid=127238)[0m mae:  0.11583415418863297
[2m[36m(func pid=127238)[0m rmse_per_class: [0.109, 0.241, 0.061, 0.311, 0.063, 0.179, 0.266, 0.131, 0.14, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.1766827404499054
[2m[36m(func pid=124933)[0m mae:  0.12954789400100708
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.258, 0.091, 0.333, 0.097, 0.189, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3225 | Steps: 2 | Val loss: 0.2754 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=119168)[0m rmse: 0.17918415367603302
[2m[36m(func pid=119168)[0m mae:  0.13151702284812927
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3712 | Steps: 2 | Val loss: 0.2912 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=130296)[0m rmse: 0.1494038701057434
[2m[36m(func pid=130296)[0m mae:  0.10242681205272675
[2m[36m(func pid=130296)[0m rmse_per_class: [0.099, 0.241, 0.029, 0.278, 0.053, 0.15, 0.258, 0.117, 0.148, 0.122]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5034 | Steps: 2 | Val loss: 0.3835 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.7668 | Steps: 2 | Val loss: 0.5982 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 07:18:27 (running for 00:26:58.88)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.77  |  0.179 |                   63 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.507 |  0.177 |                   36 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.371 |  0.159 |                   28 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.323 |  0.149 |                   15 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.15944069623947144
[2m[36m(func pid=127238)[0m mae:  0.11538021266460419
[2m[36m(func pid=127238)[0m rmse_per_class: [0.109, 0.24, 0.06, 0.31, 0.063, 0.178, 0.265, 0.13, 0.14, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17659057676792145
[2m[36m(func pid=124933)[0m mae:  0.12948007881641388
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.333, 0.097, 0.189, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3098 | Steps: 2 | Val loss: 0.2745 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=119168)[0m rmse: 0.1791725754737854
[2m[36m(func pid=119168)[0m mae:  0.1315031200647354
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3702 | Steps: 2 | Val loss: 0.2901 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=130296)[0m rmse: 0.15014629065990448
[2m[36m(func pid=130296)[0m mae:  0.10216740518808365
[2m[36m(func pid=130296)[0m rmse_per_class: [0.089, 0.244, 0.031, 0.28, 0.054, 0.15, 0.257, 0.117, 0.146, 0.133]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4983 | Steps: 2 | Val loss: 0.3808 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7642 | Steps: 2 | Val loss: 0.5963 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 07:18:32 (running for 00:27:04.30)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.767 |  0.179 |                   64 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.503 |  0.177 |                   37 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.37  |  0.159 |                   29 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.31  |  0.15  |                   16 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.15890352427959442
[2m[36m(func pid=127238)[0m mae:  0.11490927636623383
[2m[36m(func pid=127238)[0m rmse_per_class: [0.109, 0.239, 0.06, 0.309, 0.063, 0.178, 0.264, 0.13, 0.14, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17655596137046814
[2m[36m(func pid=124933)[0m mae:  0.12944619357585907
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.333, 0.097, 0.189, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3004 | Steps: 2 | Val loss: 0.2748 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=119168)[0m rmse: 0.1791517436504364
[2m[36m(func pid=119168)[0m mae:  0.13148730993270874
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3683 | Steps: 2 | Val loss: 0.2892 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=130296)[0m rmse: 0.14954030513763428
[2m[36m(func pid=130296)[0m mae:  0.10063054412603378
[2m[36m(func pid=130296)[0m rmse_per_class: [0.078, 0.246, 0.034, 0.282, 0.062, 0.151, 0.248, 0.117, 0.141, 0.136]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4934 | Steps: 2 | Val loss: 0.3780 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7630 | Steps: 2 | Val loss: 0.5956 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 07:18:37 (running for 00:27:09.35)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.764 |  0.179 |                   65 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.498 |  0.177 |                   38 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.368 |  0.159 |                   30 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.3   |  0.15  |                   17 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.1585143655538559
[2m[36m(func pid=127238)[0m mae:  0.11459797620773315
[2m[36m(func pid=127238)[0m rmse_per_class: [0.108, 0.239, 0.059, 0.309, 0.062, 0.177, 0.264, 0.129, 0.14, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.1764628142118454
[2m[36m(func pid=124933)[0m mae:  0.12936660647392273
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.333, 0.097, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17913708090782166
[2m[36m(func pid=119168)[0m mae:  0.13147467374801636
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2919 | Steps: 2 | Val loss: 0.2767 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3628 | Steps: 2 | Val loss: 0.2884 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4924 | Steps: 2 | Val loss: 0.3756 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=130296)[0m rmse: 0.1496502161026001
[2m[36m(func pid=130296)[0m mae:  0.0995529443025589
[2m[36m(func pid=130296)[0m rmse_per_class: [0.072, 0.248, 0.034, 0.284, 0.076, 0.154, 0.238, 0.115, 0.139, 0.137]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.7613 | Steps: 2 | Val loss: 0.5935 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 07:18:43 (running for 00:27:14.58)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.763 |  0.179 |                   66 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.493 |  0.176 |                   39 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.363 |  0.158 |                   31 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.292 |  0.15  |                   18 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.15819306671619415
[2m[36m(func pid=127238)[0m mae:  0.11436013132333755
[2m[36m(func pid=127238)[0m rmse_per_class: [0.108, 0.239, 0.059, 0.308, 0.062, 0.177, 0.263, 0.129, 0.14, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17635251581668854
[2m[36m(func pid=124933)[0m mae:  0.12927964329719543
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.333, 0.097, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.1791216880083084
[2m[36m(func pid=119168)[0m mae:  0.13146036863327026
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.2878 | Steps: 2 | Val loss: 0.2767 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3627 | Steps: 2 | Val loss: 0.2876 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4902 | Steps: 2 | Val loss: 0.3729 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=130296)[0m rmse: 0.14910683035850525
[2m[36m(func pid=130296)[0m mae:  0.0981377437710762
[2m[36m(func pid=130296)[0m rmse_per_class: [0.068, 0.249, 0.033, 0.285, 0.093, 0.156, 0.227, 0.113, 0.136, 0.131]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7610 | Steps: 2 | Val loss: 0.5922 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 07:18:48 (running for 00:27:19.65)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.761 |  0.179 |                   67 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.492 |  0.176 |                   40 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.363 |  0.158 |                   32 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.288 |  0.149 |                   19 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.15782910585403442
[2m[36m(func pid=127238)[0m mae:  0.11406822502613068
[2m[36m(func pid=127238)[0m rmse_per_class: [0.107, 0.238, 0.058, 0.308, 0.062, 0.176, 0.263, 0.129, 0.14, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17621758580207825
[2m[36m(func pid=124933)[0m mae:  0.12915728986263275
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.333, 0.097, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17915280163288116
[2m[36m(func pid=119168)[0m mae:  0.1314755529165268
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.2890 | Steps: 2 | Val loss: 0.2754 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3625 | Steps: 2 | Val loss: 0.2869 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=130296)[0m rmse: 0.14863334596157074
[2m[36m(func pid=130296)[0m mae:  0.0969032496213913
[2m[36m(func pid=130296)[0m rmse_per_class: [0.067, 0.249, 0.031, 0.285, 0.103, 0.159, 0.217, 0.113, 0.135, 0.127]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4843 | Steps: 2 | Val loss: 0.3701 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7585 | Steps: 2 | Val loss: 0.5897 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=127238)[0m rmse: 0.15750299394130707
[2m[36m(func pid=127238)[0m mae:  0.11379406601190567
[2m[36m(func pid=127238)[0m rmse_per_class: [0.107, 0.238, 0.058, 0.308, 0.062, 0.176, 0.262, 0.128, 0.14, 0.097]
[2m[36m(func pid=127238)[0m 
== Status ==
Current time: 2024-01-07 07:18:54 (running for 00:27:26.33)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.761 |  0.179 |                   68 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.484 |  0.176 |                   42 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.363 |  0.158 |                   33 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.289 |  0.149 |                   20 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17611852288246155
[2m[36m(func pid=124933)[0m mae:  0.12907657027244568
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.333, 0.096, 0.189, 0.289, 0.141, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.1791260838508606
[2m[36m(func pid=119168)[0m mae:  0.13146230578422546
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.2865 | Steps: 2 | Val loss: 0.2726 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3621 | Steps: 2 | Val loss: 0.2862 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=130296)[0m rmse: 0.14713308215141296
[2m[36m(func pid=130296)[0m mae:  0.09577854722738266
[2m[36m(func pid=130296)[0m rmse_per_class: [0.067, 0.25, 0.028, 0.286, 0.103, 0.16, 0.213, 0.114, 0.135, 0.116]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4803 | Steps: 2 | Val loss: 0.3676 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.7559 | Steps: 2 | Val loss: 0.5880 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=127238)[0m rmse: 0.1571005880832672
[2m[36m(func pid=127238)[0m mae:  0.11347699165344238
[2m[36m(func pid=127238)[0m rmse_per_class: [0.107, 0.237, 0.057, 0.307, 0.061, 0.175, 0.262, 0.128, 0.14, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17600268125534058
[2m[36m(func pid=124933)[0m mae:  0.12898986041545868
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.332, 0.096, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
== Status ==
Current time: 2024-01-07 07:19:00 (running for 00:27:31.58)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.758 |  0.179 |                   69 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.48  |  0.176 |                   43 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.362 |  0.157 |                   34 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.286 |  0.147 |                   21 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119168)[0m rmse: 0.1790415197610855
[2m[36m(func pid=119168)[0m mae:  0.13138669729232788
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.2958 | Steps: 2 | Val loss: 0.2693 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3599 | Steps: 2 | Val loss: 0.2856 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=130296)[0m rmse: 0.1456345170736313
[2m[36m(func pid=130296)[0m mae:  0.09458845853805542
[2m[36m(func pid=130296)[0m rmse_per_class: [0.067, 0.248, 0.026, 0.283, 0.097, 0.16, 0.211, 0.115, 0.136, 0.114]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4769 | Steps: 2 | Val loss: 0.3656 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.7526 | Steps: 2 | Val loss: 0.5872 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=127238)[0m rmse: 0.15679457783699036
[2m[36m(func pid=127238)[0m mae:  0.11321772634983063
[2m[36m(func pid=127238)[0m rmse_per_class: [0.106, 0.237, 0.057, 0.307, 0.061, 0.175, 0.262, 0.127, 0.14, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17588625848293304
[2m[36m(func pid=124933)[0m mae:  0.12891149520874023
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.332, 0.095, 0.189, 0.289, 0.14, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 07:19:05 (running for 00:27:36.79)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.756 |  0.179 |                   70 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.477 |  0.176 |                   44 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.36  |  0.157 |                   35 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.296 |  0.146 |                   22 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17902903258800507
[2m[36m(func pid=119168)[0m mae:  0.13138112425804138
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2836 | Steps: 2 | Val loss: 0.2667 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3616 | Steps: 2 | Val loss: 0.2851 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4749 | Steps: 2 | Val loss: 0.3633 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=130296)[0m rmse: 0.14427226781845093
[2m[36m(func pid=130296)[0m mae:  0.09373165667057037
[2m[36m(func pid=130296)[0m rmse_per_class: [0.068, 0.247, 0.025, 0.283, 0.087, 0.156, 0.211, 0.114, 0.138, 0.112]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.7518 | Steps: 2 | Val loss: 0.5861 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=127238)[0m rmse: 0.1565539687871933
[2m[36m(func pid=127238)[0m mae:  0.11302611976861954
[2m[36m(func pid=127238)[0m rmse_per_class: [0.106, 0.237, 0.056, 0.306, 0.061, 0.174, 0.262, 0.127, 0.14, 0.097]
[2m[36m(func pid=127238)[0m 
== Status ==
Current time: 2024-01-07 07:19:10 (running for 00:27:41.99)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.753 |  0.179 |                   71 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.475 |  0.176 |                   45 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.362 |  0.157 |                   36 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.284 |  0.144 |                   23 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.1757623255252838
[2m[36m(func pid=124933)[0m mae:  0.12880957126617432
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.332, 0.095, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.1790352612733841
[2m[36m(func pid=119168)[0m mae:  0.1313902884721756
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2799 | Steps: 2 | Val loss: 0.2657 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3561 | Steps: 2 | Val loss: 0.2847 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4720 | Steps: 2 | Val loss: 0.3612 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=130296)[0m rmse: 0.1437186300754547
[2m[36m(func pid=130296)[0m mae:  0.09359968453645706
[2m[36m(func pid=130296)[0m rmse_per_class: [0.073, 0.246, 0.026, 0.284, 0.077, 0.153, 0.213, 0.114, 0.14, 0.112]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.1562773734331131
[2m[36m(func pid=127238)[0m mae:  0.11278510093688965
[2m[36m(func pid=127238)[0m rmse_per_class: [0.106, 0.236, 0.056, 0.306, 0.061, 0.174, 0.262, 0.127, 0.14, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.7484 | Steps: 2 | Val loss: 0.5849 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 07:19:15 (running for 00:27:47.25)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.752 |  0.179 |                   72 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.472 |  0.176 |                   46 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.356 |  0.156 |                   37 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.28  |  0.144 |                   24 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.1756935566663742
[2m[36m(func pid=124933)[0m mae:  0.128743514418602
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.332, 0.095, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17906707525253296
[2m[36m(func pid=119168)[0m mae:  0.13140468299388885
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2747 | Steps: 2 | Val loss: 0.2663 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3602 | Steps: 2 | Val loss: 0.2843 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4693 | Steps: 2 | Val loss: 0.3593 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=127238)[0m rmse: 0.15603916347026825
[2m[36m(func pid=127238)[0m mae:  0.11256647109985352
[2m[36m(func pid=127238)[0m rmse_per_class: [0.106, 0.236, 0.055, 0.306, 0.061, 0.174, 0.261, 0.127, 0.14, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.14417719841003418
[2m[36m(func pid=130296)[0m mae:  0.09407080709934235
[2m[36m(func pid=130296)[0m rmse_per_class: [0.077, 0.247, 0.026, 0.286, 0.071, 0.151, 0.217, 0.114, 0.14, 0.113]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7492 | Steps: 2 | Val loss: 0.5828 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 07:19:21 (running for 00:27:52.66)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.748 |  0.179 |                   73 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.469 |  0.176 |                   47 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.36  |  0.156 |                   38 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.275 |  0.144 |                   25 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17570216953754425
[2m[36m(func pid=124933)[0m mae:  0.12875139713287354
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.332, 0.094, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17907246947288513
[2m[36m(func pid=119168)[0m mae:  0.13141094148159027
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=119168)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2793 | Steps: 2 | Val loss: 0.2681 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3563 | Steps: 2 | Val loss: 0.2836 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4677 | Steps: 2 | Val loss: 0.3573 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=130296)[0m rmse: 0.1454172432422638
[2m[36m(func pid=130296)[0m mae:  0.09501518309116364
[2m[36m(func pid=130296)[0m rmse_per_class: [0.08, 0.249, 0.026, 0.289, 0.067, 0.15, 0.222, 0.114, 0.141, 0.116]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.1556619256734848
[2m[36m(func pid=127238)[0m mae:  0.11222638934850693
[2m[36m(func pid=127238)[0m rmse_per_class: [0.105, 0.235, 0.054, 0.305, 0.061, 0.173, 0.26, 0.126, 0.14, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=119168)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.7469 | Steps: 2 | Val loss: 0.5816 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 07:19:26 (running for 00:27:57.90)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1522499993443489
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00012 | RUNNING    | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.749 |  0.179 |                   74 |
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.468 |  0.176 |                   48 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.356 |  0.156 |                   39 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.279 |  0.145 |                   26 |
| train_ccef6_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17558982968330383
[2m[36m(func pid=124933)[0m mae:  0.12865875661373138
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.332, 0.094, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=119168)[0m rmse: 0.17906348407268524
[2m[36m(func pid=119168)[0m mae:  0.13140645623207092
[2m[36m(func pid=119168)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.107]
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2723 | Steps: 2 | Val loss: 0.2689 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3505 | Steps: 2 | Val loss: 0.2836 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=130296)[0m rmse: 0.14599290490150452
[2m[36m(func pid=130296)[0m mae:  0.09520968049764633
[2m[36m(func pid=130296)[0m rmse_per_class: [0.08, 0.251, 0.026, 0.29, 0.064, 0.149, 0.226, 0.115, 0.139, 0.119]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4632 | Steps: 2 | Val loss: 0.3550 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=127238)[0m rmse: 0.15564727783203125
[2m[36m(func pid=127238)[0m mae:  0.11223191022872925
[2m[36m(func pid=127238)[0m rmse_per_class: [0.105, 0.235, 0.054, 0.305, 0.061, 0.173, 0.261, 0.126, 0.14, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.1754833459854126
[2m[36m(func pid=124933)[0m mae:  0.12857350707054138
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.256, 0.088, 0.331, 0.094, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2732 | Steps: 2 | Val loss: 0.2713 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3515 | Steps: 2 | Val loss: 0.2835 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4627 | Steps: 2 | Val loss: 0.3534 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=130296)[0m rmse: 0.1474873423576355
[2m[36m(func pid=130296)[0m mae:  0.09594696760177612
[2m[36m(func pid=130296)[0m rmse_per_class: [0.082, 0.254, 0.026, 0.293, 0.063, 0.149, 0.228, 0.116, 0.138, 0.126]
[2m[36m(func pid=127238)[0m rmse: 0.15557202696800232
[2m[36m(func pid=127238)[0m mae:  0.11219797283411026
[2m[36m(func pid=127238)[0m rmse_per_class: [0.106, 0.235, 0.054, 0.305, 0.061, 0.172, 0.261, 0.125, 0.14, 0.097]
== Status ==
Current time: 2024-01-07 07:19:31 (running for 00:28:03.11)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.463 |  0.175 |                   49 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.351 |  0.156 |                   40 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.272 |  0.146 |                   27 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=136744)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=136744)[0m Configuration completed!
[2m[36m(func pid=136744)[0m New optimizer parameters:
[2m[36m(func pid=136744)[0m SGD (
[2m[36m(func pid=136744)[0m Parameter Group 0
[2m[36m(func pid=136744)[0m     dampening: 0
[2m[36m(func pid=136744)[0m     differentiable: False
[2m[36m(func pid=136744)[0m     foreach: None
[2m[36m(func pid=136744)[0m     lr: 0.0001
[2m[36m(func pid=136744)[0m     maximize: False
[2m[36m(func pid=136744)[0m     momentum: 0.99
[2m[36m(func pid=136744)[0m     nesterov: False
[2m[36m(func pid=136744)[0m     weight_decay: 1e-05
[2m[36m(func pid=136744)[0m )
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=127238)[0m 
== Status ==
Current time: 2024-01-07 07:19:36 (running for 00:28:08.50)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.463 |  0.175 |                   50 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.351 |  0.156 |                   41 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.273 |  0.147 |                   28 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.1753704696893692
[2m[36m(func pid=124933)[0m mae:  0.12847498059272766
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.256, 0.088, 0.331, 0.094, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3524 | Steps: 2 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2797 | Steps: 2 | Val loss: 0.2731 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8939 | Steps: 2 | Val loss: 0.7101 | Batch size: 32 | lr: 0.0001 | Duration: 4.24s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4596 | Steps: 2 | Val loss: 0.3514 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=127238)[0m rmse: 0.15548090636730194
[2m[36m(func pid=127238)[0m mae:  0.11209775507450104
[2m[36m(func pid=127238)[0m rmse_per_class: [0.105, 0.235, 0.054, 0.304, 0.061, 0.172, 0.26, 0.125, 0.14, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.14843200147151947
[2m[36m(func pid=130296)[0m mae:  0.09650206565856934
[2m[36m(func pid=130296)[0m rmse_per_class: [0.082, 0.257, 0.026, 0.296, 0.064, 0.149, 0.228, 0.117, 0.137, 0.128]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.18279869854450226
[2m[36m(func pid=136744)[0m mae:  0.1345118284225464
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:19:42 (running for 00:28:13.77)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.46  |  0.175 |                   51 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.352 |  0.155 |                   42 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.28  |  0.148 |                   29 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.894 |  0.183 |                    1 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.1751823127269745
[2m[36m(func pid=124933)[0m mae:  0.12832148373126984
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.256, 0.088, 0.331, 0.093, 0.189, 0.288, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3504 | Steps: 2 | Val loss: 0.2832 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2689 | Steps: 2 | Val loss: 0.2735 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8970 | Steps: 2 | Val loss: 0.7062 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4564 | Steps: 2 | Val loss: 0.3501 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=127238)[0m rmse: 0.1554190069437027
[2m[36m(func pid=127238)[0m mae:  0.11205215752124786
[2m[36m(func pid=127238)[0m rmse_per_class: [0.105, 0.235, 0.054, 0.304, 0.061, 0.172, 0.26, 0.125, 0.14, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.14847688376903534
[2m[36m(func pid=130296)[0m mae:  0.09604708105325699
[2m[36m(func pid=130296)[0m rmse_per_class: [0.077, 0.257, 0.025, 0.296, 0.064, 0.15, 0.224, 0.119, 0.136, 0.135]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.18253958225250244
[2m[36m(func pid=136744)[0m mae:  0.13435238599777222
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.143, 0.112]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:19:47 (running for 00:28:19.03)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.456 |  0.175 |                   52 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.35  |  0.155 |                   43 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.269 |  0.148 |                   30 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.897 |  0.183 |                    2 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17518723011016846
[2m[36m(func pid=124933)[0m mae:  0.12833306193351746
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.088, 0.331, 0.093, 0.189, 0.288, 0.14, 0.142, 0.109]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3493 | Steps: 2 | Val loss: 0.2830 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2707 | Steps: 2 | Val loss: 0.2745 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8977 | Steps: 2 | Val loss: 0.7023 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4547 | Steps: 2 | Val loss: 0.3484 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=127238)[0m rmse: 0.1552850902080536
[2m[36m(func pid=127238)[0m mae:  0.11193032562732697
[2m[36m(func pid=127238)[0m rmse_per_class: [0.105, 0.235, 0.054, 0.304, 0.061, 0.171, 0.26, 0.125, 0.14, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.14877650141716003
[2m[36m(func pid=130296)[0m mae:  0.09585361182689667
[2m[36m(func pid=130296)[0m rmse_per_class: [0.074, 0.258, 0.025, 0.298, 0.065, 0.151, 0.22, 0.12, 0.137, 0.14]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.18220186233520508
[2m[36m(func pid=136744)[0m mae:  0.1341029703617096
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:19:52 (running for 00:28:24.14)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.455 |  0.175 |                   53 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.349 |  0.155 |                   44 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.271 |  0.149 |                   31 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.898 |  0.182 |                    3 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17504052817821503
[2m[36m(func pid=124933)[0m mae:  0.12821543216705322
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.087, 0.331, 0.093, 0.189, 0.288, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3434 | Steps: 2 | Val loss: 0.2825 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2685 | Steps: 2 | Val loss: 0.2753 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8920 | Steps: 2 | Val loss: 0.6995 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4539 | Steps: 2 | Val loss: 0.3471 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=127238)[0m rmse: 0.1549752652645111
[2m[36m(func pid=127238)[0m mae:  0.11165677011013031
[2m[36m(func pid=127238)[0m rmse_per_class: [0.104, 0.235, 0.053, 0.304, 0.061, 0.171, 0.259, 0.124, 0.14, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.14891140162944794
[2m[36m(func pid=130296)[0m mae:  0.09592615813016891
[2m[36m(func pid=130296)[0m rmse_per_class: [0.073, 0.258, 0.025, 0.3, 0.066, 0.152, 0.218, 0.118, 0.139, 0.141]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.18181763589382172
[2m[36m(func pid=136744)[0m mae:  0.1337939351797104
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.339, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:19:57 (running for 00:28:29.47)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.454 |  0.175 |                   54 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.343 |  0.155 |                   45 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.269 |  0.149 |                   32 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.892 |  0.182 |                    4 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.1750369369983673
[2m[36m(func pid=124933)[0m mae:  0.12822207808494568
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.088, 0.331, 0.093, 0.189, 0.288, 0.14, 0.142, 0.109]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3474 | Steps: 2 | Val loss: 0.2823 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2643 | Steps: 2 | Val loss: 0.2748 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8909 | Steps: 2 | Val loss: 0.6962 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4502 | Steps: 2 | Val loss: 0.3460 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=127238)[0m rmse: 0.1548600196838379
[2m[36m(func pid=127238)[0m mae:  0.11159545183181763
[2m[36m(func pid=127238)[0m rmse_per_class: [0.104, 0.235, 0.053, 0.303, 0.061, 0.171, 0.259, 0.124, 0.141, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.14868271350860596
[2m[36m(func pid=130296)[0m mae:  0.09552831202745438
[2m[36m(func pid=130296)[0m rmse_per_class: [0.07, 0.257, 0.025, 0.298, 0.067, 0.153, 0.216, 0.117, 0.141, 0.144]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.18140630424022675
[2m[36m(func pid=136744)[0m mae:  0.13345691561698914
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.112, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:20:03 (running for 00:28:34.63)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.45  |  0.175 |                   55 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.347 |  0.155 |                   46 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.264 |  0.149 |                   33 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.891 |  0.181 |                    5 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17487728595733643
[2m[36m(func pid=124933)[0m mae:  0.1281038373708725
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.256, 0.087, 0.331, 0.092, 0.189, 0.288, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3412 | Steps: 2 | Val loss: 0.2821 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2688 | Steps: 2 | Val loss: 0.2751 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8874 | Steps: 2 | Val loss: 0.6931 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4502 | Steps: 2 | Val loss: 0.3447 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=127238)[0m rmse: 0.15469171106815338
[2m[36m(func pid=127238)[0m mae:  0.11143239587545395
[2m[36m(func pid=127238)[0m rmse_per_class: [0.103, 0.235, 0.053, 0.303, 0.061, 0.17, 0.259, 0.124, 0.14, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.14896509051322937
[2m[36m(func pid=130296)[0m mae:  0.0954909548163414
[2m[36m(func pid=130296)[0m rmse_per_class: [0.069, 0.257, 0.025, 0.298, 0.068, 0.153, 0.216, 0.116, 0.14, 0.147]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.18104752898216248
[2m[36m(func pid=136744)[0m mae:  0.1331585794687271
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.112, 0.19, 0.294, 0.142, 0.142, 0.11]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:20:08 (running for 00:28:39.86)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.45  |  0.175 |                   56 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.341 |  0.155 |                   47 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.269 |  0.149 |                   34 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.887 |  0.181 |                    6 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17481328547000885
[2m[36m(func pid=124933)[0m mae:  0.12805548310279846
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.256, 0.087, 0.331, 0.092, 0.189, 0.288, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3434 | Steps: 2 | Val loss: 0.2816 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8855 | Steps: 2 | Val loss: 0.6905 | Batch size: 32 | lr: 0.0001 | Duration: 2.64s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2748 | Steps: 2 | Val loss: 0.2738 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4480 | Steps: 2 | Val loss: 0.3434 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=127238)[0m rmse: 0.15437568724155426
[2m[36m(func pid=127238)[0m mae:  0.11116421222686768
[2m[36m(func pid=127238)[0m rmse_per_class: [0.103, 0.235, 0.052, 0.302, 0.061, 0.17, 0.259, 0.124, 0.14, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.1807602494955063
[2m[36m(func pid=136744)[0m mae:  0.1329077184200287
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.264, 0.103, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.1483059525489807
[2m[36m(func pid=130296)[0m mae:  0.09512944519519806
[2m[36m(func pid=130296)[0m rmse_per_class: [0.072, 0.257, 0.025, 0.296, 0.069, 0.152, 0.217, 0.115, 0.141, 0.138]
[2m[36m(func pid=130296)[0m 
== Status ==
Current time: 2024-01-07 07:20:13 (running for 00:28:45.05)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.448 |  0.175 |                   57 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.343 |  0.154 |                   48 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.275 |  0.148 |                   35 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.886 |  0.181 |                    7 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17468087375164032
[2m[36m(func pid=124933)[0m mae:  0.1279527246952057
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.256, 0.087, 0.33, 0.092, 0.189, 0.288, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3476 | Steps: 2 | Val loss: 0.2815 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8841 | Steps: 2 | Val loss: 0.6878 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2657 | Steps: 2 | Val loss: 0.2739 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4454 | Steps: 2 | Val loss: 0.3421 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=127238)[0m rmse: 0.15436531603336334
[2m[36m(func pid=127238)[0m mae:  0.11115296185016632
[2m[36m(func pid=127238)[0m rmse_per_class: [0.102, 0.235, 0.052, 0.301, 0.061, 0.17, 0.259, 0.124, 0.141, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.18051645159721375
[2m[36m(func pid=136744)[0m mae:  0.13268141448497772
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.14875419437885284
[2m[36m(func pid=130296)[0m mae:  0.09535018354654312
[2m[36m(func pid=130296)[0m rmse_per_class: [0.073, 0.257, 0.025, 0.296, 0.071, 0.151, 0.219, 0.115, 0.14, 0.14]
[2m[36m(func pid=130296)[0m 
== Status ==
Current time: 2024-01-07 07:20:18 (running for 00:28:50.38)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.445 |  0.175 |                   58 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.348 |  0.154 |                   49 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.266 |  0.149 |                   36 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.884 |  0.181 |                    8 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17461848258972168
[2m[36m(func pid=124933)[0m mae:  0.12791608273983002
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.256, 0.087, 0.33, 0.091, 0.189, 0.288, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3389 | Steps: 2 | Val loss: 0.2813 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8793 | Steps: 2 | Val loss: 0.6851 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2668 | Steps: 2 | Val loss: 0.2728 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4403 | Steps: 2 | Val loss: 0.3405 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=127238)[0m rmse: 0.15421929955482483
[2m[36m(func pid=127238)[0m mae:  0.11102042347192764
[2m[36m(func pid=127238)[0m rmse_per_class: [0.102, 0.235, 0.052, 0.3, 0.062, 0.17, 0.259, 0.124, 0.141, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.18031033873558044
[2m[36m(func pid=136744)[0m mae:  0.13250184059143066
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.263, 0.101, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.14832884073257446
[2m[36m(func pid=130296)[0m mae:  0.09487147629261017
[2m[36m(func pid=130296)[0m rmse_per_class: [0.074, 0.255, 0.025, 0.293, 0.071, 0.15, 0.22, 0.116, 0.139, 0.141]
[2m[36m(func pid=130296)[0m 
== Status ==
Current time: 2024-01-07 07:20:23 (running for 00:28:55.45)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.44  |  0.175 |                   59 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.339 |  0.154 |                   50 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.267 |  0.148 |                   37 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.879 |  0.18  |                    9 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17451715469360352
[2m[36m(func pid=124933)[0m mae:  0.12781038880348206
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.087, 0.33, 0.091, 0.189, 0.288, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3394 | Steps: 2 | Val loss: 0.2811 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8750 | Steps: 2 | Val loss: 0.6816 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2667 | Steps: 2 | Val loss: 0.2726 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4419 | Steps: 2 | Val loss: 0.3391 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=127238)[0m rmse: 0.15409120917320251
[2m[36m(func pid=127238)[0m mae:  0.11091627925634384
[2m[36m(func pid=127238)[0m rmse_per_class: [0.101, 0.235, 0.051, 0.3, 0.062, 0.169, 0.259, 0.123, 0.141, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.18011006712913513
[2m[36m(func pid=136744)[0m mae:  0.13232937455177307
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.263, 0.101, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.14813467860221863
[2m[36m(func pid=130296)[0m mae:  0.09464073181152344
[2m[36m(func pid=130296)[0m rmse_per_class: [0.073, 0.256, 0.025, 0.293, 0.071, 0.15, 0.221, 0.116, 0.137, 0.141]
[2m[36m(func pid=130296)[0m 
== Status ==
Current time: 2024-01-07 07:20:29 (running for 00:29:00.69)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.442 |  0.174 |                   60 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.339 |  0.154 |                   51 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.267 |  0.148 |                   38 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.875 |  0.18  |                   10 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17438635230064392
[2m[36m(func pid=124933)[0m mae:  0.12769539654254913
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.33, 0.091, 0.189, 0.287, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3475 | Steps: 2 | Val loss: 0.2806 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8700 | Steps: 2 | Val loss: 0.6782 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2639 | Steps: 2 | Val loss: 0.2720 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4412 | Steps: 2 | Val loss: 0.3383 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=127238)[0m rmse: 0.15373067557811737
[2m[36m(func pid=127238)[0m mae:  0.11060144752264023
[2m[36m(func pid=127238)[0m rmse_per_class: [0.101, 0.235, 0.051, 0.299, 0.062, 0.169, 0.258, 0.123, 0.141, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17994612455368042
[2m[36m(func pid=136744)[0m mae:  0.13218726217746735
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.1477653682231903
[2m[36m(func pid=130296)[0m mae:  0.09413771331310272
[2m[36m(func pid=130296)[0m rmse_per_class: [0.071, 0.257, 0.024, 0.291, 0.071, 0.149, 0.222, 0.116, 0.135, 0.14]
[2m[36m(func pid=130296)[0m 
== Status ==
Current time: 2024-01-07 07:20:34 (running for 00:29:05.92)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.441 |  0.174 |                   61 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.347 |  0.154 |                   52 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.264 |  0.148 |                   39 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.87  |  0.18  |                   11 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17426548898220062
[2m[36m(func pid=124933)[0m mae:  0.12759533524513245
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.33, 0.091, 0.189, 0.287, 0.14, 0.142, 0.108]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3377 | Steps: 2 | Val loss: 0.2801 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8676 | Steps: 2 | Val loss: 0.6745 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2615 | Steps: 2 | Val loss: 0.2727 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4378 | Steps: 2 | Val loss: 0.3371 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=127238)[0m rmse: 0.1533667892217636
[2m[36m(func pid=127238)[0m mae:  0.11024832725524902
[2m[36m(func pid=127238)[0m rmse_per_class: [0.1, 0.235, 0.051, 0.299, 0.062, 0.168, 0.258, 0.123, 0.141, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17984530329704285
[2m[36m(func pid=136744)[0m mae:  0.13210073113441467
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.336, 0.111, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.1481650024652481
[2m[36m(func pid=130296)[0m mae:  0.09430713951587677
[2m[36m(func pid=130296)[0m rmse_per_class: [0.071, 0.258, 0.024, 0.292, 0.073, 0.15, 0.223, 0.116, 0.135, 0.139]
[2m[36m(func pid=130296)[0m 
== Status ==
Current time: 2024-01-07 07:20:39 (running for 00:29:10.93)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.438 |  0.174 |                   62 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.338 |  0.153 |                   53 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.262 |  0.148 |                   40 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.868 |  0.18  |                   12 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17416346073150635
[2m[36m(func pid=124933)[0m mae:  0.12749502062797546
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.33, 0.09, 0.188, 0.287, 0.14, 0.142, 0.107]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3383 | Steps: 2 | Val loss: 0.2795 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8593 | Steps: 2 | Val loss: 0.6702 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2615 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4395 | Steps: 2 | Val loss: 0.3360 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=127238)[0m rmse: 0.15303802490234375
[2m[36m(func pid=127238)[0m mae:  0.10992816835641861
[2m[36m(func pid=127238)[0m rmse_per_class: [0.099, 0.235, 0.05, 0.297, 0.062, 0.168, 0.257, 0.123, 0.141, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.1797402948141098
[2m[36m(func pid=136744)[0m mae:  0.13201388716697693
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:20:44 (running for 00:29:15.96)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.438 |  0.174 |                   62 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.338 |  0.153 |                   54 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.262 |  0.148 |                   41 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.859 |  0.18  |                   13 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.14836426079273224
[2m[36m(func pid=130296)[0m mae:  0.09426186978816986
[2m[36m(func pid=130296)[0m rmse_per_class: [0.073, 0.258, 0.025, 0.292, 0.073, 0.15, 0.222, 0.116, 0.135, 0.14]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17414039373397827
[2m[36m(func pid=124933)[0m mae:  0.12747085094451904
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.256, 0.086, 0.33, 0.09, 0.188, 0.287, 0.14, 0.142, 0.107]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3367 | Steps: 2 | Val loss: 0.2790 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8557 | Steps: 2 | Val loss: 0.6657 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2618 | Steps: 2 | Val loss: 0.2735 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4371 | Steps: 2 | Val loss: 0.3347 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=127238)[0m rmse: 0.15264317393302917
[2m[36m(func pid=127238)[0m mae:  0.10952825844287872
[2m[36m(func pid=127238)[0m rmse_per_class: [0.098, 0.235, 0.049, 0.297, 0.062, 0.167, 0.257, 0.123, 0.14, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17968711256980896
[2m[36m(func pid=136744)[0m mae:  0.13197150826454163
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:20:49 (running for 00:29:21.30)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.439 |  0.174 |                   63 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.337 |  0.153 |                   55 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.262 |  0.149 |                   42 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.856 |  0.18  |                   14 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.14862750470638275
[2m[36m(func pid=130296)[0m mae:  0.09401344507932663
[2m[36m(func pid=130296)[0m rmse_per_class: [0.073, 0.257, 0.025, 0.293, 0.073, 0.15, 0.219, 0.117, 0.135, 0.145]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=124933)[0m rmse: 0.17393676936626434
[2m[36m(func pid=124933)[0m mae:  0.12731146812438965
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.085, 0.329, 0.09, 0.188, 0.287, 0.14, 0.142, 0.107]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3359 | Steps: 2 | Val loss: 0.2787 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8450 | Steps: 2 | Val loss: 0.6608 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2644 | Steps: 2 | Val loss: 0.2742 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4363 | Steps: 2 | Val loss: 0.3337 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=127238)[0m rmse: 0.15238457918167114
[2m[36m(func pid=127238)[0m mae:  0.1092793196439743
[2m[36m(func pid=127238)[0m rmse_per_class: [0.098, 0.234, 0.049, 0.297, 0.062, 0.167, 0.256, 0.123, 0.14, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17967301607131958
[2m[36m(func pid=136744)[0m mae:  0.1319510042667389
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:20:55 (running for 00:29:26.60)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.436 |  0.174 |                   65 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.336 |  0.152 |                   56 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.262 |  0.149 |                   42 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.845 |  0.18  |                   15 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17381799221038818
[2m[36m(func pid=124933)[0m mae:  0.1272030472755432
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.085, 0.329, 0.09, 0.188, 0.286, 0.14, 0.141, 0.107]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.14908578991889954
[2m[36m(func pid=130296)[0m mae:  0.09420085698366165
[2m[36m(func pid=130296)[0m rmse_per_class: [0.073, 0.257, 0.025, 0.293, 0.072, 0.151, 0.219, 0.118, 0.137, 0.147]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3357 | Steps: 2 | Val loss: 0.2784 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8428 | Steps: 2 | Val loss: 0.6560 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4354 | Steps: 2 | Val loss: 0.3328 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=127238)[0m rmse: 0.15210333466529846
[2m[36m(func pid=127238)[0m mae:  0.10900386422872543
[2m[36m(func pid=127238)[0m rmse_per_class: [0.097, 0.234, 0.049, 0.297, 0.062, 0.167, 0.255, 0.123, 0.139, 0.097]
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2589 | Steps: 2 | Val loss: 0.2756 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17958220839500427
[2m[36m(func pid=136744)[0m mae:  0.1318611055612564
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:21:00 (running for 00:29:32.02)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.435 |  0.174 |                   66 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.336 |  0.152 |                   57 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.264 |  0.149 |                   43 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.843 |  0.18  |                   16 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17371539771556854
[2m[36m(func pid=124933)[0m mae:  0.1271285116672516
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.085, 0.329, 0.089, 0.188, 0.286, 0.14, 0.141, 0.107]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.14991241693496704
[2m[36m(func pid=130296)[0m mae:  0.09464650601148605
[2m[36m(func pid=130296)[0m rmse_per_class: [0.073, 0.258, 0.025, 0.294, 0.072, 0.151, 0.219, 0.118, 0.139, 0.15]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3321 | Steps: 2 | Val loss: 0.2779 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8364 | Steps: 2 | Val loss: 0.6506 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4325 | Steps: 2 | Val loss: 0.3320 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=127238)[0m rmse: 0.1518227756023407
[2m[36m(func pid=127238)[0m mae:  0.10868318378925323
[2m[36m(func pid=127238)[0m rmse_per_class: [0.097, 0.234, 0.048, 0.296, 0.063, 0.167, 0.255, 0.122, 0.139, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2591 | Steps: 2 | Val loss: 0.2770 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=136744)[0m rmse: 0.17950986325740814
[2m[36m(func pid=136744)[0m mae:  0.13179969787597656
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:21:05 (running for 00:29:37.19)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.433 |  0.174 |                   67 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.332 |  0.152 |                   58 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.259 |  0.15  |                   44 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.836 |  0.18  |                   17 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17365172505378723
[2m[36m(func pid=124933)[0m mae:  0.127069890499115
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.085, 0.329, 0.089, 0.188, 0.286, 0.14, 0.141, 0.107]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=130296)[0m rmse: 0.15057623386383057
[2m[36m(func pid=130296)[0m mae:  0.09510935842990875
[2m[36m(func pid=130296)[0m rmse_per_class: [0.073, 0.26, 0.025, 0.297, 0.072, 0.151, 0.219, 0.117, 0.14, 0.15]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3409 | Steps: 2 | Val loss: 0.2776 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8296 | Steps: 2 | Val loss: 0.6458 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=127238)[0m rmse: 0.15163952112197876
[2m[36m(func pid=127238)[0m mae:  0.1085021123290062
[2m[36m(func pid=127238)[0m rmse_per_class: [0.096, 0.234, 0.048, 0.296, 0.063, 0.167, 0.254, 0.122, 0.139, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4301 | Steps: 2 | Val loss: 0.3314 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2521 | Steps: 2 | Val loss: 0.2774 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=136744)[0m rmse: 0.1794699877500534
[2m[36m(func pid=136744)[0m mae:  0.13176226615905762
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:21:10 (running for 00:29:42.48)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.43  |  0.174 |                   68 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.341 |  0.152 |                   59 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.259 |  0.151 |                   45 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.83  |  0.179 |                   18 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17360226809978485
[2m[36m(func pid=124933)[0m mae:  0.12703734636306763
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.085, 0.329, 0.089, 0.188, 0.286, 0.14, 0.141, 0.107]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3354 | Steps: 2 | Val loss: 0.2772 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=130296)[0m rmse: 0.15076449513435364
[2m[36m(func pid=130296)[0m mae:  0.09532825648784637
[2m[36m(func pid=130296)[0m rmse_per_class: [0.075, 0.26, 0.025, 0.298, 0.072, 0.151, 0.219, 0.117, 0.142, 0.148]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8216 | Steps: 2 | Val loss: 0.6403 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=127238)[0m rmse: 0.15134963393211365
[2m[36m(func pid=127238)[0m mae:  0.1081911101937294
[2m[36m(func pid=127238)[0m rmse_per_class: [0.096, 0.234, 0.048, 0.296, 0.063, 0.166, 0.254, 0.122, 0.138, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4330 | Steps: 2 | Val loss: 0.3304 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2533 | Steps: 2 | Val loss: 0.2779 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=136744)[0m rmse: 0.1794549524784088
[2m[36m(func pid=136744)[0m mae:  0.13174885511398315
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:21:16 (running for 00:29:47.95)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.433 |  0.174 |                   69 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.335 |  0.151 |                   60 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.252 |  0.151 |                   46 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.822 |  0.179 |                   19 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17355823516845703
[2m[36m(func pid=124933)[0m mae:  0.12700650095939636
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.085, 0.329, 0.089, 0.188, 0.286, 0.14, 0.142, 0.107]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3336 | Steps: 2 | Val loss: 0.2768 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=130296)[0m rmse: 0.15079158544540405
[2m[36m(func pid=130296)[0m mae:  0.09530249983072281
[2m[36m(func pid=130296)[0m rmse_per_class: [0.076, 0.26, 0.025, 0.3, 0.07, 0.151, 0.218, 0.117, 0.142, 0.148]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8143 | Steps: 2 | Val loss: 0.6340 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=127238)[0m rmse: 0.15111494064331055
[2m[36m(func pid=127238)[0m mae:  0.10792608559131622
[2m[36m(func pid=127238)[0m rmse_per_class: [0.095, 0.234, 0.048, 0.295, 0.063, 0.166, 0.253, 0.122, 0.138, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2575 | Steps: 2 | Val loss: 0.2784 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=136744)[0m rmse: 0.1793912798166275
[2m[36m(func pid=136744)[0m mae:  0.13169781863689423
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4287 | Steps: 2 | Val loss: 0.3297 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:21:21 (running for 00:29:53.34)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.433 |  0.174 |                   69 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.334 |  0.151 |                   61 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.257 |  0.151 |                   48 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.814 |  0.179 |                   20 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.1735791265964508
[2m[36m(func pid=124933)[0m mae:  0.12702104449272156
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.085, 0.329, 0.089, 0.188, 0.286, 0.14, 0.142, 0.107]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3254 | Steps: 2 | Val loss: 0.2767 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=130296)[0m rmse: 0.15086551010608673
[2m[36m(func pid=130296)[0m mae:  0.09537921100854874
[2m[36m(func pid=130296)[0m rmse_per_class: [0.077, 0.26, 0.025, 0.301, 0.07, 0.151, 0.219, 0.117, 0.142, 0.148]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8066 | Steps: 2 | Val loss: 0.6282 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=127238)[0m rmse: 0.1509949117898941
[2m[36m(func pid=127238)[0m mae:  0.10777336359024048
[2m[36m(func pid=127238)[0m rmse_per_class: [0.095, 0.234, 0.047, 0.296, 0.063, 0.166, 0.252, 0.122, 0.137, 0.097]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4284 | Steps: 2 | Val loss: 0.3292 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=136744)[0m rmse: 0.17934150993824005
[2m[36m(func pid=136744)[0m mae:  0.13167396187782288
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2609 | Steps: 2 | Val loss: 0.2787 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 07:21:27 (running for 00:29:58.67)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.428 |  0.174 |                   71 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.325 |  0.151 |                   62 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.257 |  0.151 |                   48 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.807 |  0.179 |                   21 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17351111769676208
[2m[36m(func pid=124933)[0m mae:  0.1269858330488205
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.084, 0.329, 0.089, 0.188, 0.286, 0.14, 0.142, 0.107]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3267 | Steps: 2 | Val loss: 0.2765 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=130296)[0m rmse: 0.15070918202400208
[2m[36m(func pid=130296)[0m mae:  0.09529943764209747
[2m[36m(func pid=130296)[0m rmse_per_class: [0.076, 0.261, 0.025, 0.302, 0.068, 0.151, 0.218, 0.117, 0.142, 0.147]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.7986 | Steps: 2 | Val loss: 0.6213 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=127238)[0m rmse: 0.15084664523601532
[2m[36m(func pid=127238)[0m mae:  0.10760489851236343
[2m[36m(func pid=127238)[0m rmse_per_class: [0.095, 0.234, 0.047, 0.296, 0.064, 0.166, 0.252, 0.121, 0.137, 0.098]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4285 | Steps: 2 | Val loss: 0.3286 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2678 | Steps: 2 | Val loss: 0.2774 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=136744)[0m rmse: 0.1792718470096588
[2m[36m(func pid=136744)[0m mae:  0.1316075325012207
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:21:32 (running for 00:30:04.13)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.429 |  0.173 |                   72 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.327 |  0.151 |                   63 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.261 |  0.151 |                   49 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.799 |  0.179 |                   22 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.1734360009431839
[2m[36m(func pid=124933)[0m mae:  0.12691926956176758
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.084, 0.329, 0.089, 0.188, 0.286, 0.14, 0.142, 0.107]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3277 | Steps: 2 | Val loss: 0.2769 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=130296)[0m rmse: 0.1495368480682373
[2m[36m(func pid=130296)[0m mae:  0.0948181003332138
[2m[36m(func pid=130296)[0m rmse_per_class: [0.075, 0.262, 0.025, 0.302, 0.067, 0.151, 0.218, 0.119, 0.141, 0.136]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.7931 | Steps: 2 | Val loss: 0.6147 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=127238)[0m rmse: 0.15116609632968903
[2m[36m(func pid=127238)[0m mae:  0.10782521963119507
[2m[36m(func pid=127238)[0m rmse_per_class: [0.095, 0.233, 0.047, 0.297, 0.064, 0.166, 0.252, 0.121, 0.137, 0.099]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17923377454280853
[2m[36m(func pid=136744)[0m mae:  0.13157960772514343
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4257 | Steps: 2 | Val loss: 0.3277 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2594 | Steps: 2 | Val loss: 0.2777 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 07:21:37 (running for 00:30:09.39)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.426 |  0.173 |                   73 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.328 |  0.151 |                   64 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.268 |  0.15  |                   50 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.793 |  0.179 |                   23 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17329402267932892
[2m[36m(func pid=124933)[0m mae:  0.12681081891059875
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.255, 0.084, 0.328, 0.089, 0.188, 0.286, 0.14, 0.142, 0.107]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3276 | Steps: 2 | Val loss: 0.2768 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=130296)[0m rmse: 0.14945325255393982
[2m[36m(func pid=130296)[0m mae:  0.09473328292369843
[2m[36m(func pid=130296)[0m rmse_per_class: [0.075, 0.262, 0.025, 0.302, 0.066, 0.151, 0.218, 0.12, 0.139, 0.136]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7839 | Steps: 2 | Val loss: 0.6085 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=136744)[0m rmse: 0.17922194302082062
[2m[36m(func pid=136744)[0m mae:  0.13156840205192566
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.1511133909225464
[2m[36m(func pid=127238)[0m mae:  0.10771995782852173
[2m[36m(func pid=127238)[0m rmse_per_class: [0.094, 0.234, 0.047, 0.297, 0.064, 0.165, 0.252, 0.121, 0.137, 0.1]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4268 | Steps: 2 | Val loss: 0.3266 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2756 | Steps: 2 | Val loss: 0.2771 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 07:21:43 (running for 00:30:14.73)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15299999713897705
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00013 | RUNNING    | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.427 |  0.173 |                   74 |
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.328 |  0.151 |                   65 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.259 |  0.149 |                   51 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.784 |  0.179 |                   24 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=124933)[0m rmse: 0.17313210666179657
[2m[36m(func pid=124933)[0m mae:  0.12667879462242126
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.254, 0.083, 0.328, 0.088, 0.188, 0.286, 0.139, 0.142, 0.107]
[2m[36m(func pid=124933)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7752 | Steps: 2 | Val loss: 0.6018 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3277 | Steps: 2 | Val loss: 0.2767 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=130296)[0m rmse: 0.14911144971847534
[2m[36m(func pid=130296)[0m mae:  0.09433087706565857
[2m[36m(func pid=130296)[0m rmse_per_class: [0.075, 0.262, 0.025, 0.3, 0.066, 0.15, 0.219, 0.121, 0.136, 0.136]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.1791532337665558
[2m[36m(func pid=136744)[0m mae:  0.13150767982006073
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.15102866291999817
[2m[36m(func pid=127238)[0m mae:  0.10760059207677841
[2m[36m(func pid=127238)[0m rmse_per_class: [0.094, 0.233, 0.047, 0.296, 0.064, 0.165, 0.252, 0.121, 0.137, 0.101]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=124933)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4241 | Steps: 2 | Val loss: 0.3262 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2582 | Steps: 2 | Val loss: 0.2764 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=124933)[0m rmse: 0.1731266975402832
[2m[36m(func pid=124933)[0m mae:  0.12667545676231384
[2m[36m(func pid=124933)[0m rmse_per_class: [0.116, 0.254, 0.083, 0.328, 0.088, 0.188, 0.286, 0.139, 0.142, 0.107]
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7676 | Steps: 2 | Val loss: 0.5943 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 07:21:48 (running for 00:30:20.04)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15799999609589577
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 3 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.328 |  0.151 |                   66 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.276 |  0.149 |                   52 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.775 |  0.179 |                   25 |
| train_ccef6_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3262 | Steps: 2 | Val loss: 0.2765 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=130296)[0m rmse: 0.1486809104681015
[2m[36m(func pid=130296)[0m mae:  0.09392571449279785
[2m[36m(func pid=130296)[0m rmse_per_class: [0.075, 0.26, 0.025, 0.298, 0.064, 0.15, 0.219, 0.123, 0.135, 0.139]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17908641695976257
[2m[36m(func pid=136744)[0m mae:  0.13145066797733307
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.1508583128452301
[2m[36m(func pid=127238)[0m mae:  0.10742312669754028
[2m[36m(func pid=127238)[0m rmse_per_class: [0.094, 0.233, 0.047, 0.296, 0.064, 0.165, 0.252, 0.121, 0.137, 0.1]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2435 | Steps: 2 | Val loss: 0.2765 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7596 | Steps: 2 | Val loss: 0.5874 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3239 | Steps: 2 | Val loss: 0.2763 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=130296)[0m rmse: 0.14904771745204926
[2m[36m(func pid=130296)[0m mae:  0.0941154733300209
[2m[36m(func pid=130296)[0m rmse_per_class: [0.077, 0.26, 0.025, 0.297, 0.064, 0.149, 0.221, 0.123, 0.134, 0.141]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.1790046989917755
[2m[36m(func pid=136744)[0m mae:  0.1313784122467041
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=127238)[0m rmse: 0.15073922276496887
[2m[36m(func pid=127238)[0m mae:  0.10732521861791611
[2m[36m(func pid=127238)[0m rmse_per_class: [0.094, 0.233, 0.046, 0.296, 0.064, 0.165, 0.252, 0.121, 0.137, 0.1]
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2509 | Steps: 2 | Val loss: 0.2767 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 07:21:53 (running for 00:30:25.48)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15799999609589577
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.326 |  0.151 |                   67 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.244 |  0.149 |                   54 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.768 |  0.179 |                   26 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=142739)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=142739)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=142739)[0m Configuration completed!
[2m[36m(func pid=142739)[0m New optimizer parameters:
[2m[36m(func pid=142739)[0m SGD (
[2m[36m(func pid=142739)[0m Parameter Group 0
[2m[36m(func pid=142739)[0m     dampening: 0
[2m[36m(func pid=142739)[0m     differentiable: False
[2m[36m(func pid=142739)[0m     foreach: None
[2m[36m(func pid=142739)[0m     lr: 0.001
[2m[36m(func pid=142739)[0m     maximize: False
[2m[36m(func pid=142739)[0m     momentum: 0.99
[2m[36m(func pid=142739)[0m     nesterov: False
[2m[36m(func pid=142739)[0m     weight_decay: 1e-05
[2m[36m(func pid=142739)[0m )
[2m[36m(func pid=142739)[0m 
== Status ==
Current time: 2024-01-07 07:21:59 (running for 00:30:30.84)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15799999609589577
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.324 |  0.151 |                   68 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.251 |  0.15  |                   55 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.76  |  0.179 |                   27 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.149526447057724
[2m[36m(func pid=130296)[0m mae:  0.09425616264343262
[2m[36m(func pid=130296)[0m rmse_per_class: [0.077, 0.26, 0.025, 0.296, 0.065, 0.149, 0.222, 0.121, 0.134, 0.145]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3285 | Steps: 2 | Val loss: 0.2759 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.7515 | Steps: 2 | Val loss: 0.5812 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8954 | Steps: 2 | Val loss: 0.7075 | Batch size: 32 | lr: 0.001 | Duration: 4.47s
[2m[36m(func pid=127238)[0m rmse: 0.15042567253112793
[2m[36m(func pid=127238)[0m mae:  0.10710601508617401
[2m[36m(func pid=127238)[0m rmse_per_class: [0.094, 0.234, 0.045, 0.295, 0.064, 0.164, 0.251, 0.121, 0.137, 0.099]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2651 | Steps: 2 | Val loss: 0.2765 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=136744)[0m rmse: 0.1789528876543045
[2m[36m(func pid=136744)[0m mae:  0.13133274018764496
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.18274995684623718
[2m[36m(func pid=142739)[0m mae:  0.13447140157222748
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=142739)[0m 
== Status ==
Current time: 2024-01-07 07:22:04 (running for 00:30:36.16)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15799999609589577
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.329 |  0.15  |                   69 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.265 |  0.15  |                   56 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.751 |  0.179 |                   28 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.895 |  0.183 |                    1 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.1497698724269867
[2m[36m(func pid=130296)[0m mae:  0.09432616084814072
[2m[36m(func pid=130296)[0m rmse_per_class: [0.078, 0.258, 0.025, 0.294, 0.065, 0.15, 0.223, 0.121, 0.136, 0.147]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3225 | Steps: 2 | Val loss: 0.2756 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7422 | Steps: 2 | Val loss: 0.5740 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8903 | Steps: 2 | Val loss: 0.6975 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=127238)[0m rmse: 0.15025211870670319
[2m[36m(func pid=127238)[0m mae:  0.10688664019107819
[2m[36m(func pid=127238)[0m rmse_per_class: [0.093, 0.234, 0.045, 0.295, 0.064, 0.164, 0.251, 0.12, 0.137, 0.1]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2594 | Steps: 2 | Val loss: 0.2770 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=136744)[0m rmse: 0.17891588807106018
[2m[36m(func pid=136744)[0m mae:  0.1312946379184723
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.18249347805976868
[2m[36m(func pid=142739)[0m mae:  0.13431911170482635
[2m[36m(func pid=142739)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.113]
[2m[36m(func pid=142739)[0m 
== Status ==
Current time: 2024-01-07 07:22:10 (running for 00:30:41.80)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15799999609589577
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.322 |  0.15  |                   70 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.259 |  0.15  |                   57 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.742 |  0.179 |                   29 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.89  |  0.182 |                    2 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.15034142136573792
[2m[36m(func pid=130296)[0m mae:  0.09449127316474915
[2m[36m(func pid=130296)[0m rmse_per_class: [0.078, 0.258, 0.025, 0.294, 0.068, 0.151, 0.223, 0.119, 0.138, 0.15]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3267 | Steps: 2 | Val loss: 0.2755 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.7323 | Steps: 2 | Val loss: 0.5670 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8835 | Steps: 2 | Val loss: 0.6856 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=127238)[0m rmse: 0.15010538697242737
[2m[36m(func pid=127238)[0m mae:  0.10668861865997314
[2m[36m(func pid=127238)[0m rmse_per_class: [0.093, 0.233, 0.045, 0.295, 0.064, 0.164, 0.251, 0.12, 0.137, 0.1]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17889335751533508
[2m[36m(func pid=136744)[0m mae:  0.1312752217054367
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2493 | Steps: 2 | Val loss: 0.2776 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=142739)[0m rmse: 0.18227246403694153
[2m[36m(func pid=142739)[0m mae:  0.13416323065757751
[2m[36m(func pid=142739)[0m rmse_per_class: [0.117, 0.266, 0.105, 0.339, 0.113, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=142739)[0m 
== Status ==
Current time: 2024-01-07 07:22:15 (running for 00:30:47.14)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15799999609589577
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.327 |  0.15  |                   71 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.249 |  0.151 |                   58 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.732 |  0.179 |                   30 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.884 |  0.182 |                    3 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3281 | Steps: 2 | Val loss: 0.2755 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=130296)[0m rmse: 0.15051427483558655
[2m[36m(func pid=130296)[0m mae:  0.09439441561698914
[2m[36m(func pid=130296)[0m rmse_per_class: [0.077, 0.258, 0.025, 0.294, 0.068, 0.15, 0.221, 0.119, 0.139, 0.153]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.7254 | Steps: 2 | Val loss: 0.5600 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8653 | Steps: 2 | Val loss: 0.6706 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=127238)[0m rmse: 0.15011347830295563
[2m[36m(func pid=127238)[0m mae:  0.10673798620700836
[2m[36m(func pid=127238)[0m rmse_per_class: [0.093, 0.234, 0.044, 0.296, 0.065, 0.163, 0.25, 0.12, 0.137, 0.099]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.1788657009601593
[2m[36m(func pid=136744)[0m mae:  0.1312599629163742
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2547 | Steps: 2 | Val loss: 0.2787 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=142739)[0m rmse: 0.18186518549919128
[2m[36m(func pid=142739)[0m mae:  0.1338406503200531
[2m[36m(func pid=142739)[0m rmse_per_class: [0.117, 0.266, 0.104, 0.339, 0.113, 0.19, 0.294, 0.142, 0.143, 0.112]
[2m[36m(func pid=142739)[0m 
== Status ==
Current time: 2024-01-07 07:22:20 (running for 00:30:52.34)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15799999609589577
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.328 |  0.15  |                   72 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.151 |                   59 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.725 |  0.179 |                   31 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.865 |  0.182 |                    4 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.15086135268211365
[2m[36m(func pid=130296)[0m mae:  0.09463232010602951
[2m[36m(func pid=130296)[0m rmse_per_class: [0.076, 0.26, 0.025, 0.297, 0.069, 0.151, 0.221, 0.118, 0.141, 0.151]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3246 | Steps: 2 | Val loss: 0.2755 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7182 | Steps: 2 | Val loss: 0.5534 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8469 | Steps: 2 | Val loss: 0.6528 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=136744)[0m rmse: 0.17886021733283997
[2m[36m(func pid=136744)[0m mae:  0.13125455379486084
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.259, 0.097, 0.336, 0.108, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.1501150280237198
[2m[36m(func pid=127238)[0m mae:  0.1066819578409195
[2m[36m(func pid=127238)[0m rmse_per_class: [0.093, 0.234, 0.044, 0.296, 0.064, 0.163, 0.25, 0.12, 0.137, 0.099]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2529 | Steps: 2 | Val loss: 0.2794 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=142739)[0m rmse: 0.18142272531986237
[2m[36m(func pid=142739)[0m mae:  0.13347549736499786
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=142739)[0m 
== Status ==
Current time: 2024-01-07 07:22:25 (running for 00:30:57.56)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15799999609589577
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.325 |  0.15  |                   73 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.253 |  0.151 |                   60 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.718 |  0.179 |                   32 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.847 |  0.181 |                    5 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.15119224786758423
[2m[36m(func pid=130296)[0m mae:  0.0946667343378067
[2m[36m(func pid=130296)[0m rmse_per_class: [0.075, 0.261, 0.025, 0.298, 0.071, 0.151, 0.221, 0.118, 0.14, 0.152]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3163 | Steps: 2 | Val loss: 0.2755 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7091 | Steps: 2 | Val loss: 0.5464 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8257 | Steps: 2 | Val loss: 0.6339 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=127238)[0m rmse: 0.1501050591468811
[2m[36m(func pid=127238)[0m mae:  0.10662500560283661
[2m[36m(func pid=127238)[0m rmse_per_class: [0.093, 0.234, 0.044, 0.296, 0.064, 0.164, 0.25, 0.12, 0.137, 0.1]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17881156504154205
[2m[36m(func pid=136744)[0m mae:  0.13123160600662231
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.259, 0.097, 0.336, 0.107, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2586 | Steps: 2 | Val loss: 0.2799 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=142739)[0m rmse: 0.18105360865592957
[2m[36m(func pid=142739)[0m mae:  0.1331758201122284
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.265, 0.102, 0.338, 0.111, 0.189, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=142739)[0m 
== Status ==
Current time: 2024-01-07 07:22:31 (running for 00:31:02.73)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15799999609589577
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.316 |  0.15  |                   74 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.259 |  0.151 |                   61 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.709 |  0.179 |                   33 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.826 |  0.181 |                    6 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.1514493077993393
[2m[36m(func pid=130296)[0m mae:  0.09473024308681488
[2m[36m(func pid=130296)[0m rmse_per_class: [0.076, 0.261, 0.025, 0.299, 0.072, 0.151, 0.22, 0.118, 0.141, 0.152]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.7007 | Steps: 2 | Val loss: 0.5394 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3185 | Steps: 2 | Val loss: 0.2755 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8002 | Steps: 2 | Val loss: 0.6126 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2512 | Steps: 2 | Val loss: 0.2806 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=136744)[0m rmse: 0.17879244685173035
[2m[36m(func pid=136744)[0m mae:  0.13121207058429718
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.259, 0.097, 0.336, 0.107, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.15010055899620056
[2m[36m(func pid=127238)[0m mae:  0.10663507878780365
[2m[36m(func pid=127238)[0m rmse_per_class: [0.093, 0.234, 0.044, 0.296, 0.064, 0.164, 0.249, 0.12, 0.138, 0.1]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.18062123656272888
[2m[36m(func pid=142739)[0m mae:  0.13280972838401794
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.111, 0.189, 0.293, 0.141, 0.142, 0.11]
[2m[36m(func pid=142739)[0m 
== Status ==
Current time: 2024-01-07 07:22:36 (running for 00:31:08.12)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.319 |  0.15  |                   75 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.251 |  0.152 |                   62 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.701 |  0.179 |                   34 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.8   |  0.181 |                    7 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.15178093314170837
[2m[36m(func pid=130296)[0m mae:  0.09470729529857635
[2m[36m(func pid=130296)[0m rmse_per_class: [0.076, 0.262, 0.025, 0.301, 0.074, 0.151, 0.219, 0.118, 0.139, 0.153]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3214 | Steps: 2 | Val loss: 0.2752 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.6904 | Steps: 2 | Val loss: 0.5323 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7715 | Steps: 2 | Val loss: 0.5892 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=127238)[0m rmse: 0.14996036887168884
[2m[36m(func pid=127238)[0m mae:  0.10641535371541977
[2m[36m(func pid=127238)[0m rmse_per_class: [0.093, 0.234, 0.044, 0.295, 0.064, 0.164, 0.249, 0.119, 0.138, 0.101]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2592 | Steps: 2 | Val loss: 0.2807 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=136744)[0m rmse: 0.17870892584323883
[2m[36m(func pid=136744)[0m mae:  0.13114464282989502
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.259, 0.097, 0.336, 0.107, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.18027248978614807
[2m[36m(func pid=142739)[0m mae:  0.13250824809074402
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.264, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.11]
[2m[36m(func pid=142739)[0m 
== Status ==
Current time: 2024-01-07 07:22:42 (running for 00:31:13.67)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.321 |  0.15  |                   76 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.259 |  0.152 |                   63 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.69  |  0.179 |                   35 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.771 |  0.18  |                    8 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3158 | Steps: 2 | Val loss: 0.2753 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=130296)[0m rmse: 0.15201526880264282
[2m[36m(func pid=130296)[0m mae:  0.09480719268321991
[2m[36m(func pid=130296)[0m rmse_per_class: [0.078, 0.262, 0.025, 0.301, 0.076, 0.151, 0.219, 0.118, 0.139, 0.153]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6839 | Steps: 2 | Val loss: 0.5253 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7368 | Steps: 2 | Val loss: 0.5652 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=127238)[0m rmse: 0.15005408227443695
[2m[36m(func pid=127238)[0m mae:  0.10647950321435928
[2m[36m(func pid=127238)[0m rmse_per_class: [0.093, 0.234, 0.044, 0.295, 0.064, 0.164, 0.249, 0.119, 0.138, 0.101]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17863768339157104
[2m[36m(func pid=136744)[0m mae:  0.1310703307390213
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.259, 0.097, 0.336, 0.107, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2481 | Steps: 2 | Val loss: 0.2805 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=142739)[0m rmse: 0.17994673550128937
[2m[36m(func pid=142739)[0m mae:  0.13221551477909088
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.263, 0.1, 0.337, 0.109, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3260 | Steps: 2 | Val loss: 0.2751 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 07:22:47 (running for 00:31:19.07)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.316 |  0.15  |                   77 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.248 |  0.152 |                   64 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.684 |  0.179 |                   36 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.737 |  0.18  |                    9 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.1518634408712387
[2m[36m(func pid=130296)[0m mae:  0.09470205754041672
[2m[36m(func pid=130296)[0m rmse_per_class: [0.075, 0.262, 0.025, 0.3, 0.078, 0.15, 0.219, 0.118, 0.138, 0.152]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6763 | Steps: 2 | Val loss: 0.5188 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7101 | Steps: 2 | Val loss: 0.5394 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=127238)[0m rmse: 0.14990770816802979
[2m[36m(func pid=127238)[0m mae:  0.10629071295261383
[2m[36m(func pid=127238)[0m rmse_per_class: [0.092, 0.234, 0.043, 0.295, 0.064, 0.164, 0.248, 0.119, 0.138, 0.102]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17861638963222504
[2m[36m(func pid=136744)[0m mae:  0.13105365633964539
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.259, 0.097, 0.336, 0.106, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2506 | Steps: 2 | Val loss: 0.2809 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=142739)[0m rmse: 0.17958250641822815
[2m[36m(func pid=142739)[0m mae:  0.13189060986042023
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.262, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.142, 0.109]
[2m[36m(func pid=142739)[0m 
== Status ==
Current time: 2024-01-07 07:22:52 (running for 00:31:24.18)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.326 |  0.15  |                   78 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.251 |  0.152 |                   65 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.676 |  0.179 |                   37 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.71  |  0.18  |                   10 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3186 | Steps: 2 | Val loss: 0.2752 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=130296)[0m rmse: 0.15204626321792603
[2m[36m(func pid=130296)[0m mae:  0.09467753022909164
[2m[36m(func pid=130296)[0m rmse_per_class: [0.074, 0.262, 0.025, 0.3, 0.078, 0.151, 0.22, 0.119, 0.137, 0.154]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6655 | Steps: 2 | Val loss: 0.5121 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6797 | Steps: 2 | Val loss: 0.5138 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=127238)[0m rmse: 0.1499791443347931
[2m[36m(func pid=127238)[0m mae:  0.10632489621639252
[2m[36m(func pid=127238)[0m rmse_per_class: [0.092, 0.234, 0.043, 0.296, 0.064, 0.164, 0.248, 0.119, 0.138, 0.102]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2491 | Steps: 2 | Val loss: 0.2810 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=136744)[0m rmse: 0.17856621742248535
[2m[36m(func pid=136744)[0m mae:  0.1310146003961563
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.259, 0.096, 0.336, 0.106, 0.19, 0.292, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.1792244166135788
[2m[36m(func pid=142739)[0m mae:  0.13159014284610748
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.262, 0.098, 0.336, 0.107, 0.19, 0.292, 0.141, 0.142, 0.109]
[2m[36m(func pid=142739)[0m 
== Status ==
Current time: 2024-01-07 07:22:57 (running for 00:31:29.40)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.319 |  0.15  |                   79 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.249 |  0.152 |                   66 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.666 |  0.179 |                   38 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.68  |  0.179 |                   11 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.1521109938621521
[2m[36m(func pid=130296)[0m mae:  0.09469879418611526
[2m[36m(func pid=130296)[0m rmse_per_class: [0.075, 0.263, 0.025, 0.301, 0.077, 0.151, 0.22, 0.12, 0.137, 0.153]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3196 | Steps: 2 | Val loss: 0.2750 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6551 | Steps: 2 | Val loss: 0.5062 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.6490 | Steps: 2 | Val loss: 0.4884 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=127238)[0m rmse: 0.1497715562582016
[2m[36m(func pid=127238)[0m mae:  0.10609452426433563
[2m[36m(func pid=127238)[0m rmse_per_class: [0.092, 0.234, 0.042, 0.296, 0.064, 0.163, 0.247, 0.119, 0.138, 0.102]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.1784919947385788
[2m[36m(func pid=136744)[0m mae:  0.13095912337303162
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.259, 0.096, 0.335, 0.105, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2565 | Steps: 2 | Val loss: 0.2814 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=142739)[0m rmse: 0.17887218296527863
[2m[36m(func pid=142739)[0m mae:  0.13130183517932892
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.261, 0.097, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=142739)[0m 
== Status ==
Current time: 2024-01-07 07:23:02 (running for 00:31:34.55)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.32  |  0.15  |                   80 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.257 |  0.152 |                   67 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.655 |  0.178 |                   39 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.649 |  0.179 |                   12 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3153 | Steps: 2 | Val loss: 0.2744 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=130296)[0m rmse: 0.1522899866104126
[2m[36m(func pid=130296)[0m mae:  0.09479966014623642
[2m[36m(func pid=130296)[0m rmse_per_class: [0.075, 0.263, 0.025, 0.3, 0.075, 0.151, 0.221, 0.121, 0.138, 0.153]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6502 | Steps: 2 | Val loss: 0.5001 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.6172 | Steps: 2 | Val loss: 0.4642 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=127238)[0m rmse: 0.1494428962469101
[2m[36m(func pid=127238)[0m mae:  0.10576467216014862
[2m[36m(func pid=127238)[0m rmse_per_class: [0.092, 0.234, 0.042, 0.295, 0.063, 0.163, 0.247, 0.119, 0.138, 0.102]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.1784483641386032
[2m[36m(func pid=136744)[0m mae:  0.13092854619026184
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.259, 0.096, 0.335, 0.105, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2492 | Steps: 2 | Val loss: 0.2820 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=142739)[0m rmse: 0.17849516868591309
[2m[36m(func pid=142739)[0m mae:  0.13099801540374756
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.261, 0.096, 0.335, 0.105, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3225 | Steps: 2 | Val loss: 0.2745 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 07:23:08 (running for 00:31:39.93)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.149 |                   81 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.249 |  0.153 |                   68 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.65  |  0.178 |                   40 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.617 |  0.178 |                   13 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.15295937657356262
[2m[36m(func pid=130296)[0m mae:  0.09522800147533417
[2m[36m(func pid=130296)[0m rmse_per_class: [0.077, 0.263, 0.025, 0.3, 0.075, 0.151, 0.222, 0.122, 0.139, 0.154]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6415 | Steps: 2 | Val loss: 0.4936 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5871 | Steps: 2 | Val loss: 0.4414 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=127238)[0m rmse: 0.14945781230926514
[2m[36m(func pid=127238)[0m mae:  0.10581084340810776
[2m[36m(func pid=127238)[0m rmse_per_class: [0.092, 0.235, 0.042, 0.295, 0.064, 0.163, 0.246, 0.119, 0.138, 0.101]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17837730050086975
[2m[36m(func pid=136744)[0m mae:  0.1308724582195282
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.259, 0.096, 0.335, 0.105, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2579 | Steps: 2 | Val loss: 0.2818 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=142739)[0m rmse: 0.178226038813591
[2m[36m(func pid=142739)[0m mae:  0.1307755410671234
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.335, 0.103, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3179 | Steps: 2 | Val loss: 0.2739 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 07:23:13 (running for 00:31:45.29)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.322 |  0.149 |                   82 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.258 |  0.153 |                   69 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.641 |  0.178 |                   41 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.587 |  0.178 |                   14 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.15290041267871857
[2m[36m(func pid=130296)[0m mae:  0.09537848085165024
[2m[36m(func pid=130296)[0m rmse_per_class: [0.078, 0.263, 0.025, 0.299, 0.074, 0.151, 0.223, 0.122, 0.141, 0.152]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6343 | Steps: 2 | Val loss: 0.4871 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5609 | Steps: 2 | Val loss: 0.4195 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=127238)[0m rmse: 0.1490592658519745
[2m[36m(func pid=127238)[0m mae:  0.10542216151952744
[2m[36m(func pid=127238)[0m rmse_per_class: [0.091, 0.234, 0.041, 0.293, 0.063, 0.163, 0.246, 0.119, 0.138, 0.101]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17826496064662933
[2m[36m(func pid=136744)[0m mae:  0.13078708946704865
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.259, 0.096, 0.335, 0.105, 0.19, 0.292, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2547 | Steps: 2 | Val loss: 0.2820 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=142739)[0m rmse: 0.17783410847187042
[2m[36m(func pid=142739)[0m mae:  0.13044944405555725
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.334, 0.102, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3170 | Steps: 2 | Val loss: 0.2733 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6280 | Steps: 2 | Val loss: 0.4807 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 07:23:19 (running for 00:31:50.68)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.318 |  0.149 |                   83 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.153 |                   70 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.634 |  0.178 |                   42 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.561 |  0.178 |                   15 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.1530102789402008
[2m[36m(func pid=130296)[0m mae:  0.09550650417804718
[2m[36m(func pid=130296)[0m rmse_per_class: [0.079, 0.263, 0.025, 0.299, 0.073, 0.151, 0.223, 0.122, 0.142, 0.153]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5373 | Steps: 2 | Val loss: 0.4002 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=127238)[0m rmse: 0.14872410893440247
[2m[36m(func pid=127238)[0m mae:  0.1050591692328453
[2m[36m(func pid=127238)[0m rmse_per_class: [0.09, 0.234, 0.041, 0.292, 0.064, 0.163, 0.246, 0.118, 0.138, 0.101]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17817659676074982
[2m[36m(func pid=136744)[0m mae:  0.13070344924926758
[2m[36m(func pid=136744)[0m rmse_per_class: [0.115, 0.259, 0.096, 0.335, 0.104, 0.19, 0.292, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2572 | Steps: 2 | Val loss: 0.2816 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=142739)[0m rmse: 0.17746391892433167
[2m[36m(func pid=142739)[0m mae:  0.13016048073768616
[2m[36m(func pid=142739)[0m rmse_per_class: [0.117, 0.259, 0.094, 0.334, 0.1, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3132 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.6189 | Steps: 2 | Val loss: 0.4750 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=130296)[0m rmse: 0.1526697725057602
[2m[36m(func pid=130296)[0m mae:  0.09528942406177521
[2m[36m(func pid=130296)[0m rmse_per_class: [0.079, 0.262, 0.025, 0.299, 0.071, 0.151, 0.222, 0.121, 0.142, 0.154]
== Status ==
Current time: 2024-01-07 07:23:24 (running for 00:31:55.95)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.317 |  0.149 |                   84 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.257 |  0.153 |                   71 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.628 |  0.178 |                   43 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.537 |  0.177 |                   16 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5139 | Steps: 2 | Val loss: 0.3828 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=127238)[0m rmse: 0.1484670490026474
[2m[36m(func pid=127238)[0m mae:  0.10475729405879974
[2m[36m(func pid=127238)[0m rmse_per_class: [0.09, 0.234, 0.041, 0.292, 0.064, 0.162, 0.245, 0.118, 0.138, 0.101]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17813995480537415
[2m[36m(func pid=136744)[0m mae:  0.1306893527507782
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.104, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2550 | Steps: 2 | Val loss: 0.2815 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=142739)[0m rmse: 0.17707285284996033
[2m[36m(func pid=142739)[0m mae:  0.12984509766101837
[2m[36m(func pid=142739)[0m rmse_per_class: [0.117, 0.259, 0.093, 0.333, 0.098, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3148 | Steps: 2 | Val loss: 0.2727 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6122 | Steps: 2 | Val loss: 0.4687 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 07:23:29 (running for 00:32:01.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.313 |  0.148 |                   85 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   72 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.619 |  0.178 |                   44 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.514 |  0.177 |                   17 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.15239934623241425
[2m[36m(func pid=130296)[0m mae:  0.09505709260702133
[2m[36m(func pid=130296)[0m rmse_per_class: [0.078, 0.264, 0.026, 0.299, 0.07, 0.152, 0.222, 0.12, 0.141, 0.152]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4919 | Steps: 2 | Val loss: 0.3680 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=127238)[0m rmse: 0.14835304021835327
[2m[36m(func pid=127238)[0m mae:  0.10460008680820465
[2m[36m(func pid=127238)[0m rmse_per_class: [0.089, 0.234, 0.04, 0.291, 0.064, 0.163, 0.245, 0.118, 0.138, 0.101]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17803716659545898
[2m[36m(func pid=136744)[0m mae:  0.13060788810253143
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.104, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2518 | Steps: 2 | Val loss: 0.2807 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=142739)[0m rmse: 0.17671851813793182
[2m[36m(func pid=142739)[0m mae:  0.12955275177955627
[2m[36m(func pid=142739)[0m rmse_per_class: [0.117, 0.258, 0.092, 0.333, 0.096, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3165 | Steps: 2 | Val loss: 0.2723 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.6039 | Steps: 2 | Val loss: 0.4632 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 07:23:34 (running for 00:32:06.21)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.148 |                   86 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.252 |  0.152 |                   73 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.612 |  0.178 |                   45 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.492 |  0.177 |                   18 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.15178948640823364
[2m[36m(func pid=130296)[0m mae:  0.09456817060709
[2m[36m(func pid=130296)[0m rmse_per_class: [0.078, 0.263, 0.026, 0.299, 0.069, 0.152, 0.221, 0.119, 0.14, 0.152]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4725 | Steps: 2 | Val loss: 0.3552 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=127238)[0m rmse: 0.14817143976688385
[2m[36m(func pid=127238)[0m mae:  0.10438831895589828
[2m[36m(func pid=127238)[0m rmse_per_class: [0.089, 0.234, 0.04, 0.29, 0.064, 0.162, 0.245, 0.118, 0.137, 0.101]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17798882722854614
[2m[36m(func pid=136744)[0m mae:  0.13056179881095886
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.104, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2497 | Steps: 2 | Val loss: 0.2809 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=142739)[0m rmse: 0.17642126977443695
[2m[36m(func pid=142739)[0m mae:  0.12931717932224274
[2m[36m(func pid=142739)[0m rmse_per_class: [0.117, 0.258, 0.091, 0.332, 0.094, 0.19, 0.29, 0.141, 0.142, 0.109]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3142 | Steps: 2 | Val loss: 0.2723 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5980 | Steps: 2 | Val loss: 0.4576 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 07:23:39 (running for 00:32:11.49)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15150000154972076
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.317 |  0.148 |                   87 |
| train_ccef6_00015 | RUNNING    | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.25  |  0.152 |                   74 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.604 |  0.178 |                   46 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.473 |  0.176 |                   19 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=130296)[0m rmse: 0.15173804759979248
[2m[36m(func pid=130296)[0m mae:  0.09442323446273804
[2m[36m(func pid=130296)[0m rmse_per_class: [0.077, 0.263, 0.026, 0.299, 0.069, 0.152, 0.219, 0.119, 0.14, 0.153]
[2m[36m(func pid=130296)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4591 | Steps: 2 | Val loss: 0.3444 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=127238)[0m rmse: 0.1481964886188507
[2m[36m(func pid=127238)[0m mae:  0.10430464893579483
[2m[36m(func pid=127238)[0m rmse_per_class: [0.089, 0.234, 0.04, 0.29, 0.065, 0.162, 0.245, 0.118, 0.137, 0.103]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17784395813941956
[2m[36m(func pid=136744)[0m mae:  0.13045074045658112
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.334, 0.103, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=130296)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2551 | Steps: 2 | Val loss: 0.2811 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=142739)[0m rmse: 0.17603404819965363
[2m[36m(func pid=142739)[0m mae:  0.12900885939598083
[2m[36m(func pid=142739)[0m rmse_per_class: [0.118, 0.258, 0.09, 0.332, 0.093, 0.189, 0.289, 0.141, 0.142, 0.109]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3121 | Steps: 2 | Val loss: 0.2720 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5905 | Steps: 2 | Val loss: 0.4521 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=130296)[0m rmse: 0.15188562870025635
[2m[36m(func pid=130296)[0m mae:  0.09446213394403458
[2m[36m(func pid=130296)[0m rmse_per_class: [0.076, 0.263, 0.026, 0.299, 0.069, 0.152, 0.219, 0.119, 0.14, 0.156]
== Status ==
Current time: 2024-01-07 07:23:45 (running for 00:32:16.67)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 3 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.314 |  0.148 |                   88 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.598 |  0.178 |                   47 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.459 |  0.176 |                   20 |
| train_ccef6_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4437 | Steps: 2 | Val loss: 0.3350 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=136744)[0m rmse: 0.17789286375045776
[2m[36m(func pid=136744)[0m mae:  0.130478173494339
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.334, 0.103, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.1480480134487152
[2m[36m(func pid=127238)[0m mae:  0.10413482040166855
[2m[36m(func pid=127238)[0m rmse_per_class: [0.089, 0.234, 0.04, 0.289, 0.065, 0.162, 0.245, 0.118, 0.137, 0.103]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.17558011412620544
[2m[36m(func pid=142739)[0m mae:  0.12863701581954956
[2m[36m(func pid=142739)[0m rmse_per_class: [0.118, 0.257, 0.089, 0.331, 0.091, 0.189, 0.289, 0.141, 0.142, 0.109]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3109 | Steps: 2 | Val loss: 0.2718 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5832 | Steps: 2 | Val loss: 0.4464 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4320 | Steps: 2 | Val loss: 0.3276 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=127238)[0m rmse: 0.14791615307331085
[2m[36m(func pid=127238)[0m mae:  0.10399889945983887
[2m[36m(func pid=127238)[0m rmse_per_class: [0.089, 0.234, 0.039, 0.289, 0.065, 0.161, 0.245, 0.118, 0.137, 0.102]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.1778169721364975
[2m[36m(func pid=136744)[0m mae:  0.13042446970939636
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.334, 0.103, 0.19, 0.292, 0.141, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 07:23:52 (running for 00:32:23.79)
Memory usage on this node: 23.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.311 |  0.148 |                   90 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.59  |  0.178 |                   48 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.444 |  0.176 |                   21 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=147638)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=147638)[0m Configuration completed!
[2m[36m(func pid=147638)[0m New optimizer parameters:
[2m[36m(func pid=147638)[0m SGD (
[2m[36m(func pid=147638)[0m Parameter Group 0
[2m[36m(func pid=147638)[0m     dampening: 0
[2m[36m(func pid=147638)[0m     differentiable: False
[2m[36m(func pid=147638)[0m     foreach: None
[2m[36m(func pid=147638)[0m     lr: 0.01
[2m[36m(func pid=147638)[0m     maximize: False
[2m[36m(func pid=147638)[0m     momentum: 0.99
[2m[36m(func pid=147638)[0m     nesterov: False
[2m[36m(func pid=147638)[0m     weight_decay: 1e-05
[2m[36m(func pid=147638)[0m )
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.17515964806079865
[2m[36m(func pid=142739)[0m mae:  0.12830626964569092
[2m[36m(func pid=142739)[0m rmse_per_class: [0.118, 0.257, 0.088, 0.331, 0.089, 0.189, 0.288, 0.14, 0.142, 0.109]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3171 | Steps: 2 | Val loss: 0.2717 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5759 | Steps: 2 | Val loss: 0.4408 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4233 | Steps: 2 | Val loss: 0.3218 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8904 | Steps: 2 | Val loss: 0.6856 | Batch size: 32 | lr: 0.01 | Duration: 4.34s
== Status ==
Current time: 2024-01-07 07:23:57 (running for 00:32:29.17)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.317 |  0.148 |                   91 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.583 |  0.178 |                   49 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.432 |  0.175 |                   22 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.14773741364479065
[2m[36m(func pid=127238)[0m mae:  0.10384795814752579
[2m[36m(func pid=127238)[0m rmse_per_class: [0.089, 0.234, 0.039, 0.29, 0.064, 0.161, 0.244, 0.118, 0.137, 0.102]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17773257195949554
[2m[36m(func pid=136744)[0m mae:  0.13036319613456726
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.334, 0.102, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.17469680309295654
[2m[36m(func pid=142739)[0m mae:  0.12793497741222382
[2m[36m(func pid=142739)[0m rmse_per_class: [0.118, 0.256, 0.087, 0.33, 0.087, 0.189, 0.288, 0.14, 0.143, 0.109]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.1827441155910492
[2m[36m(func pid=147638)[0m mae:  0.13447633385658264
[2m[36m(func pid=147638)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3156 | Steps: 2 | Val loss: 0.2717 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5696 | Steps: 2 | Val loss: 0.4357 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4163 | Steps: 2 | Val loss: 0.3176 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8400 | Steps: 2 | Val loss: 0.6282 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 07:24:02 (running for 00:32:34.32)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.316 |  0.148 |                   92 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.576 |  0.178 |                   50 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.423 |  0.175 |                   23 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.89  |  0.183 |                    1 |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.14776532351970673
[2m[36m(func pid=127238)[0m mae:  0.1038379892706871
[2m[36m(func pid=127238)[0m rmse_per_class: [0.088, 0.234, 0.039, 0.29, 0.065, 0.161, 0.244, 0.118, 0.137, 0.102]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17767268419265747
[2m[36m(func pid=136744)[0m mae:  0.13030342757701874
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.259, 0.093, 0.334, 0.102, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.17428605258464813
[2m[36m(func pid=142739)[0m mae:  0.12760429084300995
[2m[36m(func pid=142739)[0m rmse_per_class: [0.119, 0.255, 0.086, 0.33, 0.086, 0.189, 0.287, 0.14, 0.143, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.18241146206855774
[2m[36m(func pid=147638)[0m mae:  0.13425251841545105
[2m[36m(func pid=147638)[0m rmse_per_class: [0.117, 0.267, 0.107, 0.339, 0.111, 0.19, 0.294, 0.144, 0.143, 0.113]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3079 | Steps: 2 | Val loss: 0.2718 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5629 | Steps: 2 | Val loss: 0.4309 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4098 | Steps: 2 | Val loss: 0.3148 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7573 | Steps: 2 | Val loss: 0.5521 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 07:24:08 (running for 00:32:39.60)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.308 |  0.148 |                   93 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.57  |  0.178 |                   51 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.416 |  0.174 |                   24 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.84  |  0.182 |                    2 |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.14778181910514832
[2m[36m(func pid=127238)[0m mae:  0.10383111238479614
[2m[36m(func pid=127238)[0m rmse_per_class: [0.088, 0.234, 0.039, 0.291, 0.065, 0.161, 0.244, 0.118, 0.137, 0.102]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17759624123573303
[2m[36m(func pid=136744)[0m mae:  0.13023848831653595
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.259, 0.093, 0.334, 0.102, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.17390784621238708
[2m[36m(func pid=142739)[0m mae:  0.1272917091846466
[2m[36m(func pid=142739)[0m rmse_per_class: [0.119, 0.255, 0.085, 0.329, 0.084, 0.189, 0.287, 0.14, 0.143, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.18181869387626648
[2m[36m(func pid=147638)[0m mae:  0.1338120996952057
[2m[36m(func pid=147638)[0m rmse_per_class: [0.118, 0.266, 0.105, 0.338, 0.11, 0.19, 0.293, 0.143, 0.143, 0.112]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3104 | Steps: 2 | Val loss: 0.2717 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5551 | Steps: 2 | Val loss: 0.4261 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4068 | Steps: 2 | Val loss: 0.3133 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6527 | Steps: 2 | Val loss: 0.4726 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
== Status ==
Current time: 2024-01-07 07:24:13 (running for 00:32:44.90)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.31  |  0.148 |                   94 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.563 |  0.178 |                   52 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.41  |  0.174 |                   25 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.757 |  0.182 |                    3 |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.14762356877326965
[2m[36m(func pid=127238)[0m mae:  0.10364939272403717
[2m[36m(func pid=127238)[0m rmse_per_class: [0.087, 0.234, 0.039, 0.292, 0.065, 0.16, 0.243, 0.118, 0.137, 0.101]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17743094265460968
[2m[36m(func pid=136744)[0m mae:  0.1301039308309555
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.334, 0.101, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.1735253781080246
[2m[36m(func pid=142739)[0m mae:  0.1269882321357727
[2m[36m(func pid=142739)[0m rmse_per_class: [0.119, 0.255, 0.084, 0.329, 0.082, 0.189, 0.286, 0.14, 0.143, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.1809801161289215
[2m[36m(func pid=147638)[0m mae:  0.13317391276359558
[2m[36m(func pid=147638)[0m rmse_per_class: [0.118, 0.266, 0.103, 0.337, 0.106, 0.19, 0.292, 0.143, 0.143, 0.112]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3120 | Steps: 2 | Val loss: 0.2714 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5506 | Steps: 2 | Val loss: 0.4211 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4029 | Steps: 2 | Val loss: 0.3124 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5569 | Steps: 2 | Val loss: 0.4027 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 07:24:18 (running for 00:32:50.07)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.312 |  0.147 |                   95 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.555 |  0.177 |                   53 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.407 |  0.174 |                   26 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.653 |  0.181 |                    4 |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.1473684459924698
[2m[36m(func pid=127238)[0m mae:  0.10337238013744354
[2m[36m(func pid=127238)[0m rmse_per_class: [0.087, 0.234, 0.039, 0.291, 0.065, 0.16, 0.242, 0.118, 0.137, 0.102]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.1773466318845749
[2m[36m(func pid=136744)[0m mae:  0.1300349086523056
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.334, 0.101, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.1729736030101776
[2m[36m(func pid=142739)[0m mae:  0.12652628123760223
[2m[36m(func pid=142739)[0m rmse_per_class: [0.119, 0.254, 0.083, 0.328, 0.08, 0.189, 0.285, 0.14, 0.143, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.17967045307159424
[2m[36m(func pid=147638)[0m mae:  0.13216643035411835
[2m[36m(func pid=147638)[0m rmse_per_class: [0.119, 0.265, 0.1, 0.335, 0.101, 0.189, 0.291, 0.142, 0.143, 0.112]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3048 | Steps: 2 | Val loss: 0.2711 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5441 | Steps: 2 | Val loss: 0.4165 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4031 | Steps: 2 | Val loss: 0.3128 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4792 | Steps: 2 | Val loss: 0.3525 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 07:24:23 (running for 00:32:55.20)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.305 |  0.147 |                   96 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.551 |  0.177 |                   54 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.403 |  0.173 |                   27 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.557 |  0.18  |                    5 |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.14726707339286804
[2m[36m(func pid=127238)[0m mae:  0.10320204496383667
[2m[36m(func pid=127238)[0m rmse_per_class: [0.087, 0.234, 0.038, 0.291, 0.064, 0.16, 0.241, 0.118, 0.137, 0.102]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.1772526204586029
[2m[36m(func pid=136744)[0m mae:  0.1299627721309662
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.101, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.1725529432296753
[2m[36m(func pid=142739)[0m mae:  0.1261715441942215
[2m[36m(func pid=142739)[0m rmse_per_class: [0.119, 0.254, 0.082, 0.327, 0.079, 0.189, 0.285, 0.14, 0.143, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.1780458390712738
[2m[36m(func pid=147638)[0m mae:  0.13090083003044128
[2m[36m(func pid=147638)[0m rmse_per_class: [0.12, 0.264, 0.096, 0.333, 0.094, 0.189, 0.289, 0.142, 0.143, 0.111]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3088 | Steps: 2 | Val loss: 0.2710 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5379 | Steps: 2 | Val loss: 0.4118 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4059 | Steps: 2 | Val loss: 0.3139 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4299 | Steps: 2 | Val loss: 0.3241 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 07:24:28 (running for 00:33:00.38)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.309 |  0.147 |                   97 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.544 |  0.177 |                   55 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.403 |  0.173 |                   28 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.479 |  0.178 |                    6 |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.14722105860710144
[2m[36m(func pid=127238)[0m mae:  0.10310457646846771
[2m[36m(func pid=127238)[0m rmse_per_class: [0.087, 0.234, 0.038, 0.291, 0.064, 0.16, 0.241, 0.118, 0.136, 0.102]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17717164754867554
[2m[36m(func pid=136744)[0m mae:  0.12989023327827454
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.1, 0.19, 0.291, 0.141, 0.141, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.1721678376197815
[2m[36m(func pid=142739)[0m mae:  0.12585410475730896
[2m[36m(func pid=142739)[0m rmse_per_class: [0.119, 0.254, 0.081, 0.326, 0.078, 0.188, 0.284, 0.14, 0.142, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.17617136240005493
[2m[36m(func pid=147638)[0m mae:  0.1294167935848236
[2m[36m(func pid=147638)[0m rmse_per_class: [0.12, 0.263, 0.092, 0.33, 0.087, 0.188, 0.287, 0.141, 0.143, 0.11]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3128 | Steps: 2 | Val loss: 0.2707 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5331 | Steps: 2 | Val loss: 0.4073 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4017 | Steps: 2 | Val loss: 0.3155 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4092 | Steps: 2 | Val loss: 0.3142 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 07:24:33 (running for 00:33:05.51)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.309 |  0.147 |                   97 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.533 |  0.177 |                   57 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.406 |  0.172 |                   29 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.43  |  0.176 |                    7 |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136744)[0m rmse: 0.1771257221698761
[2m[36m(func pid=136744)[0m mae:  0.12985512614250183
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.1, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=127238)[0m rmse: 0.14708837866783142
[2m[36m(func pid=127238)[0m mae:  0.10291039943695068
[2m[36m(func pid=127238)[0m rmse_per_class: [0.086, 0.234, 0.038, 0.29, 0.064, 0.16, 0.241, 0.118, 0.136, 0.103]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.17171016335487366
[2m[36m(func pid=142739)[0m mae:  0.12547942996025085
[2m[36m(func pid=142739)[0m rmse_per_class: [0.119, 0.254, 0.08, 0.325, 0.076, 0.188, 0.283, 0.14, 0.142, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.17410126328468323
[2m[36m(func pid=147638)[0m mae:  0.12772826850414276
[2m[36m(func pid=147638)[0m rmse_per_class: [0.12, 0.26, 0.088, 0.326, 0.081, 0.188, 0.285, 0.141, 0.143, 0.11]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5271 | Steps: 2 | Val loss: 0.4031 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3101 | Steps: 2 | Val loss: 0.2707 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4031 | Steps: 2 | Val loss: 0.3174 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4057 | Steps: 2 | Val loss: 0.3185 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=136744)[0m rmse: 0.17711012065410614
[2m[36m(func pid=136744)[0m mae:  0.1298479288816452
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.333, 0.1, 0.19, 0.291, 0.141, 0.142, 0.109]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:24:39 (running for 00:33:11.05)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00014 | RUNNING    | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.31  |  0.147 |                   99 |
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.527 |  0.177 |                   58 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.402 |  0.172 |                   30 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.409 |  0.174 |                    8 |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.14704693853855133
[2m[36m(func pid=127238)[0m mae:  0.10286472737789154
[2m[36m(func pid=127238)[0m rmse_per_class: [0.086, 0.234, 0.038, 0.29, 0.065, 0.16, 0.241, 0.117, 0.136, 0.103]
[2m[36m(func pid=127238)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.17112517356872559
[2m[36m(func pid=142739)[0m mae:  0.12499062716960907
[2m[36m(func pid=142739)[0m rmse_per_class: [0.119, 0.253, 0.079, 0.324, 0.075, 0.188, 0.282, 0.14, 0.142, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.17191067337989807
[2m[36m(func pid=147638)[0m mae:  0.1259201467037201
[2m[36m(func pid=147638)[0m rmse_per_class: [0.119, 0.258, 0.083, 0.322, 0.075, 0.188, 0.282, 0.14, 0.143, 0.109]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5196 | Steps: 2 | Val loss: 0.3987 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=127238)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3146 | Steps: 2 | Val loss: 0.2706 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4078 | Steps: 2 | Val loss: 0.3202 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4142 | Steps: 2 | Val loss: 0.3317 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=136744)[0m rmse: 0.1770763099193573
[2m[36m(func pid=136744)[0m mae:  0.12981484830379486
[2m[36m(func pid=136744)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.099, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:24:44 (running for 00:33:16.19)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 3 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.52  |  0.177 |                   59 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.403 |  0.171 |                   31 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.406 |  0.172 |                    9 |
| train_ccef6_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=127238)[0m rmse: 0.14705988764762878
[2m[36m(func pid=127238)[0m mae:  0.10283227264881134
[2m[36m(func pid=127238)[0m rmse_per_class: [0.086, 0.234, 0.038, 0.29, 0.065, 0.16, 0.241, 0.117, 0.137, 0.103]
[2m[36m(func pid=147638)[0m rmse: 0.16968563199043274
[2m[36m(func pid=147638)[0m mae:  0.12406593561172485
[2m[36m(func pid=147638)[0m rmse_per_class: [0.119, 0.256, 0.078, 0.318, 0.07, 0.188, 0.28, 0.139, 0.143, 0.108]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.17067991197109222
[2m[36m(func pid=142739)[0m mae:  0.1246330514550209
[2m[36m(func pid=142739)[0m rmse_per_class: [0.119, 0.253, 0.078, 0.324, 0.073, 0.188, 0.282, 0.14, 0.142, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5167 | Steps: 2 | Val loss: 0.3948 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4336 | Steps: 2 | Val loss: 0.3496 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4089 | Steps: 2 | Val loss: 0.3232 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=136744)[0m rmse: 0.1770133525133133
[2m[36m(func pid=136744)[0m mae:  0.1297617405653
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.258, 0.092, 0.333, 0.099, 0.19, 0.29, 0.141, 0.142, 0.109]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.1674499660730362
[2m[36m(func pid=147638)[0m mae:  0.12216242402791977
[2m[36m(func pid=147638)[0m rmse_per_class: [0.118, 0.253, 0.073, 0.313, 0.066, 0.187, 0.276, 0.138, 0.143, 0.107]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.17023928463459015
[2m[36m(func pid=142739)[0m mae:  0.12426922470331192
[2m[36m(func pid=142739)[0m rmse_per_class: [0.119, 0.253, 0.077, 0.323, 0.072, 0.188, 0.281, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.5117 | Steps: 2 | Val loss: 0.3910 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4533 | Steps: 2 | Val loss: 0.3698 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 07:24:50 (running for 00:33:21.95)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.517 |  0.177 |                   60 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.408 |  0.171 |                   32 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.434 |  0.167 |                   11 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=150355)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=150355)[0m Configuration completed!
[2m[36m(func pid=150355)[0m New optimizer parameters:
[2m[36m(func pid=150355)[0m SGD (
[2m[36m(func pid=150355)[0m Parameter Group 0
[2m[36m(func pid=150355)[0m     dampening: 0
[2m[36m(func pid=150355)[0m     differentiable: False
[2m[36m(func pid=150355)[0m     foreach: None
[2m[36m(func pid=150355)[0m     lr: 0.1
[2m[36m(func pid=150355)[0m     maximize: False
[2m[36m(func pid=150355)[0m     momentum: 0.99
[2m[36m(func pid=150355)[0m     nesterov: False
[2m[36m(func pid=150355)[0m     weight_decay: 1e-05
[2m[36m(func pid=150355)[0m )
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17697742581367493
[2m[36m(func pid=136744)[0m mae:  0.12974222004413605
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.258, 0.092, 0.333, 0.098, 0.19, 0.29, 0.141, 0.142, 0.109]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.16511934995651245
[2m[36m(func pid=147638)[0m mae:  0.12015952914953232
[2m[36m(func pid=147638)[0m rmse_per_class: [0.117, 0.251, 0.068, 0.309, 0.062, 0.187, 0.273, 0.136, 0.143, 0.105]
== Status ==
Current time: 2024-01-07 07:24:55 (running for 00:33:27.03)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.512 |  0.177 |                   61 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.409 |  0.17  |                   33 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.453 |  0.165 |                   12 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4132 | Steps: 2 | Val loss: 0.3265 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5063 | Steps: 2 | Val loss: 0.3873 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8395 | Steps: 2 | Val loss: 0.5172 | Batch size: 32 | lr: 0.1 | Duration: 4.56s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4803 | Steps: 2 | Val loss: 0.3899 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=142739)[0m rmse: 0.16969594359397888
[2m[36m(func pid=142739)[0m mae:  0.12381385266780853
[2m[36m(func pid=142739)[0m rmse_per_class: [0.119, 0.252, 0.076, 0.323, 0.071, 0.188, 0.28, 0.139, 0.142, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17689873278141022
[2m[36m(func pid=136744)[0m mae:  0.12968721985816956
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.258, 0.092, 0.333, 0.098, 0.19, 0.29, 0.141, 0.142, 0.109]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.1826164573431015
[2m[36m(func pid=150355)[0m mae:  0.13439832627773285
[2m[36m(func pid=150355)[0m rmse_per_class: [0.118, 0.267, 0.108, 0.338, 0.106, 0.191, 0.293, 0.146, 0.144, 0.114]
[2m[36m(func pid=150355)[0m 
== Status ==
Current time: 2024-01-07 07:25:00 (running for 00:33:32.20)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.506 |  0.177 |                   62 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.413 |  0.17  |                   34 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.48  |  0.163 |                   13 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.839 |  0.183 |                    1 |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.1626890003681183
[2m[36m(func pid=147638)[0m mae:  0.11798717826604843
[2m[36m(func pid=147638)[0m rmse_per_class: [0.115, 0.248, 0.063, 0.305, 0.06, 0.186, 0.269, 0.135, 0.143, 0.104]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4134 | Steps: 2 | Val loss: 0.3298 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5015 | Steps: 2 | Val loss: 0.3828 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5084 | Steps: 2 | Val loss: 0.4090 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5380 | Steps: 2 | Val loss: 0.3429 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=142739)[0m rmse: 0.16923901438713074
[2m[36m(func pid=142739)[0m mae:  0.12343959510326385
[2m[36m(func pid=142739)[0m rmse_per_class: [0.119, 0.251, 0.075, 0.322, 0.07, 0.187, 0.28, 0.139, 0.142, 0.108]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.1767282783985138
[2m[36m(func pid=136744)[0m mae:  0.12955380976200104
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.257, 0.091, 0.333, 0.097, 0.19, 0.29, 0.141, 0.142, 0.109]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.1603705883026123
[2m[36m(func pid=147638)[0m mae:  0.11586018651723862
[2m[36m(func pid=147638)[0m rmse_per_class: [0.113, 0.246, 0.059, 0.301, 0.057, 0.186, 0.264, 0.133, 0.142, 0.102]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:25:05 (running for 00:33:37.31)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.501 |  0.177 |                   63 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.413 |  0.169 |                   35 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.508 |  0.16  |                   14 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.538 |  0.181 |                    2 |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150355)[0m rmse: 0.1808006465435028
[2m[36m(func pid=150355)[0m mae:  0.13311490416526794
[2m[36m(func pid=150355)[0m rmse_per_class: [0.123, 0.269, 0.101, 0.334, 0.092, 0.191, 0.289, 0.146, 0.146, 0.118]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4191 | Steps: 2 | Val loss: 0.3331 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4965 | Steps: 2 | Val loss: 0.3795 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5309 | Steps: 2 | Val loss: 0.4264 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=142739)[0m rmse: 0.16862496733665466
[2m[36m(func pid=142739)[0m mae:  0.12292727082967758
[2m[36m(func pid=142739)[0m rmse_per_class: [0.118, 0.251, 0.074, 0.321, 0.068, 0.187, 0.279, 0.138, 0.142, 0.107]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4083 | Steps: 2 | Val loss: 0.3328 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=136744)[0m rmse: 0.17668762803077698
[2m[36m(func pid=136744)[0m mae:  0.1295243203639984
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.258, 0.091, 0.333, 0.097, 0.19, 0.29, 0.141, 0.142, 0.109]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.15801164507865906
[2m[36m(func pid=147638)[0m mae:  0.11360903084278107
[2m[36m(func pid=147638)[0m rmse_per_class: [0.111, 0.243, 0.055, 0.297, 0.056, 0.186, 0.26, 0.131, 0.141, 0.1]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:25:11 (running for 00:33:42.65)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.496 |  0.177 |                   64 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.419 |  0.169 |                   36 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.531 |  0.158 |                   15 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.408 |  0.176 |                    3 |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150355)[0m rmse: 0.17595651745796204
[2m[36m(func pid=150355)[0m mae:  0.129359632730484
[2m[36m(func pid=150355)[0m rmse_per_class: [0.129, 0.268, 0.085, 0.327, 0.073, 0.191, 0.279, 0.143, 0.148, 0.116]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4203 | Steps: 2 | Val loss: 0.3368 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4935 | Steps: 2 | Val loss: 0.3766 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5560 | Steps: 2 | Val loss: 0.4408 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=142739)[0m rmse: 0.16811040043830872
[2m[36m(func pid=142739)[0m mae:  0.12250647693872452
[2m[36m(func pid=142739)[0m rmse_per_class: [0.118, 0.251, 0.073, 0.32, 0.068, 0.187, 0.278, 0.138, 0.142, 0.107]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4631 | Steps: 2 | Val loss: 0.3894 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=136744)[0m rmse: 0.17661993205547333
[2m[36m(func pid=136744)[0m mae:  0.12946605682373047
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.258, 0.091, 0.333, 0.097, 0.19, 0.29, 0.141, 0.142, 0.109]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.15573202073574066
[2m[36m(func pid=147638)[0m mae:  0.11136601120233536
[2m[36m(func pid=147638)[0m rmse_per_class: [0.109, 0.24, 0.052, 0.294, 0.055, 0.185, 0.255, 0.13, 0.14, 0.098]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.16888779401779175
[2m[36m(func pid=150355)[0m mae:  0.12327772378921509
[2m[36m(func pid=150355)[0m rmse_per_class: [0.131, 0.265, 0.067, 0.315, 0.06, 0.188, 0.266, 0.138, 0.148, 0.11]
== Status ==
Current time: 2024-01-07 07:25:16 (running for 00:33:47.96)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.493 |  0.177 |                   65 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.42  |  0.168 |                   37 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.556 |  0.156 |                   16 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.463 |  0.169 |                    4 |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4268 | Steps: 2 | Val loss: 0.3404 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5629 | Steps: 2 | Val loss: 0.4530 | Batch size: 32 | lr: 0.01 | Duration: 2.61s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4881 | Steps: 2 | Val loss: 0.3732 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=142739)[0m rmse: 0.16761940717697144
[2m[36m(func pid=142739)[0m mae:  0.12208916991949081
[2m[36m(func pid=142739)[0m rmse_per_class: [0.118, 0.25, 0.071, 0.32, 0.067, 0.187, 0.277, 0.138, 0.142, 0.107]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5548 | Steps: 2 | Val loss: 0.4488 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=147638)[0m rmse: 0.15352541208267212
[2m[36m(func pid=147638)[0m mae:  0.10908415168523788
[2m[36m(func pid=147638)[0m rmse_per_class: [0.107, 0.238, 0.049, 0.291, 0.055, 0.184, 0.249, 0.128, 0.14, 0.095]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.1764739453792572
[2m[36m(func pid=136744)[0m mae:  0.12934130430221558
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.258, 0.091, 0.332, 0.096, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:25:21 (running for 00:33:53.05)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.488 |  0.176 |                   66 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.427 |  0.168 |                   38 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.563 |  0.154 |                   17 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.555 |  0.161 |                    5 |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150355)[0m rmse: 0.16092653572559357
[2m[36m(func pid=150355)[0m mae:  0.11562927067279816
[2m[36m(func pid=150355)[0m rmse_per_class: [0.128, 0.26, 0.052, 0.3, 0.055, 0.186, 0.251, 0.131, 0.145, 0.101]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4343 | Steps: 2 | Val loss: 0.3437 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.5722 | Steps: 2 | Val loss: 0.4625 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4850 | Steps: 2 | Val loss: 0.3702 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.6354 | Steps: 2 | Val loss: 0.4870 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=142739)[0m rmse: 0.16703549027442932
[2m[36m(func pid=142739)[0m mae:  0.12159384787082672
[2m[36m(func pid=142739)[0m rmse_per_class: [0.118, 0.249, 0.07, 0.319, 0.066, 0.187, 0.276, 0.137, 0.142, 0.106]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.1514425277709961
[2m[36m(func pid=147638)[0m mae:  0.1068335622549057
[2m[36m(func pid=147638)[0m rmse_per_class: [0.104, 0.236, 0.046, 0.288, 0.055, 0.184, 0.244, 0.127, 0.139, 0.093]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17640207707881927
[2m[36m(func pid=136744)[0m mae:  0.12927749752998352
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.258, 0.091, 0.332, 0.096, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:25:26 (running for 00:33:58.23)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.485 |  0.176 |                   67 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.434 |  0.167 |                   39 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.572 |  0.151 |                   18 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.635 |  0.153 |                    6 |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150355)[0m rmse: 0.15308722853660583
[2m[36m(func pid=150355)[0m mae:  0.10698401927947998
[2m[36m(func pid=150355)[0m rmse_per_class: [0.117, 0.255, 0.045, 0.285, 0.055, 0.186, 0.234, 0.124, 0.141, 0.09]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4385 | Steps: 2 | Val loss: 0.3469 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.5900 | Steps: 2 | Val loss: 0.4683 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4790 | Steps: 2 | Val loss: 0.3670 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=142739)[0m rmse: 0.16637207567691803
[2m[36m(func pid=142739)[0m mae:  0.1210002675652504
[2m[36m(func pid=142739)[0m rmse_per_class: [0.117, 0.248, 0.069, 0.318, 0.065, 0.186, 0.275, 0.137, 0.142, 0.106]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.6628 | Steps: 2 | Val loss: 0.4989 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=147638)[0m rmse: 0.14962168037891388
[2m[36m(func pid=147638)[0m mae:  0.10469631105661392
[2m[36m(func pid=147638)[0m rmse_per_class: [0.101, 0.234, 0.045, 0.286, 0.055, 0.183, 0.239, 0.126, 0.138, 0.091]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17633672058582306
[2m[36m(func pid=136744)[0m mae:  0.12922212481498718
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.257, 0.09, 0.332, 0.096, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:25:32 (running for 00:34:03.56)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.479 |  0.176 |                   68 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.438 |  0.166 |                   40 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.59  |  0.15  |                   19 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.663 |  0.148 |                    7 |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150355)[0m rmse: 0.14782071113586426
[2m[36m(func pid=150355)[0m mae:  0.09917163848876953
[2m[36m(func pid=150355)[0m rmse_per_class: [0.103, 0.249, 0.044, 0.274, 0.056, 0.186, 0.227, 0.119, 0.136, 0.083]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4390 | Steps: 2 | Val loss: 0.3503 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5942 | Steps: 2 | Val loss: 0.4718 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4757 | Steps: 2 | Val loss: 0.3639 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=142739)[0m rmse: 0.16589049994945526
[2m[36m(func pid=142739)[0m mae:  0.12059684097766876
[2m[36m(func pid=142739)[0m rmse_per_class: [0.117, 0.247, 0.069, 0.317, 0.064, 0.186, 0.274, 0.137, 0.142, 0.106]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.6620 | Steps: 2 | Val loss: 0.4820 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=147638)[0m rmse: 0.14783358573913574
[2m[36m(func pid=147638)[0m mae:  0.10245373100042343
[2m[36m(func pid=147638)[0m rmse_per_class: [0.097, 0.232, 0.043, 0.283, 0.055, 0.183, 0.235, 0.125, 0.136, 0.089]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.1763341724872589
[2m[36m(func pid=136744)[0m mae:  0.1292254477739334
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.257, 0.09, 0.332, 0.096, 0.19, 0.29, 0.141, 0.142, 0.109]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:25:37 (running for 00:34:08.71)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.476 |  0.176 |                   69 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.439 |  0.166 |                   41 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.594 |  0.148 |                   20 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.662 |  0.147 |                    8 |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150355)[0m rmse: 0.14746049046516418
[2m[36m(func pid=150355)[0m mae:  0.09435819089412689
[2m[36m(func pid=150355)[0m rmse_per_class: [0.09, 0.244, 0.045, 0.268, 0.056, 0.19, 0.25, 0.119, 0.133, 0.08]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4444 | Steps: 2 | Val loss: 0.3540 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5938 | Steps: 2 | Val loss: 0.4728 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4731 | Steps: 2 | Val loss: 0.3610 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=142739)[0m rmse: 0.16526448726654053
[2m[36m(func pid=142739)[0m mae:  0.12004037946462631
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.247, 0.067, 0.317, 0.063, 0.186, 0.273, 0.136, 0.142, 0.106]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6320 | Steps: 2 | Val loss: 0.4388 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=147638)[0m rmse: 0.14631438255310059
[2m[36m(func pid=147638)[0m mae:  0.10029558092355728
[2m[36m(func pid=147638)[0m rmse_per_class: [0.093, 0.231, 0.042, 0.281, 0.055, 0.182, 0.233, 0.124, 0.135, 0.087]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17616257071495056
[2m[36m(func pid=136744)[0m mae:  0.12908527255058289
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.257, 0.09, 0.332, 0.095, 0.19, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:25:42 (running for 00:34:13.89)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.473 |  0.176 |                   70 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.444 |  0.165 |                   42 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.594 |  0.146 |                   21 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.632 |  0.151 |                    9 |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150355)[0m rmse: 0.151035338640213
[2m[36m(func pid=150355)[0m mae:  0.09312330931425095
[2m[36m(func pid=150355)[0m rmse_per_class: [0.081, 0.241, 0.047, 0.267, 0.056, 0.193, 0.292, 0.122, 0.131, 0.08]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4463 | Steps: 2 | Val loss: 0.3570 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5907 | Steps: 2 | Val loss: 0.4707 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4695 | Steps: 2 | Val loss: 0.3585 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=142739)[0m rmse: 0.16470806300640106
[2m[36m(func pid=142739)[0m mae:  0.1195613369345665
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.246, 0.066, 0.316, 0.062, 0.186, 0.272, 0.136, 0.142, 0.105]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5845 | Steps: 2 | Val loss: 0.3786 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=147638)[0m rmse: 0.14509990811347961
[2m[36m(func pid=147638)[0m mae:  0.09832809865474701
[2m[36m(func pid=147638)[0m rmse_per_class: [0.09, 0.23, 0.041, 0.278, 0.055, 0.182, 0.231, 0.124, 0.134, 0.086]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17607997357845306
[2m[36m(func pid=136744)[0m mae:  0.12900704145431519
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.257, 0.09, 0.332, 0.095, 0.19, 0.289, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:25:47 (running for 00:34:19.23)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.469 |  0.176 |                   71 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.446 |  0.165 |                   43 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.591 |  0.145 |                   22 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.584 |  0.156 |                   10 |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150355)[0m rmse: 0.1559591293334961
[2m[36m(func pid=150355)[0m mae:  0.09427231550216675
[2m[36m(func pid=150355)[0m rmse_per_class: [0.075, 0.244, 0.048, 0.27, 0.056, 0.196, 0.333, 0.125, 0.131, 0.082]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4474 | Steps: 2 | Val loss: 0.3601 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5927 | Steps: 2 | Val loss: 0.4668 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4667 | Steps: 2 | Val loss: 0.3560 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=142739)[0m rmse: 0.1642037332057953
[2m[36m(func pid=142739)[0m mae:  0.11910338699817657
[2m[36m(func pid=142739)[0m rmse_per_class: [0.116, 0.246, 0.065, 0.315, 0.062, 0.185, 0.271, 0.136, 0.142, 0.105]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4932 | Steps: 2 | Val loss: 0.3220 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=147638)[0m rmse: 0.14444437623023987
[2m[36m(func pid=147638)[0m mae:  0.09672120213508606
[2m[36m(func pid=147638)[0m rmse_per_class: [0.087, 0.229, 0.041, 0.277, 0.055, 0.182, 0.232, 0.124, 0.133, 0.084]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=136744)[0m rmse: 0.17602631449699402
[2m[36m(func pid=136744)[0m mae:  0.1289697140455246
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.257, 0.09, 0.332, 0.095, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:25:53 (running for 00:34:24.65)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.467 |  0.176 |                   72 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.447 |  0.164 |                   44 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.593 |  0.144 |                   23 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.493 |  0.156 |                   11 |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4548 | Steps: 2 | Val loss: 0.3627 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=150355)[0m rmse: 0.15594102442264557
[2m[36m(func pid=150355)[0m mae:  0.09329508244991302
[2m[36m(func pid=150355)[0m rmse_per_class: [0.072, 0.247, 0.048, 0.272, 0.056, 0.194, 0.331, 0.124, 0.131, 0.084]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5917 | Steps: 2 | Val loss: 0.4602 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4627 | Steps: 2 | Val loss: 0.3535 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=142739)[0m rmse: 0.16358144581317902
[2m[36m(func pid=142739)[0m mae:  0.11852798610925674
[2m[36m(func pid=142739)[0m rmse_per_class: [0.115, 0.246, 0.064, 0.314, 0.061, 0.185, 0.27, 0.135, 0.141, 0.104]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.14415426552295685
[2m[36m(func pid=147638)[0m mae:  0.09550650417804718
[2m[36m(func pid=147638)[0m rmse_per_class: [0.084, 0.229, 0.04, 0.276, 0.056, 0.183, 0.234, 0.124, 0.132, 0.084]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4283 | Steps: 2 | Val loss: 0.2977 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=136744)[0m rmse: 0.17590518295764923
[2m[36m(func pid=136744)[0m mae:  0.12888208031654358
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.257, 0.089, 0.332, 0.094, 0.19, 0.289, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
== Status ==
Current time: 2024-01-07 07:25:58 (running for 00:34:29.99)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.463 |  0.176 |                   73 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.455 |  0.164 |                   45 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.592 |  0.144 |                   24 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.428 |  0.149 |                   12 |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4559 | Steps: 2 | Val loss: 0.3653 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=150355)[0m rmse: 0.1486334353685379
[2m[36m(func pid=150355)[0m mae:  0.08908072859048843
[2m[36m(func pid=150355)[0m rmse_per_class: [0.074, 0.246, 0.048, 0.268, 0.056, 0.179, 0.279, 0.119, 0.129, 0.087]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5845 | Steps: 2 | Val loss: 0.4498 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4599 | Steps: 2 | Val loss: 0.3512 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=142739)[0m rmse: 0.16326101124286652
[2m[36m(func pid=142739)[0m mae:  0.1182483658194542
[2m[36m(func pid=142739)[0m rmse_per_class: [0.115, 0.245, 0.064, 0.314, 0.06, 0.185, 0.269, 0.135, 0.142, 0.104]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.14403176307678223
[2m[36m(func pid=147638)[0m mae:  0.09419678151607513
[2m[36m(func pid=147638)[0m rmse_per_class: [0.081, 0.229, 0.04, 0.275, 0.056, 0.184, 0.237, 0.125, 0.131, 0.083]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4137 | Steps: 2 | Val loss: 0.3182 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=136744)[0m rmse: 0.17579102516174316
[2m[36m(func pid=136744)[0m mae:  0.1287892460823059
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.257, 0.089, 0.332, 0.094, 0.19, 0.289, 0.141, 0.142, 0.108]
[2m[36m(func pid=136744)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4609 | Steps: 2 | Val loss: 0.3684 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 07:26:03 (running for 00:34:35.35)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00016 | RUNNING    | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.46  |  0.176 |                   74 |
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.456 |  0.163 |                   46 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.585 |  0.144 |                   25 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.414 |  0.141 |                   13 |
| train_ccef6_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5632 | Steps: 2 | Val loss: 0.4388 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=150355)[0m rmse: 0.1414695382118225
[2m[36m(func pid=150355)[0m mae:  0.08698569238185883
[2m[36m(func pid=150355)[0m rmse_per_class: [0.083, 0.243, 0.047, 0.266, 0.056, 0.161, 0.22, 0.116, 0.13, 0.093]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=136744)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4543 | Steps: 2 | Val loss: 0.3488 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=142739)[0m rmse: 0.16265332698822021
[2m[36m(func pid=142739)[0m mae:  0.11768988519906998
[2m[36m(func pid=142739)[0m rmse_per_class: [0.115, 0.245, 0.062, 0.313, 0.06, 0.184, 0.268, 0.134, 0.141, 0.104]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.14427106082439423
[2m[36m(func pid=147638)[0m mae:  0.09320074319839478
[2m[36m(func pid=147638)[0m rmse_per_class: [0.078, 0.229, 0.04, 0.273, 0.056, 0.184, 0.242, 0.126, 0.131, 0.083]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4387 | Steps: 2 | Val loss: 0.3694 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=136744)[0m rmse: 0.1756746470928192
[2m[36m(func pid=136744)[0m mae:  0.12868735194206238
[2m[36m(func pid=136744)[0m rmse_per_class: [0.117, 0.256, 0.089, 0.331, 0.093, 0.19, 0.289, 0.141, 0.142, 0.108]
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4674 | Steps: 2 | Val loss: 0.3701 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5438 | Steps: 2 | Val loss: 0.4260 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=150355)[0m rmse: 0.14319664239883423
[2m[36m(func pid=150355)[0m mae:  0.09109420329332352
[2m[36m(func pid=150355)[0m rmse_per_class: [0.099, 0.241, 0.045, 0.268, 0.056, 0.152, 0.217, 0.114, 0.14, 0.099]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.16194893419742584
[2m[36m(func pid=142739)[0m mae:  0.1170368418097496
[2m[36m(func pid=142739)[0m rmse_per_class: [0.115, 0.244, 0.061, 0.311, 0.059, 0.184, 0.267, 0.134, 0.141, 0.103]
[2m[36m(func pid=147638)[0m rmse: 0.14517518877983093
[2m[36m(func pid=147638)[0m mae:  0.0926593765616417
[2m[36m(func pid=147638)[0m rmse_per_class: [0.076, 0.23, 0.04, 0.274, 0.056, 0.185, 0.251, 0.127, 0.13, 0.082]
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5184 | Steps: 2 | Val loss: 0.4218 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=150355)[0m rmse: 0.1508222371339798
[2m[36m(func pid=150355)[0m mae:  0.0978083536028862
[2m[36m(func pid=150355)[0m rmse_per_class: [0.113, 0.24, 0.042, 0.271, 0.056, 0.152, 0.252, 0.116, 0.16, 0.107]
== Status ==
Current time: 2024-01-07 07:26:09 (running for 00:34:40.76)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.461 |  0.163 |                   47 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.563 |  0.144 |                   26 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.439 |  0.143 |                   14 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=153954)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=153954)[0m Configuration completed!
[2m[36m(func pid=153954)[0m New optimizer parameters:
[2m[36m(func pid=153954)[0m SGD (
[2m[36m(func pid=153954)[0m Parameter Group 0
[2m[36m(func pid=153954)[0m     dampening: 0
[2m[36m(func pid=153954)[0m     differentiable: False
[2m[36m(func pid=153954)[0m     foreach: None
[2m[36m(func pid=153954)[0m     lr: 0.0001
[2m[36m(func pid=153954)[0m     maximize: False
[2m[36m(func pid=153954)[0m     momentum: 0.9
[2m[36m(func pid=153954)[0m     nesterov: False
[2m[36m(func pid=153954)[0m     weight_decay: 1e-05
[2m[36m(func pid=153954)[0m )
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=142739)[0m 
== Status ==
Current time: 2024-01-07 07:26:15 (running for 00:34:46.75)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.467 |  0.162 |                   48 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.563 |  0.144 |                   26 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.439 |  0.143 |                   14 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4649 | Steps: 2 | Val loss: 0.3722 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5362 | Steps: 2 | Val loss: 0.4102 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5363 | Steps: 2 | Val loss: 0.4368 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8983 | Steps: 2 | Val loss: 0.7084 | Batch size: 32 | lr: 0.0001 | Duration: 4.51s
== Status ==
Current time: 2024-01-07 07:26:20 (running for 00:34:51.77)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.467 |  0.162 |                   48 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.544 |  0.145 |                   27 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.518 |  0.151 |                   15 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.16149601340293884
[2m[36m(func pid=142739)[0m mae:  0.11664412170648575
[2m[36m(func pid=142739)[0m rmse_per_class: [0.115, 0.244, 0.06, 0.31, 0.059, 0.184, 0.266, 0.134, 0.141, 0.103]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.14608606696128845
[2m[36m(func pid=147638)[0m mae:  0.09227481484413147
[2m[36m(func pid=147638)[0m rmse_per_class: [0.074, 0.231, 0.04, 0.274, 0.056, 0.186, 0.259, 0.128, 0.13, 0.083]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.15928223729133606
[2m[36m(func pid=150355)[0m mae:  0.10343994945287704
[2m[36m(func pid=150355)[0m rmse_per_class: [0.113, 0.245, 0.036, 0.274, 0.056, 0.158, 0.287, 0.121, 0.174, 0.129]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.18270020186901093
[2m[36m(func pid=153954)[0m mae:  0.13444431126117706
[2m[36m(func pid=153954)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5150 | Steps: 2 | Val loss: 0.3923 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4678 | Steps: 2 | Val loss: 0.3736 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5365 | Steps: 2 | Val loss: 0.4213 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8963 | Steps: 2 | Val loss: 0.7047 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=142739)[0m rmse: 0.1608639657497406
[2m[36m(func pid=142739)[0m mae:  0.11605872958898544
[2m[36m(func pid=142739)[0m rmse_per_class: [0.114, 0.243, 0.059, 0.309, 0.058, 0.184, 0.265, 0.133, 0.141, 0.102]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.14729192852973938
[2m[36m(func pid=147638)[0m mae:  0.09213428199291229
[2m[36m(func pid=147638)[0m rmse_per_class: [0.072, 0.234, 0.041, 0.275, 0.056, 0.186, 0.268, 0.128, 0.13, 0.083]
== Status ==
Current time: 2024-01-07 07:26:25 (running for 00:34:57.16)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.468 |  0.161 |                   50 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.536 |  0.146 |                   28 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.536 |  0.159 |                   16 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.898 |  0.183 |                    1 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.1691282093524933
[2m[36m(func pid=150355)[0m mae:  0.10750248283147812
[2m[36m(func pid=150355)[0m rmse_per_class: [0.106, 0.261, 0.028, 0.282, 0.056, 0.17, 0.31, 0.131, 0.172, 0.174]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.1824377328157425
[2m[36m(func pid=153954)[0m mae:  0.13428139686584473
[2m[36m(func pid=153954)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5016 | Steps: 2 | Val loss: 0.3727 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4694 | Steps: 2 | Val loss: 0.3756 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4850 | Steps: 2 | Val loss: 0.4068 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8966 | Steps: 2 | Val loss: 0.7014 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 07:26:30 (running for 00:35:02.32)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.468 |  0.161 |                   50 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.502 |  0.148 |                   30 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.536 |  0.169 |                   17 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.896 |  0.182 |                    2 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.14786913990974426
[2m[36m(func pid=147638)[0m mae:  0.09169810265302658
[2m[36m(func pid=147638)[0m rmse_per_class: [0.07, 0.236, 0.042, 0.274, 0.056, 0.186, 0.273, 0.129, 0.13, 0.083]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.16039451956748962
[2m[36m(func pid=142739)[0m mae:  0.11564183235168457
[2m[36m(func pid=142739)[0m rmse_per_class: [0.114, 0.242, 0.058, 0.309, 0.058, 0.183, 0.264, 0.133, 0.141, 0.102]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.1828782558441162
[2m[36m(func pid=150355)[0m mae:  0.11195433139801025
[2m[36m(func pid=150355)[0m rmse_per_class: [0.102, 0.287, 0.025, 0.296, 0.056, 0.186, 0.321, 0.144, 0.166, 0.244]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.18216267228126526
[2m[36m(func pid=153954)[0m mae:  0.13408175110816956
[2m[36m(func pid=153954)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4653 | Steps: 2 | Val loss: 0.3535 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4726 | Steps: 2 | Val loss: 0.3768 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4516 | Steps: 2 | Val loss: 0.4143 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8964 | Steps: 2 | Val loss: 0.6989 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
== Status ==
Current time: 2024-01-07 07:26:35 (running for 00:35:07.49)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.469 |  0.16  |                   51 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.465 |  0.148 |                   31 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.485 |  0.183 |                   18 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.897 |  0.182 |                    3 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.14817474782466888
[2m[36m(func pid=147638)[0m mae:  0.09136155247688293
[2m[36m(func pid=147638)[0m rmse_per_class: [0.069, 0.239, 0.042, 0.275, 0.056, 0.184, 0.274, 0.13, 0.13, 0.083]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.15992635488510132
[2m[36m(func pid=142739)[0m mae:  0.11524222791194916
[2m[36m(func pid=142739)[0m rmse_per_class: [0.113, 0.242, 0.058, 0.308, 0.057, 0.183, 0.262, 0.133, 0.141, 0.102]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.19732244312763214
[2m[36m(func pid=150355)[0m mae:  0.11687362194061279
[2m[36m(func pid=150355)[0m rmse_per_class: [0.105, 0.313, 0.03, 0.316, 0.056, 0.193, 0.32, 0.159, 0.161, 0.321]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.18180376291275024
[2m[36m(func pid=153954)[0m mae:  0.13380160927772522
[2m[36m(func pid=153954)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.339, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4412 | Steps: 2 | Val loss: 0.3353 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4716 | Steps: 2 | Val loss: 0.3779 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4224 | Steps: 2 | Val loss: 0.4321 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8920 | Steps: 2 | Val loss: 0.6967 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 07:26:41 (running for 00:35:12.77)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.473 |  0.16  |                   52 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.441 |  0.149 |                   32 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.452 |  0.197 |                   19 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.896 |  0.182 |                    4 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.1486075222492218
[2m[36m(func pid=147638)[0m mae:  0.09112004190683365
[2m[36m(func pid=147638)[0m rmse_per_class: [0.068, 0.243, 0.042, 0.275, 0.056, 0.183, 0.275, 0.13, 0.13, 0.084]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.1592099368572235
[2m[36m(func pid=142739)[0m mae:  0.11457151174545288
[2m[36m(func pid=142739)[0m rmse_per_class: [0.113, 0.241, 0.057, 0.308, 0.057, 0.183, 0.261, 0.132, 0.14, 0.101]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.20409250259399414
[2m[36m(func pid=150355)[0m mae:  0.11801360547542572
[2m[36m(func pid=150355)[0m rmse_per_class: [0.106, 0.327, 0.037, 0.329, 0.056, 0.192, 0.302, 0.17, 0.156, 0.365]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.18149909377098083
[2m[36m(func pid=153954)[0m mae:  0.13355669379234314
[2m[36m(func pid=153954)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4716 | Steps: 2 | Val loss: 0.3791 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4229 | Steps: 2 | Val loss: 0.3175 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4362 | Steps: 2 | Val loss: 0.4584 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8945 | Steps: 2 | Val loss: 0.6944 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 07:26:46 (running for 00:35:18.03)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.472 |  0.159 |                   54 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.441 |  0.149 |                   32 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.422 |  0.204 |                   20 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.892 |  0.181 |                    5 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.15855571627616882
[2m[36m(func pid=142739)[0m mae:  0.11396008729934692
[2m[36m(func pid=142739)[0m rmse_per_class: [0.112, 0.24, 0.056, 0.307, 0.057, 0.182, 0.26, 0.131, 0.14, 0.1]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.1484188288450241
[2m[36m(func pid=147638)[0m mae:  0.0906573161482811
[2m[36m(func pid=147638)[0m rmse_per_class: [0.068, 0.244, 0.042, 0.274, 0.056, 0.181, 0.274, 0.13, 0.13, 0.084]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.20204265415668488
[2m[36m(func pid=150355)[0m mae:  0.11429284512996674
[2m[36m(func pid=150355)[0m rmse_per_class: [0.109, 0.332, 0.046, 0.335, 0.056, 0.184, 0.262, 0.162, 0.152, 0.382]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.18113063275814056
[2m[36m(func pid=153954)[0m mae:  0.13325008749961853
[2m[36m(func pid=153954)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.112, 0.19, 0.294, 0.142, 0.142, 0.11]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4724 | Steps: 2 | Val loss: 0.3800 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4046 | Steps: 2 | Val loss: 0.3021 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4347 | Steps: 2 | Val loss: 0.4836 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8911 | Steps: 2 | Val loss: 0.6927 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 07:26:51 (running for 00:35:23.14)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.472 |  0.159 |                   54 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.405 |  0.148 |                   34 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.436 |  0.202 |                   21 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.894 |  0.181 |                    6 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.1477127969264984
[2m[36m(func pid=147638)[0m mae:  0.09009037166833878
[2m[36m(func pid=147638)[0m rmse_per_class: [0.068, 0.245, 0.042, 0.273, 0.056, 0.178, 0.27, 0.129, 0.13, 0.085]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.15800325572490692
[2m[36m(func pid=142739)[0m mae:  0.11343103647232056
[2m[36m(func pid=142739)[0m rmse_per_class: [0.112, 0.24, 0.055, 0.306, 0.056, 0.182, 0.259, 0.131, 0.14, 0.099]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.1945585459470749
[2m[36m(func pid=150355)[0m mae:  0.10717346519231796
[2m[36m(func pid=150355)[0m rmse_per_class: [0.105, 0.33, 0.051, 0.333, 0.059, 0.175, 0.235, 0.137, 0.151, 0.371]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.18080838024616241
[2m[36m(func pid=153954)[0m mae:  0.1329687386751175
[2m[36m(func pid=153954)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.338, 0.112, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3931 | Steps: 2 | Val loss: 0.2902 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4783 | Steps: 2 | Val loss: 0.3801 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4421 | Steps: 2 | Val loss: 0.5006 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8852 | Steps: 2 | Val loss: 0.6916 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 07:26:56 (running for 00:35:28.36)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.472 |  0.158 |                   55 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.393 |  0.147 |                   35 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.435 |  0.195 |                   22 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.891 |  0.181 |                    7 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.1465456187725067
[2m[36m(func pid=147638)[0m mae:  0.08941642940044403
[2m[36m(func pid=147638)[0m rmse_per_class: [0.068, 0.246, 0.042, 0.272, 0.056, 0.175, 0.262, 0.128, 0.129, 0.087]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.1575152724981308
[2m[36m(func pid=142739)[0m mae:  0.11299137771129608
[2m[36m(func pid=142739)[0m rmse_per_class: [0.111, 0.24, 0.054, 0.305, 0.056, 0.182, 0.258, 0.131, 0.14, 0.099]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.19564256072044373
[2m[36m(func pid=150355)[0m mae:  0.10729752480983734
[2m[36m(func pid=150355)[0m rmse_per_class: [0.1, 0.324, 0.05, 0.33, 0.065, 0.179, 0.304, 0.122, 0.147, 0.335]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.18061226606369019
[2m[36m(func pid=153954)[0m mae:  0.13279235363006592
[2m[36m(func pid=153954)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4908 | Steps: 2 | Val loss: 0.3787 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3705 | Steps: 2 | Val loss: 0.2815 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4602 | Steps: 2 | Val loss: 0.5080 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8833 | Steps: 2 | Val loss: 0.6909 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=147638)[0m rmse: 0.14482592046260834
[2m[36m(func pid=147638)[0m mae:  0.08858127892017365
[2m[36m(func pid=147638)[0m rmse_per_class: [0.07, 0.246, 0.042, 0.271, 0.056, 0.17, 0.25, 0.126, 0.129, 0.088]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:27:01 (running for 00:35:33.53)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.478 |  0.158 |                   56 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.37  |  0.145 |                   36 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.442 |  0.196 |                   23 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.885 |  0.181 |                    8 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.15700392425060272
[2m[36m(func pid=142739)[0m mae:  0.11252725124359131
[2m[36m(func pid=142739)[0m rmse_per_class: [0.11, 0.24, 0.053, 0.304, 0.056, 0.182, 0.256, 0.13, 0.14, 0.099]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.2042054384946823
[2m[36m(func pid=150355)[0m mae:  0.1142285093665123
[2m[36m(func pid=150355)[0m rmse_per_class: [0.093, 0.317, 0.043, 0.335, 0.082, 0.197, 0.423, 0.13, 0.142, 0.281]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.18040822446346283
[2m[36m(func pid=153954)[0m mae:  0.1325952708721161
[2m[36m(func pid=153954)[0m rmse_per_class: [0.116, 0.263, 0.102, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3638 | Steps: 2 | Val loss: 0.2772 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4752 | Steps: 2 | Val loss: 0.3786 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8832 | Steps: 2 | Val loss: 0.6896 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4836 | Steps: 2 | Val loss: 0.5139 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 07:27:07 (running for 00:35:38.66)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.491 |  0.157 |                   57 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.364 |  0.143 |                   37 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.46  |  0.204 |                   24 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.883 |  0.18  |                    9 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.14319196343421936
[2m[36m(func pid=147638)[0m mae:  0.08804343640804291
[2m[36m(func pid=147638)[0m rmse_per_class: [0.071, 0.246, 0.041, 0.27, 0.056, 0.166, 0.238, 0.125, 0.129, 0.09]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.15646463632583618
[2m[36m(func pid=142739)[0m mae:  0.1119905337691307
[2m[36m(func pid=142739)[0m rmse_per_class: [0.11, 0.239, 0.053, 0.303, 0.056, 0.182, 0.255, 0.13, 0.14, 0.098]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.18027864396572113
[2m[36m(func pid=153954)[0m mae:  0.13247931003570557
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.263, 0.101, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.2106003761291504
[2m[36m(func pid=150355)[0m mae:  0.12069062143564224
[2m[36m(func pid=150355)[0m rmse_per_class: [0.089, 0.311, 0.036, 0.342, 0.103, 0.211, 0.505, 0.139, 0.138, 0.234]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3560 | Steps: 2 | Val loss: 0.2770 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4739 | Steps: 2 | Val loss: 0.3783 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8825 | Steps: 2 | Val loss: 0.6873 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4506 | Steps: 2 | Val loss: 0.5163 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 07:27:12 (running for 00:35:43.92)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.475 |  0.156 |                   58 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.356 |  0.142 |                   38 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.484 |  0.211 |                   25 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.883 |  0.18  |                   10 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.1418587565422058
[2m[36m(func pid=147638)[0m mae:  0.08786974847316742
[2m[36m(func pid=147638)[0m rmse_per_class: [0.073, 0.247, 0.04, 0.271, 0.056, 0.162, 0.226, 0.123, 0.129, 0.091]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.15576761960983276
[2m[36m(func pid=142739)[0m mae:  0.11132270097732544
[2m[36m(func pid=142739)[0m rmse_per_class: [0.11, 0.238, 0.052, 0.302, 0.055, 0.181, 0.254, 0.13, 0.139, 0.097]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.18009111285209656
[2m[36m(func pid=153954)[0m mae:  0.13231885433197021
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.263, 0.101, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.21275129914283752
[2m[36m(func pid=150355)[0m mae:  0.12362785637378693
[2m[36m(func pid=150355)[0m rmse_per_class: [0.087, 0.307, 0.032, 0.346, 0.124, 0.216, 0.537, 0.143, 0.137, 0.198]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3544 | Steps: 2 | Val loss: 0.2798 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4737 | Steps: 2 | Val loss: 0.3783 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8779 | Steps: 2 | Val loss: 0.6861 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4865 | Steps: 2 | Val loss: 0.5061 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=142739)[0m rmse: 0.15518084168434143
[2m[36m(func pid=142739)[0m mae:  0.11072085052728653
[2m[36m(func pid=142739)[0m rmse_per_class: [0.109, 0.237, 0.051, 0.301, 0.055, 0.181, 0.253, 0.129, 0.139, 0.097]
[2m[36m(func pid=142739)[0m 
== Status ==
Current time: 2024-01-07 07:27:17 (running for 00:35:49.17)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.474 |  0.155 |                   60 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.356 |  0.142 |                   38 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.451 |  0.213 |                   26 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.882 |  0.18  |                   11 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.14048898220062256
[2m[36m(func pid=147638)[0m mae:  0.08773595094680786
[2m[36m(func pid=147638)[0m rmse_per_class: [0.077, 0.245, 0.039, 0.269, 0.056, 0.158, 0.217, 0.12, 0.129, 0.093]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.179934561252594
[2m[36m(func pid=153954)[0m mae:  0.13218741118907928
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.21082425117492676
[2m[36m(func pid=150355)[0m mae:  0.1219116672873497
[2m[36m(func pid=150355)[0m rmse_per_class: [0.086, 0.305, 0.031, 0.343, 0.147, 0.215, 0.519, 0.144, 0.137, 0.181]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4760 | Steps: 2 | Val loss: 0.3780 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3641 | Steps: 2 | Val loss: 0.2863 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8758 | Steps: 2 | Val loss: 0.6846 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4568 | Steps: 2 | Val loss: 0.4848 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 07:27:22 (running for 00:35:54.32)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.476 |  0.155 |                   61 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.354 |  0.14  |                   39 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.486 |  0.211 |                   27 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.878 |  0.18  |                   12 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.15470144152641296
[2m[36m(func pid=142739)[0m mae:  0.11021140962839127
[2m[36m(func pid=142739)[0m rmse_per_class: [0.108, 0.237, 0.05, 0.301, 0.055, 0.18, 0.252, 0.129, 0.139, 0.096]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.1400189995765686
[2m[36m(func pid=147638)[0m mae:  0.08830420672893524
[2m[36m(func pid=147638)[0m rmse_per_class: [0.081, 0.245, 0.039, 0.269, 0.056, 0.155, 0.211, 0.119, 0.13, 0.095]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.1798570156097412
[2m[36m(func pid=153954)[0m mae:  0.13212808966636658
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.20653584599494934
[2m[36m(func pid=150355)[0m mae:  0.11714305728673935
[2m[36m(func pid=150355)[0m rmse_per_class: [0.086, 0.308, 0.03, 0.339, 0.18, 0.209, 0.461, 0.144, 0.138, 0.171]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3663 | Steps: 2 | Val loss: 0.2949 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4755 | Steps: 2 | Val loss: 0.3777 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8758 | Steps: 2 | Val loss: 0.6829 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4425 | Steps: 2 | Val loss: 0.4615 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 07:27:27 (running for 00:35:59.43)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.476 |  0.155 |                   61 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.366 |  0.14  |                   41 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.457 |  0.207 |                   28 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.876 |  0.18  |                   13 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.139949232339859
[2m[36m(func pid=147638)[0m mae:  0.08904097974300385
[2m[36m(func pid=147638)[0m rmse_per_class: [0.086, 0.244, 0.037, 0.269, 0.056, 0.152, 0.21, 0.118, 0.131, 0.096]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.1540694683790207
[2m[36m(func pid=142739)[0m mae:  0.10956206172704697
[2m[36m(func pid=142739)[0m rmse_per_class: [0.108, 0.236, 0.049, 0.299, 0.055, 0.18, 0.251, 0.129, 0.139, 0.095]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17976301908493042
[2m[36m(func pid=153954)[0m mae:  0.1320418268442154
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.19945624470710754
[2m[36m(func pid=150355)[0m mae:  0.1106375902891159
[2m[36m(func pid=150355)[0m rmse_per_class: [0.086, 0.319, 0.031, 0.336, 0.209, 0.195, 0.373, 0.141, 0.14, 0.165]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3804 | Steps: 2 | Val loss: 0.3057 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4749 | Steps: 2 | Val loss: 0.3767 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8713 | Steps: 2 | Val loss: 0.6814 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4268 | Steps: 2 | Val loss: 0.4529 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 07:27:32 (running for 00:36:04.52)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.475 |  0.154 |                   62 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.38  |  0.141 |                   42 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.442 |  0.199 |                   29 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.876 |  0.18  |                   14 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.14053380489349365
[2m[36m(func pid=147638)[0m mae:  0.09007124602794647
[2m[36m(func pid=147638)[0m rmse_per_class: [0.09, 0.243, 0.036, 0.268, 0.056, 0.15, 0.213, 0.116, 0.133, 0.098]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.15347988903522491
[2m[36m(func pid=142739)[0m mae:  0.10895168781280518
[2m[36m(func pid=142739)[0m rmse_per_class: [0.107, 0.236, 0.049, 0.298, 0.055, 0.179, 0.25, 0.128, 0.138, 0.095]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17972473800182343
[2m[36m(func pid=153954)[0m mae:  0.13200727105140686
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.11, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.19309112429618835
[2m[36m(func pid=150355)[0m mae:  0.10648147761821747
[2m[36m(func pid=150355)[0m rmse_per_class: [0.087, 0.338, 0.032, 0.343, 0.221, 0.179, 0.289, 0.136, 0.145, 0.16]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4184 | Steps: 2 | Val loss: 0.3161 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8705 | Steps: 2 | Val loss: 0.6798 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4783 | Steps: 2 | Val loss: 0.3762 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4429 | Steps: 2 | Val loss: 0.4535 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=147638)[0m rmse: 0.14158165454864502
[2m[36m(func pid=147638)[0m mae:  0.09142566472291946
[2m[36m(func pid=147638)[0m rmse_per_class: [0.095, 0.241, 0.035, 0.268, 0.056, 0.15, 0.219, 0.115, 0.136, 0.101]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:27:38 (running for 00:36:09.90)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.475 |  0.153 |                   63 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.418 |  0.142 |                   43 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.427 |  0.193 |                   30 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.871 |  0.18  |                   15 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.15302674472332
[2m[36m(func pid=142739)[0m mae:  0.10844401270151138
[2m[36m(func pid=142739)[0m rmse_per_class: [0.107, 0.235, 0.048, 0.298, 0.055, 0.179, 0.249, 0.128, 0.138, 0.094]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17966313660144806
[2m[36m(func pid=153954)[0m mae:  0.1319577991962433
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.294, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.18865209817886353
[2m[36m(func pid=150355)[0m mae:  0.10540811717510223
[2m[36m(func pid=150355)[0m rmse_per_class: [0.087, 0.35, 0.034, 0.354, 0.21, 0.176, 0.246, 0.13, 0.153, 0.146]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3921 | Steps: 2 | Val loss: 0.3265 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4785 | Steps: 2 | Val loss: 0.3748 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8686 | Steps: 2 | Val loss: 0.6775 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4243 | Steps: 2 | Val loss: 0.4682 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 07:27:43 (running for 00:36:15.13)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.478 |  0.153 |                   64 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.392 |  0.143 |                   44 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.443 |  0.189 |                   31 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.87  |  0.18  |                   16 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.14345256984233856
[2m[36m(func pid=147638)[0m mae:  0.09319630265235901
[2m[36m(func pid=147638)[0m rmse_per_class: [0.1, 0.24, 0.034, 0.269, 0.056, 0.149, 0.227, 0.114, 0.139, 0.105]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.1525058001279831
[2m[36m(func pid=142739)[0m mae:  0.10788683593273163
[2m[36m(func pid=142739)[0m rmse_per_class: [0.106, 0.234, 0.048, 0.298, 0.055, 0.178, 0.248, 0.127, 0.137, 0.093]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17958596348762512
[2m[36m(func pid=153954)[0m mae:  0.13188575208187103
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.18910416960716248
[2m[36m(func pid=150355)[0m mae:  0.10838355869054794
[2m[36m(func pid=150355)[0m rmse_per_class: [0.085, 0.355, 0.033, 0.361, 0.186, 0.188, 0.261, 0.134, 0.159, 0.129]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4007 | Steps: 2 | Val loss: 0.3350 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4716 | Steps: 2 | Val loss: 0.3735 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8648 | Steps: 2 | Val loss: 0.6758 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4193 | Steps: 2 | Val loss: 0.4845 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 07:27:48 (running for 00:36:20.16)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.479 |  0.153 |                   65 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.401 |  0.146 |                   45 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.424 |  0.189 |                   32 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.869 |  0.18  |                   17 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.1456110179424286
[2m[36m(func pid=147638)[0m mae:  0.09506667405366898
[2m[36m(func pid=147638)[0m rmse_per_class: [0.104, 0.24, 0.033, 0.27, 0.056, 0.15, 0.237, 0.114, 0.144, 0.109]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.15211956202983856
[2m[36m(func pid=142739)[0m mae:  0.10744927078485489
[2m[36m(func pid=142739)[0m rmse_per_class: [0.105, 0.234, 0.047, 0.298, 0.055, 0.178, 0.247, 0.127, 0.137, 0.093]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17954231798648834
[2m[36m(func pid=153954)[0m mae:  0.13184303045272827
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.18928363919258118
[2m[36m(func pid=150355)[0m mae:  0.11055729538202286
[2m[36m(func pid=150355)[0m rmse_per_class: [0.085, 0.34, 0.028, 0.357, 0.165, 0.202, 0.288, 0.14, 0.164, 0.124]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4309 | Steps: 2 | Val loss: 0.3420 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4693 | Steps: 2 | Val loss: 0.3727 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8624 | Steps: 2 | Val loss: 0.6744 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=147638)[0m rmse: 0.14783890545368195
[2m[36m(func pid=147638)[0m mae:  0.09691863507032394
[2m[36m(func pid=147638)[0m rmse_per_class: [0.107, 0.24, 0.031, 0.271, 0.056, 0.15, 0.246, 0.114, 0.149, 0.114]
== Status ==
Current time: 2024-01-07 07:27:53 (running for 00:36:25.27)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.472 |  0.152 |                   66 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.431 |  0.148 |                   46 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.419 |  0.189 |                   33 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.865 |  0.18  |                   18 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4383 | Steps: 2 | Val loss: 0.4943 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=142739)[0m rmse: 0.15159140527248383
[2m[36m(func pid=142739)[0m mae:  0.10687843710184097
[2m[36m(func pid=142739)[0m rmse_per_class: [0.105, 0.233, 0.046, 0.297, 0.055, 0.178, 0.246, 0.127, 0.137, 0.092]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17953479290008545
[2m[36m(func pid=153954)[0m mae:  0.131827712059021
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.18760469555854797
[2m[36m(func pid=150355)[0m mae:  0.11098073422908783
[2m[36m(func pid=150355)[0m rmse_per_class: [0.085, 0.321, 0.027, 0.345, 0.144, 0.213, 0.303, 0.145, 0.169, 0.124]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4203 | Steps: 2 | Val loss: 0.3474 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4657 | Steps: 2 | Val loss: 0.3710 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8569 | Steps: 2 | Val loss: 0.6731 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 07:27:58 (running for 00:36:30.33)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.469 |  0.152 |                   67 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.42  |  0.15  |                   47 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.438 |  0.188 |                   34 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.862 |  0.18  |                   19 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.1502668410539627
[2m[36m(func pid=147638)[0m mae:  0.0987323522567749
[2m[36m(func pid=147638)[0m rmse_per_class: [0.109, 0.241, 0.03, 0.273, 0.056, 0.151, 0.257, 0.114, 0.153, 0.119]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.15115566551685333
[2m[36m(func pid=142739)[0m mae:  0.1064213365316391
[2m[36m(func pid=142739)[0m rmse_per_class: [0.104, 0.233, 0.046, 0.297, 0.054, 0.178, 0.245, 0.126, 0.137, 0.092]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4690 | Steps: 2 | Val loss: 0.4943 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=153954)[0m rmse: 0.17955051362514496
[2m[36m(func pid=153954)[0m mae:  0.13182185590267181
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.111, 0.19, 0.293, 0.141, 0.142, 0.107]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.18585510551929474
[2m[36m(func pid=150355)[0m mae:  0.11047879606485367
[2m[36m(func pid=150355)[0m rmse_per_class: [0.088, 0.306, 0.03, 0.338, 0.128, 0.213, 0.305, 0.151, 0.175, 0.124]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4208 | Steps: 2 | Val loss: 0.3496 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4628 | Steps: 2 | Val loss: 0.3693 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8602 | Steps: 2 | Val loss: 0.6714 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 07:28:04 (running for 00:36:35.59)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.466 |  0.151 |                   68 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.421 |  0.153 |                   48 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.469 |  0.186 |                   35 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.857 |  0.18  |                   20 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.15251949429512024
[2m[36m(func pid=147638)[0m mae:  0.1001935601234436
[2m[36m(func pid=147638)[0m rmse_per_class: [0.11, 0.242, 0.029, 0.273, 0.056, 0.152, 0.267, 0.115, 0.156, 0.126]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4821 | Steps: 2 | Val loss: 0.4779 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=142739)[0m rmse: 0.15054838359355927
[2m[36m(func pid=142739)[0m mae:  0.1057848334312439
[2m[36m(func pid=142739)[0m rmse_per_class: [0.103, 0.232, 0.046, 0.295, 0.054, 0.177, 0.243, 0.126, 0.137, 0.091]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17958080768585205
[2m[36m(func pid=153954)[0m mae:  0.1318562924861908
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4113 | Steps: 2 | Val loss: 0.3485 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=150355)[0m rmse: 0.18487468361854553
[2m[36m(func pid=150355)[0m mae:  0.10914069414138794
[2m[36m(func pid=150355)[0m rmse_per_class: [0.095, 0.3, 0.035, 0.335, 0.112, 0.201, 0.295, 0.164, 0.184, 0.128]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4614 | Steps: 2 | Val loss: 0.3676 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8551 | Steps: 2 | Val loss: 0.6693 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 07:28:09 (running for 00:36:40.68)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.463 |  0.151 |                   69 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.411 |  0.155 |                   49 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.482 |  0.185 |                   36 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.86  |  0.18  |                   21 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.15465296804904938
[2m[36m(func pid=147638)[0m mae:  0.10142195224761963
[2m[36m(func pid=147638)[0m rmse_per_class: [0.109, 0.243, 0.027, 0.273, 0.056, 0.153, 0.275, 0.116, 0.159, 0.133]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.15000441670417786
[2m[36m(func pid=142739)[0m mae:  0.10518469661474228
[2m[36m(func pid=142739)[0m rmse_per_class: [0.102, 0.231, 0.045, 0.294, 0.054, 0.177, 0.242, 0.126, 0.136, 0.091]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4563 | Steps: 2 | Val loss: 0.4587 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=153954)[0m rmse: 0.17958450317382812
[2m[36m(func pid=153954)[0m mae:  0.13186705112457275
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4067 | Steps: 2 | Val loss: 0.3464 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=150355)[0m rmse: 0.18447181582450867
[2m[36m(func pid=150355)[0m mae:  0.10723482072353363
[2m[36m(func pid=150355)[0m rmse_per_class: [0.103, 0.301, 0.04, 0.333, 0.102, 0.186, 0.274, 0.178, 0.193, 0.135]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4595 | Steps: 2 | Val loss: 0.3650 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8532 | Steps: 2 | Val loss: 0.6674 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 07:28:14 (running for 00:36:45.79)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.461 |  0.15  |                   70 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.407 |  0.157 |                   50 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.456 |  0.184 |                   37 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.855 |  0.18  |                   22 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=147638)[0m rmse: 0.15733537077903748
[2m[36m(func pid=147638)[0m mae:  0.10277275741100311
[2m[36m(func pid=147638)[0m rmse_per_class: [0.108, 0.245, 0.026, 0.275, 0.056, 0.154, 0.282, 0.119, 0.162, 0.145]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.149640291929245
[2m[36m(func pid=142739)[0m mae:  0.10477211326360703
[2m[36m(func pid=142739)[0m rmse_per_class: [0.102, 0.231, 0.045, 0.294, 0.054, 0.177, 0.241, 0.126, 0.136, 0.091]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4401 | Steps: 2 | Val loss: 0.4462 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=153954)[0m rmse: 0.17955553531646729
[2m[36m(func pid=153954)[0m mae:  0.13183990120887756
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4175 | Steps: 2 | Val loss: 0.3438 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=150355)[0m rmse: 0.1831001192331314
[2m[36m(func pid=150355)[0m mae:  0.10444042831659317
[2m[36m(func pid=150355)[0m rmse_per_class: [0.109, 0.303, 0.043, 0.332, 0.099, 0.173, 0.249, 0.184, 0.194, 0.145]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4498 | Steps: 2 | Val loss: 0.3624 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.8500 | Steps: 2 | Val loss: 0.6651 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=147638)[0m rmse: 0.16007523238658905
[2m[36m(func pid=147638)[0m mae:  0.10413078218698502
[2m[36m(func pid=147638)[0m rmse_per_class: [0.107, 0.249, 0.026, 0.276, 0.056, 0.154, 0.289, 0.123, 0.164, 0.157]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:28:19 (running for 00:36:51.39)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.45  |  0.149 |                   72 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.417 |  0.16  |                   51 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.44  |  0.183 |                   38 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.853 |  0.18  |                   23 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14917981624603271
[2m[36m(func pid=142739)[0m mae:  0.10427138954401016
[2m[36m(func pid=142739)[0m rmse_per_class: [0.101, 0.23, 0.044, 0.293, 0.054, 0.176, 0.241, 0.125, 0.136, 0.09]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3867 | Steps: 2 | Val loss: 0.4490 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=153954)[0m rmse: 0.17952248454093933
[2m[36m(func pid=153954)[0m mae:  0.13181164860725403
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3942 | Steps: 2 | Val loss: 0.3404 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=150355)[0m rmse: 0.18384937942028046
[2m[36m(func pid=150355)[0m mae:  0.10324933379888535
[2m[36m(func pid=150355)[0m rmse_per_class: [0.112, 0.304, 0.044, 0.331, 0.097, 0.175, 0.252, 0.18, 0.186, 0.158]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4523 | Steps: 2 | Val loss: 0.3597 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.8471 | Steps: 2 | Val loss: 0.6641 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=147638)[0m rmse: 0.1626940220594406
[2m[36m(func pid=147638)[0m mae:  0.1049826368689537
[2m[36m(func pid=147638)[0m rmse_per_class: [0.104, 0.253, 0.025, 0.278, 0.056, 0.156, 0.293, 0.126, 0.165, 0.171]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:28:25 (running for 00:36:56.57)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.452 |  0.149 |                   73 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.394 |  0.163 |                   52 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.387 |  0.184 |                   39 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.85  |  0.18  |                   24 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14871063828468323
[2m[36m(func pid=142739)[0m mae:  0.10370208323001862
[2m[36m(func pid=142739)[0m rmse_per_class: [0.1, 0.23, 0.044, 0.293, 0.054, 0.176, 0.24, 0.125, 0.136, 0.09]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3909 | Steps: 2 | Val loss: 0.4653 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=153954)[0m rmse: 0.17949141561985016
[2m[36m(func pid=153954)[0m mae:  0.13178454339504242
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3833 | Steps: 2 | Val loss: 0.3362 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=150355)[0m rmse: 0.1889725625514984
[2m[36m(func pid=150355)[0m mae:  0.10570810735225677
[2m[36m(func pid=150355)[0m rmse_per_class: [0.109, 0.304, 0.045, 0.331, 0.093, 0.19, 0.297, 0.169, 0.178, 0.173]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4518 | Steps: 2 | Val loss: 0.3574 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.8464 | Steps: 2 | Val loss: 0.6620 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=147638)[0m rmse: 0.16527043282985687
[2m[36m(func pid=147638)[0m mae:  0.105830118060112
[2m[36m(func pid=147638)[0m rmse_per_class: [0.102, 0.258, 0.025, 0.28, 0.056, 0.157, 0.297, 0.129, 0.162, 0.187]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:28:30 (running for 00:37:01.75)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.452 |  0.148 |                   74 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.383 |  0.165 |                   53 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.391 |  0.189 |                   40 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.847 |  0.179 |                   25 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.1483469307422638
[2m[36m(func pid=142739)[0m mae:  0.10321688652038574
[2m[36m(func pid=142739)[0m rmse_per_class: [0.1, 0.229, 0.044, 0.293, 0.054, 0.175, 0.239, 0.125, 0.135, 0.089]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4390 | Steps: 2 | Val loss: 0.4847 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=153954)[0m rmse: 0.1794937551021576
[2m[36m(func pid=153954)[0m mae:  0.13177961111068726
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.337, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3925 | Steps: 2 | Val loss: 0.3347 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=150355)[0m rmse: 0.194545179605484
[2m[36m(func pid=150355)[0m mae:  0.10945983976125717
[2m[36m(func pid=150355)[0m rmse_per_class: [0.106, 0.306, 0.045, 0.335, 0.084, 0.204, 0.35, 0.159, 0.17, 0.186]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4476 | Steps: 2 | Val loss: 0.3546 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.8446 | Steps: 2 | Val loss: 0.6606 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=147638)[0m rmse: 0.16845154762268066
[2m[36m(func pid=147638)[0m mae:  0.10692892968654633
[2m[36m(func pid=147638)[0m rmse_per_class: [0.101, 0.265, 0.025, 0.284, 0.056, 0.158, 0.299, 0.133, 0.159, 0.204]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:28:35 (running for 00:37:06.86)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.448 |  0.148 |                   75 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.393 |  0.168 |                   54 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.439 |  0.195 |                   41 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.846 |  0.179 |                   26 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14799058437347412
[2m[36m(func pid=142739)[0m mae:  0.10284192860126495
[2m[36m(func pid=142739)[0m rmse_per_class: [0.099, 0.229, 0.043, 0.293, 0.054, 0.175, 0.238, 0.125, 0.135, 0.089]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17946091294288635
[2m[36m(func pid=153954)[0m mae:  0.13176073133945465
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3799 | Steps: 2 | Val loss: 0.5017 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3556 | Steps: 2 | Val loss: 0.3324 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.4511 | Steps: 2 | Val loss: 0.3519 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=150355)[0m rmse: 0.1995707005262375
[2m[36m(func pid=150355)[0m mae:  0.1125364750623703
[2m[36m(func pid=150355)[0m rmse_per_class: [0.104, 0.311, 0.043, 0.339, 0.074, 0.212, 0.382, 0.151, 0.168, 0.211]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.8410 | Steps: 2 | Val loss: 0.6587 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=147638)[0m rmse: 0.17078687250614166
[2m[36m(func pid=147638)[0m mae:  0.10733838379383087
[2m[36m(func pid=147638)[0m rmse_per_class: [0.098, 0.27, 0.026, 0.287, 0.056, 0.158, 0.3, 0.137, 0.157, 0.219]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:28:40 (running for 00:37:12.19)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.451 |  0.148 |                   76 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.356 |  0.171 |                   55 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.38  |  0.2   |                   42 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.845 |  0.179 |                   27 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14770276844501495
[2m[36m(func pid=142739)[0m mae:  0.10248719155788422
[2m[36m(func pid=142739)[0m rmse_per_class: [0.099, 0.229, 0.043, 0.293, 0.055, 0.175, 0.237, 0.125, 0.135, 0.088]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.1794837862253189
[2m[36m(func pid=153954)[0m mae:  0.13177867233753204
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3861 | Steps: 2 | Val loss: 0.5212 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3485 | Steps: 2 | Val loss: 0.3322 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=150355)[0m rmse: 0.20346109569072723
[2m[36m(func pid=150355)[0m mae:  0.1145181655883789
[2m[36m(func pid=150355)[0m rmse_per_class: [0.101, 0.317, 0.039, 0.347, 0.065, 0.215, 0.392, 0.148, 0.167, 0.244]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.4423 | Steps: 2 | Val loss: 0.3490 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.8377 | Steps: 2 | Val loss: 0.6568 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=147638)[0m rmse: 0.1733592450618744
[2m[36m(func pid=147638)[0m mae:  0.10783173888921738
[2m[36m(func pid=147638)[0m rmse_per_class: [0.095, 0.276, 0.027, 0.29, 0.056, 0.159, 0.299, 0.141, 0.152, 0.238]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:28:45 (running for 00:37:17.48)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.442 |  0.147 |                   77 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.349 |  0.173 |                   56 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.386 |  0.203 |                   43 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.841 |  0.179 |                   28 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14730039238929749
[2m[36m(func pid=142739)[0m mae:  0.1019982099533081
[2m[36m(func pid=142739)[0m rmse_per_class: [0.098, 0.228, 0.043, 0.293, 0.055, 0.174, 0.236, 0.124, 0.135, 0.088]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17949150502681732
[2m[36m(func pid=153954)[0m mae:  0.1317773461341858
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3976 | Steps: 2 | Val loss: 0.5420 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3503 | Steps: 2 | Val loss: 0.3341 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=150355)[0m rmse: 0.20638203620910645
[2m[36m(func pid=150355)[0m mae:  0.11582218110561371
[2m[36m(func pid=150355)[0m rmse_per_class: [0.099, 0.323, 0.031, 0.356, 0.061, 0.212, 0.385, 0.149, 0.168, 0.28]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.8366 | Steps: 2 | Val loss: 0.6545 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.4356 | Steps: 2 | Val loss: 0.3449 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=147638)[0m rmse: 0.17586752772331238
[2m[36m(func pid=147638)[0m mae:  0.10833851248025894
[2m[36m(func pid=147638)[0m rmse_per_class: [0.091, 0.281, 0.028, 0.291, 0.056, 0.159, 0.296, 0.148, 0.151, 0.257]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:28:51 (running for 00:37:22.78)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.442 |  0.147 |                   77 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.35  |  0.176 |                   57 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.398 |  0.206 |                   44 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.837 |  0.18  |                   30 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17951568961143494
[2m[36m(func pid=153954)[0m mae:  0.13179554045200348
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.14697562158107758
[2m[36m(func pid=142739)[0m mae:  0.10162146389484406
[2m[36m(func pid=142739)[0m rmse_per_class: [0.097, 0.228, 0.042, 0.292, 0.055, 0.174, 0.236, 0.124, 0.135, 0.087]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3921 | Steps: 2 | Val loss: 0.5643 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3444 | Steps: 2 | Val loss: 0.3372 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.8325 | Steps: 2 | Val loss: 0.6517 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.4424 | Steps: 2 | Val loss: 0.3411 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=150355)[0m rmse: 0.20882479846477509
[2m[36m(func pid=150355)[0m mae:  0.11666037887334824
[2m[36m(func pid=150355)[0m rmse_per_class: [0.096, 0.328, 0.031, 0.365, 0.06, 0.206, 0.364, 0.151, 0.167, 0.321]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.17810410261154175
[2m[36m(func pid=147638)[0m mae:  0.10875338315963745
[2m[36m(func pid=147638)[0m rmse_per_class: [0.09, 0.285, 0.03, 0.294, 0.056, 0.159, 0.293, 0.152, 0.149, 0.273]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:28:56 (running for 00:37:27.98)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.436 |  0.147 |                   78 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.344 |  0.178 |                   58 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.392 |  0.209 |                   45 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.833 |  0.179 |                   31 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17949964106082916
[2m[36m(func pid=153954)[0m mae:  0.13178230822086334
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.14641667902469635
[2m[36m(func pid=142739)[0m mae:  0.10092612355947495
[2m[36m(func pid=142739)[0m rmse_per_class: [0.096, 0.229, 0.042, 0.29, 0.055, 0.173, 0.234, 0.124, 0.134, 0.087]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3902 | Steps: 2 | Val loss: 0.5844 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3329 | Steps: 2 | Val loss: 0.3393 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.4275 | Steps: 2 | Val loss: 0.3379 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.8301 | Steps: 2 | Val loss: 0.6502 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=150355)[0m rmse: 0.20971600711345673
[2m[36m(func pid=150355)[0m mae:  0.11727525293827057
[2m[36m(func pid=150355)[0m rmse_per_class: [0.094, 0.331, 0.032, 0.369, 0.059, 0.195, 0.343, 0.154, 0.162, 0.358]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.17931029200553894
[2m[36m(func pid=147638)[0m mae:  0.10868369042873383
[2m[36m(func pid=147638)[0m rmse_per_class: [0.089, 0.29, 0.032, 0.297, 0.055, 0.159, 0.288, 0.152, 0.147, 0.284]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:29:01 (running for 00:37:33.12)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.428 |  0.146 |                   80 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.333 |  0.179 |                   59 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.39  |  0.21  |                   46 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.833 |  0.179 |                   31 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14607271552085876
[2m[36m(func pid=142739)[0m mae:  0.10048471391201019
[2m[36m(func pid=142739)[0m rmse_per_class: [0.095, 0.228, 0.042, 0.29, 0.055, 0.173, 0.234, 0.124, 0.134, 0.087]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17949488759040833
[2m[36m(func pid=153954)[0m mae:  0.13178160786628723
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4162 | Steps: 2 | Val loss: 0.5965 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3486 | Steps: 2 | Val loss: 0.3437 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.4276 | Steps: 2 | Val loss: 0.3350 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=150355)[0m rmse: 0.20879244804382324
[2m[36m(func pid=150355)[0m mae:  0.11721435934305191
[2m[36m(func pid=150355)[0m rmse_per_class: [0.093, 0.333, 0.032, 0.372, 0.058, 0.185, 0.318, 0.157, 0.158, 0.382]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8280 | Steps: 2 | Val loss: 0.6489 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=147638)[0m rmse: 0.1804925501346588
[2m[36m(func pid=147638)[0m mae:  0.10860941559076309
[2m[36m(func pid=147638)[0m rmse_per_class: [0.088, 0.293, 0.034, 0.3, 0.055, 0.158, 0.28, 0.153, 0.145, 0.299]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:29:06 (running for 00:37:38.25)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.428 |  0.146 |                   81 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.349 |  0.18  |                   60 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.416 |  0.209 |                   47 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.83  |  0.179 |                   32 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.145695760846138
[2m[36m(func pid=142739)[0m mae:  0.10001400858163834
[2m[36m(func pid=142739)[0m rmse_per_class: [0.094, 0.228, 0.042, 0.289, 0.055, 0.173, 0.233, 0.123, 0.134, 0.086]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17946115136146545
[2m[36m(func pid=153954)[0m mae:  0.13176019489765167
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4154 | Steps: 2 | Val loss: 0.5963 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3344 | Steps: 2 | Val loss: 0.3485 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.4205 | Steps: 2 | Val loss: 0.3315 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=150355)[0m rmse: 0.20759406685829163
[2m[36m(func pid=150355)[0m mae:  0.11731996387243271
[2m[36m(func pid=150355)[0m rmse_per_class: [0.094, 0.335, 0.031, 0.374, 0.058, 0.191, 0.299, 0.154, 0.152, 0.388]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8286 | Steps: 2 | Val loss: 0.6471 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=147638)[0m rmse: 0.18094654381275177
[2m[36m(func pid=147638)[0m mae:  0.10817348957061768
[2m[36m(func pid=147638)[0m rmse_per_class: [0.086, 0.295, 0.036, 0.301, 0.055, 0.159, 0.271, 0.153, 0.144, 0.311]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:29:11 (running for 00:37:43.32)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.421 |  0.145 |                   82 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.334 |  0.181 |                   61 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.415 |  0.208 |                   48 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.828 |  0.179 |                   33 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14532046020030975
[2m[36m(func pid=142739)[0m mae:  0.09948930889368057
[2m[36m(func pid=142739)[0m rmse_per_class: [0.093, 0.228, 0.042, 0.288, 0.055, 0.172, 0.233, 0.123, 0.134, 0.086]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17948193848133087
[2m[36m(func pid=153954)[0m mae:  0.1317823976278305
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4209 | Steps: 2 | Val loss: 0.5889 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3306 | Steps: 2 | Val loss: 0.3527 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.4287 | Steps: 2 | Val loss: 0.3275 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=150355)[0m rmse: 0.20572137832641602
[2m[36m(func pid=150355)[0m mae:  0.11698149144649506
[2m[36m(func pid=150355)[0m rmse_per_class: [0.094, 0.333, 0.029, 0.37, 0.056, 0.22, 0.282, 0.15, 0.148, 0.373]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8241 | Steps: 2 | Val loss: 0.6450 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=147638)[0m rmse: 0.1806318759918213
[2m[36m(func pid=147638)[0m mae:  0.10725291818380356
[2m[36m(func pid=147638)[0m rmse_per_class: [0.084, 0.298, 0.038, 0.303, 0.055, 0.159, 0.259, 0.151, 0.142, 0.317]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:29:17 (running for 00:37:48.57)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.429 |  0.145 |                   83 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.331 |  0.181 |                   62 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.421 |  0.206 |                   49 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.829 |  0.179 |                   34 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14506518840789795
[2m[36m(func pid=142739)[0m mae:  0.09912607818841934
[2m[36m(func pid=142739)[0m rmse_per_class: [0.093, 0.228, 0.042, 0.288, 0.055, 0.172, 0.233, 0.123, 0.133, 0.085]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.1794375777244568
[2m[36m(func pid=153954)[0m mae:  0.13174089789390564
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3828 | Steps: 2 | Val loss: 0.5770 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3384 | Steps: 2 | Val loss: 0.3564 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.4155 | Steps: 2 | Val loss: 0.3244 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=150355)[0m rmse: 0.20314034819602966
[2m[36m(func pid=150355)[0m mae:  0.11682046949863434
[2m[36m(func pid=150355)[0m rmse_per_class: [0.092, 0.329, 0.028, 0.364, 0.056, 0.262, 0.273, 0.145, 0.143, 0.339]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.8242 | Steps: 2 | Val loss: 0.6429 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=147638)[0m rmse: 0.17960795760154724
[2m[36m(func pid=147638)[0m mae:  0.10588395595550537
[2m[36m(func pid=147638)[0m rmse_per_class: [0.083, 0.301, 0.039, 0.305, 0.056, 0.158, 0.247, 0.147, 0.141, 0.32]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.14479286968708038
[2m[36m(func pid=142739)[0m mae:  0.0988125205039978
[2m[36m(func pid=142739)[0m rmse_per_class: [0.092, 0.227, 0.042, 0.287, 0.055, 0.171, 0.232, 0.123, 0.133, 0.085]
== Status ==
Current time: 2024-01-07 07:29:22 (running for 00:37:53.76)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.416 |  0.145 |                   84 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.338 |  0.18  |                   63 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.383 |  0.203 |                   50 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.824 |  0.179 |                   35 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.1794135570526123
[2m[36m(func pid=153954)[0m mae:  0.1317184865474701
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3991 | Steps: 2 | Val loss: 0.5603 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3251 | Steps: 2 | Val loss: 0.3581 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.4083 | Steps: 2 | Val loss: 0.3211 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=150355)[0m rmse: 0.1981024295091629
[2m[36m(func pid=150355)[0m mae:  0.11513330787420273
[2m[36m(func pid=150355)[0m rmse_per_class: [0.091, 0.326, 0.027, 0.361, 0.056, 0.283, 0.266, 0.143, 0.14, 0.288]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.8225 | Steps: 2 | Val loss: 0.6415 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=147638)[0m rmse: 0.17733927071094513
[2m[36m(func pid=147638)[0m mae:  0.10365866124629974
[2m[36m(func pid=147638)[0m rmse_per_class: [0.081, 0.303, 0.039, 0.306, 0.06, 0.159, 0.233, 0.139, 0.14, 0.313]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:29:27 (running for 00:37:58.87)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.408 |  0.145 |                   85 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.325 |  0.177 |                   64 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.399 |  0.198 |                   51 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.824 |  0.179 |                   36 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14453060925006866
[2m[36m(func pid=142739)[0m mae:  0.09848107397556305
[2m[36m(func pid=142739)[0m rmse_per_class: [0.091, 0.227, 0.041, 0.287, 0.055, 0.171, 0.231, 0.123, 0.133, 0.085]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.1793772578239441
[2m[36m(func pid=153954)[0m mae:  0.13168254494667053
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4196 | Steps: 2 | Val loss: 0.5511 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3277 | Steps: 2 | Val loss: 0.3605 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.4033 | Steps: 2 | Val loss: 0.3174 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=150355)[0m rmse: 0.1916114091873169
[2m[36m(func pid=150355)[0m mae:  0.1118939146399498
[2m[36m(func pid=150355)[0m rmse_per_class: [0.092, 0.318, 0.026, 0.356, 0.056, 0.269, 0.259, 0.142, 0.139, 0.257]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.8159 | Steps: 2 | Val loss: 0.6399 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=147638)[0m rmse: 0.17580345273017883
[2m[36m(func pid=147638)[0m mae:  0.10176446288824081
[2m[36m(func pid=147638)[0m rmse_per_class: [0.082, 0.304, 0.04, 0.308, 0.066, 0.16, 0.222, 0.132, 0.139, 0.305]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:29:32 (running for 00:38:04.04)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.403 |  0.144 |                   86 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.328 |  0.176 |                   65 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.42  |  0.192 |                   52 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.822 |  0.179 |                   37 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.1441958248615265
[2m[36m(func pid=142739)[0m mae:  0.09806078672409058
[2m[36m(func pid=142739)[0m rmse_per_class: [0.091, 0.227, 0.041, 0.286, 0.055, 0.17, 0.231, 0.123, 0.133, 0.085]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17939849197864532
[2m[36m(func pid=153954)[0m mae:  0.131703719496727
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3801 | Steps: 2 | Val loss: 0.5361 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3404 | Steps: 2 | Val loss: 0.3657 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3992 | Steps: 2 | Val loss: 0.3134 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=150355)[0m rmse: 0.1853853166103363
[2m[36m(func pid=150355)[0m mae:  0.10833625495433807
[2m[36m(func pid=150355)[0m rmse_per_class: [0.099, 0.322, 0.026, 0.353, 0.056, 0.233, 0.258, 0.145, 0.139, 0.222]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8176 | Steps: 2 | Val loss: 0.6375 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=147638)[0m rmse: 0.1760077327489853
[2m[36m(func pid=147638)[0m mae:  0.10111027956008911
[2m[36m(func pid=147638)[0m rmse_per_class: [0.084, 0.305, 0.04, 0.309, 0.074, 0.162, 0.215, 0.126, 0.14, 0.304]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:29:37 (running for 00:38:09.43)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.399 |  0.144 |                   87 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.34  |  0.176 |                   66 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.38  |  0.185 |                   53 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.816 |  0.179 |                   38 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.1439104825258255
[2m[36m(func pid=142739)[0m mae:  0.09774575382471085
[2m[36m(func pid=142739)[0m rmse_per_class: [0.09, 0.227, 0.041, 0.285, 0.055, 0.17, 0.23, 0.122, 0.133, 0.085]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17936988174915314
[2m[36m(func pid=153954)[0m mae:  0.1316790133714676
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.337, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4002 | Steps: 2 | Val loss: 0.5267 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3339 | Steps: 2 | Val loss: 0.3686 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.4011 | Steps: 2 | Val loss: 0.3097 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=150355)[0m rmse: 0.18243959546089172
[2m[36m(func pid=150355)[0m mae:  0.10632266104221344
[2m[36m(func pid=150355)[0m rmse_per_class: [0.116, 0.324, 0.026, 0.35, 0.056, 0.191, 0.259, 0.154, 0.14, 0.209]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.8153 | Steps: 2 | Val loss: 0.6360 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=147638)[0m rmse: 0.1759917438030243
[2m[36m(func pid=147638)[0m mae:  0.10018950700759888
[2m[36m(func pid=147638)[0m rmse_per_class: [0.084, 0.306, 0.041, 0.311, 0.086, 0.165, 0.212, 0.121, 0.139, 0.296]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:29:43 (running for 00:38:14.65)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.401 |  0.144 |                   88 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.334 |  0.176 |                   67 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.4   |  0.182 |                   54 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.818 |  0.179 |                   39 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14364784955978394
[2m[36m(func pid=142739)[0m mae:  0.09735189378261566
[2m[36m(func pid=142739)[0m rmse_per_class: [0.089, 0.227, 0.041, 0.285, 0.055, 0.17, 0.23, 0.122, 0.133, 0.085]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17933176457881927
[2m[36m(func pid=153954)[0m mae:  0.13165585696697235
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3994 | Steps: 2 | Val loss: 0.5233 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3387 | Steps: 2 | Val loss: 0.3706 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3921 | Steps: 2 | Val loss: 0.3064 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=150355)[0m rmse: 0.18452958762645721
[2m[36m(func pid=150355)[0m mae:  0.10735426843166351
[2m[36m(func pid=150355)[0m rmse_per_class: [0.14, 0.327, 0.026, 0.348, 0.056, 0.175, 0.259, 0.169, 0.141, 0.204]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.8119 | Steps: 2 | Val loss: 0.6347 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=147638)[0m rmse: 0.17646117508411407
[2m[36m(func pid=147638)[0m mae:  0.09976216405630112
[2m[36m(func pid=147638)[0m rmse_per_class: [0.08, 0.307, 0.04, 0.312, 0.097, 0.168, 0.216, 0.117, 0.138, 0.289]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:29:48 (running for 00:38:19.92)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.392 |  0.143 |                   89 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.339 |  0.176 |                   68 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.399 |  0.185 |                   55 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.815 |  0.179 |                   40 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14341972768306732
[2m[36m(func pid=142739)[0m mae:  0.09704137593507767
[2m[36m(func pid=142739)[0m rmse_per_class: [0.088, 0.227, 0.041, 0.284, 0.055, 0.169, 0.23, 0.122, 0.133, 0.085]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17933067679405212
[2m[36m(func pid=153954)[0m mae:  0.13165846467018127
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3911 | Steps: 2 | Val loss: 0.5149 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3235 | Steps: 2 | Val loss: 0.3711 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.4130 | Steps: 2 | Val loss: 0.3027 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=150355)[0m rmse: 0.1884618103504181
[2m[36m(func pid=150355)[0m mae:  0.10957854986190796
[2m[36m(func pid=150355)[0m rmse_per_class: [0.154, 0.337, 0.026, 0.348, 0.056, 0.183, 0.258, 0.186, 0.143, 0.194]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.8106 | Steps: 2 | Val loss: 0.6330 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=147638)[0m rmse: 0.1774328202009201
[2m[36m(func pid=147638)[0m mae:  0.09966858476400375
[2m[36m(func pid=147638)[0m rmse_per_class: [0.078, 0.306, 0.04, 0.313, 0.108, 0.171, 0.227, 0.115, 0.136, 0.28]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:29:53 (running for 00:38:25.35)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.413 |  0.143 |                   90 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.323 |  0.177 |                   69 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.391 |  0.188 |                   56 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.812 |  0.179 |                   41 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14320436120033264
[2m[36m(func pid=142739)[0m mae:  0.09674932062625885
[2m[36m(func pid=142739)[0m rmse_per_class: [0.088, 0.228, 0.041, 0.284, 0.055, 0.169, 0.229, 0.122, 0.133, 0.084]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17931099236011505
[2m[36m(func pid=153954)[0m mae:  0.131641685962677
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3867 | Steps: 2 | Val loss: 0.5134 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3371 | Steps: 2 | Val loss: 0.3713 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3867 | Steps: 2 | Val loss: 0.2995 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=147638)[0m rmse: 0.1788979470729828
[2m[36m(func pid=147638)[0m mae:  0.1000799685716629
[2m[36m(func pid=147638)[0m rmse_per_class: [0.077, 0.305, 0.039, 0.314, 0.12, 0.175, 0.241, 0.115, 0.135, 0.268]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.19325369596481323
[2m[36m(func pid=150355)[0m mae:  0.11212725937366486
[2m[36m(func pid=150355)[0m rmse_per_class: [0.157, 0.347, 0.025, 0.35, 0.056, 0.194, 0.261, 0.203, 0.146, 0.193]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8074 | Steps: 2 | Val loss: 0.6317 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 07:29:58 (running for 00:38:30.53)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.387 |  0.143 |                   91 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.337 |  0.179 |                   70 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.387 |  0.193 |                   57 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.811 |  0.179 |                   42 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14289113879203796
[2m[36m(func pid=142739)[0m mae:  0.09642858803272247
[2m[36m(func pid=142739)[0m rmse_per_class: [0.087, 0.228, 0.041, 0.283, 0.055, 0.169, 0.228, 0.122, 0.132, 0.084]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.1793549358844757
[2m[36m(func pid=153954)[0m mae:  0.13166914880275726
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3397 | Steps: 2 | Val loss: 0.3719 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3975 | Steps: 2 | Val loss: 0.5130 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3905 | Steps: 2 | Val loss: 0.2958 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=150355)[0m rmse: 0.1977682113647461
[2m[36m(func pid=150355)[0m mae:  0.11446992307901382
[2m[36m(func pid=150355)[0m rmse_per_class: [0.156, 0.356, 0.026, 0.36, 0.056, 0.202, 0.268, 0.207, 0.153, 0.193]
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8072 | Steps: 2 | Val loss: 0.6298 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=147638)[0m rmse: 0.18126118183135986
[2m[36m(func pid=147638)[0m mae:  0.10124590247869492
[2m[36m(func pid=147638)[0m rmse_per_class: [0.075, 0.304, 0.037, 0.317, 0.133, 0.178, 0.261, 0.116, 0.135, 0.257]
[2m[36m(func pid=147638)[0m 
== Status ==
Current time: 2024-01-07 07:30:04 (running for 00:38:35.88)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.39  |  0.143 |                   92 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.34  |  0.181 |                   71 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.398 |  0.198 |                   58 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.807 |  0.179 |                   43 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14264705777168274
[2m[36m(func pid=142739)[0m mae:  0.09608049690723419
[2m[36m(func pid=142739)[0m rmse_per_class: [0.087, 0.228, 0.041, 0.282, 0.055, 0.169, 0.227, 0.121, 0.132, 0.084]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17935405671596527
[2m[36m(func pid=153954)[0m mae:  0.13166919350624084
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3334 | Steps: 2 | Val loss: 0.3703 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3689 | Steps: 2 | Val loss: 0.5122 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3783 | Steps: 2 | Val loss: 0.2930 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=147638)[0m rmse: 0.18294987082481384
[2m[36m(func pid=147638)[0m mae:  0.10225007683038712
[2m[36m(func pid=147638)[0m rmse_per_class: [0.074, 0.301, 0.036, 0.318, 0.143, 0.181, 0.279, 0.117, 0.134, 0.245]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.8034 | Steps: 2 | Val loss: 0.6276 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=150355)[0m rmse: 0.2016495168209076
[2m[36m(func pid=150355)[0m mae:  0.11612015962600708
[2m[36m(func pid=150355)[0m rmse_per_class: [0.149, 0.363, 0.028, 0.377, 0.057, 0.207, 0.276, 0.204, 0.162, 0.196]
[2m[36m(func pid=150355)[0m 
== Status ==
Current time: 2024-01-07 07:30:09 (running for 00:38:41.15)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.39  |  0.143 |                   92 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.333 |  0.183 |                   72 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.369 |  0.202 |                   59 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.803 |  0.179 |                   45 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17938022315502167
[2m[36m(func pid=153954)[0m mae:  0.13168299198150635
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.14247381687164307
[2m[36m(func pid=142739)[0m mae:  0.09587694704532623
[2m[36m(func pid=142739)[0m rmse_per_class: [0.086, 0.228, 0.041, 0.282, 0.055, 0.168, 0.227, 0.121, 0.132, 0.084]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3281 | Steps: 2 | Val loss: 0.3678 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3863 | Steps: 2 | Val loss: 0.5122 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.7992 | Steps: 2 | Val loss: 0.6261 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3717 | Steps: 2 | Val loss: 0.2903 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=147638)[0m rmse: 0.18431465327739716
[2m[36m(func pid=147638)[0m mae:  0.10336162894964218
[2m[36m(func pid=147638)[0m rmse_per_class: [0.073, 0.299, 0.034, 0.319, 0.155, 0.184, 0.295, 0.12, 0.134, 0.231]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.20536136627197266
[2m[36m(func pid=150355)[0m mae:  0.1172683984041214
[2m[36m(func pid=150355)[0m rmse_per_class: [0.145, 0.363, 0.029, 0.391, 0.059, 0.207, 0.281, 0.194, 0.179, 0.206]
[2m[36m(func pid=150355)[0m 
== Status ==
Current time: 2024-01-07 07:30:14 (running for 00:38:46.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.372 |  0.142 |                   94 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.328 |  0.184 |                   73 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.386 |  0.205 |                   60 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.803 |  0.179 |                   45 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.1423792541027069
[2m[36m(func pid=142739)[0m mae:  0.09572532027959824
[2m[36m(func pid=142739)[0m rmse_per_class: [0.086, 0.228, 0.041, 0.283, 0.055, 0.167, 0.227, 0.121, 0.132, 0.083]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17933854460716248
[2m[36m(func pid=153954)[0m mae:  0.13164761662483215
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3529 | Steps: 2 | Val loss: 0.3654 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3687 | Steps: 2 | Val loss: 0.5132 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=147638)[0m rmse: 0.18510571122169495
[2m[36m(func pid=147638)[0m mae:  0.10446027666330338
[2m[36m(func pid=147638)[0m rmse_per_class: [0.073, 0.298, 0.032, 0.323, 0.162, 0.187, 0.311, 0.123, 0.133, 0.21]
[2m[36m(func pid=147638)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3698 | Steps: 2 | Val loss: 0.2876 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.7994 | Steps: 2 | Val loss: 0.6247 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=150355)[0m rmse: 0.20694024860858917
[2m[36m(func pid=150355)[0m mae:  0.11696101725101471
[2m[36m(func pid=150355)[0m rmse_per_class: [0.14, 0.357, 0.03, 0.398, 0.061, 0.207, 0.284, 0.185, 0.191, 0.218]
[2m[36m(func pid=150355)[0m 
== Status ==
Current time: 2024-01-07 07:30:20 (running for 00:38:51.75)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15050000324845314
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.37  |  0.142 |                   95 |
| train_ccef6_00018 | RUNNING    | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.353 |  0.185 |                   74 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.369 |  0.207 |                   61 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.799 |  0.179 |                   46 |
| train_ccef6_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14221295714378357
[2m[36m(func pid=142739)[0m mae:  0.09564367681741714
[2m[36m(func pid=142739)[0m rmse_per_class: [0.086, 0.228, 0.041, 0.283, 0.055, 0.167, 0.226, 0.121, 0.132, 0.083]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17932502925395966
[2m[36m(func pid=153954)[0m mae:  0.1316395103931427
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=147638)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3430 | Steps: 2 | Val loss: 0.3627 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3792 | Steps: 2 | Val loss: 0.5109 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=147638)[0m rmse: 0.18517586588859558
[2m[36m(func pid=147638)[0m mae:  0.10493671894073486
[2m[36m(func pid=147638)[0m rmse_per_class: [0.072, 0.295, 0.031, 0.323, 0.164, 0.188, 0.321, 0.125, 0.133, 0.201]
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3638 | Steps: 2 | Val loss: 0.2851 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=150355)[0m rmse: 0.2068943977355957
[2m[36m(func pid=150355)[0m mae:  0.11569911241531372
[2m[36m(func pid=150355)[0m rmse_per_class: [0.137, 0.347, 0.03, 0.397, 0.066, 0.204, 0.29, 0.171, 0.204, 0.224]
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7958 | Steps: 2 | Val loss: 0.6232 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=142739)[0m rmse: 0.14214302599430084
[2m[36m(func pid=142739)[0m mae:  0.09548001736402512
[2m[36m(func pid=142739)[0m rmse_per_class: [0.086, 0.229, 0.041, 0.283, 0.055, 0.166, 0.226, 0.121, 0.132, 0.083]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.1793164163827896
[2m[36m(func pid=153954)[0m mae:  0.13162891566753387
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3750 | Steps: 2 | Val loss: 0.5087 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3663 | Steps: 2 | Val loss: 0.2826 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=150355)[0m rmse: 0.20620135962963104
[2m[36m(func pid=150355)[0m mae:  0.11399552971124649
[2m[36m(func pid=150355)[0m rmse_per_class: [0.139, 0.336, 0.03, 0.386, 0.077, 0.198, 0.302, 0.158, 0.211, 0.225]
[2m[36m(func pid=142739)[0m rmse: 0.14198528230190277
[2m[36m(func pid=142739)[0m mae:  0.09521877765655518
[2m[36m(func pid=142739)[0m rmse_per_class: [0.085, 0.229, 0.041, 0.283, 0.055, 0.166, 0.225, 0.121, 0.132, 0.083]
== Status ==
Current time: 2024-01-07 07:30:25 (running for 00:38:56.98)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.364 |  0.142 |                   96 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.379 |  0.207 |                   62 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.799 |  0.179 |                   47 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 07:30:30 (running for 00:39:02.56)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.364 |  0.142 |                   96 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.379 |  0.207 |                   62 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.796 |  0.179 |                   48 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=164771)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=164771)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=164771)[0m Configuration completed!
[2m[36m(func pid=164771)[0m New optimizer parameters:
[2m[36m(func pid=164771)[0m SGD (
[2m[36m(func pid=164771)[0m Parameter Group 0
[2m[36m(func pid=164771)[0m     dampening: 0
[2m[36m(func pid=164771)[0m     differentiable: False
[2m[36m(func pid=164771)[0m     foreach: None
[2m[36m(func pid=164771)[0m     lr: 0.001
[2m[36m(func pid=164771)[0m     maximize: False
[2m[36m(func pid=164771)[0m     momentum: 0.9
[2m[36m(func pid=164771)[0m     nesterov: False
[2m[36m(func pid=164771)[0m     weight_decay: 1e-05
[2m[36m(func pid=164771)[0m )
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3596 | Steps: 2 | Val loss: 0.2805 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3612 | Steps: 2 | Val loss: 0.5061 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7954 | Steps: 2 | Val loss: 0.6218 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8967 | Steps: 2 | Val loss: 0.7075 | Batch size: 32 | lr: 0.001 | Duration: 4.72s
== Status ==
Current time: 2024-01-07 07:30:36 (running for 00:39:07.59)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.366 |  0.142 |                   97 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.375 |  0.206 |                   63 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.796 |  0.179 |                   48 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14186401665210724
[2m[36m(func pid=142739)[0m mae:  0.09509813785552979
[2m[36m(func pid=142739)[0m rmse_per_class: [0.085, 0.229, 0.041, 0.284, 0.055, 0.165, 0.225, 0.121, 0.132, 0.083]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17932075262069702
[2m[36m(func pid=153954)[0m mae:  0.13163387775421143
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.20578674972057343
[2m[36m(func pid=150355)[0m mae:  0.11301616579294205
[2m[36m(func pid=150355)[0m rmse_per_class: [0.143, 0.326, 0.029, 0.372, 0.093, 0.191, 0.324, 0.148, 0.214, 0.218]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.18267600238323212
[2m[36m(func pid=164771)[0m mae:  0.13440671563148499
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3567 | Steps: 2 | Val loss: 0.2780 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7924 | Steps: 2 | Val loss: 0.6206 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3599 | Steps: 2 | Val loss: 0.5075 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8947 | Steps: 2 | Val loss: 0.6988 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 07:30:41 (running for 00:39:13.04)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00017 | RUNNING    | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.357 |  0.142 |                   99 |
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.361 |  0.206 |                   64 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.795 |  0.179 |                   49 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.897 |  0.183 |                    1 |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=142739)[0m rmse: 0.14170530438423157
[2m[36m(func pid=142739)[0m mae:  0.09492761641740799
[2m[36m(func pid=142739)[0m rmse_per_class: [0.085, 0.23, 0.04, 0.283, 0.055, 0.164, 0.224, 0.12, 0.132, 0.083]
[2m[36m(func pid=142739)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17933477461338043
[2m[36m(func pid=153954)[0m mae:  0.13164235651493073
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m rmse: 0.2062959223985672
[2m[36m(func pid=150355)[0m mae:  0.11317773908376694
[2m[36m(func pid=150355)[0m rmse_per_class: [0.15, 0.317, 0.029, 0.36, 0.105, 0.187, 0.348, 0.142, 0.211, 0.213]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.18252021074295044
[2m[36m(func pid=164771)[0m mae:  0.13433592021465302
[2m[36m(func pid=164771)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.143, 0.113]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=142739)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3476 | Steps: 2 | Val loss: 0.2760 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7890 | Steps: 2 | Val loss: 0.6185 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3630 | Steps: 2 | Val loss: 0.5115 | Batch size: 32 | lr: 0.1 | Duration: 3.28s
[2m[36m(func pid=142739)[0m rmse: 0.14160533249378204
[2m[36m(func pid=142739)[0m mae:  0.09480829536914825
[2m[36m(func pid=142739)[0m rmse_per_class: [0.085, 0.23, 0.04, 0.283, 0.055, 0.164, 0.224, 0.12, 0.132, 0.083]
== Status ==
Current time: 2024-01-07 07:30:46 (running for 00:39:18.45)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 3 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.36  |  0.206 |                   65 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.792 |  0.179 |                   50 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.895 |  0.183 |                    2 |
| train_ccef6_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17931528389453888
[2m[36m(func pid=153954)[0m mae:  0.13162873685359955
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8834 | Steps: 2 | Val loss: 0.6881 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=150355)[0m rmse: 0.2065364420413971
[2m[36m(func pid=150355)[0m mae:  0.11378679424524307
[2m[36m(func pid=150355)[0m rmse_per_class: [0.16, 0.313, 0.029, 0.356, 0.113, 0.19, 0.359, 0.138, 0.201, 0.207]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.18218228220939636
[2m[36m(func pid=164771)[0m mae:  0.13409391045570374
[2m[36m(func pid=164771)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7895 | Steps: 2 | Val loss: 0.6163 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3638 | Steps: 2 | Val loss: 0.5083 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=153954)[0m rmse: 0.17924486100673676
[2m[36m(func pid=153954)[0m mae:  0.13157811760902405
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.107]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8731 | Steps: 2 | Val loss: 0.6758 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=150355)[0m rmse: 0.2059188187122345
[2m[36m(func pid=150355)[0m mae:  0.1143440455198288
[2m[36m(func pid=150355)[0m rmse_per_class: [0.176, 0.314, 0.03, 0.358, 0.12, 0.206, 0.351, 0.135, 0.188, 0.182]
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.7882 | Steps: 2 | Val loss: 0.6152 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=164771)[0m rmse: 0.18179716169834137
[2m[36m(func pid=164771)[0m mae:  0.13379907608032227
[2m[36m(func pid=164771)[0m rmse_per_class: [0.117, 0.266, 0.105, 0.339, 0.112, 0.19, 0.294, 0.142, 0.143, 0.112]
== Status ==
Current time: 2024-01-07 07:30:52 (running for 00:39:23.58)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.363 |  0.207 |                   66 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.79  |  0.179 |                   52 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.883 |  0.182 |                    3 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=165914)[0m Configuration completed!
[2m[36m(func pid=165914)[0m New optimizer parameters:
[2m[36m(func pid=165914)[0m SGD (
[2m[36m(func pid=165914)[0m Parameter Group 0
[2m[36m(func pid=165914)[0m     dampening: 0
[2m[36m(func pid=165914)[0m     differentiable: False
[2m[36m(func pid=165914)[0m     foreach: None
[2m[36m(func pid=165914)[0m     lr: 0.01
[2m[36m(func pid=165914)[0m     maximize: False
[2m[36m(func pid=165914)[0m     momentum: 0.9
[2m[36m(func pid=165914)[0m     nesterov: False
[2m[36m(func pid=165914)[0m     weight_decay: 1e-05
[2m[36m(func pid=165914)[0m )
[2m[36m(func pid=165914)[0m 
== Status ==
Current time: 2024-01-07 07:30:57 (running for 00:39:28.89)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.364 |  0.206 |                   67 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.788 |  0.179 |                   53 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.873 |  0.182 |                    4 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.1792331039905548
[2m[36m(func pid=153954)[0m mae:  0.13157165050506592
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3322 | Steps: 2 | Val loss: 0.5185 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8562 | Steps: 2 | Val loss: 0.6631 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8944 | Steps: 2 | Val loss: 0.6862 | Batch size: 32 | lr: 0.01 | Duration: 4.62s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.7854 | Steps: 2 | Val loss: 0.6132 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=150355)[0m rmse: 0.20687386393547058
[2m[36m(func pid=150355)[0m mae:  0.11574150621891022
[2m[36m(func pid=150355)[0m rmse_per_class: [0.202, 0.313, 0.03, 0.359, 0.124, 0.228, 0.325, 0.133, 0.176, 0.18]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.18134655058383942
[2m[36m(func pid=164771)[0m mae:  0.13342756032943726
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.111, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.1827126145362854
[2m[36m(func pid=165914)[0m mae:  0.13444390892982483
[2m[36m(func pid=165914)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.11, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=165914)[0m 
== Status ==
Current time: 2024-01-07 07:31:02 (running for 00:39:34.01)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.332 |  0.207 |                   68 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.785 |  0.179 |                   54 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.856 |  0.181 |                    5 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.894 |  0.183 |                    1 |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17916862666606903
[2m[36m(func pid=153954)[0m mae:  0.13151654601097107
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3839 | Steps: 2 | Val loss: 0.5256 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8442 | Steps: 2 | Val loss: 0.6498 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8457 | Steps: 2 | Val loss: 0.6344 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.7852 | Steps: 2 | Val loss: 0.6114 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=150355)[0m rmse: 0.2076515257358551
[2m[36m(func pid=150355)[0m mae:  0.117425337433815
[2m[36m(func pid=150355)[0m rmse_per_class: [0.233, 0.312, 0.03, 0.36, 0.123, 0.247, 0.298, 0.131, 0.165, 0.177]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.18096956610679626
[2m[36m(func pid=164771)[0m mae:  0.13311733305454254
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.111, 0.189, 0.294, 0.142, 0.142, 0.11]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.18251194059848785
[2m[36m(func pid=165914)[0m mae:  0.134338840842247
[2m[36m(func pid=165914)[0m rmse_per_class: [0.117, 0.267, 0.107, 0.339, 0.111, 0.19, 0.294, 0.144, 0.144, 0.113]
[2m[36m(func pid=165914)[0m 
== Status ==
Current time: 2024-01-07 07:31:07 (running for 00:39:39.25)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.384 |  0.208 |                   69 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.785 |  0.179 |                   55 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.844 |  0.181 |                    6 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.846 |  0.183 |                    2 |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17918242514133453
[2m[36m(func pid=153954)[0m mae:  0.13154253363609314
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3640 | Steps: 2 | Val loss: 0.5338 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8242 | Steps: 2 | Val loss: 0.6369 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7674 | Steps: 2 | Val loss: 0.5707 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.7809 | Steps: 2 | Val loss: 0.6101 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=150355)[0m rmse: 0.2095508575439453
[2m[36m(func pid=150355)[0m mae:  0.1195499449968338
[2m[36m(func pid=150355)[0m rmse_per_class: [0.268, 0.312, 0.031, 0.361, 0.126, 0.25, 0.281, 0.131, 0.158, 0.177]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.1805732548236847
[2m[36m(func pid=164771)[0m mae:  0.13277378678321838
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.11, 0.189, 0.293, 0.141, 0.142, 0.11]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.18217185139656067
[2m[36m(func pid=165914)[0m mae:  0.13410882651805878
[2m[36m(func pid=165914)[0m rmse_per_class: [0.118, 0.267, 0.105, 0.339, 0.11, 0.19, 0.293, 0.143, 0.143, 0.113]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.1791468858718872
== Status ==
Current time: 2024-01-07 07:31:12 (running for 00:39:44.48)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.364 |  0.21  |                   70 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.781 |  0.179 |                   56 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.824 |  0.181 |                    7 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.767 |  0.182 |                    3 |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=153954)[0m mae:  0.13151690363883972

[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3730 | Steps: 2 | Val loss: 0.5350 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8062 | Steps: 2 | Val loss: 0.6230 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6817 | Steps: 2 | Val loss: 0.5045 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7774 | Steps: 2 | Val loss: 0.6084 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=150355)[0m rmse: 0.2116227149963379
[2m[36m(func pid=150355)[0m mae:  0.12093691527843475
[2m[36m(func pid=150355)[0m rmse_per_class: [0.298, 0.313, 0.032, 0.361, 0.132, 0.237, 0.271, 0.136, 0.154, 0.182]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.18023666739463806
[2m[36m(func pid=164771)[0m mae:  0.13248378038406372
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.263, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.181406632065773
[2m[36m(func pid=165914)[0m mae:  0.133521169424057
[2m[36m(func pid=165914)[0m rmse_per_class: [0.118, 0.266, 0.103, 0.338, 0.108, 0.19, 0.293, 0.143, 0.143, 0.112]
[2m[36m(func pid=165914)[0m 
== Status ==
Current time: 2024-01-07 07:31:18 (running for 00:39:49.56)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.373 |  0.212 |                   71 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.777 |  0.179 |                   57 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.806 |  0.18  |                    8 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.682 |  0.181 |                    4 |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17917528748512268
[2m[36m(func pid=153954)[0m mae:  0.13152773678302765
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3659 | Steps: 2 | Val loss: 0.5357 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7892 | Steps: 2 | Val loss: 0.6091 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.6001 | Steps: 2 | Val loss: 0.4460 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7767 | Steps: 2 | Val loss: 0.6067 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=150355)[0m rmse: 0.21397848427295685
[2m[36m(func pid=150355)[0m mae:  0.12203021347522736
[2m[36m(func pid=150355)[0m rmse_per_class: [0.318, 0.315, 0.033, 0.36, 0.142, 0.214, 0.267, 0.148, 0.151, 0.191]
[2m[36m(func pid=150355)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17995494604110718
[2m[36m(func pid=164771)[0m mae:  0.13223519921302795
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.263, 0.101, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.1804696023464203
[2m[36m(func pid=165914)[0m mae:  0.13277778029441833
[2m[36m(func pid=165914)[0m rmse_per_class: [0.119, 0.266, 0.1, 0.337, 0.105, 0.189, 0.292, 0.143, 0.143, 0.112]
[2m[36m(func pid=165914)[0m 
== Status ==
Current time: 2024-01-07 07:31:23 (running for 00:39:54.69)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.366 |  0.214 |                   72 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.777 |  0.179 |                   58 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.789 |  0.18  |                    9 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.6   |  0.18  |                    5 |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17914043366909027
[2m[36m(func pid=153954)[0m mae:  0.13150526583194733
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3532 | Steps: 2 | Val loss: 0.5339 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7712 | Steps: 2 | Val loss: 0.5955 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5339 | Steps: 2 | Val loss: 0.4000 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7760 | Steps: 2 | Val loss: 0.6055 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=150355)[0m rmse: 0.21528103947639465
[2m[36m(func pid=150355)[0m mae:  0.12214715778827667
[2m[36m(func pid=150355)[0m rmse_per_class: [0.315, 0.318, 0.033, 0.358, 0.149, 0.192, 0.266, 0.165, 0.15, 0.206]
[2m[36m(func pid=150355)[0m 
== Status ==
Current time: 2024-01-07 07:31:28 (running for 00:39:59.92)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.353 |  0.215 |                   73 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.776 |  0.179 |                   59 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.789 |  0.18  |                    9 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.6   |  0.18  |                    5 |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17917022109031677
[2m[36m(func pid=153954)[0m mae:  0.13152197003364563
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17968353629112244
[2m[36m(func pid=164771)[0m mae:  0.13199445605278015
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.262, 0.1, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.17955566942691803
[2m[36m(func pid=165914)[0m mae:  0.1320265829563141
[2m[36m(func pid=165914)[0m rmse_per_class: [0.119, 0.266, 0.098, 0.336, 0.101, 0.189, 0.29, 0.143, 0.142, 0.112]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3720 | Steps: 2 | Val loss: 0.5285 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7753 | Steps: 2 | Val loss: 0.6042 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4857 | Steps: 2 | Val loss: 0.3664 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7549 | Steps: 2 | Val loss: 0.5821 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=150355)[0m rmse: 0.21497520804405212
[2m[36m(func pid=150355)[0m mae:  0.1212078183889389
[2m[36m(func pid=150355)[0m rmse_per_class: [0.287, 0.324, 0.034, 0.355, 0.153, 0.182, 0.263, 0.181, 0.149, 0.222]
[2m[36m(func pid=150355)[0m 
== Status ==
Current time: 2024-01-07 07:31:33 (running for 00:40:05.05)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00019 | RUNNING    | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.372 |  0.215 |                   74 |
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.775 |  0.179 |                   60 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.771 |  0.18  |                   10 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.534 |  0.18  |                    6 |
| train_ccef6_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17916321754455566
[2m[36m(func pid=153954)[0m mae:  0.13151192665100098
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.17844074964523315
[2m[36m(func pid=165914)[0m mae:  0.1311267763376236
[2m[36m(func pid=165914)[0m rmse_per_class: [0.119, 0.265, 0.095, 0.334, 0.097, 0.188, 0.289, 0.143, 0.142, 0.111]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.1794639378786087
[2m[36m(func pid=164771)[0m mae:  0.13181081414222717
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.262, 0.099, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=150355)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4027 | Steps: 2 | Val loss: 0.5214 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.7728 | Steps: 2 | Val loss: 0.6026 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4547 | Steps: 2 | Val loss: 0.3437 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7402 | Steps: 2 | Val loss: 0.5687 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=150355)[0m rmse: 0.21181528270244598
[2m[36m(func pid=150355)[0m mae:  0.1189470887184143
[2m[36m(func pid=150355)[0m rmse_per_class: [0.241, 0.334, 0.034, 0.353, 0.146, 0.186, 0.257, 0.185, 0.15, 0.234]
[2m[36m(func pid=153954)[0m rmse: 0.17917463183403015
[2m[36m(func pid=153954)[0m mae:  0.1315196007490158
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=165914)[0m rmse: 0.17729875445365906
[2m[36m(func pid=165914)[0m mae:  0.1301918774843216
[2m[36m(func pid=165914)[0m rmse_per_class: [0.12, 0.263, 0.093, 0.333, 0.093, 0.188, 0.288, 0.142, 0.142, 0.111]
[2m[36m(func pid=164771)[0m rmse: 0.1792459487915039
[2m[36m(func pid=164771)[0m mae:  0.13163353502750397
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 07:31:38 (running for 00:40:10.16)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.775 |  0.179 |                   60 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.755 |  0.179 |                   11 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.486 |  0.178 |                    7 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 07:31:46 (running for 00:40:18.04)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.773 |  0.179 |                   61 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.755 |  0.179 |                   11 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.486 |  0.178 |                    7 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=168155)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=168155)[0m Configuration completed!
[2m[36m(func pid=168155)[0m New optimizer parameters:
[2m[36m(func pid=168155)[0m SGD (
[2m[36m(func pid=168155)[0m Parameter Group 0
[2m[36m(func pid=168155)[0m     dampening: 0
[2m[36m(func pid=168155)[0m     differentiable: False
[2m[36m(func pid=168155)[0m     foreach: None
[2m[36m(func pid=168155)[0m     lr: 0.1
[2m[36m(func pid=168155)[0m     maximize: False
[2m[36m(func pid=168155)[0m     momentum: 0.9
[2m[36m(func pid=168155)[0m     nesterov: False
[2m[36m(func pid=168155)[0m     weight_decay: 1e-05
[2m[36m(func pid=168155)[0m )
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7703 | Steps: 2 | Val loss: 0.6009 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7186 | Steps: 2 | Val loss: 0.5547 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4282 | Steps: 2 | Val loss: 0.3284 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8347 | Steps: 2 | Val loss: 0.5215 | Batch size: 32 | lr: 0.1 | Duration: 4.48s
== Status ==
Current time: 2024-01-07 07:31:51 (running for 00:40:23.07)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.773 |  0.179 |                   61 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.74  |  0.179 |                   12 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.455 |  0.177 |                    8 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17917093634605408
[2m[36m(func pid=153954)[0m mae:  0.13150838017463684
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.1759527325630188
[2m[36m(func pid=165914)[0m mae:  0.12905865907669067
[2m[36m(func pid=165914)[0m rmse_per_class: [0.119, 0.262, 0.09, 0.331, 0.09, 0.188, 0.286, 0.142, 0.142, 0.11]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17900940775871277
[2m[36m(func pid=164771)[0m mae:  0.1314283311367035
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.18261049687862396
[2m[36m(func pid=168155)[0m mae:  0.13437071442604065
[2m[36m(func pid=168155)[0m rmse_per_class: [0.118, 0.267, 0.109, 0.338, 0.106, 0.191, 0.293, 0.146, 0.144, 0.114]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.7680 | Steps: 2 | Val loss: 0.5990 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4148 | Steps: 2 | Val loss: 0.3194 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7050 | Steps: 2 | Val loss: 0.5420 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5484 | Steps: 2 | Val loss: 0.3537 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 07:31:57 (running for 00:40:28.70)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.768 |  0.179 |                   63 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.719 |  0.179 |                   13 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.428 |  0.176 |                    9 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.835 |  0.183 |                    1 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17918242514133453
[2m[36m(func pid=153954)[0m mae:  0.13151445984840393
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.17483988404273987
[2m[36m(func pid=165914)[0m mae:  0.12811945378780365
[2m[36m(func pid=165914)[0m rmse_per_class: [0.119, 0.261, 0.087, 0.329, 0.087, 0.187, 0.285, 0.141, 0.142, 0.11]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17887726426124573
[2m[36m(func pid=164771)[0m mae:  0.13130657374858856
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.261, 0.097, 0.335, 0.107, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.1809677630662918
[2m[36m(func pid=168155)[0m mae:  0.13324271142482758
[2m[36m(func pid=168155)[0m rmse_per_class: [0.122, 0.267, 0.101, 0.337, 0.094, 0.191, 0.289, 0.145, 0.146, 0.115]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.7671 | Steps: 2 | Val loss: 0.5975 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4085 | Steps: 2 | Val loss: 0.3141 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.6901 | Steps: 2 | Val loss: 0.5299 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4116 | Steps: 2 | Val loss: 0.3216 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 07:32:02 (running for 00:40:33.90)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.767 |  0.179 |                   64 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.705 |  0.179 |                   14 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.415 |  0.175 |                   10 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.548 |  0.181 |                    2 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17918509244918823
[2m[36m(func pid=153954)[0m mae:  0.13152256608009338
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.17365577816963196
[2m[36m(func pid=165914)[0m mae:  0.12712976336479187
[2m[36m(func pid=165914)[0m rmse_per_class: [0.118, 0.26, 0.085, 0.328, 0.083, 0.187, 0.284, 0.141, 0.142, 0.109]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.1787046492099762
[2m[36m(func pid=164771)[0m mae:  0.13115353882312775
[2m[36m(func pid=164771)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.335, 0.107, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.17672905325889587
[2m[36m(func pid=168155)[0m mae:  0.1299668848514557
[2m[36m(func pid=168155)[0m rmse_per_class: [0.125, 0.266, 0.091, 0.332, 0.079, 0.19, 0.281, 0.143, 0.148, 0.113]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7652 | Steps: 2 | Val loss: 0.5959 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4001 | Steps: 2 | Val loss: 0.3109 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4265 | Steps: 2 | Val loss: 0.3401 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6753 | Steps: 2 | Val loss: 0.5186 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 07:32:07 (running for 00:40:39.30)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.765 |  0.179 |                   65 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.69  |  0.179 |                   15 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.408 |  0.174 |                   11 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.412 |  0.177 |                    3 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.1792023777961731
[2m[36m(func pid=153954)[0m mae:  0.13153915107250214
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.17238740622997284
[2m[36m(func pid=165914)[0m mae:  0.126068577170372
[2m[36m(func pid=165914)[0m rmse_per_class: [0.118, 0.259, 0.082, 0.326, 0.081, 0.187, 0.282, 0.14, 0.141, 0.109]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.17081227898597717
[2m[36m(func pid=168155)[0m mae:  0.12505218386650085
[2m[36m(func pid=168155)[0m rmse_per_class: [0.126, 0.263, 0.078, 0.321, 0.067, 0.189, 0.27, 0.139, 0.148, 0.107]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.1785990297794342
[2m[36m(func pid=164771)[0m mae:  0.13106556236743927
[2m[36m(func pid=164771)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7604 | Steps: 2 | Val loss: 0.5940 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4027 | Steps: 2 | Val loss: 0.3091 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4591 | Steps: 2 | Val loss: 0.3567 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6638 | Steps: 2 | Val loss: 0.5073 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 07:32:12 (running for 00:40:44.55)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.76  |  0.179 |                   66 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.675 |  0.179 |                   16 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.4   |  0.172 |                   12 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.427 |  0.171 |                    4 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.1791762262582779
[2m[36m(func pid=153954)[0m mae:  0.13151279091835022
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.1711677759885788
[2m[36m(func pid=165914)[0m mae:  0.12505844235420227
[2m[36m(func pid=165914)[0m rmse_per_class: [0.117, 0.257, 0.079, 0.324, 0.078, 0.186, 0.281, 0.14, 0.141, 0.109]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.16370603442192078
[2m[36m(func pid=168155)[0m mae:  0.11885231733322144
[2m[36m(func pid=168155)[0m rmse_per_class: [0.122, 0.259, 0.063, 0.311, 0.059, 0.185, 0.258, 0.134, 0.147, 0.1]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.1784062683582306
[2m[36m(func pid=164771)[0m mae:  0.13090470433235168
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.259, 0.096, 0.335, 0.106, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.7603 | Steps: 2 | Val loss: 0.5925 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3947 | Steps: 2 | Val loss: 0.3082 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4785 | Steps: 2 | Val loss: 0.3561 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6492 | Steps: 2 | Val loss: 0.4968 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 07:32:18 (running for 00:40:49.76)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.76  |  0.179 |                   67 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.664 |  0.178 |                   17 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.403 |  0.171 |                   13 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.459 |  0.164 |                    5 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.179107666015625
[2m[36m(func pid=153954)[0m mae:  0.1314510703086853
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.17017459869384766
[2m[36m(func pid=165914)[0m mae:  0.1242629736661911
[2m[36m(func pid=165914)[0m rmse_per_class: [0.117, 0.256, 0.077, 0.322, 0.076, 0.186, 0.28, 0.139, 0.141, 0.108]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.15659737586975098
[2m[36m(func pid=168155)[0m mae:  0.11211302131414413
[2m[36m(func pid=168155)[0m rmse_per_class: [0.115, 0.252, 0.051, 0.299, 0.055, 0.181, 0.246, 0.129, 0.144, 0.093]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17826415598392487
[2m[36m(func pid=164771)[0m mae:  0.13079950213432312
[2m[36m(func pid=164771)[0m rmse_per_class: [0.115, 0.259, 0.096, 0.335, 0.105, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7604 | Steps: 2 | Val loss: 0.5913 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3991 | Steps: 2 | Val loss: 0.3077 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4765 | Steps: 2 | Val loss: 0.3394 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6379 | Steps: 2 | Val loss: 0.4872 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 07:32:23 (running for 00:40:54.86)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.76  |  0.179 |                   68 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.649 |  0.178 |                   18 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.395 |  0.17  |                   14 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.478 |  0.157 |                    6 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17904067039489746
[2m[36m(func pid=153954)[0m mae:  0.1314011514186859
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.169218510389328
[2m[36m(func pid=165914)[0m mae:  0.12351159006357193
[2m[36m(func pid=165914)[0m rmse_per_class: [0.117, 0.254, 0.075, 0.321, 0.074, 0.185, 0.279, 0.138, 0.141, 0.108]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.1504293531179428
[2m[36m(func pid=168155)[0m mae:  0.10568650811910629
[2m[36m(func pid=168155)[0m rmse_per_class: [0.105, 0.245, 0.045, 0.288, 0.054, 0.177, 0.236, 0.125, 0.141, 0.088]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.1781473457813263
[2m[36m(func pid=164771)[0m mae:  0.13068567216396332
[2m[36m(func pid=164771)[0m rmse_per_class: [0.115, 0.259, 0.095, 0.335, 0.105, 0.19, 0.292, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7577 | Steps: 2 | Val loss: 0.5893 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3948 | Steps: 2 | Val loss: 0.3069 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4399 | Steps: 2 | Val loss: 0.3135 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 07:32:28 (running for 00:40:59.92)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.758 |  0.179 |                   69 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.638 |  0.178 |                   19 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.399 |  0.169 |                   15 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.476 |  0.15  |                    7 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17907020449638367
[2m[36m(func pid=153954)[0m mae:  0.1314472109079361
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6238 | Steps: 2 | Val loss: 0.4780 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=165914)[0m rmse: 0.16813968122005463
[2m[36m(func pid=165914)[0m mae:  0.12262023985385895
[2m[36m(func pid=165914)[0m rmse_per_class: [0.116, 0.253, 0.073, 0.32, 0.072, 0.185, 0.277, 0.137, 0.141, 0.107]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.14629890024662018
[2m[36m(func pid=168155)[0m mae:  0.10101939737796783
[2m[36m(func pid=168155)[0m rmse_per_class: [0.099, 0.238, 0.041, 0.282, 0.054, 0.174, 0.23, 0.122, 0.139, 0.084]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17801541090011597
[2m[36m(func pid=164771)[0m mae:  0.13058777153491974
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.335, 0.104, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.7541 | Steps: 2 | Val loss: 0.5874 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3913 | Steps: 2 | Val loss: 0.3062 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4033 | Steps: 2 | Val loss: 0.2858 | Batch size: 32 | lr: 0.1 | Duration: 2.60s
== Status ==
Current time: 2024-01-07 07:32:33 (running for 00:41:05.42)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.754 |  0.179 |                   70 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.624 |  0.178 |                   20 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.395 |  0.168 |                   16 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.44  |  0.146 |                    8 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17906202375888824
[2m[36m(func pid=153954)[0m mae:  0.13143806159496307
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.16719484329223633
[2m[36m(func pid=165914)[0m mae:  0.12183170020580292
[2m[36m(func pid=165914)[0m rmse_per_class: [0.116, 0.251, 0.071, 0.319, 0.071, 0.184, 0.276, 0.137, 0.141, 0.106]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6148 | Steps: 2 | Val loss: 0.4694 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=168155)[0m rmse: 0.14331486821174622
[2m[36m(func pid=168155)[0m mae:  0.09764282405376434
[2m[36m(func pid=168155)[0m rmse_per_class: [0.094, 0.234, 0.039, 0.277, 0.055, 0.17, 0.226, 0.12, 0.137, 0.082]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17796194553375244
[2m[36m(func pid=164771)[0m mae:  0.130527064204216
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.334, 0.104, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.7547 | Steps: 2 | Val loss: 0.5858 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3934 | Steps: 2 | Val loss: 0.3050 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3593 | Steps: 2 | Val loss: 0.2670 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 07:32:39 (running for 00:41:10.60)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.754 |  0.179 |                   70 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.615 |  0.178 |                   21 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.393 |  0.166 |                   18 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.403 |  0.143 |                    9 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17905865609645844
[2m[36m(func pid=153954)[0m mae:  0.13142645359039307
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.097, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.16624793410301208
[2m[36m(func pid=165914)[0m mae:  0.12105722725391388
[2m[36m(func pid=165914)[0m rmse_per_class: [0.115, 0.249, 0.069, 0.318, 0.07, 0.184, 0.275, 0.136, 0.141, 0.106]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.1416955292224884
[2m[36m(func pid=168155)[0m mae:  0.09627722203731537
[2m[36m(func pid=168155)[0m rmse_per_class: [0.092, 0.231, 0.038, 0.275, 0.055, 0.165, 0.223, 0.118, 0.139, 0.081]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6034 | Steps: 2 | Val loss: 0.4612 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3903 | Steps: 2 | Val loss: 0.3037 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.7510 | Steps: 2 | Val loss: 0.5850 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=164771)[0m rmse: 0.17790870368480682
[2m[36m(func pid=164771)[0m mae:  0.13047246634960175
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.334, 0.104, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3376 | Steps: 2 | Val loss: 0.2632 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 07:32:44 (running for 00:41:15.93)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.751 |  0.179 |                   72 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.603 |  0.178 |                   22 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.393 |  0.166 |                   18 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.359 |  0.142 |                   10 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.1791391223669052
[2m[36m(func pid=153954)[0m mae:  0.13150648772716522
[2m[36m(func pid=153954)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.1653570830821991
[2m[36m(func pid=165914)[0m mae:  0.1203225627541542
[2m[36m(func pid=165914)[0m rmse_per_class: [0.115, 0.248, 0.067, 0.317, 0.069, 0.183, 0.274, 0.135, 0.141, 0.105]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.14145240187644958
[2m[36m(func pid=168155)[0m mae:  0.09654863178730011
[2m[36m(func pid=168155)[0m rmse_per_class: [0.095, 0.231, 0.036, 0.275, 0.055, 0.16, 0.225, 0.116, 0.14, 0.081]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5939 | Steps: 2 | Val loss: 0.4532 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.7538 | Steps: 2 | Val loss: 0.5840 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3895 | Steps: 2 | Val loss: 0.3023 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3258 | Steps: 2 | Val loss: 0.2700 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=164771)[0m rmse: 0.17775997519493103
[2m[36m(func pid=164771)[0m mae:  0.13035272061824799
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.334, 0.104, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
== Status ==
Current time: 2024-01-07 07:32:49 (running for 00:41:21.24)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.751 |  0.179 |                   72 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.594 |  0.178 |                   23 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.389 |  0.164 |                   20 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.338 |  0.141 |                   11 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.17910680174827576
[2m[36m(func pid=153954)[0m mae:  0.13146591186523438
[2m[36m(func pid=153954)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.16447493433952332
[2m[36m(func pid=165914)[0m mae:  0.11958475410938263
[2m[36m(func pid=165914)[0m rmse_per_class: [0.114, 0.247, 0.066, 0.316, 0.068, 0.182, 0.273, 0.135, 0.14, 0.104]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.1428268402814865
[2m[36m(func pid=168155)[0m mae:  0.09822414815425873
[2m[36m(func pid=168155)[0m rmse_per_class: [0.098, 0.232, 0.034, 0.276, 0.055, 0.155, 0.235, 0.115, 0.144, 0.085]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5854 | Steps: 2 | Val loss: 0.4461 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7494 | Steps: 2 | Val loss: 0.5827 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3862 | Steps: 2 | Val loss: 0.3009 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3250 | Steps: 2 | Val loss: 0.2771 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=164771)[0m rmse: 0.1776142567396164
[2m[36m(func pid=164771)[0m mae:  0.13024082779884338
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.334, 0.103, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
== Status ==
Current time: 2024-01-07 07:32:54 (running for 00:41:26.55)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.754 |  0.179 |                   73 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.585 |  0.178 |                   24 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.386 |  0.164 |                   21 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.326 |  0.143 |                   12 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=153954)[0m rmse: 0.1790773570537567
[2m[36m(func pid=153954)[0m mae:  0.13143470883369446
[2m[36m(func pid=153954)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.336, 0.108, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=153954)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.16367590427398682
[2m[36m(func pid=165914)[0m mae:  0.11893564462661743
[2m[36m(func pid=165914)[0m rmse_per_class: [0.113, 0.246, 0.065, 0.315, 0.067, 0.182, 0.272, 0.134, 0.14, 0.104]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.14525505900382996
[2m[36m(func pid=168155)[0m mae:  0.10038609802722931
[2m[36m(func pid=168155)[0m rmse_per_class: [0.099, 0.234, 0.031, 0.278, 0.055, 0.152, 0.25, 0.115, 0.146, 0.093]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5764 | Steps: 2 | Val loss: 0.4395 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3829 | Steps: 2 | Val loss: 0.2993 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3252 | Steps: 2 | Val loss: 0.2785 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=153954)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.7456 | Steps: 2 | Val loss: 0.5812 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=164771)[0m rmse: 0.17750561237335205
[2m[36m(func pid=164771)[0m mae:  0.13013961911201477
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.259, 0.093, 0.334, 0.103, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
== Status ==
Current time: 2024-01-07 07:33:00 (running for 00:41:31.67)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00020 | RUNNING    | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.749 |  0.179 |                   74 |
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.576 |  0.178 |                   25 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.386 |  0.164 |                   21 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.325 |  0.148 |                   14 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.1475687026977539
[2m[36m(func pid=168155)[0m mae:  0.10197974741458893
[2m[36m(func pid=168155)[0m rmse_per_class: [0.097, 0.237, 0.03, 0.28, 0.054, 0.151, 0.261, 0.115, 0.147, 0.104]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.16287758946418762
[2m[36m(func pid=165914)[0m mae:  0.11828341335058212
[2m[36m(func pid=165914)[0m rmse_per_class: [0.112, 0.245, 0.063, 0.314, 0.066, 0.181, 0.271, 0.134, 0.14, 0.103]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=153954)[0m rmse: 0.17911066114902496
[2m[36m(func pid=153954)[0m mae:  0.13145670294761658
[2m[36m(func pid=153954)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5689 | Steps: 2 | Val loss: 0.4338 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3255 | Steps: 2 | Val loss: 0.2767 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3839 | Steps: 2 | Val loss: 0.2978 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=164771)[0m rmse: 0.17735853791236877
[2m[36m(func pid=164771)[0m mae:  0.13001671433448792
[2m[36m(func pid=164771)[0m rmse_per_class: [0.115, 0.259, 0.093, 0.333, 0.102, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.14971432089805603
[2m[36m(func pid=168155)[0m mae:  0.1029619574546814
[2m[36m(func pid=168155)[0m rmse_per_class: [0.096, 0.24, 0.031, 0.282, 0.053, 0.15, 0.265, 0.115, 0.146, 0.119]
[2m[36m(func pid=168155)[0m 
== Status ==
Current time: 2024-01-07 07:33:05 (running for 00:41:37.15)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.569 |  0.177 |                   26 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.383 |  0.163 |                   22 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.325 |  0.15  |                   15 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.16216588020324707
[2m[36m(func pid=165914)[0m mae:  0.11768845468759537
[2m[36m(func pid=165914)[0m rmse_per_class: [0.111, 0.244, 0.062, 0.313, 0.065, 0.181, 0.269, 0.133, 0.14, 0.102]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5609 | Steps: 2 | Val loss: 0.4280 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3068 | Steps: 2 | Val loss: 0.2760 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3804 | Steps: 2 | Val loss: 0.2963 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=164771)[0m rmse: 0.17723168432712555
[2m[36m(func pid=164771)[0m mae:  0.12991397082805634
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.333, 0.102, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
== Status ==
Current time: 2024-01-07 07:33:10 (running for 00:41:42.49)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.561 |  0.177 |                   27 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.384 |  0.162 |                   23 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.307 |  0.151 |                   16 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.15079572796821594
[2m[36m(func pid=168155)[0m mae:  0.10274107754230499
[2m[36m(func pid=168155)[0m rmse_per_class: [0.09, 0.245, 0.033, 0.286, 0.054, 0.15, 0.261, 0.115, 0.142, 0.132]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.16150447726249695
[2m[36m(func pid=165914)[0m mae:  0.11714138835668564
[2m[36m(func pid=165914)[0m rmse_per_class: [0.111, 0.243, 0.061, 0.313, 0.064, 0.18, 0.269, 0.133, 0.14, 0.102]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5526 | Steps: 2 | Val loss: 0.4220 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3070 | Steps: 2 | Val loss: 0.2769 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3795 | Steps: 2 | Val loss: 0.2950 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=164771)[0m rmse: 0.17715613543987274
[2m[36m(func pid=164771)[0m mae:  0.12984254956245422
[2m[36m(func pid=164771)[0m rmse_per_class: [0.115, 0.258, 0.093, 0.333, 0.101, 0.19, 0.291, 0.141, 0.141, 0.108]
[2m[36m(func pid=164771)[0m 
== Status ==
Current time: 2024-01-07 07:33:16 (running for 00:41:47.77)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.553 |  0.177 |                   28 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.38  |  0.162 |                   24 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.307 |  0.151 |                   17 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.15075373649597168
[2m[36m(func pid=168155)[0m mae:  0.10157120227813721
[2m[36m(func pid=168155)[0m rmse_per_class: [0.087, 0.248, 0.036, 0.289, 0.061, 0.151, 0.25, 0.115, 0.138, 0.134]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.1609741747379303
[2m[36m(func pid=165914)[0m mae:  0.11670806258916855
[2m[36m(func pid=165914)[0m rmse_per_class: [0.111, 0.242, 0.06, 0.312, 0.064, 0.179, 0.268, 0.132, 0.14, 0.101]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5460 | Steps: 2 | Val loss: 0.4163 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2874 | Steps: 2 | Val loss: 0.2781 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3777 | Steps: 2 | Val loss: 0.2935 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=164771)[0m rmse: 0.17700731754302979
[2m[36m(func pid=164771)[0m mae:  0.1297248750925064
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.258, 0.093, 0.333, 0.101, 0.19, 0.291, 0.141, 0.141, 0.108]
[2m[36m(func pid=164771)[0m 
== Status ==
Current time: 2024-01-07 07:33:21 (running for 00:41:52.95)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.546 |  0.177 |                   29 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.379 |  0.161 |                   25 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.287 |  0.15  |                   18 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.15041320025920868
[2m[36m(func pid=168155)[0m mae:  0.10002382844686508
[2m[36m(func pid=168155)[0m rmse_per_class: [0.081, 0.25, 0.036, 0.291, 0.077, 0.153, 0.235, 0.113, 0.135, 0.134]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.16032767295837402
[2m[36m(func pid=165914)[0m mae:  0.1161731481552124
[2m[36m(func pid=165914)[0m rmse_per_class: [0.11, 0.241, 0.06, 0.311, 0.063, 0.179, 0.267, 0.131, 0.14, 0.101]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5412 | Steps: 2 | Val loss: 0.4108 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.2938 | Steps: 2 | Val loss: 0.2788 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3805 | Steps: 2 | Val loss: 0.2920 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 07:33:26 (running for 00:41:58.08)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.541 |  0.177 |                   30 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.378 |  0.16  |                   26 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.287 |  0.15  |                   18 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=164771)[0m rmse: 0.17693080008029938
[2m[36m(func pid=164771)[0m mae:  0.12967225909233093
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.1, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.15018577873706818
[2m[36m(func pid=168155)[0m mae:  0.09873280674219131
[2m[36m(func pid=168155)[0m rmse_per_class: [0.075, 0.251, 0.034, 0.293, 0.096, 0.157, 0.222, 0.112, 0.134, 0.128]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15973082184791565
[2m[36m(func pid=165914)[0m mae:  0.11565817892551422
[2m[36m(func pid=165914)[0m rmse_per_class: [0.11, 0.241, 0.059, 0.311, 0.063, 0.178, 0.266, 0.131, 0.139, 0.1]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5339 | Steps: 2 | Val loss: 0.4065 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3078 | Steps: 2 | Val loss: 0.2770 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3706 | Steps: 2 | Val loss: 0.2909 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 07:33:31 (running for 00:42:03.54)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.534 |  0.177 |                   31 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.38  |  0.16  |                   27 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.294 |  0.15  |                   19 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=164771)[0m rmse: 0.17692497372627258
[2m[36m(func pid=164771)[0m mae:  0.12968984246253967
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.1, 0.19, 0.291, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.1490483433008194
[2m[36m(func pid=168155)[0m mae:  0.09722129255533218
[2m[36m(func pid=168155)[0m rmse_per_class: [0.071, 0.25, 0.03, 0.293, 0.107, 0.159, 0.214, 0.112, 0.133, 0.122]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15927833318710327
[2m[36m(func pid=165914)[0m mae:  0.11529558897018433
[2m[36m(func pid=165914)[0m rmse_per_class: [0.11, 0.24, 0.058, 0.31, 0.062, 0.178, 0.266, 0.131, 0.139, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5253 | Steps: 2 | Val loss: 0.4025 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.2855 | Steps: 2 | Val loss: 0.2746 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3723 | Steps: 2 | Val loss: 0.2900 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 07:33:37 (running for 00:42:09.05)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.534 |  0.177 |                   31 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.371 |  0.159 |                   28 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.308 |  0.149 |                   20 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=164771)[0m rmse: 0.17692585289478302
[2m[36m(func pid=164771)[0m mae:  0.12968987226486206
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.099, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.14809608459472656
[2m[36m(func pid=168155)[0m mae:  0.09621386229991913
[2m[36m(func pid=168155)[0m rmse_per_class: [0.068, 0.249, 0.028, 0.294, 0.11, 0.158, 0.211, 0.113, 0.133, 0.116]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.1587751805782318
[2m[36m(func pid=165914)[0m mae:  0.11485733836889267
[2m[36m(func pid=165914)[0m rmse_per_class: [0.109, 0.239, 0.058, 0.31, 0.062, 0.177, 0.265, 0.13, 0.139, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.2802 | Steps: 2 | Val loss: 0.2722 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.5221 | Steps: 2 | Val loss: 0.3981 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3711 | Steps: 2 | Val loss: 0.2889 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 07:33:42 (running for 00:42:14.35)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.525 |  0.177 |                   32 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.372 |  0.159 |                   29 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.286 |  0.148 |                   21 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.1472000777721405
[2m[36m(func pid=168155)[0m mae:  0.09563572704792023
[2m[36m(func pid=168155)[0m rmse_per_class: [0.068, 0.249, 0.026, 0.292, 0.106, 0.158, 0.211, 0.114, 0.134, 0.114]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.176772803068161
[2m[36m(func pid=164771)[0m mae:  0.12957271933555603
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.257, 0.092, 0.333, 0.099, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15834084153175354
[2m[36m(func pid=165914)[0m mae:  0.11450475454330444
[2m[36m(func pid=165914)[0m rmse_per_class: [0.108, 0.239, 0.057, 0.309, 0.062, 0.177, 0.264, 0.13, 0.139, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2845 | Steps: 2 | Val loss: 0.2708 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3630 | Steps: 2 | Val loss: 0.2882 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5192 | Steps: 2 | Val loss: 0.3946 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 07:33:48 (running for 00:42:19.71)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.522 |  0.177 |                   33 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.371 |  0.158 |                   30 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.28  |  0.147 |                   22 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14632992446422577
[2m[36m(func pid=168155)[0m mae:  0.09523659944534302
[2m[36m(func pid=168155)[0m rmse_per_class: [0.069, 0.251, 0.026, 0.292, 0.097, 0.155, 0.214, 0.113, 0.134, 0.112]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17676474153995514
[2m[36m(func pid=164771)[0m mae:  0.12955337762832642
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.099, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15807786583900452
[2m[36m(func pid=165914)[0m mae:  0.11428163945674896
[2m[36m(func pid=165914)[0m rmse_per_class: [0.108, 0.238, 0.057, 0.309, 0.062, 0.176, 0.263, 0.13, 0.139, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2878 | Steps: 2 | Val loss: 0.2697 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3724 | Steps: 2 | Val loss: 0.2877 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.5114 | Steps: 2 | Val loss: 0.3909 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 07:33:53 (running for 00:42:25.16)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.519 |  0.177 |                   34 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.363 |  0.158 |                   31 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.285 |  0.146 |                   23 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14573423564434052
[2m[36m(func pid=168155)[0m mae:  0.09504034370183945
[2m[36m(func pid=168155)[0m rmse_per_class: [0.07, 0.252, 0.026, 0.291, 0.087, 0.153, 0.219, 0.113, 0.135, 0.111]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17664650082588196
[2m[36m(func pid=164771)[0m mae:  0.12944908440113068
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.258, 0.092, 0.333, 0.099, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.1578449308872223
[2m[36m(func pid=165914)[0m mae:  0.11408670991659164
[2m[36m(func pid=165914)[0m rmse_per_class: [0.107, 0.238, 0.057, 0.308, 0.061, 0.176, 0.263, 0.129, 0.139, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2727 | Steps: 2 | Val loss: 0.2687 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.5086 | Steps: 2 | Val loss: 0.3876 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3636 | Steps: 2 | Val loss: 0.2869 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 07:33:58 (running for 00:42:30.52)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.511 |  0.177 |                   35 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.372 |  0.158 |                   32 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.273 |  0.145 |                   25 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14542458951473236
[2m[36m(func pid=168155)[0m mae:  0.09487105906009674
[2m[36m(func pid=168155)[0m rmse_per_class: [0.073, 0.25, 0.027, 0.288, 0.078, 0.151, 0.225, 0.113, 0.137, 0.112]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15753071010112762
[2m[36m(func pid=165914)[0m mae:  0.1138058453798294
[2m[36m(func pid=165914)[0m rmse_per_class: [0.106, 0.239, 0.056, 0.307, 0.061, 0.176, 0.262, 0.129, 0.139, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.176579087972641
[2m[36m(func pid=164771)[0m mae:  0.12939178943634033
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.258, 0.091, 0.333, 0.098, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2803 | Steps: 2 | Val loss: 0.2690 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3649 | Steps: 2 | Val loss: 0.2865 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5037 | Steps: 2 | Val loss: 0.3845 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 07:34:04 (running for 00:42:35.70)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.509 |  0.177 |                   36 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.364 |  0.158 |                   33 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.28  |  0.146 |                   26 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14592091739177704
[2m[36m(func pid=168155)[0m mae:  0.09519661217927933
[2m[36m(func pid=168155)[0m rmse_per_class: [0.077, 0.25, 0.027, 0.288, 0.072, 0.15, 0.229, 0.113, 0.138, 0.115]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15740013122558594
[2m[36m(func pid=165914)[0m mae:  0.11374296993017197
[2m[36m(func pid=165914)[0m rmse_per_class: [0.107, 0.238, 0.056, 0.307, 0.061, 0.176, 0.262, 0.128, 0.14, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17645443975925446
[2m[36m(func pid=164771)[0m mae:  0.1293100118637085
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.332, 0.098, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2758 | Steps: 2 | Val loss: 0.2691 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3681 | Steps: 2 | Val loss: 0.2861 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4985 | Steps: 2 | Val loss: 0.3810 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 07:34:09 (running for 00:42:41.27)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.504 |  0.176 |                   37 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.365 |  0.157 |                   34 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.276 |  0.146 |                   27 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14638786017894745
[2m[36m(func pid=168155)[0m mae:  0.0953172892332077
[2m[36m(func pid=168155)[0m rmse_per_class: [0.079, 0.249, 0.027, 0.287, 0.067, 0.15, 0.231, 0.114, 0.139, 0.122]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15721330046653748
[2m[36m(func pid=165914)[0m mae:  0.11359037458896637
[2m[36m(func pid=165914)[0m rmse_per_class: [0.107, 0.238, 0.056, 0.307, 0.061, 0.176, 0.262, 0.128, 0.14, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17639552056789398
[2m[36m(func pid=164771)[0m mae:  0.12925127148628235
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.332, 0.098, 0.19, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2696 | Steps: 2 | Val loss: 0.2704 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3604 | Steps: 2 | Val loss: 0.2855 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4932 | Steps: 2 | Val loss: 0.3780 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 07:34:15 (running for 00:42:46.66)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.499 |  0.176 |                   38 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.368 |  0.157 |                   35 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.27  |  0.147 |                   28 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14749011397361755
[2m[36m(func pid=168155)[0m mae:  0.09573879092931747
[2m[36m(func pid=168155)[0m rmse_per_class: [0.08, 0.25, 0.027, 0.287, 0.065, 0.15, 0.23, 0.115, 0.141, 0.13]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15690353512763977
[2m[36m(func pid=165914)[0m mae:  0.11331085860729218
[2m[36m(func pid=165914)[0m rmse_per_class: [0.107, 0.238, 0.056, 0.306, 0.061, 0.175, 0.261, 0.128, 0.14, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17628316581249237
[2m[36m(func pid=164771)[0m mae:  0.12916143238544464
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.332, 0.098, 0.189, 0.29, 0.141, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2764 | Steps: 2 | Val loss: 0.2723 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3579 | Steps: 2 | Val loss: 0.2848 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4911 | Steps: 2 | Val loss: 0.3754 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 07:34:20 (running for 00:42:51.82)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.493 |  0.176 |                   39 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.36  |  0.157 |                   36 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.276 |  0.149 |                   29 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14854981005191803
[2m[36m(func pid=168155)[0m mae:  0.09608520567417145
[2m[36m(func pid=168155)[0m rmse_per_class: [0.079, 0.252, 0.026, 0.291, 0.065, 0.15, 0.227, 0.115, 0.142, 0.139]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15654690563678741
[2m[36m(func pid=165914)[0m mae:  0.11299896240234375
[2m[36m(func pid=165914)[0m rmse_per_class: [0.106, 0.237, 0.055, 0.306, 0.061, 0.175, 0.261, 0.127, 0.139, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17615708708763123
[2m[36m(func pid=164771)[0m mae:  0.12906934320926666
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.332, 0.097, 0.189, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2738 | Steps: 2 | Val loss: 0.2728 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3563 | Steps: 2 | Val loss: 0.2844 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4859 | Steps: 2 | Val loss: 0.3729 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 07:34:25 (running for 00:42:57.30)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.491 |  0.176 |                   40 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.358 |  0.157 |                   37 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.274 |  0.148 |                   30 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14828674495220184
[2m[36m(func pid=168155)[0m mae:  0.09555234760046005
[2m[36m(func pid=168155)[0m rmse_per_class: [0.075, 0.253, 0.026, 0.291, 0.063, 0.151, 0.222, 0.116, 0.142, 0.144]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15627451241016388
[2m[36m(func pid=165914)[0m mae:  0.11277146637439728
[2m[36m(func pid=165914)[0m rmse_per_class: [0.106, 0.237, 0.055, 0.306, 0.061, 0.174, 0.26, 0.127, 0.139, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17606359720230103
[2m[36m(func pid=164771)[0m mae:  0.1290072202682495
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.332, 0.097, 0.189, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2734 | Steps: 2 | Val loss: 0.2748 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3592 | Steps: 2 | Val loss: 0.2838 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4831 | Steps: 2 | Val loss: 0.3703 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=168155)[0m rmse: 0.14901569485664368
[2m[36m(func pid=168155)[0m mae:  0.09575440734624863
[2m[36m(func pid=168155)[0m rmse_per_class: [0.074, 0.254, 0.026, 0.296, 0.065, 0.152, 0.219, 0.115, 0.142, 0.148]
== Status ==
Current time: 2024-01-07 07:34:30 (running for 00:43:02.45)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.486 |  0.176 |                   41 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.356 |  0.156 |                   38 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.273 |  0.149 |                   31 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15595167875289917
[2m[36m(func pid=165914)[0m mae:  0.11248254776000977
[2m[36m(func pid=165914)[0m rmse_per_class: [0.105, 0.237, 0.054, 0.304, 0.061, 0.174, 0.26, 0.127, 0.139, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17598631978034973
[2m[36m(func pid=164771)[0m mae:  0.12893930077552795
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.332, 0.097, 0.189, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2623 | Steps: 2 | Val loss: 0.2758 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3526 | Steps: 2 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4815 | Steps: 2 | Val loss: 0.3678 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 07:34:36 (running for 00:43:07.57)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.483 |  0.176 |                   42 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.359 |  0.156 |                   39 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.262 |  0.149 |                   32 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.1489480435848236
[2m[36m(func pid=168155)[0m mae:  0.09541962295770645
[2m[36m(func pid=168155)[0m rmse_per_class: [0.073, 0.255, 0.026, 0.298, 0.068, 0.152, 0.215, 0.116, 0.14, 0.147]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.1556798666715622
[2m[36m(func pid=165914)[0m mae:  0.11226041615009308
[2m[36m(func pid=165914)[0m rmse_per_class: [0.105, 0.237, 0.054, 0.304, 0.061, 0.173, 0.26, 0.127, 0.139, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17582115530967712
[2m[36m(func pid=164771)[0m mae:  0.12881110608577728
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.257, 0.09, 0.331, 0.096, 0.189, 0.29, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2728 | Steps: 2 | Val loss: 0.2765 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3515 | Steps: 2 | Val loss: 0.2830 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4756 | Steps: 2 | Val loss: 0.3653 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 07:34:41 (running for 00:43:12.84)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.482 |  0.176 |                   43 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.353 |  0.156 |                   40 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.273 |  0.149 |                   33 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14915886521339417
[2m[36m(func pid=168155)[0m mae:  0.09530916064977646
[2m[36m(func pid=168155)[0m rmse_per_class: [0.07, 0.255, 0.025, 0.299, 0.069, 0.153, 0.213, 0.116, 0.14, 0.15]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15547379851341248
[2m[36m(func pid=165914)[0m mae:  0.11209625005722046
[2m[36m(func pid=165914)[0m rmse_per_class: [0.104, 0.236, 0.054, 0.303, 0.061, 0.173, 0.26, 0.126, 0.139, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17574934661388397
[2m[36m(func pid=164771)[0m mae:  0.12875089049339294
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.331, 0.096, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2705 | Steps: 2 | Val loss: 0.2762 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3511 | Steps: 2 | Val loss: 0.2825 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4715 | Steps: 2 | Val loss: 0.3627 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 07:34:46 (running for 00:43:17.88)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.476 |  0.176 |                   44 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.351 |  0.155 |                   41 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.271 |  0.149 |                   34 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14887076616287231
[2m[36m(func pid=168155)[0m mae:  0.0948680192232132
[2m[36m(func pid=168155)[0m rmse_per_class: [0.069, 0.255, 0.025, 0.299, 0.07, 0.153, 0.213, 0.115, 0.14, 0.149]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.1551654189825058
[2m[36m(func pid=165914)[0m mae:  0.1117999404668808
[2m[36m(func pid=165914)[0m rmse_per_class: [0.104, 0.236, 0.053, 0.303, 0.061, 0.173, 0.26, 0.126, 0.139, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17563912272453308
[2m[36m(func pid=164771)[0m mae:  0.12866854667663574
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.332, 0.095, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2783 | Steps: 2 | Val loss: 0.2751 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3516 | Steps: 2 | Val loss: 0.2822 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4701 | Steps: 2 | Val loss: 0.3608 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 07:34:51 (running for 00:43:23.15)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.471 |  0.176 |                   45 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.351 |  0.155 |                   42 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.278 |  0.148 |                   35 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14838345348834991
[2m[36m(func pid=168155)[0m mae:  0.09447029232978821
[2m[36m(func pid=168155)[0m rmse_per_class: [0.068, 0.255, 0.025, 0.298, 0.072, 0.152, 0.214, 0.115, 0.139, 0.147]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15499289333820343
[2m[36m(func pid=165914)[0m mae:  0.1116497740149498
[2m[36m(func pid=165914)[0m rmse_per_class: [0.104, 0.235, 0.053, 0.302, 0.062, 0.172, 0.26, 0.126, 0.139, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17559322714805603
[2m[36m(func pid=164771)[0m mae:  0.12862451374530792
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.332, 0.095, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2610 | Steps: 2 | Val loss: 0.2746 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3486 | Steps: 2 | Val loss: 0.2820 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 07:34:56 (running for 00:43:28.26)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.47  |  0.176 |                   46 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.352 |  0.155 |                   43 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.261 |  0.148 |                   36 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14830917119979858
[2m[36m(func pid=168155)[0m mae:  0.0943201556801796
[2m[36m(func pid=168155)[0m rmse_per_class: [0.07, 0.256, 0.025, 0.297, 0.073, 0.151, 0.215, 0.115, 0.137, 0.145]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4680 | Steps: 2 | Val loss: 0.3585 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=165914)[0m rmse: 0.1548091322183609
[2m[36m(func pid=165914)[0m mae:  0.11147820949554443
[2m[36m(func pid=165914)[0m rmse_per_class: [0.103, 0.235, 0.052, 0.301, 0.062, 0.172, 0.26, 0.125, 0.139, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.1754484921693802
[2m[36m(func pid=164771)[0m mae:  0.12851056456565857
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.332, 0.095, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2634 | Steps: 2 | Val loss: 0.2739 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3474 | Steps: 2 | Val loss: 0.2819 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 07:35:01 (running for 00:43:33.52)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.468 |  0.175 |                   47 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.349 |  0.155 |                   44 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.263 |  0.148 |                   37 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14812922477722168
[2m[36m(func pid=168155)[0m mae:  0.09418467432260513
[2m[36m(func pid=168155)[0m rmse_per_class: [0.072, 0.257, 0.025, 0.296, 0.073, 0.15, 0.217, 0.114, 0.135, 0.141]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4647 | Steps: 2 | Val loss: 0.3574 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=165914)[0m rmse: 0.15470555424690247
[2m[36m(func pid=165914)[0m mae:  0.11138185113668442
[2m[36m(func pid=165914)[0m rmse_per_class: [0.103, 0.235, 0.052, 0.302, 0.062, 0.171, 0.259, 0.125, 0.139, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2571 | Steps: 2 | Val loss: 0.2740 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=164771)[0m rmse: 0.17543891072273254
[2m[36m(func pid=164771)[0m mae:  0.1285075545310974
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.256, 0.089, 0.331, 0.095, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3513 | Steps: 2 | Val loss: 0.2818 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 07:35:07 (running for 00:43:38.79)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.465 |  0.175 |                   48 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.347 |  0.155 |                   45 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.257 |  0.148 |                   38 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.1484294831752777
[2m[36m(func pid=168155)[0m mae:  0.0944327786564827
[2m[36m(func pid=168155)[0m rmse_per_class: [0.076, 0.257, 0.026, 0.296, 0.073, 0.15, 0.219, 0.114, 0.134, 0.14]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4659 | Steps: 2 | Val loss: 0.3554 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=165914)[0m rmse: 0.15460403263568878
[2m[36m(func pid=165914)[0m mae:  0.11125143617391586
[2m[36m(func pid=165914)[0m rmse_per_class: [0.103, 0.235, 0.052, 0.302, 0.062, 0.171, 0.259, 0.125, 0.139, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2602 | Steps: 2 | Val loss: 0.2744 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=164771)[0m rmse: 0.17530640959739685
[2m[36m(func pid=164771)[0m mae:  0.12838886678218842
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.256, 0.088, 0.331, 0.095, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3441 | Steps: 2 | Val loss: 0.2813 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 07:35:12 (running for 00:43:44.02)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.466 |  0.175 |                   49 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.351 |  0.155 |                   46 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.26  |  0.149 |                   39 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14896413683891296
[2m[36m(func pid=168155)[0m mae:  0.09479623287916183
[2m[36m(func pid=168155)[0m rmse_per_class: [0.079, 0.257, 0.026, 0.296, 0.073, 0.15, 0.222, 0.115, 0.134, 0.139]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4633 | Steps: 2 | Val loss: 0.3539 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=165914)[0m rmse: 0.15428023040294647
[2m[36m(func pid=165914)[0m mae:  0.11095956712961197
[2m[36m(func pid=165914)[0m rmse_per_class: [0.102, 0.235, 0.052, 0.301, 0.062, 0.171, 0.259, 0.125, 0.139, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2773 | Steps: 2 | Val loss: 0.2749 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=164771)[0m rmse: 0.17519238591194153
[2m[36m(func pid=164771)[0m mae:  0.1283055543899536
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.256, 0.088, 0.331, 0.094, 0.189, 0.289, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3430 | Steps: 2 | Val loss: 0.2810 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 07:35:17 (running for 00:43:49.22)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.463 |  0.175 |                   50 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.344 |  0.154 |                   47 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.277 |  0.149 |                   40 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14929582178592682
[2m[36m(func pid=168155)[0m mae:  0.09499058872461319
[2m[36m(func pid=168155)[0m rmse_per_class: [0.08, 0.257, 0.026, 0.295, 0.071, 0.15, 0.224, 0.115, 0.134, 0.141]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4601 | Steps: 2 | Val loss: 0.3518 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=165914)[0m rmse: 0.15409162640571594
[2m[36m(func pid=165914)[0m mae:  0.11076663434505463
[2m[36m(func pid=165914)[0m rmse_per_class: [0.102, 0.234, 0.051, 0.301, 0.062, 0.17, 0.259, 0.124, 0.138, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2601 | Steps: 2 | Val loss: 0.2746 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=164771)[0m rmse: 0.17510026693344116
[2m[36m(func pid=164771)[0m mae:  0.1282377988100052
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.256, 0.088, 0.331, 0.094, 0.189, 0.288, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3457 | Steps: 2 | Val loss: 0.2811 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 07:35:23 (running for 00:43:54.70)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.46  |  0.175 |                   51 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.343 |  0.154 |                   48 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.26  |  0.149 |                   41 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14927972853183746
[2m[36m(func pid=168155)[0m mae:  0.09492569416761398
[2m[36m(func pid=168155)[0m rmse_per_class: [0.081, 0.256, 0.026, 0.295, 0.072, 0.151, 0.223, 0.115, 0.134, 0.141]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4569 | Steps: 2 | Val loss: 0.3499 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=165914)[0m rmse: 0.1541198194026947
[2m[36m(func pid=165914)[0m mae:  0.11081433296203613
[2m[36m(func pid=165914)[0m rmse_per_class: [0.102, 0.235, 0.051, 0.301, 0.063, 0.171, 0.259, 0.124, 0.138, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2645 | Steps: 2 | Val loss: 0.2752 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=164771)[0m rmse: 0.17496061325073242
[2m[36m(func pid=164771)[0m mae:  0.12813208997249603
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.256, 0.088, 0.331, 0.094, 0.189, 0.288, 0.14, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3442 | Steps: 2 | Val loss: 0.2805 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 07:35:28 (running for 00:43:59.73)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.457 |  0.175 |                   52 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.346 |  0.154 |                   49 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.265 |  0.149 |                   42 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.1493217647075653
[2m[36m(func pid=168155)[0m mae:  0.09487636387348175
[2m[36m(func pid=168155)[0m rmse_per_class: [0.079, 0.255, 0.025, 0.295, 0.071, 0.151, 0.221, 0.116, 0.136, 0.145]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4537 | Steps: 2 | Val loss: 0.3482 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=165914)[0m rmse: 0.1537451446056366
[2m[36m(func pid=165914)[0m mae:  0.1104431301355362
[2m[36m(func pid=165914)[0m rmse_per_class: [0.101, 0.234, 0.051, 0.3, 0.063, 0.17, 0.258, 0.124, 0.138, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2572 | Steps: 2 | Val loss: 0.2761 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=164771)[0m rmse: 0.17485174536705017
[2m[36m(func pid=164771)[0m mae:  0.1280486285686493
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.256, 0.087, 0.33, 0.093, 0.189, 0.288, 0.14, 0.141, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3402 | Steps: 2 | Val loss: 0.2798 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 07:35:33 (running for 00:44:05.04)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.454 |  0.175 |                   53 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.344 |  0.154 |                   50 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.257 |  0.15  |                   43 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14960215985774994
[2m[36m(func pid=168155)[0m mae:  0.09491665661334991
[2m[36m(func pid=168155)[0m rmse_per_class: [0.077, 0.256, 0.025, 0.296, 0.072, 0.153, 0.219, 0.117, 0.136, 0.145]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4551 | Steps: 2 | Val loss: 0.3469 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=165914)[0m rmse: 0.15336152911186218
[2m[36m(func pid=165914)[0m mae:  0.1100902408361435
[2m[36m(func pid=165914)[0m rmse_per_class: [0.101, 0.234, 0.051, 0.298, 0.063, 0.17, 0.258, 0.124, 0.138, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2595 | Steps: 2 | Val loss: 0.2766 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=164771)[0m rmse: 0.1747734695672989
[2m[36m(func pid=164771)[0m mae:  0.1279800832271576
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.087, 0.33, 0.093, 0.189, 0.288, 0.14, 0.141, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3424 | Steps: 2 | Val loss: 0.2796 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 07:35:38 (running for 00:44:10.25)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.455 |  0.175 |                   54 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.34  |  0.153 |                   51 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.26  |  0.15  |                   44 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14954304695129395
[2m[36m(func pid=168155)[0m mae:  0.09463836252689362
[2m[36m(func pid=168155)[0m rmse_per_class: [0.073, 0.257, 0.025, 0.297, 0.072, 0.153, 0.217, 0.118, 0.137, 0.145]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4521 | Steps: 2 | Val loss: 0.3455 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=165914)[0m rmse: 0.15316951274871826
[2m[36m(func pid=165914)[0m mae:  0.10990331321954727
[2m[36m(func pid=165914)[0m rmse_per_class: [0.1, 0.234, 0.051, 0.298, 0.063, 0.17, 0.257, 0.123, 0.138, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2568 | Steps: 2 | Val loss: 0.2771 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=164771)[0m rmse: 0.17478999495506287
[2m[36m(func pid=164771)[0m mae:  0.1280118227005005
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.087, 0.33, 0.093, 0.189, 0.288, 0.139, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
== Status ==
Current time: 2024-01-07 07:35:43 (running for 00:44:15.38)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.452 |  0.175 |                   55 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.342 |  0.153 |                   52 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.257 |  0.15  |                   45 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.14959511160850525
[2m[36m(func pid=168155)[0m mae:  0.09460079669952393
[2m[36m(func pid=168155)[0m rmse_per_class: [0.073, 0.258, 0.025, 0.298, 0.073, 0.153, 0.217, 0.118, 0.136, 0.144]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3419 | Steps: 2 | Val loss: 0.2794 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4497 | Steps: 2 | Val loss: 0.3441 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=165914)[0m rmse: 0.15306200087070465
[2m[36m(func pid=165914)[0m mae:  0.10977796465158463
[2m[36m(func pid=165914)[0m rmse_per_class: [0.1, 0.234, 0.051, 0.298, 0.063, 0.169, 0.257, 0.123, 0.138, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2550 | Steps: 2 | Val loss: 0.2781 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 07:35:48 (running for 00:44:20.49)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.45  |  0.175 |                   56 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.342 |  0.153 |                   53 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.257 |  0.15  |                   45 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=164771)[0m rmse: 0.17471401393413544
[2m[36m(func pid=164771)[0m mae:  0.1279568076133728
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.087, 0.33, 0.093, 0.189, 0.288, 0.139, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3401 | Steps: 2 | Val loss: 0.2795 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=168155)[0m rmse: 0.14986437559127808
[2m[36m(func pid=168155)[0m mae:  0.09470857679843903
[2m[36m(func pid=168155)[0m rmse_per_class: [0.073, 0.26, 0.025, 0.3, 0.073, 0.153, 0.217, 0.118, 0.136, 0.144]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.1530151665210724
[2m[36m(func pid=165914)[0m mae:  0.10973048210144043
[2m[36m(func pid=165914)[0m rmse_per_class: [0.099, 0.234, 0.05, 0.299, 0.063, 0.169, 0.256, 0.123, 0.138, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4478 | Steps: 2 | Val loss: 0.3428 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2577 | Steps: 2 | Val loss: 0.2787 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 07:35:54 (running for 00:44:25.95)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.448 |  0.175 |                   57 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.34  |  0.153 |                   54 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.255 |  0.15  |                   46 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=164771)[0m rmse: 0.17462670803070068
[2m[36m(func pid=164771)[0m mae:  0.1278746873140335
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.087, 0.33, 0.092, 0.189, 0.288, 0.139, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.14997270703315735
[2m[36m(func pid=168155)[0m mae:  0.09463782608509064
[2m[36m(func pid=168155)[0m rmse_per_class: [0.072, 0.261, 0.025, 0.301, 0.071, 0.152, 0.217, 0.119, 0.136, 0.146]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3394 | Steps: 2 | Val loss: 0.2792 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=165914)[0m rmse: 0.1527605503797531
[2m[36m(func pid=165914)[0m mae:  0.10949919372797012
[2m[36m(func pid=165914)[0m rmse_per_class: [0.099, 0.234, 0.05, 0.299, 0.063, 0.168, 0.256, 0.123, 0.138, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2614 | Steps: 2 | Val loss: 0.2787 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4465 | Steps: 2 | Val loss: 0.3415 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=168155)[0m rmse: 0.1500265896320343
[2m[36m(func pid=168155)[0m mae:  0.09465459734201431
[2m[36m(func pid=168155)[0m rmse_per_class: [0.072, 0.261, 0.025, 0.3, 0.07, 0.151, 0.218, 0.119, 0.137, 0.147]
[2m[36m(func pid=168155)[0m 
== Status ==
Current time: 2024-01-07 07:35:59 (running for 00:44:31.29)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.448 |  0.175 |                   57 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.339 |  0.153 |                   55 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.261 |  0.15  |                   48 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=164771)[0m rmse: 0.17450563609600067
[2m[36m(func pid=164771)[0m mae:  0.12778253853321075
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.33, 0.092, 0.189, 0.288, 0.139, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3397 | Steps: 2 | Val loss: 0.2787 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=165914)[0m rmse: 0.15242937207221985
[2m[36m(func pid=165914)[0m mae:  0.10919077694416046
[2m[36m(func pid=165914)[0m rmse_per_class: [0.098, 0.234, 0.05, 0.299, 0.063, 0.168, 0.255, 0.123, 0.138, 0.097]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2526 | Steps: 2 | Val loss: 0.2783 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4436 | Steps: 2 | Val loss: 0.3402 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 07:36:05 (running for 00:44:36.91)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.447 |  0.175 |                   58 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.34  |  0.152 |                   56 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.253 |  0.15  |                   49 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.1502523571252823
[2m[36m(func pid=168155)[0m mae:  0.09498675912618637
[2m[36m(func pid=168155)[0m rmse_per_class: [0.073, 0.261, 0.025, 0.3, 0.07, 0.151, 0.221, 0.118, 0.138, 0.146]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3339 | Steps: 2 | Val loss: 0.2786 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=164771)[0m rmse: 0.17443165183067322
[2m[36m(func pid=164771)[0m mae:  0.12772667407989502
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.33, 0.092, 0.189, 0.287, 0.139, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15232186019420624
[2m[36m(func pid=165914)[0m mae:  0.10906632989645004
[2m[36m(func pid=165914)[0m rmse_per_class: [0.097, 0.234, 0.05, 0.299, 0.063, 0.168, 0.255, 0.123, 0.138, 0.097]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2605 | Steps: 2 | Val loss: 0.2787 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4415 | Steps: 2 | Val loss: 0.3388 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=168155)[0m rmse: 0.15045194327831268
[2m[36m(func pid=168155)[0m mae:  0.09511514753103256
[2m[36m(func pid=168155)[0m rmse_per_class: [0.073, 0.262, 0.025, 0.3, 0.07, 0.15, 0.222, 0.117, 0.139, 0.146]
== Status ==
Current time: 2024-01-07 07:36:10 (running for 00:44:42.00)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.444 |  0.174 |                   59 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.334 |  0.152 |                   57 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.261 |  0.15  |                   50 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3376 | Steps: 2 | Val loss: 0.2785 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=164771)[0m rmse: 0.17431725561618805
[2m[36m(func pid=164771)[0m mae:  0.12762770056724548
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.33, 0.092, 0.189, 0.287, 0.139, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2732 | Steps: 2 | Val loss: 0.2782 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=165914)[0m rmse: 0.15223154425621033
[2m[36m(func pid=165914)[0m mae:  0.10890122503042221
[2m[36m(func pid=165914)[0m rmse_per_class: [0.097, 0.234, 0.049, 0.299, 0.063, 0.167, 0.255, 0.122, 0.137, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4395 | Steps: 2 | Val loss: 0.3378 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 07:36:15 (running for 00:44:47.40)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.442 |  0.174 |                   60 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.338 |  0.152 |                   58 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.273 |  0.15  |                   51 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.1501532644033432
[2m[36m(func pid=168155)[0m mae:  0.09520956128835678
[2m[36m(func pid=168155)[0m rmse_per_class: [0.076, 0.262, 0.025, 0.302, 0.072, 0.15, 0.221, 0.117, 0.14, 0.136]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m rmse: 0.17425493896007538
[2m[36m(func pid=164771)[0m mae:  0.12756551802158356
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.254, 0.086, 0.33, 0.091, 0.189, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3343 | Steps: 2 | Val loss: 0.2780 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2609 | Steps: 2 | Val loss: 0.2784 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=165914)[0m rmse: 0.15195700526237488
[2m[36m(func pid=165914)[0m mae:  0.10861793905496597
[2m[36m(func pid=165914)[0m rmse_per_class: [0.097, 0.234, 0.049, 0.298, 0.063, 0.167, 0.254, 0.122, 0.137, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4399 | Steps: 2 | Val loss: 0.3370 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 07:36:21 (running for 00:44:52.77)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.439 |  0.174 |                   61 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.334 |  0.152 |                   59 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.261 |  0.15  |                   52 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.1502756029367447
[2m[36m(func pid=168155)[0m mae:  0.09513596445322037
[2m[36m(func pid=168155)[0m rmse_per_class: [0.075, 0.261, 0.025, 0.302, 0.072, 0.15, 0.22, 0.117, 0.141, 0.14]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3352 | Steps: 2 | Val loss: 0.2777 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=164771)[0m rmse: 0.1741977035999298
[2m[36m(func pid=164771)[0m mae:  0.1275300234556198
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.33, 0.091, 0.188, 0.287, 0.139, 0.142, 0.108]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.151760071516037
[2m[36m(func pid=165914)[0m mae:  0.10838981717824936
[2m[36m(func pid=165914)[0m rmse_per_class: [0.096, 0.234, 0.048, 0.298, 0.063, 0.167, 0.254, 0.122, 0.137, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2552 | Steps: 2 | Val loss: 0.2798 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4380 | Steps: 2 | Val loss: 0.3360 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 07:36:26 (running for 00:44:58.06)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.44  |  0.174 |                   62 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.335 |  0.152 |                   60 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.255 |  0.151 |                   53 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.150889053940773
[2m[36m(func pid=168155)[0m mae:  0.09537506103515625
[2m[36m(func pid=168155)[0m rmse_per_class: [0.075, 0.262, 0.026, 0.304, 0.071, 0.15, 0.219, 0.117, 0.141, 0.145]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3296 | Steps: 2 | Val loss: 0.2776 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=164771)[0m rmse: 0.1741149127483368
[2m[36m(func pid=164771)[0m mae:  0.12745532393455505
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.33, 0.091, 0.188, 0.287, 0.14, 0.142, 0.107]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2544 | Steps: 2 | Val loss: 0.2809 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=165914)[0m rmse: 0.15165744721889496
[2m[36m(func pid=165914)[0m mae:  0.10826961696147919
[2m[36m(func pid=165914)[0m rmse_per_class: [0.096, 0.234, 0.048, 0.297, 0.063, 0.166, 0.253, 0.122, 0.137, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4382 | Steps: 2 | Val loss: 0.3350 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 07:36:31 (running for 00:45:03.22)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.438 |  0.174 |                   63 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.33  |  0.152 |                   61 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.254 |  0.151 |                   54 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.15137979388237
[2m[36m(func pid=168155)[0m mae:  0.0956358090043068
[2m[36m(func pid=168155)[0m rmse_per_class: [0.076, 0.261, 0.025, 0.304, 0.07, 0.151, 0.218, 0.117, 0.143, 0.149]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3348 | Steps: 2 | Val loss: 0.2778 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=164771)[0m rmse: 0.17399916052818298
[2m[36m(func pid=164771)[0m mae:  0.12735804915428162
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.329, 0.091, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2686 | Steps: 2 | Val loss: 0.2808 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=165914)[0m rmse: 0.15176008641719818
[2m[36m(func pid=165914)[0m mae:  0.10836607217788696
[2m[36m(func pid=165914)[0m rmse_per_class: [0.096, 0.234, 0.048, 0.298, 0.063, 0.166, 0.253, 0.122, 0.138, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4357 | Steps: 2 | Val loss: 0.3344 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 07:36:36 (running for 00:45:08.53)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.438 |  0.174 |                   64 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.335 |  0.152 |                   62 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.269 |  0.151 |                   55 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.1508462131023407
[2m[36m(func pid=168155)[0m mae:  0.09529101103544235
[2m[36m(func pid=168155)[0m rmse_per_class: [0.074, 0.261, 0.025, 0.304, 0.068, 0.15, 0.218, 0.118, 0.142, 0.149]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3297 | Steps: 2 | Val loss: 0.2776 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=164771)[0m rmse: 0.1738978773355484
[2m[36m(func pid=164771)[0m mae:  0.12727580964565277
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.329, 0.09, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2569 | Steps: 2 | Val loss: 0.2803 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=165914)[0m rmse: 0.15154916048049927
[2m[36m(func pid=165914)[0m mae:  0.10825147479772568
[2m[36m(func pid=165914)[0m rmse_per_class: [0.096, 0.234, 0.048, 0.298, 0.063, 0.166, 0.253, 0.121, 0.138, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4341 | Steps: 2 | Val loss: 0.3336 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 07:36:42 (running for 00:45:13.67)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.436 |  0.174 |                   65 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.33  |  0.152 |                   63 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.257 |  0.15  |                   56 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.15038946270942688
[2m[36m(func pid=168155)[0m mae:  0.09499432891607285
[2m[36m(func pid=168155)[0m rmse_per_class: [0.074, 0.262, 0.025, 0.305, 0.068, 0.15, 0.218, 0.118, 0.139, 0.145]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3308 | Steps: 2 | Val loss: 0.2776 | Batch size: 32 | lr: 0.01 | Duration: 2.60s
[2m[36m(func pid=164771)[0m rmse: 0.1739417463541031
[2m[36m(func pid=164771)[0m mae:  0.12732453644275665
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.329, 0.09, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2524 | Steps: 2 | Val loss: 0.2815 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=165914)[0m rmse: 0.1515248566865921
[2m[36m(func pid=165914)[0m mae:  0.10819678008556366
[2m[36m(func pid=165914)[0m rmse_per_class: [0.096, 0.234, 0.048, 0.299, 0.063, 0.166, 0.253, 0.121, 0.138, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4298 | Steps: 2 | Val loss: 0.3329 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=168155)[0m rmse: 0.15105637907981873
[2m[36m(func pid=168155)[0m mae:  0.09521640092134476
[2m[36m(func pid=168155)[0m rmse_per_class: [0.075, 0.263, 0.025, 0.305, 0.068, 0.15, 0.219, 0.118, 0.138, 0.149]
== Status ==
Current time: 2024-01-07 07:36:47 (running for 00:45:18.80)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.434 |  0.174 |                   66 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.331 |  0.152 |                   64 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.252 |  0.151 |                   57 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3326 | Steps: 2 | Val loss: 0.2771 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=164771)[0m rmse: 0.17391592264175415
[2m[36m(func pid=164771)[0m mae:  0.127288818359375
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.329, 0.09, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2456 | Steps: 2 | Val loss: 0.2817 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=165914)[0m rmse: 0.15121375024318695
[2m[36m(func pid=165914)[0m mae:  0.10787080228328705
[2m[36m(func pid=165914)[0m rmse_per_class: [0.095, 0.234, 0.047, 0.298, 0.063, 0.166, 0.252, 0.121, 0.138, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4315 | Steps: 2 | Val loss: 0.3320 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 07:36:52 (running for 00:45:24.18)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.43  |  0.174 |                   67 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.333 |  0.151 |                   65 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.246 |  0.151 |                   58 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.15141989290714264
[2m[36m(func pid=168155)[0m mae:  0.095200315117836
[2m[36m(func pid=168155)[0m rmse_per_class: [0.076, 0.263, 0.025, 0.304, 0.068, 0.15, 0.22, 0.118, 0.137, 0.153]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3345 | Steps: 2 | Val loss: 0.2769 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=164771)[0m rmse: 0.17383889853954315
[2m[36m(func pid=164771)[0m mae:  0.12723705172538757
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.086, 0.329, 0.09, 0.188, 0.287, 0.139, 0.142, 0.107]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2594 | Steps: 2 | Val loss: 0.2820 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=165914)[0m rmse: 0.15106745064258575
[2m[36m(func pid=165914)[0m mae:  0.10767298936843872
[2m[36m(func pid=165914)[0m rmse_per_class: [0.094, 0.234, 0.047, 0.298, 0.063, 0.165, 0.252, 0.121, 0.138, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4299 | Steps: 2 | Val loss: 0.3310 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 07:36:58 (running for 00:45:29.58)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.432 |  0.174 |                   68 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.335 |  0.151 |                   66 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.259 |  0.152 |                   59 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.1517871767282486
[2m[36m(func pid=168155)[0m mae:  0.0951722115278244
[2m[36m(func pid=168155)[0m rmse_per_class: [0.076, 0.263, 0.025, 0.303, 0.069, 0.151, 0.221, 0.118, 0.136, 0.156]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3314 | Steps: 2 | Val loss: 0.2767 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=164771)[0m rmse: 0.17366287112236023
[2m[36m(func pid=164771)[0m mae:  0.12709233164787292
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.255, 0.085, 0.329, 0.09, 0.188, 0.286, 0.139, 0.142, 0.107]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2485 | Steps: 2 | Val loss: 0.2824 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=165914)[0m rmse: 0.15085768699645996
[2m[36m(func pid=165914)[0m mae:  0.10757763683795929
[2m[36m(func pid=165914)[0m rmse_per_class: [0.094, 0.234, 0.047, 0.297, 0.063, 0.165, 0.252, 0.121, 0.138, 0.098]
[2m[36m(func pid=165914)[0m 
== Status ==
Current time: 2024-01-07 07:37:03 (running for 00:45:34.60)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.43  |  0.174 |                   69 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.331 |  0.151 |                   67 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.249 |  0.152 |                   60 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.15229156613349915
[2m[36m(func pid=168155)[0m mae:  0.09539030492305756
[2m[36m(func pid=168155)[0m rmse_per_class: [0.078, 0.265, 0.025, 0.303, 0.072, 0.152, 0.221, 0.118, 0.135, 0.154]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4290 | Steps: 2 | Val loss: 0.3299 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3263 | Steps: 2 | Val loss: 0.2766 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2537 | Steps: 2 | Val loss: 0.2832 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=164771)[0m rmse: 0.17353864014148712
[2m[36m(func pid=164771)[0m mae:  0.12699900567531586
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.254, 0.085, 0.329, 0.089, 0.188, 0.286, 0.139, 0.142, 0.107]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15086707472801208
[2m[36m(func pid=165914)[0m mae:  0.10754171758890152
[2m[36m(func pid=165914)[0m rmse_per_class: [0.094, 0.234, 0.047, 0.297, 0.063, 0.165, 0.252, 0.121, 0.138, 0.098]
[2m[36m(func pid=165914)[0m 
== Status ==
Current time: 2024-01-07 07:37:08 (running for 00:45:39.85)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.429 |  0.174 |                   70 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.326 |  0.151 |                   68 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.254 |  0.153 |                   61 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.15280607342720032
[2m[36m(func pid=168155)[0m mae:  0.09552634507417679
[2m[36m(func pid=168155)[0m rmse_per_class: [0.078, 0.265, 0.025, 0.304, 0.073, 0.152, 0.222, 0.117, 0.135, 0.156]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4280 | Steps: 2 | Val loss: 0.3292 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3286 | Steps: 2 | Val loss: 0.2761 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2535 | Steps: 2 | Val loss: 0.2833 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=164771)[0m rmse: 0.17347471415996552
[2m[36m(func pid=164771)[0m mae:  0.1269579827785492
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.254, 0.085, 0.329, 0.089, 0.188, 0.286, 0.139, 0.142, 0.107]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.1505964994430542
[2m[36m(func pid=165914)[0m mae:  0.10724649578332901
[2m[36m(func pid=165914)[0m rmse_per_class: [0.094, 0.234, 0.046, 0.297, 0.063, 0.165, 0.251, 0.12, 0.138, 0.099]
[2m[36m(func pid=165914)[0m 
== Status ==
Current time: 2024-01-07 07:37:13 (running for 00:45:45.11)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.428 |  0.173 |                   71 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.329 |  0.151 |                   69 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.253 |  0.153 |                   62 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.15301808714866638
[2m[36m(func pid=168155)[0m mae:  0.09554709494113922
[2m[36m(func pid=168155)[0m rmse_per_class: [0.079, 0.264, 0.025, 0.305, 0.073, 0.152, 0.22, 0.117, 0.137, 0.156]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4266 | Steps: 2 | Val loss: 0.3282 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3237 | Steps: 2 | Val loss: 0.2761 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2530 | Steps: 2 | Val loss: 0.2817 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=164771)[0m rmse: 0.17328207194805145
[2m[36m(func pid=164771)[0m mae:  0.12680846452713013
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.254, 0.084, 0.329, 0.088, 0.188, 0.286, 0.139, 0.141, 0.107]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.1505916714668274
[2m[36m(func pid=165914)[0m mae:  0.10719402134418488
[2m[36m(func pid=165914)[0m rmse_per_class: [0.093, 0.234, 0.046, 0.297, 0.063, 0.165, 0.25, 0.12, 0.139, 0.099]
[2m[36m(func pid=165914)[0m 
== Status ==
Current time: 2024-01-07 07:37:18 (running for 00:45:50.45)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.427 |  0.173 |                   72 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.324 |  0.151 |                   70 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.253 |  0.152 |                   63 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.15212571620941162
[2m[36m(func pid=168155)[0m mae:  0.09475962072610855
[2m[36m(func pid=168155)[0m rmse_per_class: [0.078, 0.262, 0.025, 0.302, 0.073, 0.152, 0.219, 0.117, 0.138, 0.156]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4249 | Steps: 2 | Val loss: 0.3271 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3255 | Steps: 2 | Val loss: 0.2760 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2490 | Steps: 2 | Val loss: 0.2807 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=164771)[0m rmse: 0.17320755124092102
[2m[36m(func pid=164771)[0m mae:  0.12673547863960266
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.254, 0.084, 0.329, 0.088, 0.188, 0.286, 0.139, 0.141, 0.107]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.1503927856683731
[2m[36m(func pid=165914)[0m mae:  0.10704322159290314
[2m[36m(func pid=165914)[0m rmse_per_class: [0.093, 0.234, 0.046, 0.297, 0.063, 0.165, 0.25, 0.12, 0.139, 0.098]
[2m[36m(func pid=165914)[0m 
== Status ==
Current time: 2024-01-07 07:37:24 (running for 00:45:55.69)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.425 |  0.173 |                   73 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.326 |  0.15  |                   71 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.249 |  0.151 |                   64 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.15140189230442047
[2m[36m(func pid=168155)[0m mae:  0.09427666664123535
[2m[36m(func pid=168155)[0m rmse_per_class: [0.078, 0.26, 0.025, 0.301, 0.07, 0.151, 0.218, 0.117, 0.139, 0.154]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4226 | Steps: 2 | Val loss: 0.3265 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3230 | Steps: 2 | Val loss: 0.2757 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2580 | Steps: 2 | Val loss: 0.2797 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=164771)[0m rmse: 0.17315658926963806
[2m[36m(func pid=164771)[0m mae:  0.12669065594673157
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.254, 0.084, 0.328, 0.088, 0.188, 0.286, 0.139, 0.141, 0.107]
[2m[36m(func pid=164771)[0m 
[2m[36m(func pid=165914)[0m rmse: 0.15027020871639252
[2m[36m(func pid=165914)[0m mae:  0.10685040056705475
[2m[36m(func pid=165914)[0m rmse_per_class: [0.092, 0.234, 0.046, 0.296, 0.063, 0.164, 0.249, 0.12, 0.139, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.15064264833927155
[2m[36m(func pid=168155)[0m mae:  0.09373198449611664
[2m[36m(func pid=168155)[0m rmse_per_class: [0.076, 0.259, 0.025, 0.299, 0.068, 0.151, 0.218, 0.118, 0.139, 0.153]
[2m[36m(func pid=168155)[0m 
== Status ==
Current time: 2024-01-07 07:37:29 (running for 00:46:01.01)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15199999511241913
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00021 | RUNNING    | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.423 |  0.173 |                   74 |
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.323 |  0.15  |                   72 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.258 |  0.151 |                   65 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=164771)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4230 | Steps: 2 | Val loss: 0.3260 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3251 | Steps: 2 | Val loss: 0.2757 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2530 | Steps: 2 | Val loss: 0.2789 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=164771)[0m rmse: 0.17308607697486877
[2m[36m(func pid=164771)[0m mae:  0.12664783000946045
[2m[36m(func pid=164771)[0m rmse_per_class: [0.116, 0.254, 0.084, 0.328, 0.088, 0.188, 0.286, 0.139, 0.141, 0.107]
[2m[36m(func pid=165914)[0m rmse: 0.1502610594034195
[2m[36m(func pid=165914)[0m mae:  0.10674391686916351
[2m[36m(func pid=165914)[0m rmse_per_class: [0.092, 0.234, 0.045, 0.296, 0.063, 0.164, 0.25, 0.12, 0.138, 0.1]
[2m[36m(func pid=165914)[0m 
== Status ==
Current time: 2024-01-07 07:37:34 (running for 00:46:06.32)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1522499956190586
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.325 |  0.15  |                   73 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.253 |  0.15  |                   66 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.15009678900241852
[2m[36m(func pid=168155)[0m mae:  0.09361346811056137
[2m[36m(func pid=168155)[0m rmse_per_class: [0.077, 0.259, 0.025, 0.299, 0.066, 0.15, 0.218, 0.118, 0.14, 0.148]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3229 | Steps: 2 | Val loss: 0.2754 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2527 | Steps: 2 | Val loss: 0.2787 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=165914)[0m rmse: 0.1500554233789444
[2m[36m(func pid=165914)[0m mae:  0.10645625740289688
[2m[36m(func pid=165914)[0m rmse_per_class: [0.092, 0.234, 0.045, 0.296, 0.063, 0.164, 0.249, 0.12, 0.138, 0.1]
[2m[36m(func pid=165914)[0m 
== Status ==
Current time: 2024-01-07 07:37:40 (running for 00:46:11.64)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1522499956190586
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.323 |  0.15  |                   74 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.253 |  0.15  |                   67 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.1498505026102066
[2m[36m(func pid=168155)[0m mae:  0.09350740164518356
[2m[36m(func pid=168155)[0m rmse_per_class: [0.076, 0.258, 0.025, 0.298, 0.063, 0.15, 0.218, 0.12, 0.142, 0.149]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3267 | Steps: 2 | Val loss: 0.2752 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2512 | Steps: 2 | Val loss: 0.2802 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=165914)[0m rmse: 0.14990083873271942
[2m[36m(func pid=165914)[0m mae:  0.10631438344717026
[2m[36m(func pid=165914)[0m rmse_per_class: [0.092, 0.234, 0.045, 0.296, 0.063, 0.163, 0.249, 0.119, 0.138, 0.1]
[2m[36m(func pid=165914)[0m 
== Status ==
Current time: 2024-01-07 07:37:45 (running for 00:46:17.05)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.327 |  0.15  |                   75 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.251 |  0.151 |                   68 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.1506943255662918
[2m[36m(func pid=168155)[0m mae:  0.09411432594060898
[2m[36m(func pid=168155)[0m rmse_per_class: [0.076, 0.26, 0.025, 0.3, 0.063, 0.15, 0.219, 0.121, 0.143, 0.148]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3279 | Steps: 2 | Val loss: 0.2751 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2521 | Steps: 2 | Val loss: 0.2803 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=165914)[0m rmse: 0.14984476566314697
[2m[36m(func pid=165914)[0m mae:  0.10622842609882355
[2m[36m(func pid=165914)[0m rmse_per_class: [0.092, 0.234, 0.045, 0.296, 0.064, 0.163, 0.249, 0.119, 0.138, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.1506173312664032
[2m[36m(func pid=168155)[0m mae:  0.09394723176956177
[2m[36m(func pid=168155)[0m rmse_per_class: [0.075, 0.26, 0.025, 0.3, 0.064, 0.15, 0.22, 0.122, 0.141, 0.148]
[2m[36m(func pid=168155)[0m 
== Status ==
Current time: 2024-01-07 07:37:50 (running for 00:46:22.21)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.328 |  0.15  |                   76 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.252 |  0.151 |                   69 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3211 | Steps: 2 | Val loss: 0.2751 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2561 | Steps: 2 | Val loss: 0.2809 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=165914)[0m rmse: 0.14980216324329376
[2m[36m(func pid=165914)[0m mae:  0.10616608709096909
[2m[36m(func pid=165914)[0m rmse_per_class: [0.092, 0.234, 0.044, 0.296, 0.064, 0.163, 0.249, 0.119, 0.138, 0.099]
[2m[36m(func pid=165914)[0m 
== Status ==
Current time: 2024-01-07 07:37:55 (running for 00:46:27.50)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.321 |  0.15  |                   77 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.256 |  0.151 |                   70 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.15098968148231506
[2m[36m(func pid=168155)[0m mae:  0.09439369291067123
[2m[36m(func pid=168155)[0m rmse_per_class: [0.077, 0.262, 0.025, 0.301, 0.064, 0.15, 0.222, 0.123, 0.142, 0.144]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3194 | Steps: 2 | Val loss: 0.2751 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2466 | Steps: 2 | Val loss: 0.2822 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 07:38:00 (running for 00:46:32.53)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.321 |  0.15  |                   77 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.256 |  0.151 |                   70 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14982564747333527
[2m[36m(func pid=165914)[0m mae:  0.10615549236536026
[2m[36m(func pid=165914)[0m rmse_per_class: [0.092, 0.234, 0.044, 0.296, 0.064, 0.163, 0.248, 0.119, 0.138, 0.1]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.15161406993865967
[2m[36m(func pid=168155)[0m mae:  0.09472937881946564
[2m[36m(func pid=168155)[0m rmse_per_class: [0.078, 0.263, 0.025, 0.303, 0.064, 0.15, 0.222, 0.123, 0.142, 0.147]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3268 | Steps: 2 | Val loss: 0.2750 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2619 | Steps: 2 | Val loss: 0.2834 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 07:38:06 (running for 00:46:38.01)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.319 |  0.15  |                   78 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.247 |  0.152 |                   71 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14978231489658356
[2m[36m(func pid=165914)[0m mae:  0.10607518255710602
[2m[36m(func pid=165914)[0m rmse_per_class: [0.092, 0.234, 0.044, 0.296, 0.064, 0.163, 0.248, 0.119, 0.138, 0.1]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.15237239003181458
[2m[36m(func pid=168155)[0m mae:  0.09533383697271347
[2m[36m(func pid=168155)[0m rmse_per_class: [0.079, 0.264, 0.025, 0.305, 0.065, 0.149, 0.223, 0.122, 0.143, 0.148]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3198 | Steps: 2 | Val loss: 0.2747 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2595 | Steps: 2 | Val loss: 0.2836 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 07:38:11 (running for 00:46:43.43)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.327 |  0.15  |                   79 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.262 |  0.152 |                   72 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.1496240198612213
[2m[36m(func pid=165914)[0m mae:  0.10590578615665436
[2m[36m(func pid=165914)[0m rmse_per_class: [0.092, 0.234, 0.043, 0.295, 0.064, 0.163, 0.248, 0.119, 0.138, 0.1]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=168155)[0m rmse: 0.15224149823188782
[2m[36m(func pid=168155)[0m mae:  0.09501783549785614
[2m[36m(func pid=168155)[0m rmse_per_class: [0.079, 0.265, 0.025, 0.305, 0.066, 0.149, 0.22, 0.122, 0.141, 0.149]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3130 | Steps: 2 | Val loss: 0.2743 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2500 | Steps: 2 | Val loss: 0.2850 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 07:38:17 (running for 00:46:48.78)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.32  |  0.15  |                   80 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.26  |  0.152 |                   73 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14925633370876312
[2m[36m(func pid=165914)[0m mae:  0.10567019879817963
[2m[36m(func pid=165914)[0m rmse_per_class: [0.091, 0.234, 0.043, 0.295, 0.064, 0.163, 0.247, 0.119, 0.138, 0.099]
[2m[36m(func pid=168155)[0m rmse: 0.15299835801124573
[2m[36m(func pid=168155)[0m mae:  0.09533663839101791
[2m[36m(func pid=168155)[0m rmse_per_class: [0.08, 0.264, 0.025, 0.306, 0.068, 0.149, 0.22, 0.121, 0.141, 0.155]
[2m[36m(func pid=168155)[0m 
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3141 | Steps: 2 | Val loss: 0.2740 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=168155)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2487 | Steps: 2 | Val loss: 0.2854 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 07:38:22 (running for 00:46:54.18)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1510000005364418
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.313 |  0.149 |                   81 |
| train_ccef6_00023 | RUNNING    | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.25  |  0.153 |                   74 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=168155)[0m rmse: 0.15326325595378876
[2m[36m(func pid=168155)[0m mae:  0.09544836729764938
[2m[36m(func pid=168155)[0m rmse_per_class: [0.081, 0.264, 0.025, 0.307, 0.069, 0.149, 0.219, 0.12, 0.141, 0.156]
[2m[36m(func pid=165914)[0m rmse: 0.14900779724121094
[2m[36m(func pid=165914)[0m mae:  0.10544154793024063
[2m[36m(func pid=165914)[0m rmse_per_class: [0.091, 0.234, 0.043, 0.295, 0.064, 0.163, 0.246, 0.119, 0.138, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3157 | Steps: 2 | Val loss: 0.2739 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 07:38:27 (running for 00:46:59.56)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.314 |  0.149 |                   82 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14886851608753204
[2m[36m(func pid=165914)[0m mae:  0.10531780868768692
[2m[36m(func pid=165914)[0m rmse_per_class: [0.091, 0.234, 0.042, 0.295, 0.065, 0.162, 0.246, 0.119, 0.138, 0.097]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3168 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 07:38:33 (running for 00:47:04.99)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.316 |  0.149 |                   83 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14880985021591187
[2m[36m(func pid=165914)[0m mae:  0.10516098886728287
[2m[36m(func pid=165914)[0m rmse_per_class: [0.09, 0.234, 0.042, 0.295, 0.064, 0.162, 0.246, 0.119, 0.138, 0.098]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3187 | Steps: 2 | Val loss: 0.2735 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 07:38:38 (running for 00:47:10.31)
Memory usage on this node: 16.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.317 |  0.149 |                   84 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.1486506462097168
[2m[36m(func pid=165914)[0m mae:  0.10494653135538101
[2m[36m(func pid=165914)[0m rmse_per_class: [0.09, 0.234, 0.042, 0.294, 0.064, 0.162, 0.245, 0.119, 0.137, 0.099]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3123 | Steps: 2 | Val loss: 0.2736 | Batch size: 32 | lr: 0.01 | Duration: 3.26s
== Status ==
Current time: 2024-01-07 07:38:44 (running for 00:47:15.58)
Memory usage on this node: 16.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.319 |  0.149 |                   85 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.1487288773059845
[2m[36m(func pid=165914)[0m mae:  0.10494927316904068
[2m[36m(func pid=165914)[0m rmse_per_class: [0.09, 0.234, 0.042, 0.295, 0.064, 0.161, 0.245, 0.119, 0.137, 0.101]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3118 | Steps: 2 | Val loss: 0.2735 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 07:38:49 (running for 00:47:21.31)
Memory usage on this node: 16.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.312 |  0.149 |                   86 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14863170683383942
[2m[36m(func pid=165914)[0m mae:  0.10482176393270493
[2m[36m(func pid=165914)[0m rmse_per_class: [0.089, 0.234, 0.041, 0.295, 0.064, 0.161, 0.245, 0.119, 0.137, 0.101]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3188 | Steps: 2 | Val loss: 0.2733 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 07:38:55 (running for 00:47:26.63)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.312 |  0.149 |                   87 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14843469858169556
[2m[36m(func pid=165914)[0m mae:  0.10469925403594971
[2m[36m(func pid=165914)[0m rmse_per_class: [0.089, 0.235, 0.041, 0.295, 0.064, 0.16, 0.245, 0.119, 0.137, 0.1]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3145 | Steps: 2 | Val loss: 0.2731 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 07:39:00 (running for 00:47:32.16)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.319 |  0.148 |                   88 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.1483772099018097
[2m[36m(func pid=165914)[0m mae:  0.10453088581562042
[2m[36m(func pid=165914)[0m rmse_per_class: [0.089, 0.235, 0.041, 0.294, 0.064, 0.16, 0.244, 0.118, 0.137, 0.101]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3159 | Steps: 2 | Val loss: 0.2734 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 07:39:06 (running for 00:47:37.65)
Memory usage on this node: 16.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.315 |  0.148 |                   89 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14865462481975555
[2m[36m(func pid=165914)[0m mae:  0.10473344475030899
[2m[36m(func pid=165914)[0m rmse_per_class: [0.089, 0.235, 0.041, 0.295, 0.064, 0.16, 0.244, 0.118, 0.137, 0.103]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3140 | Steps: 2 | Val loss: 0.2736 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 07:39:11 (running for 00:47:43.00)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.316 |  0.149 |                   90 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14875566959381104
[2m[36m(func pid=165914)[0m mae:  0.10475903749465942
[2m[36m(func pid=165914)[0m rmse_per_class: [0.089, 0.235, 0.04, 0.295, 0.064, 0.16, 0.245, 0.118, 0.137, 0.103]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3108 | Steps: 2 | Val loss: 0.2732 | Batch size: 32 | lr: 0.01 | Duration: 3.32s
== Status ==
Current time: 2024-01-07 07:39:16 (running for 00:47:48.32)
Memory usage on this node: 16.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.314 |  0.149 |                   91 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14856740832328796
[2m[36m(func pid=165914)[0m mae:  0.10454615205526352
[2m[36m(func pid=165914)[0m rmse_per_class: [0.088, 0.235, 0.04, 0.294, 0.064, 0.16, 0.244, 0.118, 0.137, 0.104]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3116 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 07:39:22 (running for 00:47:54.13)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.311 |  0.149 |                   92 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14841778576374054
[2m[36m(func pid=165914)[0m mae:  0.10430965572595596
[2m[36m(func pid=165914)[0m rmse_per_class: [0.087, 0.235, 0.04, 0.294, 0.064, 0.16, 0.244, 0.118, 0.137, 0.105]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3088 | Steps: 2 | Val loss: 0.2727 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 07:39:28 (running for 00:47:59.75)
Memory usage on this node: 16.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.312 |  0.148 |                   93 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14821599423885345
[2m[36m(func pid=165914)[0m mae:  0.10404304414987564
[2m[36m(func pid=165914)[0m rmse_per_class: [0.087, 0.235, 0.039, 0.294, 0.064, 0.16, 0.243, 0.118, 0.137, 0.105]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3195 | Steps: 2 | Val loss: 0.2722 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 07:39:33 (running for 00:48:04.96)
Memory usage on this node: 16.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.309 |  0.148 |                   94 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14788135886192322
[2m[36m(func pid=165914)[0m mae:  0.10368108749389648
[2m[36m(func pid=165914)[0m rmse_per_class: [0.086, 0.235, 0.039, 0.293, 0.064, 0.159, 0.243, 0.117, 0.137, 0.105]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3176 | Steps: 2 | Val loss: 0.2721 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 07:39:38 (running for 00:48:10.40)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.32  |  0.148 |                   95 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14790627360343933
[2m[36m(func pid=165914)[0m mae:  0.10360150039196014
[2m[36m(func pid=165914)[0m rmse_per_class: [0.086, 0.235, 0.039, 0.292, 0.064, 0.159, 0.243, 0.117, 0.137, 0.106]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3105 | Steps: 2 | Val loss: 0.2722 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 07:39:44 (running for 00:48:15.87)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.318 |  0.148 |                   96 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14803124964237213
[2m[36m(func pid=165914)[0m mae:  0.10366325080394745
[2m[36m(func pid=165914)[0m rmse_per_class: [0.086, 0.235, 0.039, 0.293, 0.064, 0.159, 0.243, 0.117, 0.137, 0.107]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3105 | Steps: 2 | Val loss: 0.2723 | Batch size: 32 | lr: 0.01 | Duration: 3.31s
== Status ==
Current time: 2024-01-07 07:39:49 (running for 00:48:21.37)
Memory usage on this node: 16.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.311 |  0.148 |                   97 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14805004000663757
[2m[36m(func pid=165914)[0m mae:  0.10364975780248642
[2m[36m(func pid=165914)[0m rmse_per_class: [0.087, 0.235, 0.039, 0.293, 0.064, 0.159, 0.243, 0.117, 0.137, 0.107]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3050 | Steps: 2 | Val loss: 0.2722 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 07:39:55 (running for 00:48:27.32)
Memory usage on this node: 16.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.311 |  0.148 |                   98 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=165914)[0m rmse: 0.14814458787441254
[2m[36m(func pid=165914)[0m mae:  0.10361284017562866
[2m[36m(func pid=165914)[0m rmse_per_class: [0.087, 0.235, 0.038, 0.292, 0.064, 0.159, 0.244, 0.117, 0.137, 0.108]
[2m[36m(func pid=165914)[0m 
[2m[36m(func pid=165914)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3046 | Steps: 2 | Val loss: 0.2721 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 07:40:01 (running for 00:48:32.83)
Memory usage on this node: 16.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00022 | RUNNING    | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.305 |  0.148 |                   99 |
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 07:40:01 (running for 00:48:33.42)
Memory usage on this node: 16.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: -0.15149999782443047
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_ccef6_00000 | TERMINATED | 192.168.7.53:66954  | 0.0001 |       0.99 |         0      |  0.455 |  0.176 |                   75 |
| train_ccef6_00001 | TERMINATED | 192.168.7.53:67327  | 0.001  |       0.99 |         0      |  0.35  |  0.142 |                  100 |
| train_ccef6_00002 | TERMINATED | 192.168.7.53:67743  | 0.01   |       0.99 |         0      |  0.343 |  0.185 |                   75 |
| train_ccef6_00003 | TERMINATED | 192.168.7.53:68170  | 0.1    |       0.99 |         0      |  0.36  |  0.214 |                   75 |
| train_ccef6_00004 | TERMINATED | 192.168.7.53:84353  | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   75 |
| train_ccef6_00005 | TERMINATED | 192.168.7.53:84788  | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   75 |
| train_ccef6_00006 | TERMINATED | 192.168.7.53:86022  | 0.01   |       0.9  |         0      |  0.305 |  0.148 |                  100 |
| train_ccef6_00007 | TERMINATED | 192.168.7.53:90307  | 0.1    |       0.9  |         0      |  0.251 |  0.155 |                  100 |
| train_ccef6_00008 | TERMINATED | 192.168.7.53:101638 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.176 |                   75 |
| train_ccef6_00009 | TERMINATED | 192.168.7.53:102191 | 0.001  |       0.99 |         0.0001 |  0.355 |  0.142 |                  100 |
| train_ccef6_00010 | TERMINATED | 192.168.7.53:109115 | 0.01   |       0.99 |         0.0001 |  0.338 |  0.183 |                   75 |
| train_ccef6_00011 | TERMINATED | 192.168.7.53:113492 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.217 |                   75 |
| train_ccef6_00012 | TERMINATED | 192.168.7.53:119168 | 0.0001 |       0.9  |         0.0001 |  0.747 |  0.179 |                   75 |
| train_ccef6_00013 | TERMINATED | 192.168.7.53:124933 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   75 |
| train_ccef6_00014 | TERMINATED | 192.168.7.53:127238 | 0.01   |       0.9  |         0.0001 |  0.315 |  0.147 |                  100 |
| train_ccef6_00015 | TERMINATED | 192.168.7.53:130296 | 0.1    |       0.9  |         0.0001 |  0.255 |  0.152 |                   75 |
| train_ccef6_00016 | TERMINATED | 192.168.7.53:136744 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.176 |                   75 |
| train_ccef6_00017 | TERMINATED | 192.168.7.53:142739 | 0.001  |       0.99 |         1e-05  |  0.348 |  0.142 |                  100 |
| train_ccef6_00018 | TERMINATED | 192.168.7.53:147638 | 0.01   |       0.99 |         1e-05  |  0.343 |  0.185 |                   75 |
| train_ccef6_00019 | TERMINATED | 192.168.7.53:150355 | 0.1    |       0.99 |         1e-05  |  0.403 |  0.212 |                   75 |
| train_ccef6_00020 | TERMINATED | 192.168.7.53:153954 | 0.0001 |       0.9  |         1e-05  |  0.746 |  0.179 |                   75 |
| train_ccef6_00021 | TERMINATED | 192.168.7.53:164771 | 0.001  |       0.9  |         1e-05  |  0.423 |  0.173 |                   75 |
| train_ccef6_00022 | TERMINATED | 192.168.7.53:165914 | 0.01   |       0.9  |         1e-05  |  0.305 |  0.148 |                  100 |
| train_ccef6_00023 | TERMINATED | 192.168.7.53:168155 | 0.1    |       0.9  |         1e-05  |  0.249 |  0.153 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+


2024-01-07 07:40:01,866	INFO tune.py:798 -- Total run time: 2914.53 seconds (2913.40 seconds for the tuning loop).
[2m[36m(func pid=165914)[0m rmse: 0.14798925817012787
[2m[36m(func pid=165914)[0m mae:  0.10350097715854645
[2m[36m(func pid=165914)[0m rmse_per_class: [0.087, 0.235, 0.038, 0.292, 0.064, 0.159, 0.243, 0.117, 0.137, 0.107]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341341.1 ON aap04 CANCELLED AT 2024-01-07T07:40:06 ***
srun: error: aap04: task 0: Exited with exit code 1

IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 05:00:08,833	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 05:00:08,833	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 05:00:11,486	SUCC scripts.py:747 -- --------------------
2024-01-07 05:00:11,487	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 05:00:11,487	SUCC scripts.py:749 -- --------------------
2024-01-07 05:00:11,487	INFO scripts.py:751 -- Next steps
2024-01-07 05:00:11,487	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 05:00:11,487	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 05:00:11,487	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 05:00:11,487	INFO scripts.py:773 -- import ray
2024-01-07 05:00:11,487	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 05:00:11,487	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 05:00:11,487	INFO scripts.py:791 --   ray status
2024-01-07 05:00:11,488	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 05:00:11,488	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 05:00:11,488	INFO scripts.py:810 --   ray stop
2024-01-07 05:00:11,488	INFO scripts.py:891 -- --block
2024-01-07 05:00:11,488	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 05:00:11,488	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              8731986252889533712
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7fa5c37670d0>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          Supervised
task_name:           multilabel
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          5
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         random
seed:                42
dropout:             None
transfer_learning:   LP
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1


Supervised model resnet18 with random weights
Old final fully-connected layer: Linear(in_features=512, out_features=1000, bias=True)
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Linear probing adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 05:00:52,771	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 05:00:52,794	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 05:01:16,381	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-07 05:01:16 (running for 00:00:22.68)
Memory usage on this node: 13.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |
| train_5a6ec_00001 | PENDING  |                    | 0.001  |       0.99 |         0      |
| train_5a6ec_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_5a6ec_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14462)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=14462)[0m Configuration completed!
[2m[36m(func pid=14462)[0m New optimizer parameters:
[2m[36m(func pid=14462)[0m SGD (
[2m[36m(func pid=14462)[0m Parameter Group 0
[2m[36m(func pid=14462)[0m     dampening: 0
[2m[36m(func pid=14462)[0m     differentiable: False
[2m[36m(func pid=14462)[0m     foreach: None
[2m[36m(func pid=14462)[0m     lr: 0.0001
[2m[36m(func pid=14462)[0m     maximize: False
[2m[36m(func pid=14462)[0m     momentum: 0.99
[2m[36m(func pid=14462)[0m     nesterov: False
[2m[36m(func pid=14462)[0m     weight_decay: 0
[2m[36m(func pid=14462)[0m )
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0439 | Steps: 2 | Val loss: 0.7189 | Batch size: 32 | lr: 0.0001 | Duration: 4.88s
[2m[36m(func pid=14462)[0m rmse: 0.17990460991859436
[2m[36m(func pid=14462)[0m mae:  0.1324189007282257
[2m[36m(func pid=14462)[0m rmse_per_class: [0.114, 0.263, 0.097, 0.333, 0.1, 0.191, 0.298, 0.143, 0.141, 0.119]
== Status ==
Current time: 2024-01-07 05:01:26 (running for 00:00:32.77)
Memory usage on this node: 15.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |
| train_5a6ec_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_5a6ec_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=14835)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=14835)[0m Configuration completed!
[2m[36m(func pid=14835)[0m New optimizer parameters:
[2m[36m(func pid=14835)[0m SGD (
[2m[36m(func pid=14835)[0m Parameter Group 0
[2m[36m(func pid=14835)[0m     dampening: 0
[2m[36m(func pid=14835)[0m     differentiable: False
[2m[36m(func pid=14835)[0m     foreach: None
[2m[36m(func pid=14835)[0m     lr: 0.001
[2m[36m(func pid=14835)[0m     maximize: False
[2m[36m(func pid=14835)[0m     momentum: 0.99
[2m[36m(func pid=14835)[0m     nesterov: False
[2m[36m(func pid=14835)[0m     weight_decay: 0
[2m[36m(func pid=14835)[0m )
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0423 | Steps: 2 | Val loss: 0.7127 | Batch size: 32 | lr: 0.001 | Duration: 4.64s
[2m[36m(func pid=14835)[0m rmse: 0.17987872660160065
[2m[36m(func pid=14835)[0m mae:  0.13239093124866486
[2m[36m(func pid=14835)[0m rmse_per_class: [0.114, 0.263, 0.097, 0.333, 0.1, 0.191, 0.298, 0.143, 0.141, 0.119]
== Status ==
Current time: 2024-01-07 05:01:34 (running for 00:00:41.10)
Memory usage on this node: 17.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |
| train_5a6ec_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15258)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=15258)[0m Configuration completed!
[2m[36m(func pid=15258)[0m New optimizer parameters:
[2m[36m(func pid=15258)[0m SGD (
[2m[36m(func pid=15258)[0m Parameter Group 0
[2m[36m(func pid=15258)[0m     dampening: 0
[2m[36m(func pid=15258)[0m     differentiable: False
[2m[36m(func pid=15258)[0m     foreach: None
[2m[36m(func pid=15258)[0m     lr: 0.01
[2m[36m(func pid=15258)[0m     maximize: False
[2m[36m(func pid=15258)[0m     momentum: 0.99
[2m[36m(func pid=15258)[0m     nesterov: False
[2m[36m(func pid=15258)[0m     weight_decay: 0
[2m[36m(func pid=15258)[0m )
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0029 | Steps: 2 | Val loss: 0.6591 | Batch size: 32 | lr: 0.01 | Duration: 4.16s
[2m[36m(func pid=15258)[0m rmse: 0.1797683984041214
[2m[36m(func pid=15258)[0m mae:  0.1322588175535202
[2m[36m(func pid=15258)[0m rmse_per_class: [0.115, 0.263, 0.098, 0.334, 0.099, 0.191, 0.297, 0.143, 0.141, 0.118]
== Status ==
Current time: 2024-01-07 05:01:43 (running for 00:00:49.42)
Memory usage on this node: 20.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 05:01:50 (running for 00:00:56.99)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |        |        |                      |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  1.042 |   0.18 |                    1 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |        |        |                      |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |        |        |                      |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15692)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=15692)[0m Configuration completed!
[2m[36m(func pid=15692)[0m New optimizer parameters:
[2m[36m(func pid=15692)[0m SGD (
[2m[36m(func pid=15692)[0m Parameter Group 0
[2m[36m(func pid=15692)[0m     dampening: 0
[2m[36m(func pid=15692)[0m     differentiable: False
[2m[36m(func pid=15692)[0m     foreach: None
[2m[36m(func pid=15692)[0m     lr: 0.1
[2m[36m(func pid=15692)[0m     maximize: False
[2m[36m(func pid=15692)[0m     momentum: 0.99
[2m[36m(func pid=15692)[0m     nesterov: False
[2m[36m(func pid=15692)[0m     weight_decay: 0
[2m[36m(func pid=15692)[0m )
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0399 | Steps: 2 | Val loss: 0.7190 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0067 | Steps: 2 | Val loss: 0.7035 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.7559 | Steps: 2 | Val loss: 0.5841 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.7928 | Steps: 2 | Val loss: 0.4310 | Batch size: 32 | lr: 0.1 | Duration: 4.30s
== Status ==
Current time: 2024-01-07 05:01:55 (running for 00:01:02.03)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  1.044 |   0.18 |                    1 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  1.042 |   0.18 |                    1 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  1.003 |   0.18 |                    1 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |        |        |                      |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.18043045699596405
[2m[36m(func pid=14462)[0m mae:  0.13288508355617523
[2m[36m(func pid=14462)[0m rmse_per_class: [0.114, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17985032498836517
[2m[36m(func pid=15258)[0m mae:  0.13234248757362366
[2m[36m(func pid=15258)[0m rmse_per_class: [0.114, 0.264, 0.104, 0.334, 0.101, 0.191, 0.294, 0.141, 0.144, 0.113]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.18036362528800964
[2m[36m(func pid=14835)[0m mae:  0.1328185647726059
[2m[36m(func pid=14835)[0m rmse_per_class: [0.113, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1792464703321457
[2m[36m(func pid=15692)[0m mae:  0.13111598789691925
[2m[36m(func pid=15692)[0m rmse_per_class: [0.12, 0.264, 0.118, 0.342, 0.088, 0.191, 0.282, 0.14, 0.144, 0.105]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5362 | Steps: 2 | Val loss: 0.4805 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0362 | Steps: 2 | Val loss: 0.7227 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.9521 | Steps: 2 | Val loss: 0.6881 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5001 | Steps: 2 | Val loss: 0.3563 | Batch size: 32 | lr: 0.1 | Duration: 2.60s
[2m[36m(func pid=14462)[0m rmse: 0.1806679219007492
[2m[36m(func pid=14462)[0m mae:  0.1331070214509964
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.105, 0.192, 0.3, 0.142, 0.143, 0.116]
== Status ==
Current time: 2024-01-07 05:02:01 (running for 00:01:07.35)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  1.036 |  0.181 |                    3 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  1.007 |  0.18  |                    2 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.756 |  0.18  |                    2 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.793 |  0.179 |                    1 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.18055006861686707
[2m[36m(func pid=14835)[0m mae:  0.13299091160297394
[2m[36m(func pid=14835)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.104, 0.191, 0.299, 0.142, 0.143, 0.115]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17932121455669403
[2m[36m(func pid=15258)[0m mae:  0.13185623288154602
[2m[36m(func pid=15258)[0m rmse_per_class: [0.114, 0.264, 0.106, 0.335, 0.099, 0.191, 0.291, 0.139, 0.146, 0.108]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18119649589061737
[2m[36m(func pid=15692)[0m mae:  0.1312110722064972
[2m[36m(func pid=15692)[0m rmse_per_class: [0.125, 0.266, 0.146, 0.348, 0.075, 0.19, 0.273, 0.14, 0.153, 0.096]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4449 | Steps: 2 | Val loss: 0.3861 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.0268 | Steps: 2 | Val loss: 0.7261 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8730 | Steps: 2 | Val loss: 0.6611 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7296 | Steps: 2 | Val loss: 0.3414 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 05:02:06 (running for 00:01:12.36)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  1.036 |  0.181 |                    3 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.952 |  0.181 |                    3 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.445 |  0.178 |                    4 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.5   |  0.181 |                    2 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.17826639115810394
[2m[36m(func pid=15258)[0m mae:  0.13080117106437683
[2m[36m(func pid=15258)[0m rmse_per_class: [0.116, 0.264, 0.11, 0.336, 0.093, 0.19, 0.285, 0.136, 0.148, 0.103]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.18076902627944946
[2m[36m(func pid=14462)[0m mae:  0.13321015238761902
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.106, 0.192, 0.302, 0.142, 0.143, 0.115]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.18057872354984283
[2m[36m(func pid=14835)[0m mae:  0.13302594423294067
[2m[36m(func pid=14835)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.332, 0.105, 0.191, 0.3, 0.142, 0.143, 0.114]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.183736652135849
[2m[36m(func pid=15692)[0m mae:  0.1304270625114441
[2m[36m(func pid=15692)[0m rmse_per_class: [0.14, 0.27, 0.16, 0.359, 0.057, 0.19, 0.261, 0.145, 0.166, 0.091]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4690 | Steps: 2 | Val loss: 0.3338 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.0126 | Steps: 2 | Val loss: 0.7276 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.7879 | Steps: 2 | Val loss: 0.6234 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8599 | Steps: 2 | Val loss: 0.3708 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 05:02:11 (running for 00:01:17.57)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  1.027 |  0.181 |                    4 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.873 |  0.181 |                    4 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.469 |  0.177 |                    5 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.73  |  0.184 |                    3 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.17698971927165985
[2m[36m(func pid=15258)[0m mae:  0.12933126091957092
[2m[36m(func pid=15258)[0m rmse_per_class: [0.118, 0.265, 0.113, 0.34, 0.084, 0.189, 0.277, 0.135, 0.151, 0.098]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.18085357546806335
[2m[36m(func pid=14462)[0m mae:  0.13328607380390167
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.107, 0.192, 0.303, 0.142, 0.143, 0.116]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.18050554394721985
[2m[36m(func pid=14835)[0m mae:  0.13296692073345184
[2m[36m(func pid=14835)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.331, 0.105, 0.191, 0.3, 0.141, 0.143, 0.113]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1890215277671814
[2m[36m(func pid=15692)[0m mae:  0.1305026262998581
[2m[36m(func pid=15692)[0m rmse_per_class: [0.159, 0.274, 0.148, 0.371, 0.055, 0.194, 0.28, 0.151, 0.166, 0.093]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5328 | Steps: 2 | Val loss: 0.3210 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.0004 | Steps: 2 | Val loss: 0.7265 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.7036 | Steps: 2 | Val loss: 0.5774 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.9071 | Steps: 2 | Val loss: 0.3987 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=15258)[0m rmse: 0.17610657215118408
[2m[36m(func pid=15258)[0m mae:  0.12776753306388855
[2m[36m(func pid=15258)[0m rmse_per_class: [0.119, 0.266, 0.115, 0.344, 0.073, 0.188, 0.27, 0.137, 0.155, 0.094]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:02:16 (running for 00:01:22.84)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  1.013 |  0.181 |                    5 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.788 |  0.181 |                    5 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.533 |  0.176 |                    6 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.86  |  0.189 |                    4 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.18091434240341187
[2m[36m(func pid=14462)[0m mae:  0.13333946466445923
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.107, 0.191, 0.303, 0.142, 0.142, 0.116]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.18037736415863037
[2m[36m(func pid=14835)[0m mae:  0.1328449547290802
[2m[36m(func pid=14835)[0m rmse_per_class: [0.114, 0.264, 0.103, 0.331, 0.105, 0.191, 0.299, 0.14, 0.144, 0.112]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1935700625181198
[2m[36m(func pid=15692)[0m mae:  0.131959468126297
[2m[36m(func pid=15692)[0m rmse_per_class: [0.194, 0.277, 0.099, 0.379, 0.056, 0.2, 0.318, 0.154, 0.162, 0.096]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5962 | Steps: 2 | Val loss: 0.3345 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.9808 | Steps: 2 | Val loss: 0.7225 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.6215 | Steps: 2 | Val loss: 0.5274 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8697 | Steps: 2 | Val loss: 0.3949 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=15258)[0m rmse: 0.17606180906295776
[2m[36m(func pid=15258)[0m mae:  0.12628573179244995
[2m[36m(func pid=15258)[0m rmse_per_class: [0.121, 0.267, 0.113, 0.35, 0.064, 0.187, 0.27, 0.139, 0.158, 0.092]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:02:21 (running for 00:01:28.03)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.981 |  0.181 |                    7 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.704 |  0.18  |                    6 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.596 |  0.176 |                    7 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.907 |  0.194 |                    5 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.18094155192375183
[2m[36m(func pid=14462)[0m mae:  0.13335800170898438
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.33, 0.108, 0.191, 0.304, 0.142, 0.142, 0.116]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.18021951615810394
[2m[36m(func pid=14835)[0m mae:  0.1326819807291031
[2m[36m(func pid=14835)[0m rmse_per_class: [0.114, 0.264, 0.105, 0.332, 0.105, 0.191, 0.298, 0.139, 0.144, 0.111]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18807891011238098
[2m[36m(func pid=15692)[0m mae:  0.1292896270751953
[2m[36m(func pid=15692)[0m rmse_per_class: [0.172, 0.267, 0.052, 0.377, 0.056, 0.192, 0.301, 0.155, 0.212, 0.097]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.6535 | Steps: 2 | Val loss: 0.3607 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.9617 | Steps: 2 | Val loss: 0.7160 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.5620 | Steps: 2 | Val loss: 0.4781 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8221 | Steps: 2 | Val loss: 0.3619 | Batch size: 32 | lr: 0.1 | Duration: 2.60s
[2m[36m(func pid=15258)[0m rmse: 0.17718520760536194
[2m[36m(func pid=15258)[0m mae:  0.1251918375492096
[2m[36m(func pid=15258)[0m rmse_per_class: [0.122, 0.268, 0.108, 0.356, 0.058, 0.187, 0.281, 0.142, 0.159, 0.091]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:02:26 (running for 00:01:33.12)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.962 |  0.181 |                    8 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.622 |  0.18  |                    7 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.654 |  0.177 |                    8 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.87  |  0.188 |                    6 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.18096865713596344
[2m[36m(func pid=14462)[0m mae:  0.13337679207324982
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.33, 0.108, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.1799635887145996
[2m[36m(func pid=14835)[0m mae:  0.1324223130941391
[2m[36m(func pid=14835)[0m rmse_per_class: [0.114, 0.264, 0.107, 0.332, 0.104, 0.19, 0.297, 0.139, 0.145, 0.109]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.17744119465351105
[2m[36m(func pid=15692)[0m mae:  0.12488635629415512
[2m[36m(func pid=15692)[0m rmse_per_class: [0.098, 0.249, 0.044, 0.357, 0.056, 0.181, 0.255, 0.155, 0.282, 0.097]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7118 | Steps: 2 | Val loss: 0.3914 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.9428 | Steps: 2 | Val loss: 0.7071 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.5127 | Steps: 2 | Val loss: 0.4336 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7215 | Steps: 2 | Val loss: 0.4095 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=15258)[0m rmse: 0.17976129055023193
[2m[36m(func pid=15258)[0m mae:  0.12481112778186798
[2m[36m(func pid=15258)[0m rmse_per_class: [0.123, 0.27, 0.1, 0.362, 0.055, 0.188, 0.305, 0.146, 0.159, 0.091]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.1809752881526947
[2m[36m(func pid=14462)[0m mae:  0.1333729326725006
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.329, 0.108, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=14462)[0m 
== Status ==
Current time: 2024-01-07 05:02:32 (running for 00:01:38.48)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.943 |  0.181 |                    9 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.513 |  0.18  |                    9 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.712 |  0.18  |                    9 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.822 |  0.177 |                    7 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.17970219254493713
[2m[36m(func pid=14835)[0m mae:  0.1321355402469635
[2m[36m(func pid=14835)[0m rmse_per_class: [0.115, 0.264, 0.109, 0.333, 0.102, 0.189, 0.294, 0.138, 0.145, 0.107]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1784619390964508
[2m[36m(func pid=15692)[0m mae:  0.12262631952762604
[2m[36m(func pid=15692)[0m rmse_per_class: [0.108, 0.328, 0.048, 0.301, 0.056, 0.239, 0.316, 0.155, 0.136, 0.097]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7621 | Steps: 2 | Val loss: 0.4201 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.9190 | Steps: 2 | Val loss: 0.6964 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4777 | Steps: 2 | Val loss: 0.3963 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7893 | Steps: 2 | Val loss: 0.4208 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=15258)[0m rmse: 0.18329480290412903
[2m[36m(func pid=15258)[0m mae:  0.12524619698524475
[2m[36m(func pid=15258)[0m rmse_per_class: [0.123, 0.272, 0.09, 0.367, 0.055, 0.19, 0.339, 0.149, 0.157, 0.092]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.18100225925445557
[2m[36m(func pid=14462)[0m mae:  0.13338902592658997
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.329, 0.109, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=14462)[0m 
== Status ==
Current time: 2024-01-07 05:02:37 (running for 00:01:43.48)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.919 |  0.181 |                   10 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.478 |  0.179 |                   10 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.762 |  0.183 |                   10 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.721 |  0.178 |                    8 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.17944949865341187
[2m[36m(func pid=14835)[0m mae:  0.13182228803634644
[2m[36m(func pid=14835)[0m rmse_per_class: [0.116, 0.264, 0.112, 0.334, 0.1, 0.189, 0.292, 0.137, 0.146, 0.105]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.17745205760002136
[2m[36m(func pid=15692)[0m mae:  0.11862216144800186
[2m[36m(func pid=15692)[0m rmse_per_class: [0.11, 0.239, 0.049, 0.335, 0.056, 0.275, 0.325, 0.153, 0.136, 0.096]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7695 | Steps: 2 | Val loss: 0.4427 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8958 | Steps: 2 | Val loss: 0.6835 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4596 | Steps: 2 | Val loss: 0.3671 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7411 | Steps: 2 | Val loss: 0.3170 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=15258)[0m rmse: 0.18705794215202332
[2m[36m(func pid=15258)[0m mae:  0.1262023001909256
[2m[36m(func pid=15258)[0m rmse_per_class: [0.123, 0.274, 0.079, 0.372, 0.055, 0.193, 0.375, 0.151, 0.155, 0.093]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.1809961497783661
[2m[36m(func pid=14462)[0m mae:  0.13337939977645874
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=14462)[0m 
== Status ==
Current time: 2024-01-07 05:02:42 (running for 00:01:48.55)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.896 |  0.181 |                   11 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.46  |  0.179 |                   11 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.769 |  0.187 |                   11 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.789 |  0.177 |                    9 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.17915575206279755
[2m[36m(func pid=14835)[0m mae:  0.13143785297870636
[2m[36m(func pid=14835)[0m rmse_per_class: [0.116, 0.264, 0.115, 0.336, 0.097, 0.188, 0.288, 0.136, 0.148, 0.103]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.16974255442619324
[2m[36m(func pid=15692)[0m mae:  0.10935994237661362
[2m[36m(func pid=15692)[0m rmse_per_class: [0.106, 0.254, 0.053, 0.289, 0.056, 0.305, 0.28, 0.126, 0.136, 0.091]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7812 | Steps: 2 | Val loss: 0.4588 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8692 | Steps: 2 | Val loss: 0.6692 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4534 | Steps: 2 | Val loss: 0.3464 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5969 | Steps: 2 | Val loss: 0.3760 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=15258)[0m rmse: 0.19052961468696594
[2m[36m(func pid=15258)[0m mae:  0.12748828530311584
[2m[36m(func pid=15258)[0m rmse_per_class: [0.123, 0.276, 0.07, 0.376, 0.056, 0.197, 0.408, 0.153, 0.152, 0.094]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:02:47 (running for 00:01:53.56)
Memory usage on this node: 24.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.896 |  0.181 |                   11 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.453 |  0.179 |                   12 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.781 |  0.191 |                   12 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.741 |  0.17  |                   10 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.1809934377670288
[2m[36m(func pid=14462)[0m mae:  0.1333717405796051
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.17889457941055298
[2m[36m(func pid=14835)[0m mae:  0.1310424506664276
[2m[36m(func pid=14835)[0m rmse_per_class: [0.117, 0.265, 0.118, 0.337, 0.094, 0.188, 0.285, 0.136, 0.149, 0.1]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.19488660991191864
[2m[36m(func pid=15692)[0m mae:  0.1291484832763672
[2m[36m(func pid=15692)[0m rmse_per_class: [0.102, 0.284, 0.05, 0.367, 0.056, 0.181, 0.306, 0.293, 0.136, 0.173]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7998 | Steps: 2 | Val loss: 0.4693 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8435 | Steps: 2 | Val loss: 0.6535 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4475 | Steps: 2 | Val loss: 0.3335 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.6819 | Steps: 2 | Val loss: 0.5927 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=15258)[0m rmse: 0.19362179934978485
[2m[36m(func pid=15258)[0m mae:  0.1288510262966156
[2m[36m(func pid=15258)[0m rmse_per_class: [0.125, 0.277, 0.062, 0.379, 0.056, 0.201, 0.437, 0.154, 0.149, 0.095]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:02:52 (running for 00:01:58.60)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.869 |  0.181 |                   12 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.448 |  0.179 |                   13 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.8   |  0.194 |                   13 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.597 |  0.195 |                   11 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.17867757380008698
[2m[36m(func pid=14835)[0m mae:  0.13062533736228943
[2m[36m(func pid=14835)[0m rmse_per_class: [0.118, 0.265, 0.121, 0.339, 0.09, 0.187, 0.281, 0.136, 0.15, 0.098]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.18096336722373962
[2m[36m(func pid=14462)[0m mae:  0.13334660232067108
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.109, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.2182033509016037
[2m[36m(func pid=15692)[0m mae:  0.14781787991523743
[2m[36m(func pid=15692)[0m rmse_per_class: [0.124, 0.296, 0.048, 0.388, 0.056, 0.221, 0.264, 0.323, 0.137, 0.325]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7657 | Steps: 2 | Val loss: 0.4699 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4513 | Steps: 2 | Val loss: 0.3273 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8181 | Steps: 2 | Val loss: 0.6372 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8776 | Steps: 2 | Val loss: 0.6056 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=15258)[0m rmse: 0.19566316902637482
[2m[36m(func pid=15258)[0m mae:  0.12995842099189758
[2m[36m(func pid=15258)[0m rmse_per_class: [0.131, 0.278, 0.056, 0.381, 0.056, 0.205, 0.451, 0.155, 0.148, 0.096]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.17850878834724426
[2m[36m(func pid=14835)[0m mae:  0.1302211880683899
[2m[36m(func pid=14835)[0m rmse_per_class: [0.119, 0.265, 0.124, 0.342, 0.086, 0.187, 0.278, 0.136, 0.152, 0.097]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:02:57 (running for 00:02:03.87)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.818 |  0.181 |                   14 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.451 |  0.179 |                   14 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.766 |  0.196 |                   14 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.682 |  0.218 |                   12 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.18090982735157013
[2m[36m(func pid=14462)[0m mae:  0.13330116868019104
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.108, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.22571244835853577
[2m[36m(func pid=15692)[0m mae:  0.1538955718278885
[2m[36m(func pid=15692)[0m rmse_per_class: [0.143, 0.291, 0.048, 0.389, 0.056, 0.225, 0.284, 0.268, 0.133, 0.421]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7478 | Steps: 2 | Val loss: 0.4662 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4602 | Steps: 2 | Val loss: 0.3265 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7901 | Steps: 2 | Val loss: 0.6200 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7482 | Steps: 2 | Val loss: 0.4867 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=15258)[0m rmse: 0.19697630405426025
[2m[36m(func pid=15258)[0m mae:  0.1307561695575714
[2m[36m(func pid=15258)[0m rmse_per_class: [0.147, 0.278, 0.053, 0.382, 0.056, 0.207, 0.447, 0.155, 0.147, 0.096]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.17843559384346008
[2m[36m(func pid=14835)[0m mae:  0.1298370063304901
[2m[36m(func pid=14835)[0m rmse_per_class: [0.12, 0.266, 0.127, 0.344, 0.081, 0.186, 0.274, 0.137, 0.153, 0.095]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:03:02 (running for 00:02:08.93)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.79  |  0.181 |                   15 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.46  |  0.178 |                   15 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.748 |  0.197 |                   15 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.878 |  0.226 |                   13 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.18086305260658264
[2m[36m(func pid=14462)[0m mae:  0.13325920701026917
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.108, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.21722611784934998
[2m[36m(func pid=15692)[0m mae:  0.1500461995601654
[2m[36m(func pid=15692)[0m rmse_per_class: [0.101, 0.238, 0.05, 0.388, 0.056, 0.214, 0.3, 0.118, 0.47, 0.238]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7184 | Steps: 2 | Val loss: 0.4589 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4741 | Steps: 2 | Val loss: 0.3302 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7651 | Steps: 2 | Val loss: 0.6023 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.6164 | Steps: 2 | Val loss: 0.4449 | Batch size: 32 | lr: 0.1 | Duration: 2.61s
[2m[36m(func pid=15258)[0m rmse: 0.19747747480869293
[2m[36m(func pid=15258)[0m mae:  0.1310780644416809
[2m[36m(func pid=15258)[0m rmse_per_class: [0.173, 0.276, 0.052, 0.383, 0.056, 0.209, 0.426, 0.155, 0.148, 0.097]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.178463414311409
[2m[36m(func pid=14835)[0m mae:  0.12946903705596924
[2m[36m(func pid=14835)[0m rmse_per_class: [0.122, 0.266, 0.13, 0.346, 0.077, 0.186, 0.272, 0.137, 0.154, 0.094]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.18079601228237152
[2m[36m(func pid=14462)[0m mae:  0.13320335745811462
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.108, 0.19, 0.304, 0.143, 0.142, 0.115]
== Status ==
Current time: 2024-01-07 05:03:07 (running for 00:02:14.06)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.765 |  0.181 |                   16 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.474 |  0.178 |                   16 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.718 |  0.197 |                   16 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.748 |  0.217 |                   14 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.20295289158821106
[2m[36m(func pid=15692)[0m mae:  0.1359124332666397
[2m[36m(func pid=15692)[0m rmse_per_class: [0.107, 0.401, 0.054, 0.387, 0.056, 0.176, 0.3, 0.135, 0.322, 0.091]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4831 | Steps: 2 | Val loss: 0.3365 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6861 | Steps: 2 | Val loss: 0.4492 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7396 | Steps: 2 | Val loss: 0.5847 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5983 | Steps: 2 | Val loss: 0.4545 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=14835)[0m rmse: 0.1785215437412262
[2m[36m(func pid=14835)[0m mae:  0.1290939301252365
[2m[36m(func pid=14835)[0m rmse_per_class: [0.123, 0.267, 0.132, 0.349, 0.073, 0.186, 0.27, 0.138, 0.155, 0.093]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.19648221135139465
[2m[36m(func pid=15258)[0m mae:  0.1309826672077179
[2m[36m(func pid=15258)[0m rmse_per_class: [0.199, 0.272, 0.054, 0.383, 0.056, 0.208, 0.387, 0.155, 0.153, 0.097]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.1807509809732437
[2m[36m(func pid=14462)[0m mae:  0.13315941393375397
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.108, 0.19, 0.304, 0.143, 0.143, 0.115]
== Status ==
Current time: 2024-01-07 05:03:12 (running for 00:02:19.12)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.74  |  0.181 |                   17 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.483 |  0.179 |                   17 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.686 |  0.196 |                   17 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.616 |  0.203 |                   15 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.204986572265625
[2m[36m(func pid=15692)[0m mae:  0.13376343250274658
[2m[36m(func pid=15692)[0m rmse_per_class: [0.109, 0.58, 0.05, 0.38, 0.056, 0.193, 0.3, 0.153, 0.133, 0.097]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4967 | Steps: 2 | Val loss: 0.3452 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6837 | Steps: 2 | Val loss: 0.4395 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.7165 | Steps: 2 | Val loss: 0.5666 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6165 | Steps: 2 | Val loss: 0.3628 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=14835)[0m rmse: 0.1787400245666504
[2m[36m(func pid=14835)[0m mae:  0.1287911832332611
[2m[36m(func pid=14835)[0m rmse_per_class: [0.124, 0.267, 0.134, 0.351, 0.07, 0.186, 0.268, 0.139, 0.156, 0.092]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.19369825720787048
[2m[36m(func pid=15258)[0m mae:  0.13025659322738647
[2m[36m(func pid=15258)[0m rmse_per_class: [0.207, 0.264, 0.06, 0.382, 0.056, 0.205, 0.343, 0.155, 0.167, 0.097]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:03:18 (running for 00:02:24.22)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.717 |  0.181 |                   18 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.497 |  0.179 |                   18 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.684 |  0.194 |                   18 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.598 |  0.205 |                   16 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.180670827627182
[2m[36m(func pid=14462)[0m mae:  0.1331002414226532
[2m[36m(func pid=14462)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.107, 0.19, 0.303, 0.143, 0.143, 0.115]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.17433767020702362
[2m[36m(func pid=15692)[0m mae:  0.11006458103656769
[2m[36m(func pid=15692)[0m rmse_per_class: [0.097, 0.274, 0.054, 0.292, 0.056, 0.318, 0.262, 0.155, 0.137, 0.097]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.7404 | Steps: 2 | Val loss: 0.4266 | Batch size: 32 | lr: 0.01 | Duration: 2.62s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.5123 | Steps: 2 | Val loss: 0.3559 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6942 | Steps: 2 | Val loss: 0.5483 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6485 | Steps: 2 | Val loss: 0.4172 | Batch size: 32 | lr: 0.1 | Duration: 2.61s
[2m[36m(func pid=15258)[0m rmse: 0.18909147381782532
[2m[36m(func pid=15258)[0m mae:  0.12896659970283508
[2m[36m(func pid=15258)[0m rmse_per_class: [0.194, 0.252, 0.068, 0.38, 0.056, 0.199, 0.295, 0.155, 0.194, 0.097]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.17912279069423676
[2m[36m(func pid=14835)[0m mae:  0.12854556739330292
[2m[36m(func pid=14835)[0m rmse_per_class: [0.125, 0.268, 0.136, 0.353, 0.067, 0.186, 0.268, 0.14, 0.157, 0.092]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:03:23 (running for 00:02:29.26)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.694 |  0.181 |                   19 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.512 |  0.179 |                   19 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.74  |  0.189 |                   19 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.617 |  0.174 |                   17 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.18058165907859802
[2m[36m(func pid=14462)[0m mae:  0.13303425908088684
[2m[36m(func pid=14462)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.329, 0.106, 0.19, 0.303, 0.143, 0.143, 0.115]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1660165637731552
[2m[36m(func pid=15692)[0m mae:  0.10510410368442535
[2m[36m(func pid=15692)[0m rmse_per_class: [0.119, 0.262, 0.05, 0.311, 0.053, 0.233, 0.241, 0.156, 0.139, 0.097]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6783 | Steps: 2 | Val loss: 0.4089 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5271 | Steps: 2 | Val loss: 0.3669 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6676 | Steps: 2 | Val loss: 0.5302 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.7601 | Steps: 2 | Val loss: 0.3717 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=14835)[0m rmse: 0.1795533001422882
[2m[36m(func pid=14835)[0m mae:  0.12833669781684875
[2m[36m(func pid=14835)[0m rmse_per_class: [0.126, 0.268, 0.138, 0.355, 0.064, 0.186, 0.268, 0.141, 0.158, 0.092]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.18347617983818054
[2m[36m(func pid=15258)[0m mae:  0.12734468281269073
[2m[36m(func pid=15258)[0m rmse_per_class: [0.159, 0.242, 0.07, 0.377, 0.056, 0.19, 0.255, 0.155, 0.233, 0.097]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:03:28 (running for 00:02:34.56)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.668 |  0.181 |                   20 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.527 |  0.18  |                   20 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.678 |  0.183 |                   20 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.649 |  0.166 |                   18 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.18050844967365265
[2m[36m(func pid=14462)[0m mae:  0.13296689093112946
[2m[36m(func pid=14462)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.106, 0.19, 0.302, 0.143, 0.143, 0.115]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.17195382714271545
[2m[36m(func pid=15692)[0m mae:  0.10732942819595337
[2m[36m(func pid=15692)[0m rmse_per_class: [0.208, 0.283, 0.049, 0.27, 0.099, 0.195, 0.227, 0.155, 0.137, 0.097]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5457 | Steps: 2 | Val loss: 0.3783 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6578 | Steps: 2 | Val loss: 0.3928 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6481 | Steps: 2 | Val loss: 0.5131 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6416 | Steps: 2 | Val loss: 0.4505 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=14835)[0m rmse: 0.18004894256591797
[2m[36m(func pid=14835)[0m mae:  0.12815067172050476
[2m[36m(func pid=14835)[0m rmse_per_class: [0.127, 0.269, 0.14, 0.357, 0.062, 0.186, 0.269, 0.142, 0.158, 0.091]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17971032857894897
[2m[36m(func pid=15258)[0m mae:  0.12622325122356415
[2m[36m(func pid=15258)[0m rmse_per_class: [0.119, 0.249, 0.067, 0.37, 0.056, 0.18, 0.243, 0.155, 0.261, 0.097]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:03:33 (running for 00:02:39.60)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.648 |  0.18  |                   21 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.546 |  0.18  |                   21 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.658 |  0.18  |                   21 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.76  |  0.172 |                   19 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.1804390251636505
[2m[36m(func pid=14462)[0m mae:  0.13290287554264069
[2m[36m(func pid=14462)[0m rmse_per_class: [0.114, 0.264, 0.1, 0.33, 0.105, 0.19, 0.301, 0.143, 0.143, 0.115]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.21656878292560577
[2m[36m(func pid=15692)[0m mae:  0.13498644530773163
[2m[36m(func pid=15692)[0m rmse_per_class: [0.202, 0.277, 0.08, 0.337, 0.447, 0.172, 0.254, 0.152, 0.148, 0.097]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5575 | Steps: 2 | Val loss: 0.3900 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6477 | Steps: 2 | Val loss: 0.3805 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6311 | Steps: 2 | Val loss: 0.4968 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6230 | Steps: 2 | Val loss: 0.5882 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=14835)[0m rmse: 0.1806568205356598
[2m[36m(func pid=14835)[0m mae:  0.12805941700935364
[2m[36m(func pid=14835)[0m rmse_per_class: [0.128, 0.269, 0.142, 0.359, 0.06, 0.187, 0.27, 0.143, 0.158, 0.091]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17916998267173767
[2m[36m(func pid=15258)[0m mae:  0.1257363110780716
[2m[36m(func pid=15258)[0m rmse_per_class: [0.097, 0.284, 0.06, 0.358, 0.056, 0.172, 0.259, 0.155, 0.253, 0.097]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:03:38 (running for 00:02:44.79)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.631 |  0.18  |                   22 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.557 |  0.181 |                   22 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.648 |  0.179 |                   22 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.642 |  0.217 |                   20 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.18034011125564575
[2m[36m(func pid=14462)[0m mae:  0.132818341255188
[2m[36m(func pid=14462)[0m rmse_per_class: [0.114, 0.264, 0.1, 0.33, 0.104, 0.19, 0.3, 0.142, 0.144, 0.114]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.25875481963157654
[2m[36m(func pid=15692)[0m mae:  0.16806018352508545
[2m[36m(func pid=15692)[0m rmse_per_class: [0.102, 0.266, 0.09, 0.386, 0.513, 0.213, 0.31, 0.137, 0.472, 0.097]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5704 | Steps: 2 | Val loss: 0.4015 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.6061 | Steps: 2 | Val loss: 0.3705 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.6095 | Steps: 2 | Val loss: 0.4810 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6972 | Steps: 2 | Val loss: 0.6148 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
== Status ==
Current time: 2024-01-07 05:03:43 (running for 00:02:49.79)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.61  |  0.18  |                   23 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.57  |  0.181 |                   23 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.606 |  0.178 |                   23 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.623 |  0.259 |                   21 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.18025043606758118
[2m[36m(func pid=14462)[0m mae:  0.13274063169956207
[2m[36m(func pid=14462)[0m rmse_per_class: [0.114, 0.264, 0.101, 0.33, 0.104, 0.19, 0.3, 0.142, 0.144, 0.114]
[2m[36m(func pid=14835)[0m rmse: 0.1813301146030426
[2m[36m(func pid=14835)[0m mae:  0.12800420820713043
[2m[36m(func pid=14835)[0m rmse_per_class: [0.13, 0.27, 0.143, 0.361, 0.058, 0.187, 0.272, 0.144, 0.158, 0.091]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.1783411204814911
[2m[36m(func pid=15258)[0m mae:  0.12447305023670197
[2m[36m(func pid=15258)[0m rmse_per_class: [0.098, 0.317, 0.054, 0.337, 0.056, 0.174, 0.281, 0.155, 0.214, 0.097]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.2427949607372284
[2m[36m(func pid=15692)[0m mae:  0.16082656383514404
[2m[36m(func pid=15692)[0m rmse_per_class: [0.107, 0.221, 0.067, 0.389, 0.399, 0.222, 0.323, 0.1, 0.504, 0.097]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5828 | Steps: 2 | Val loss: 0.4125 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5792 | Steps: 2 | Val loss: 0.3603 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5755 | Steps: 2 | Val loss: 0.4649 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.6275 | Steps: 2 | Val loss: 0.5237 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 05:03:48 (running for 00:02:54.95)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.61  |  0.18  |                   23 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.583 |  0.182 |                   24 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.606 |  0.178 |                   23 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.697 |  0.243 |                   22 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.18211671710014343
[2m[36m(func pid=14835)[0m mae:  0.12805604934692383
[2m[36m(func pid=14835)[0m rmse_per_class: [0.131, 0.27, 0.145, 0.363, 0.057, 0.188, 0.275, 0.144, 0.158, 0.091]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17418460547924042
[2m[36m(func pid=15258)[0m mae:  0.12106417119503021
[2m[36m(func pid=15258)[0m rmse_per_class: [0.104, 0.315, 0.05, 0.308, 0.056, 0.192, 0.299, 0.154, 0.166, 0.096]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.18019269406795502
[2m[36m(func pid=14462)[0m mae:  0.132680743932724
[2m[36m(func pid=14462)[0m rmse_per_class: [0.114, 0.264, 0.102, 0.331, 0.103, 0.19, 0.299, 0.141, 0.144, 0.113]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.21449629962444305
[2m[36m(func pid=15692)[0m mae:  0.13531139492988586
[2m[36m(func pid=15692)[0m rmse_per_class: [0.1, 0.377, 0.059, 0.389, 0.251, 0.218, 0.29, 0.192, 0.175, 0.094]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5994 | Steps: 2 | Val loss: 0.4233 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5734 | Steps: 2 | Val loss: 0.3552 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5741 | Steps: 2 | Val loss: 0.4510 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5568 | Steps: 2 | Val loss: 0.4923 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=14835)[0m rmse: 0.18296851217746735
[2m[36m(func pid=14835)[0m mae:  0.12815208733081818
[2m[36m(func pid=14835)[0m rmse_per_class: [0.132, 0.271, 0.147, 0.364, 0.056, 0.188, 0.278, 0.145, 0.157, 0.091]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17005546391010284
[2m[36m(func pid=15258)[0m mae:  0.1173509955406189
[2m[36m(func pid=15258)[0m rmse_per_class: [0.108, 0.278, 0.046, 0.294, 0.056, 0.22, 0.31, 0.153, 0.139, 0.096]
== Status ==
Current time: 2024-01-07 05:03:53 (running for 00:03:00.13)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.576 |  0.18  |                   24 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.599 |  0.183 |                   25 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.573 |  0.17  |                   25 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.627 |  0.214 |                   23 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.18008238077163696
[2m[36m(func pid=14462)[0m mae:  0.13258345425128937
[2m[36m(func pid=14462)[0m rmse_per_class: [0.115, 0.264, 0.103, 0.331, 0.102, 0.19, 0.298, 0.141, 0.144, 0.112]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.189099982380867
[2m[36m(func pid=15692)[0m mae:  0.11904235184192657
[2m[36m(func pid=15692)[0m rmse_per_class: [0.126, 0.348, 0.045, 0.388, 0.095, 0.202, 0.245, 0.22, 0.136, 0.085]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.6279 | Steps: 2 | Val loss: 0.4359 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5602 | Steps: 2 | Val loss: 0.4383 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5631 | Steps: 2 | Val loss: 0.3564 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5537 | Steps: 2 | Val loss: 0.4815 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=14835)[0m rmse: 0.184016615152359
[2m[36m(func pid=14835)[0m mae:  0.12833495438098907
[2m[36m(func pid=14835)[0m rmse_per_class: [0.134, 0.271, 0.149, 0.366, 0.055, 0.189, 0.281, 0.146, 0.157, 0.092]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.1712505966424942
[2m[36m(func pid=15258)[0m mae:  0.11548177897930145
[2m[36m(func pid=15258)[0m rmse_per_class: [0.109, 0.244, 0.044, 0.316, 0.056, 0.248, 0.315, 0.151, 0.133, 0.095]
[2m[36m(func pid=14462)[0m rmse: 0.17997026443481445
[2m[36m(func pid=14462)[0m mae:  0.13248084485530853
[2m[36m(func pid=14462)[0m rmse_per_class: [0.115, 0.265, 0.103, 0.331, 0.102, 0.19, 0.297, 0.14, 0.145, 0.112]
== Status ==
Current time: 2024-01-07 05:03:59 (running for 00:03:05.53)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.574 |  0.18  |                   25 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.628 |  0.184 |                   26 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.573 |  0.17  |                   25 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.557 |  0.189 |                   24 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18490424752235413
[2m[36m(func pid=15692)[0m mae:  0.1222197636961937
[2m[36m(func pid=15692)[0m rmse_per_class: [0.233, 0.206, 0.05, 0.386, 0.051, 0.17, 0.242, 0.125, 0.139, 0.248]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.6289 | Steps: 2 | Val loss: 0.4455 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5437 | Steps: 2 | Val loss: 0.4261 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5847 | Steps: 2 | Val loss: 0.3553 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.6010 | Steps: 2 | Val loss: 0.3532 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=14835)[0m rmse: 0.18495362997055054
[2m[36m(func pid=14835)[0m mae:  0.12851017713546753
[2m[36m(func pid=14835)[0m rmse_per_class: [0.135, 0.272, 0.151, 0.367, 0.055, 0.19, 0.285, 0.147, 0.157, 0.092]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:04:04 (running for 00:03:10.70)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.56  |  0.18  |                   26 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.629 |  0.185 |                   27 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.585 |  0.176 |                   27 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.554 |  0.185 |                   25 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.17988058924674988
[2m[36m(func pid=14462)[0m mae:  0.13239111006259918
[2m[36m(func pid=14462)[0m rmse_per_class: [0.115, 0.265, 0.104, 0.332, 0.101, 0.19, 0.296, 0.14, 0.145, 0.111]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17618849873542786
[2m[36m(func pid=15258)[0m mae:  0.11556736379861832
[2m[36m(func pid=15258)[0m rmse_per_class: [0.11, 0.245, 0.045, 0.346, 0.056, 0.269, 0.317, 0.147, 0.133, 0.093]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18317648768424988
[2m[36m(func pid=15692)[0m mae:  0.11525465548038483
[2m[36m(func pid=15692)[0m rmse_per_class: [0.27, 0.23, 0.054, 0.327, 0.054, 0.233, 0.264, 0.1, 0.139, 0.16]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6254 | Steps: 2 | Val loss: 0.4522 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5324 | Steps: 2 | Val loss: 0.4151 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5915 | Steps: 2 | Val loss: 0.3442 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5202 | Steps: 2 | Val loss: 0.3707 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=14835)[0m rmse: 0.18567028641700745
[2m[36m(func pid=14835)[0m mae:  0.12858322262763977
[2m[36m(func pid=14835)[0m rmse_per_class: [0.137, 0.272, 0.151, 0.368, 0.055, 0.19, 0.288, 0.148, 0.156, 0.092]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:04:09 (running for 00:03:15.96)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.544 |  0.18  |                   27 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.625 |  0.186 |                   28 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.585 |  0.176 |                   27 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.52  |  0.177 |                   27 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.17977946996688843
[2m[36m(func pid=14462)[0m mae:  0.13228808343410492
[2m[36m(func pid=14462)[0m rmse_per_class: [0.115, 0.265, 0.105, 0.332, 0.1, 0.19, 0.295, 0.139, 0.145, 0.11]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.1777985841035843
[2m[36m(func pid=15258)[0m mae:  0.11509086191654205
[2m[36m(func pid=15258)[0m rmse_per_class: [0.11, 0.26, 0.047, 0.351, 0.056, 0.279, 0.313, 0.137, 0.134, 0.091]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.177435502409935
[2m[36m(func pid=15692)[0m mae:  0.10838796943426132
[2m[36m(func pid=15692)[0m rmse_per_class: [0.095, 0.26, 0.053, 0.385, 0.056, 0.276, 0.306, 0.119, 0.137, 0.088]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5215 | Steps: 2 | Val loss: 0.4054 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.6353 | Steps: 2 | Val loss: 0.4578 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5506 | Steps: 2 | Val loss: 0.3331 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5845 | Steps: 2 | Val loss: 0.4117 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 05:04:14 (running for 00:03:21.01)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.532 |  0.18  |                   28 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.625 |  0.186 |                   28 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.592 |  0.178 |                   28 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.52  |  0.177 |                   27 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.17969070374965668
[2m[36m(func pid=14462)[0m mae:  0.1321912705898285
[2m[36m(func pid=14462)[0m rmse_per_class: [0.115, 0.265, 0.106, 0.333, 0.099, 0.19, 0.294, 0.139, 0.146, 0.11]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.18638312816619873
[2m[36m(func pid=14835)[0m mae:  0.1286768615245819
[2m[36m(func pid=14835)[0m rmse_per_class: [0.138, 0.273, 0.151, 0.369, 0.054, 0.191, 0.291, 0.148, 0.156, 0.092]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17391380667686462
[2m[36m(func pid=15258)[0m mae:  0.11341670900583267
[2m[36m(func pid=15258)[0m rmse_per_class: [0.109, 0.27, 0.049, 0.325, 0.056, 0.27, 0.303, 0.125, 0.134, 0.098]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18359147012233734
[2m[36m(func pid=15692)[0m mae:  0.11484052985906601
[2m[36m(func pid=15692)[0m rmse_per_class: [0.106, 0.257, 0.053, 0.511, 0.056, 0.169, 0.315, 0.14, 0.133, 0.094]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.6430 | Steps: 2 | Val loss: 0.4626 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.6151 | Steps: 2 | Val loss: 0.4078 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5127 | Steps: 2 | Val loss: 0.3964 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5169 | Steps: 2 | Val loss: 0.3378 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 05:04:20 (running for 00:03:26.24)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.522 |  0.18  |                   29 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.643 |  0.187 |                   30 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.551 |  0.174 |                   29 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.585 |  0.184 |                   28 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.18706002831459045
[2m[36m(func pid=14835)[0m mae:  0.12877210974693298
[2m[36m(func pid=14835)[0m rmse_per_class: [0.14, 0.273, 0.151, 0.37, 0.054, 0.192, 0.294, 0.149, 0.156, 0.092]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17961344122886658
[2m[36m(func pid=14462)[0m mae:  0.13210119307041168
[2m[36m(func pid=14462)[0m rmse_per_class: [0.115, 0.265, 0.107, 0.333, 0.099, 0.19, 0.293, 0.139, 0.146, 0.109]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.1714712679386139
[2m[36m(func pid=15258)[0m mae:  0.11443328857421875
[2m[36m(func pid=15258)[0m rmse_per_class: [0.106, 0.277, 0.051, 0.288, 0.056, 0.231, 0.292, 0.14, 0.133, 0.14]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18966729938983917
[2m[36m(func pid=15692)[0m mae:  0.12185555696487427
[2m[36m(func pid=15692)[0m rmse_per_class: [0.103, 0.216, 0.053, 0.284, 0.056, 0.16, 0.293, 0.148, 0.487, 0.096]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.6320 | Steps: 2 | Val loss: 0.4262 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.6566 | Steps: 2 | Val loss: 0.4672 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5325 | Steps: 2 | Val loss: 0.3639 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5031 | Steps: 2 | Val loss: 0.3877 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 05:04:25 (running for 00:03:31.40)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.513 |  0.18  |                   30 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.643 |  0.187 |                   30 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.517 |  0.171 |                   30 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.615 |  0.19  |                   29 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15692)[0m rmse: 0.19238583743572235
[2m[36m(func pid=15692)[0m mae:  0.11985962092876434
[2m[36m(func pid=15692)[0m rmse_per_class: [0.114, 0.29, 0.05, 0.356, 0.056, 0.186, 0.25, 0.151, 0.375, 0.096]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17952626943588257
[2m[36m(func pid=14462)[0m mae:  0.13199421763420105
[2m[36m(func pid=14462)[0m rmse_per_class: [0.116, 0.265, 0.108, 0.334, 0.098, 0.19, 0.292, 0.138, 0.147, 0.108]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.18782125413417816
[2m[36m(func pid=14835)[0m mae:  0.12890855967998505
[2m[36m(func pid=14835)[0m rmse_per_class: [0.141, 0.273, 0.152, 0.371, 0.054, 0.193, 0.297, 0.149, 0.155, 0.093]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.18414126336574554
[2m[36m(func pid=15258)[0m mae:  0.12438275665044785
[2m[36m(func pid=15258)[0m rmse_per_class: [0.102, 0.284, 0.053, 0.3, 0.056, 0.184, 0.297, 0.208, 0.134, 0.224]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.6436 | Steps: 2 | Val loss: 0.4702 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4918 | Steps: 2 | Val loss: 0.3802 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5344 | Steps: 2 | Val loss: 0.4048 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5708 | Steps: 2 | Val loss: 0.4917 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 05:04:30 (running for 00:03:36.73)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.503 |  0.18  |                   31 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.657 |  0.188 |                   31 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.533 |  0.184 |                   31 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.632 |  0.192 |                   30 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.18851572275161743
[2m[36m(func pid=14835)[0m mae:  0.12904511392116547
[2m[36m(func pid=14835)[0m rmse_per_class: [0.143, 0.273, 0.152, 0.371, 0.054, 0.193, 0.3, 0.15, 0.155, 0.093]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17946882545948029
[2m[36m(func pid=14462)[0m mae:  0.13191092014312744
[2m[36m(func pid=14462)[0m rmse_per_class: [0.116, 0.265, 0.109, 0.335, 0.097, 0.19, 0.291, 0.138, 0.147, 0.108]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1980309784412384
[2m[36m(func pid=15692)[0m mae:  0.1174103245139122
[2m[36m(func pid=15692)[0m rmse_per_class: [0.243, 0.361, 0.042, 0.384, 0.056, 0.205, 0.31, 0.15, 0.132, 0.096]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.2040378749370575
[2m[36m(func pid=15258)[0m mae:  0.13822847604751587
[2m[36m(func pid=15258)[0m rmse_per_class: [0.1, 0.29, 0.05, 0.348, 0.056, 0.181, 0.304, 0.276, 0.135, 0.299]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.6515 | Steps: 2 | Val loss: 0.4744 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4755 | Steps: 2 | Val loss: 0.3729 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.6879 | Steps: 2 | Val loss: 0.5109 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.5803 | Steps: 2 | Val loss: 0.4513 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 05:04:35 (running for 00:03:42.01)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.492 |  0.179 |                   32 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.652 |  0.189 |                   33 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.534 |  0.204 |                   32 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.571 |  0.198 |                   31 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.189473956823349
[2m[36m(func pid=14835)[0m mae:  0.12935477495193481
[2m[36m(func pid=14835)[0m rmse_per_class: [0.146, 0.274, 0.154, 0.372, 0.055, 0.194, 0.302, 0.15, 0.154, 0.093]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.1793980896472931
[2m[36m(func pid=14462)[0m mae:  0.13180549442768097
[2m[36m(func pid=14462)[0m rmse_per_class: [0.116, 0.265, 0.111, 0.335, 0.096, 0.189, 0.29, 0.137, 0.147, 0.107]
[2m[36m(func pid=15692)[0m rmse: 0.19515250623226166
[2m[36m(func pid=15692)[0m mae:  0.11553510278463364
[2m[36m(func pid=15692)[0m rmse_per_class: [0.184, 0.398, 0.041, 0.387, 0.056, 0.195, 0.315, 0.143, 0.137, 0.096]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.21490070223808289
[2m[36m(func pid=15258)[0m mae:  0.14611200988292694
[2m[36m(func pid=15258)[0m rmse_per_class: [0.102, 0.294, 0.047, 0.372, 0.056, 0.197, 0.299, 0.308, 0.136, 0.337]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.6429 | Steps: 2 | Val loss: 0.4764 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5866 | Steps: 2 | Val loss: 0.4846 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.6334 | Steps: 2 | Val loss: 0.4403 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4754 | Steps: 2 | Val loss: 0.3669 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 05:04:41 (running for 00:03:47.21)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.476 |  0.179 |                   33 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.652 |  0.189 |                   33 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.58  |  0.215 |                   33 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.688 |  0.195 |                   32 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.19030366837978363
[2m[36m(func pid=14835)[0m mae:  0.12960503995418549
[2m[36m(func pid=14835)[0m rmse_per_class: [0.148, 0.274, 0.156, 0.373, 0.055, 0.195, 0.304, 0.151, 0.154, 0.093]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.21785899996757507
[2m[36m(func pid=15258)[0m mae:  0.1479417383670807
[2m[36m(func pid=15258)[0m rmse_per_class: [0.112, 0.295, 0.046, 0.381, 0.056, 0.208, 0.287, 0.313, 0.136, 0.345]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17933917045593262
[2m[36m(func pid=14462)[0m mae:  0.13171052932739258
[2m[36m(func pid=14462)[0m rmse_per_class: [0.117, 0.265, 0.112, 0.336, 0.095, 0.189, 0.289, 0.137, 0.148, 0.106]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18322953581809998
[2m[36m(func pid=15692)[0m mae:  0.10529708862304688
[2m[36m(func pid=15692)[0m rmse_per_class: [0.118, 0.426, 0.085, 0.383, 0.056, 0.173, 0.243, 0.121, 0.134, 0.093]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.6545 | Steps: 2 | Val loss: 0.4771 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.6008 | Steps: 2 | Val loss: 0.4955 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4730 | Steps: 2 | Val loss: 0.3618 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5725 | Steps: 2 | Val loss: 0.4127 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 05:04:46 (running for 00:03:52.29)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.475 |  0.179 |                   34 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.655 |  0.191 |                   35 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.587 |  0.218 |                   34 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.633 |  0.183 |                   33 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.19101351499557495
[2m[36m(func pid=14835)[0m mae:  0.12980051338672638
[2m[36m(func pid=14835)[0m rmse_per_class: [0.151, 0.274, 0.156, 0.374, 0.055, 0.196, 0.306, 0.151, 0.153, 0.094]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.2169342339038849
[2m[36m(func pid=15258)[0m mae:  0.14604488015174866
[2m[36m(func pid=15258)[0m rmse_per_class: [0.133, 0.295, 0.045, 0.384, 0.056, 0.215, 0.275, 0.298, 0.135, 0.332]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.1792999655008316
[2m[36m(func pid=14462)[0m mae:  0.131630539894104
[2m[36m(func pid=14462)[0m rmse_per_class: [0.117, 0.265, 0.113, 0.336, 0.094, 0.189, 0.288, 0.137, 0.148, 0.106]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1951679289340973
[2m[36m(func pid=15692)[0m mae:  0.12107843160629272
[2m[36m(func pid=15692)[0m rmse_per_class: [0.105, 0.228, 0.063, 0.353, 0.056, 0.536, 0.285, 0.102, 0.133, 0.09]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6052 | Steps: 2 | Val loss: 0.4870 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6467 | Steps: 2 | Val loss: 0.4762 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4567 | Steps: 2 | Val loss: 0.3574 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.5763 | Steps: 2 | Val loss: 0.3786 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 05:04:51 (running for 00:03:57.47)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.473 |  0.179 |                   35 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.655 |  0.191 |                   35 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.605 |  0.214 |                   36 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.572 |  0.195 |                   34 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.19159889221191406
[2m[36m(func pid=14835)[0m mae:  0.12995843589305878
[2m[36m(func pid=14835)[0m rmse_per_class: [0.154, 0.274, 0.156, 0.374, 0.055, 0.197, 0.308, 0.152, 0.153, 0.094]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.2139509618282318
[2m[36m(func pid=15258)[0m mae:  0.14277181029319763
[2m[36m(func pid=15258)[0m rmse_per_class: [0.16, 0.293, 0.044, 0.385, 0.056, 0.218, 0.266, 0.275, 0.134, 0.308]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.1792815625667572
[2m[36m(func pid=14462)[0m mae:  0.13156412541866302
[2m[36m(func pid=14462)[0m rmse_per_class: [0.117, 0.265, 0.114, 0.337, 0.094, 0.189, 0.287, 0.137, 0.148, 0.105]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18088829517364502
[2m[36m(func pid=15692)[0m mae:  0.11231490224599838
[2m[36m(func pid=15692)[0m rmse_per_class: [0.108, 0.227, 0.054, 0.277, 0.056, 0.387, 0.318, 0.129, 0.142, 0.112]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5857 | Steps: 2 | Val loss: 0.4653 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6345 | Steps: 2 | Val loss: 0.4759 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4521 | Steps: 2 | Val loss: 0.3529 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.5011 | Steps: 2 | Val loss: 0.3986 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 05:04:56 (running for 00:04:02.50)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.457 |  0.179 |                   36 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.647 |  0.192 |                   36 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.586 |  0.209 |                   37 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.576 |  0.181 |                   35 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.20917873084545135
[2m[36m(func pid=15258)[0m mae:  0.1386379897594452
[2m[36m(func pid=15258)[0m rmse_per_class: [0.195, 0.289, 0.042, 0.386, 0.056, 0.219, 0.258, 0.241, 0.133, 0.272]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.1792387068271637
[2m[36m(func pid=14462)[0m mae:  0.13146401941776276
[2m[36m(func pid=14462)[0m rmse_per_class: [0.117, 0.265, 0.115, 0.337, 0.093, 0.189, 0.286, 0.136, 0.149, 0.104]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.19242727756500244
[2m[36m(func pid=14835)[0m mae:  0.13028395175933838
[2m[36m(func pid=14835)[0m rmse_per_class: [0.158, 0.274, 0.159, 0.375, 0.055, 0.197, 0.308, 0.152, 0.152, 0.094]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1876441389322281
[2m[36m(func pid=15692)[0m mae:  0.11626823246479034
[2m[36m(func pid=15692)[0m rmse_per_class: [0.105, 0.26, 0.055, 0.31, 0.055, 0.177, 0.319, 0.143, 0.233, 0.22]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.5421 | Steps: 2 | Val loss: 0.4397 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6436 | Steps: 2 | Val loss: 0.4752 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4599 | Steps: 2 | Val loss: 0.3494 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5507 | Steps: 2 | Val loss: 0.4496 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 05:05:01 (running for 00:04:07.69)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.452 |  0.179 |                   37 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.644 |  0.193 |                   38 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.586 |  0.209 |                   37 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.501 |  0.188 |                   36 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.19328424334526062
[2m[36m(func pid=14835)[0m mae:  0.13064679503440857
[2m[36m(func pid=14835)[0m rmse_per_class: [0.162, 0.274, 0.162, 0.375, 0.055, 0.198, 0.308, 0.152, 0.152, 0.094]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.20240230858325958
[2m[36m(func pid=15258)[0m mae:  0.13455845415592194
[2m[36m(func pid=15258)[0m rmse_per_class: [0.224, 0.28, 0.043, 0.385, 0.056, 0.218, 0.249, 0.199, 0.138, 0.231]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.1791803538799286
[2m[36m(func pid=14462)[0m mae:  0.13136930763721466
[2m[36m(func pid=14462)[0m rmse_per_class: [0.118, 0.265, 0.116, 0.338, 0.092, 0.189, 0.285, 0.136, 0.149, 0.104]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18537387251853943
[2m[36m(func pid=15692)[0m mae:  0.11908048391342163
[2m[36m(func pid=15692)[0m rmse_per_class: [0.108, 0.261, 0.053, 0.275, 0.054, 0.215, 0.292, 0.118, 0.195, 0.282]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6320 | Steps: 2 | Val loss: 0.4728 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.5261 | Steps: 2 | Val loss: 0.4202 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4576 | Steps: 2 | Val loss: 0.3464 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.5750 | Steps: 2 | Val loss: 0.3967 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 05:05:06 (running for 00:04:12.82)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.46  |  0.179 |                   38 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.632 |  0.194 |                   39 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.542 |  0.202 |                   38 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.551 |  0.185 |                   37 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.19398577511310577
[2m[36m(func pid=14835)[0m mae:  0.13092507421970367
[2m[36m(func pid=14835)[0m rmse_per_class: [0.166, 0.274, 0.164, 0.376, 0.055, 0.199, 0.308, 0.153, 0.151, 0.094]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.19478997588157654
[2m[36m(func pid=15258)[0m mae:  0.1311001181602478
[2m[36m(func pid=15258)[0m rmse_per_class: [0.231, 0.265, 0.046, 0.384, 0.056, 0.215, 0.24, 0.156, 0.171, 0.184]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17911572754383087
[2m[36m(func pid=14462)[0m mae:  0.13126076757907867
[2m[36m(func pid=14462)[0m rmse_per_class: [0.118, 0.265, 0.117, 0.338, 0.091, 0.189, 0.284, 0.136, 0.149, 0.103]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.17983368039131165
[2m[36m(func pid=15692)[0m mae:  0.10881955921649933
[2m[36m(func pid=15692)[0m rmse_per_class: [0.213, 0.225, 0.05, 0.269, 0.05, 0.218, 0.227, 0.117, 0.13, 0.298]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6281 | Steps: 2 | Val loss: 0.4692 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.5105 | Steps: 2 | Val loss: 0.4085 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4508 | Steps: 2 | Val loss: 0.3436 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.5130 | Steps: 2 | Val loss: 0.3683 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=14835)[0m rmse: 0.19453775882720947
[2m[36m(func pid=14835)[0m mae:  0.131135955452919
[2m[36m(func pid=14835)[0m rmse_per_class: [0.17, 0.274, 0.165, 0.376, 0.055, 0.199, 0.308, 0.153, 0.151, 0.095]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:05:11 (running for 00:04:18.02)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.458 |  0.179 |                   39 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.628 |  0.195 |                   40 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.511 |  0.189 |                   40 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.575 |  0.18  |                   38 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.18920540809631348
[2m[36m(func pid=15258)[0m mae:  0.12910816073417664
[2m[36m(func pid=15258)[0m rmse_per_class: [0.203, 0.242, 0.049, 0.383, 0.056, 0.212, 0.232, 0.128, 0.251, 0.136]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17908865213394165
[2m[36m(func pid=14462)[0m mae:  0.13118083775043488
[2m[36m(func pid=14462)[0m rmse_per_class: [0.118, 0.265, 0.118, 0.339, 0.09, 0.189, 0.283, 0.136, 0.15, 0.102]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.17927315831184387
[2m[36m(func pid=15692)[0m mae:  0.10321097075939178
[2m[36m(func pid=15692)[0m rmse_per_class: [0.226, 0.237, 0.05, 0.334, 0.079, 0.201, 0.238, 0.112, 0.133, 0.183]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6199 | Steps: 2 | Val loss: 0.4643 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4805 | Steps: 2 | Val loss: 0.4014 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4512 | Steps: 2 | Val loss: 0.3413 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4468 | Steps: 2 | Val loss: 0.4311 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=14835)[0m rmse: 0.19489744305610657
[2m[36m(func pid=14835)[0m mae:  0.1312749683856964
[2m[36m(func pid=14835)[0m rmse_per_class: [0.174, 0.274, 0.164, 0.376, 0.056, 0.2, 0.307, 0.153, 0.151, 0.095]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:05:17 (running for 00:04:23.15)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.451 |  0.179 |                   40 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.62  |  0.195 |                   41 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.481 |  0.187 |                   41 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.513 |  0.179 |                   39 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.1872808337211609
[2m[36m(func pid=15258)[0m mae:  0.1288912147283554
[2m[36m(func pid=15258)[0m rmse_per_class: [0.146, 0.219, 0.053, 0.382, 0.056, 0.208, 0.234, 0.122, 0.349, 0.104]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17903883755207062
[2m[36m(func pid=14462)[0m mae:  0.13108237087726593
[2m[36m(func pid=14462)[0m rmse_per_class: [0.118, 0.265, 0.119, 0.339, 0.089, 0.189, 0.282, 0.136, 0.15, 0.102]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18682599067687988
[2m[36m(func pid=15692)[0m mae:  0.10893182456493378
[2m[36m(func pid=15692)[0m rmse_per_class: [0.111, 0.446, 0.048, 0.374, 0.163, 0.166, 0.23, 0.106, 0.133, 0.091]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6153 | Steps: 2 | Val loss: 0.4606 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4873 | Steps: 2 | Val loss: 0.3978 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4474 | Steps: 2 | Val loss: 0.3392 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4840 | Steps: 2 | Val loss: 0.4907 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=14835)[0m rmse: 0.19551505148410797
[2m[36m(func pid=14835)[0m mae:  0.13157527148723602
[2m[36m(func pid=14835)[0m rmse_per_class: [0.18, 0.274, 0.165, 0.376, 0.056, 0.2, 0.306, 0.153, 0.151, 0.095]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:05:22 (running for 00:04:28.28)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.451 |  0.179 |                   41 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.615 |  0.196 |                   42 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.487 |  0.189 |                   42 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.447 |  0.187 |                   40 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.1892075091600418
[2m[36m(func pid=15258)[0m mae:  0.12980972230434418
[2m[36m(func pid=15258)[0m rmse_per_class: [0.102, 0.215, 0.061, 0.381, 0.056, 0.204, 0.244, 0.13, 0.407, 0.092]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17899072170257568
[2m[36m(func pid=14462)[0m mae:  0.13098512589931488
[2m[36m(func pid=14462)[0m rmse_per_class: [0.119, 0.266, 0.12, 0.34, 0.088, 0.189, 0.281, 0.136, 0.151, 0.101]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.2097717523574829
[2m[36m(func pid=15692)[0m mae:  0.12458518892526627
[2m[36m(func pid=15692)[0m rmse_per_class: [0.102, 0.44, 0.051, 0.379, 0.231, 0.299, 0.265, 0.111, 0.132, 0.089]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6110 | Steps: 2 | Val loss: 0.4551 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4859 | Steps: 2 | Val loss: 0.3932 | Batch size: 32 | lr: 0.01 | Duration: 2.63s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4491 | Steps: 2 | Val loss: 0.3381 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5700 | Steps: 2 | Val loss: 0.5260 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=14835)[0m rmse: 0.1957942545413971
[2m[36m(func pid=14835)[0m mae:  0.13170000910758972
[2m[36m(func pid=14835)[0m rmse_per_class: [0.185, 0.273, 0.163, 0.376, 0.056, 0.2, 0.305, 0.153, 0.151, 0.095]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.19309648871421814
[2m[36m(func pid=15258)[0m mae:  0.13046793639659882
[2m[36m(func pid=15258)[0m rmse_per_class: [0.095, 0.256, 0.069, 0.378, 0.056, 0.199, 0.256, 0.138, 0.392, 0.091]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.1789896935224533
[2m[36m(func pid=14462)[0m mae:  0.13091492652893066
[2m[36m(func pid=14462)[0m rmse_per_class: [0.119, 0.265, 0.121, 0.341, 0.087, 0.189, 0.281, 0.136, 0.151, 0.101]
== Status ==
Current time: 2024-01-07 05:05:27 (running for 00:04:33.69)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.449 |  0.179 |                   43 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.611 |  0.196 |                   43 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.486 |  0.193 |                   43 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.484 |  0.21  |                   41 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.2211255133152008
[2m[36m(func pid=15692)[0m mae:  0.13432908058166504
[2m[36m(func pid=15692)[0m rmse_per_class: [0.101, 0.285, 0.099, 0.367, 0.271, 0.426, 0.301, 0.129, 0.141, 0.093]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.6465 | Steps: 2 | Val loss: 0.4543 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4849 | Steps: 2 | Val loss: 0.3873 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4508 | Steps: 2 | Val loss: 0.3369 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.5861 | Steps: 2 | Val loss: 0.5029 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=14835)[0m rmse: 0.1969064623117447
[2m[36m(func pid=14835)[0m mae:  0.13225555419921875
[2m[36m(func pid=14835)[0m rmse_per_class: [0.192, 0.273, 0.169, 0.377, 0.056, 0.201, 0.302, 0.153, 0.15, 0.095]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.19550630450248718
[2m[36m(func pid=15258)[0m mae:  0.13084940612316132
[2m[36m(func pid=15258)[0m rmse_per_class: [0.099, 0.335, 0.071, 0.374, 0.056, 0.192, 0.267, 0.144, 0.324, 0.092]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:05:32 (running for 00:04:38.97)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.451 |  0.179 |                   44 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.646 |  0.197 |                   44 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.485 |  0.196 |                   44 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.57  |  0.221 |                   42 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.17896626889705658
[2m[36m(func pid=14462)[0m mae:  0.13083665072917938
[2m[36m(func pid=14462)[0m rmse_per_class: [0.119, 0.266, 0.122, 0.341, 0.086, 0.189, 0.28, 0.136, 0.151, 0.1]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.21726182103157043
[2m[36m(func pid=15692)[0m mae:  0.13072867691516876
[2m[36m(func pid=15692)[0m rmse_per_class: [0.1, 0.237, 0.118, 0.32, 0.287, 0.32, 0.295, 0.139, 0.264, 0.094]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6475 | Steps: 2 | Val loss: 0.4496 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5264 | Steps: 2 | Val loss: 0.3798 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4470 | Steps: 2 | Val loss: 0.3356 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5429 | Steps: 2 | Val loss: 0.4490 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=14835)[0m rmse: 0.1972476840019226
[2m[36m(func pid=14835)[0m mae:  0.13242219388484955
[2m[36m(func pid=14835)[0m rmse_per_class: [0.198, 0.273, 0.169, 0.377, 0.056, 0.201, 0.301, 0.154, 0.15, 0.095]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.1948932707309723
[2m[36m(func pid=15258)[0m mae:  0.1299184262752533
[2m[36m(func pid=15258)[0m rmse_per_class: [0.102, 0.418, 0.068, 0.366, 0.056, 0.182, 0.274, 0.147, 0.241, 0.094]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:05:37 (running for 00:04:43.99)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.447 |  0.179 |                   45 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.648 |  0.197 |                   45 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.526 |  0.195 |                   45 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.586 |  0.217 |                   43 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.17893336713314056
[2m[36m(func pid=14462)[0m mae:  0.13075664639472961
[2m[36m(func pid=14462)[0m rmse_per_class: [0.119, 0.266, 0.122, 0.341, 0.085, 0.189, 0.279, 0.136, 0.152, 0.1]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1986308991909027
[2m[36m(func pid=15692)[0m mae:  0.12030640989542007
[2m[36m(func pid=15692)[0m rmse_per_class: [0.12, 0.261, 0.082, 0.282, 0.256, 0.166, 0.262, 0.142, 0.322, 0.093]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5861 | Steps: 2 | Val loss: 0.4427 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5437 | Steps: 2 | Val loss: 0.3674 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4468 | Steps: 2 | Val loss: 0.3348 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5062 | Steps: 2 | Val loss: 0.4569 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=14835)[0m rmse: 0.19709078967571259
[2m[36m(func pid=14835)[0m mae:  0.13243257999420166
[2m[36m(func pid=14835)[0m rmse_per_class: [0.203, 0.272, 0.164, 0.377, 0.056, 0.201, 0.299, 0.154, 0.15, 0.095]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.19078388810157776
[2m[36m(func pid=15258)[0m mae:  0.12648293375968933
[2m[36m(func pid=15258)[0m rmse_per_class: [0.103, 0.457, 0.065, 0.35, 0.056, 0.172, 0.277, 0.15, 0.183, 0.095]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:05:42 (running for 00:04:49.06)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.447 |  0.179 |                   46 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.586 |  0.197 |                   46 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.544 |  0.191 |                   46 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.543 |  0.199 |                   44 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.17891111969947815
[2m[36m(func pid=14462)[0m mae:  0.13066649436950684
[2m[36m(func pid=14462)[0m rmse_per_class: [0.119, 0.266, 0.123, 0.342, 0.084, 0.188, 0.278, 0.136, 0.152, 0.1]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1789156198501587
[2m[36m(func pid=15692)[0m mae:  0.11038649082183838
[2m[36m(func pid=15692)[0m rmse_per_class: [0.164, 0.26, 0.049, 0.267, 0.192, 0.17, 0.233, 0.14, 0.222, 0.091]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5729 | Steps: 2 | Val loss: 0.4367 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5129 | Steps: 2 | Val loss: 0.3462 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4465 | Steps: 2 | Val loss: 0.3345 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5415 | Steps: 2 | Val loss: 0.4067 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=14835)[0m rmse: 0.19701191782951355
[2m[36m(func pid=14835)[0m mae:  0.13257093727588654
[2m[36m(func pid=14835)[0m rmse_per_class: [0.209, 0.271, 0.16, 0.377, 0.056, 0.201, 0.295, 0.154, 0.151, 0.095]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.18113195896148682
[2m[36m(func pid=15258)[0m mae:  0.11922701448202133
[2m[36m(func pid=15258)[0m rmse_per_class: [0.103, 0.43, 0.064, 0.321, 0.056, 0.163, 0.276, 0.151, 0.153, 0.095]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.1789076179265976
[2m[36m(func pid=14462)[0m mae:  0.13058698177337646
[2m[36m(func pid=14462)[0m rmse_per_class: [0.12, 0.266, 0.124, 0.342, 0.084, 0.188, 0.277, 0.136, 0.152, 0.099]
[2m[36m(func pid=14462)[0m 
== Status ==
Current time: 2024-01-07 05:05:48 (running for 00:04:54.31)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.447 |  0.179 |                   47 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.573 |  0.197 |                   47 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.513 |  0.181 |                   47 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.541 |  0.169 |                   46 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15692)[0m rmse: 0.16938592493534088
[2m[36m(func pid=15692)[0m mae:  0.10165892541408539
[2m[36m(func pid=15692)[0m rmse_per_class: [0.204, 0.234, 0.047, 0.275, 0.141, 0.181, 0.248, 0.13, 0.145, 0.089]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5732 | Steps: 2 | Val loss: 0.4328 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4814 | Steps: 2 | Val loss: 0.3226 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4454 | Steps: 2 | Val loss: 0.3344 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4815 | Steps: 2 | Val loss: 0.3437 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=14835)[0m rmse: 0.19716046750545502
[2m[36m(func pid=14835)[0m mae:  0.1327960044145584
[2m[36m(func pid=14835)[0m rmse_per_class: [0.217, 0.271, 0.158, 0.377, 0.056, 0.201, 0.292, 0.154, 0.151, 0.095]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.166395902633667
[2m[36m(func pid=15258)[0m mae:  0.10924079269170761
[2m[36m(func pid=15258)[0m rmse_per_class: [0.1, 0.336, 0.063, 0.286, 0.055, 0.164, 0.272, 0.152, 0.139, 0.095]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17893125116825104
[2m[36m(func pid=14462)[0m mae:  0.13052073121070862
[2m[36m(func pid=14462)[0m rmse_per_class: [0.12, 0.266, 0.125, 0.343, 0.083, 0.188, 0.277, 0.136, 0.152, 0.099]
[2m[36m(func pid=14462)[0m 
== Status ==
Current time: 2024-01-07 05:05:53 (running for 00:04:59.42)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.445 |  0.179 |                   48 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.573 |  0.197 |                   48 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.481 |  0.166 |                   48 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.481 |  0.16  |                   47 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15692)[0m rmse: 0.15958400070667267
[2m[36m(func pid=15692)[0m mae:  0.08937348425388336
[2m[36m(func pid=15692)[0m rmse_per_class: [0.217, 0.2, 0.065, 0.278, 0.092, 0.162, 0.244, 0.11, 0.134, 0.094]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5656 | Steps: 2 | Val loss: 0.4274 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4666 | Steps: 2 | Val loss: 0.3097 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4489 | Steps: 2 | Val loss: 0.3345 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4387 | Steps: 2 | Val loss: 0.3908 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=14835)[0m rmse: 0.19686710834503174
[2m[36m(func pid=14835)[0m mae:  0.13287313282489777
[2m[36m(func pid=14835)[0m rmse_per_class: [0.224, 0.27, 0.152, 0.377, 0.056, 0.201, 0.288, 0.154, 0.152, 0.095]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.15690290927886963
[2m[36m(func pid=15258)[0m mae:  0.10223381221294403
[2m[36m(func pid=15258)[0m rmse_per_class: [0.098, 0.235, 0.055, 0.298, 0.054, 0.177, 0.27, 0.153, 0.133, 0.096]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17891272902488708
[2m[36m(func pid=14462)[0m mae:  0.13043542206287384
[2m[36m(func pid=14462)[0m rmse_per_class: [0.12, 0.266, 0.126, 0.343, 0.081, 0.188, 0.276, 0.136, 0.153, 0.098]
[2m[36m(func pid=14462)[0m 
== Status ==
Current time: 2024-01-07 05:05:58 (running for 00:05:04.49)
Memory usage on this node: 24.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.449 |  0.179 |                   49 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.566 |  0.197 |                   49 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.467 |  0.157 |                   49 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.439 |  0.18  |                   48 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15692)[0m rmse: 0.17962418496608734
[2m[36m(func pid=15692)[0m mae:  0.10351693630218506
[2m[36m(func pid=15692)[0m rmse_per_class: [0.119, 0.373, 0.066, 0.324, 0.054, 0.166, 0.248, 0.137, 0.134, 0.176]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5538 | Steps: 2 | Val loss: 0.4224 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4649 | Steps: 2 | Val loss: 0.3113 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4503 | Steps: 2 | Val loss: 0.3345 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4767 | Steps: 2 | Val loss: 0.4457 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=14835)[0m rmse: 0.19647216796875
[2m[36m(func pid=14835)[0m mae:  0.13286174833774567
[2m[36m(func pid=14835)[0m rmse_per_class: [0.23, 0.269, 0.147, 0.376, 0.056, 0.201, 0.285, 0.154, 0.153, 0.096]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.1586979776620865
[2m[36m(func pid=15258)[0m mae:  0.10167355835437775
[2m[36m(func pid=15258)[0m rmse_per_class: [0.096, 0.216, 0.044, 0.338, 0.053, 0.19, 0.268, 0.153, 0.133, 0.096]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:06:03 (running for 00:05:09.49)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.45  |  0.179 |                   50 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.554 |  0.196 |                   50 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.465 |  0.159 |                   50 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.439 |  0.18  |                   48 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.17888197302818298
[2m[36m(func pid=14462)[0m mae:  0.13033010065555573
[2m[36m(func pid=14462)[0m rmse_per_class: [0.121, 0.266, 0.127, 0.344, 0.081, 0.188, 0.275, 0.136, 0.153, 0.098]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.20514658093452454
[2m[36m(func pid=15692)[0m mae:  0.12336608022451401
[2m[36m(func pid=15692)[0m rmse_per_class: [0.105, 0.401, 0.046, 0.368, 0.051, 0.169, 0.296, 0.163, 0.134, 0.318]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5919 | Steps: 2 | Val loss: 0.4183 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4745 | Steps: 2 | Val loss: 0.3162 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4499 | Steps: 2 | Val loss: 0.3345 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5831 | Steps: 2 | Val loss: 0.4373 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=14835)[0m rmse: 0.19602391123771667
[2m[36m(func pid=14835)[0m mae:  0.1327521950006485
[2m[36m(func pid=14835)[0m rmse_per_class: [0.234, 0.267, 0.143, 0.376, 0.056, 0.2, 0.281, 0.154, 0.154, 0.096]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.16153718531131744
[2m[36m(func pid=15258)[0m mae:  0.10272469371557236
[2m[36m(func pid=15258)[0m rmse_per_class: [0.095, 0.234, 0.045, 0.341, 0.052, 0.205, 0.261, 0.153, 0.133, 0.096]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:06:08 (running for 00:05:14.82)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.45  |  0.179 |                   51 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.592 |  0.196 |                   51 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.474 |  0.162 |                   51 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.477 |  0.205 |                   49 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.17887328565120697
[2m[36m(func pid=14462)[0m mae:  0.13026772439479828
[2m[36m(func pid=14462)[0m rmse_per_class: [0.121, 0.266, 0.128, 0.344, 0.08, 0.188, 0.275, 0.136, 0.153, 0.098]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.19497628509998322
[2m[36m(func pid=15692)[0m mae:  0.11892648786306381
[2m[36m(func pid=15692)[0m rmse_per_class: [0.107, 0.327, 0.043, 0.369, 0.053, 0.187, 0.289, 0.152, 0.135, 0.287]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5625 | Steps: 2 | Val loss: 0.4133 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4713 | Steps: 2 | Val loss: 0.3174 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4490 | Steps: 2 | Val loss: 0.3348 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5475 | Steps: 2 | Val loss: 0.3815 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=14835)[0m rmse: 0.19527503848075867
[2m[36m(func pid=14835)[0m mae:  0.13251307606697083
[2m[36m(func pid=14835)[0m rmse_per_class: [0.237, 0.266, 0.137, 0.375, 0.056, 0.199, 0.277, 0.154, 0.155, 0.096]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.16205467283725739
[2m[36m(func pid=15258)[0m mae:  0.10370700061321259
[2m[36m(func pid=15258)[0m rmse_per_class: [0.112, 0.252, 0.048, 0.308, 0.052, 0.218, 0.25, 0.153, 0.134, 0.096]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17884428799152374
[2m[36m(func pid=14462)[0m mae:  0.13017390668392181
[2m[36m(func pid=14462)[0m rmse_per_class: [0.121, 0.266, 0.128, 0.345, 0.079, 0.188, 0.274, 0.136, 0.153, 0.097]
[2m[36m(func pid=14462)[0m 
== Status ==
Current time: 2024-01-07 05:06:13 (running for 00:05:20.05)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.449 |  0.179 |                   52 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.563 |  0.195 |                   52 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.471 |  0.162 |                   52 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.548 |  0.17  |                   51 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15692)[0m rmse: 0.17043772339820862
[2m[36m(func pid=15692)[0m mae:  0.10500230640172958
[2m[36m(func pid=15692)[0m rmse_per_class: [0.102, 0.221, 0.05, 0.346, 0.055, 0.228, 0.257, 0.123, 0.219, 0.103]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5428 | Steps: 2 | Val loss: 0.4074 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4757 | Steps: 2 | Val loss: 0.3189 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4524 | Steps: 2 | Val loss: 0.3354 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4672 | Steps: 2 | Val loss: 0.3565 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=14835)[0m rmse: 0.19417646527290344
[2m[36m(func pid=14835)[0m mae:  0.1320430338382721
[2m[36m(func pid=14835)[0m rmse_per_class: [0.238, 0.264, 0.131, 0.374, 0.056, 0.198, 0.274, 0.154, 0.157, 0.096]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.16596992313861847
[2m[36m(func pid=15258)[0m mae:  0.10633288323879242
[2m[36m(func pid=15258)[0m rmse_per_class: [0.175, 0.263, 0.049, 0.272, 0.057, 0.224, 0.239, 0.152, 0.134, 0.095]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17881521582603455
[2m[36m(func pid=14462)[0m mae:  0.13007842004299164
[2m[36m(func pid=14462)[0m rmse_per_class: [0.121, 0.266, 0.129, 0.345, 0.078, 0.188, 0.273, 0.136, 0.154, 0.097]
[2m[36m(func pid=14462)[0m 
== Status ==
Current time: 2024-01-07 05:06:19 (running for 00:05:25.20)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.452 |  0.179 |                   53 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.543 |  0.194 |                   53 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.476 |  0.166 |                   53 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.467 |  0.158 |                   52 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15692)[0m rmse: 0.15766598284244537
[2m[36m(func pid=15692)[0m mae:  0.09868265688419342
[2m[36m(func pid=15692)[0m rmse_per_class: [0.1, 0.207, 0.046, 0.281, 0.055, 0.222, 0.254, 0.098, 0.226, 0.087]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5446 | Steps: 2 | Val loss: 0.4009 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4538 | Steps: 2 | Val loss: 0.3270 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4811 | Steps: 2 | Val loss: 0.3821 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4526 | Steps: 2 | Val loss: 0.3360 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=14835)[0m rmse: 0.1926819086074829
[2m[36m(func pid=14835)[0m mae:  0.13130275905132294
[2m[36m(func pid=14835)[0m rmse_per_class: [0.235, 0.261, 0.124, 0.373, 0.056, 0.197, 0.271, 0.154, 0.16, 0.096]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17508485913276672
[2m[36m(func pid=15258)[0m mae:  0.11157004535198212
[2m[36m(func pid=15258)[0m rmse_per_class: [0.251, 0.27, 0.049, 0.281, 0.073, 0.217, 0.231, 0.151, 0.133, 0.095]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:06:24 (running for 00:05:30.20)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.452 |  0.179 |                   53 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.545 |  0.193 |                   54 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.454 |  0.175 |                   54 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.481 |  0.164 |                   53 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.17880722880363464
[2m[36m(func pid=14462)[0m mae:  0.12999698519706726
[2m[36m(func pid=14462)[0m rmse_per_class: [0.122, 0.266, 0.129, 0.346, 0.077, 0.188, 0.273, 0.137, 0.154, 0.097]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1637498289346695
[2m[36m(func pid=15692)[0m mae:  0.09996341168880463
[2m[36m(func pid=15692)[0m rmse_per_class: [0.205, 0.224, 0.058, 0.275, 0.055, 0.203, 0.276, 0.107, 0.144, 0.091]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5602 | Steps: 2 | Val loss: 0.3965 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4711 | Steps: 2 | Val loss: 0.3376 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5101 | Steps: 2 | Val loss: 0.4274 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4504 | Steps: 2 | Val loss: 0.3360 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=14835)[0m rmse: 0.1914617419242859
[2m[36m(func pid=14835)[0m mae:  0.13070844113826752
[2m[36m(func pid=14835)[0m rmse_per_class: [0.231, 0.259, 0.121, 0.372, 0.056, 0.196, 0.267, 0.154, 0.163, 0.096]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.18394380807876587
[2m[36m(func pid=15258)[0m mae:  0.11691424995660782
[2m[36m(func pid=15258)[0m rmse_per_class: [0.297, 0.274, 0.049, 0.313, 0.102, 0.202, 0.226, 0.149, 0.133, 0.094]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:06:29 (running for 00:05:35.37)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.453 |  0.179 |                   54 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.56  |  0.191 |                   55 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.471 |  0.184 |                   55 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.51  |  0.172 |                   54 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.17875342071056366
[2m[36m(func pid=14462)[0m mae:  0.12990690767765045
[2m[36m(func pid=14462)[0m rmse_per_class: [0.122, 0.266, 0.13, 0.346, 0.077, 0.188, 0.272, 0.137, 0.154, 0.097]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1717156618833542
[2m[36m(func pid=15692)[0m mae:  0.1048964262008667
[2m[36m(func pid=15692)[0m rmse_per_class: [0.292, 0.224, 0.053, 0.292, 0.054, 0.174, 0.286, 0.116, 0.134, 0.093]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5479 | Steps: 2 | Val loss: 0.3887 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4425 | Steps: 2 | Val loss: 0.3456 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4915 | Steps: 2 | Val loss: 0.4044 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4549 | Steps: 2 | Val loss: 0.3365 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=14835)[0m rmse: 0.18938235938549042
[2m[36m(func pid=14835)[0m mae:  0.12963107228279114
[2m[36m(func pid=14835)[0m rmse_per_class: [0.223, 0.256, 0.113, 0.371, 0.056, 0.194, 0.264, 0.154, 0.167, 0.095]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.1899900734424591
[2m[36m(func pid=15258)[0m mae:  0.12082089483737946
[2m[36m(func pid=15258)[0m rmse_per_class: [0.312, 0.276, 0.049, 0.338, 0.145, 0.183, 0.226, 0.146, 0.133, 0.092]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:06:34 (running for 00:05:40.46)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.45  |  0.179 |                   55 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.548 |  0.189 |                   56 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.443 |  0.19  |                   56 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.492 |  0.173 |                   55 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15692)[0m rmse: 0.17302358150482178
[2m[36m(func pid=15692)[0m mae:  0.10084059089422226
[2m[36m(func pid=15692)[0m rmse_per_class: [0.356, 0.215, 0.069, 0.275, 0.052, 0.163, 0.255, 0.118, 0.135, 0.092]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17874005436897278
[2m[36m(func pid=14462)[0m mae:  0.1298338770866394
[2m[36m(func pid=14462)[0m rmse_per_class: [0.122, 0.266, 0.13, 0.346, 0.076, 0.188, 0.272, 0.137, 0.154, 0.096]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5409 | Steps: 2 | Val loss: 0.3830 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4660 | Steps: 2 | Val loss: 0.3506 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4622 | Steps: 2 | Val loss: 0.4148 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4555 | Steps: 2 | Val loss: 0.3362 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=14835)[0m rmse: 0.18761277198791504
[2m[36m(func pid=14835)[0m mae:  0.1287701278924942
[2m[36m(func pid=14835)[0m rmse_per_class: [0.215, 0.254, 0.109, 0.369, 0.056, 0.193, 0.261, 0.154, 0.171, 0.095]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:06:39 (running for 00:05:45.47)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.455 |  0.179 |                   56 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.541 |  0.188 |                   57 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.466 |  0.193 |                   57 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.492 |  0.173 |                   55 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.19323796033859253
[2m[36m(func pid=15258)[0m mae:  0.12328241765499115
[2m[36m(func pid=15258)[0m rmse_per_class: [0.29, 0.275, 0.05, 0.353, 0.2, 0.172, 0.228, 0.139, 0.136, 0.09]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1787186861038208
[2m[36m(func pid=15692)[0m mae:  0.09958989918231964
[2m[36m(func pid=15692)[0m rmse_per_class: [0.299, 0.252, 0.088, 0.319, 0.05, 0.158, 0.28, 0.118, 0.131, 0.092]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17865267395973206
[2m[36m(func pid=14462)[0m mae:  0.1297435462474823
[2m[36m(func pid=14462)[0m rmse_per_class: [0.122, 0.266, 0.13, 0.346, 0.075, 0.188, 0.272, 0.137, 0.154, 0.096]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5387 | Steps: 2 | Val loss: 0.3750 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4305 | Steps: 2 | Val loss: 0.3592 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4752 | Steps: 2 | Val loss: 0.4403 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4595 | Steps: 2 | Val loss: 0.3374 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=14835)[0m rmse: 0.18523280322551727
[2m[36m(func pid=14835)[0m mae:  0.1275515854358673
[2m[36m(func pid=14835)[0m rmse_per_class: [0.203, 0.251, 0.103, 0.367, 0.056, 0.191, 0.257, 0.154, 0.176, 0.095]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:06:44 (running for 00:05:50.61)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.455 |  0.179 |                   57 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.539 |  0.185 |                   58 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.431 |  0.195 |                   58 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.462 |  0.179 |                   56 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.19472405314445496
[2m[36m(func pid=15258)[0m mae:  0.12571166455745697
[2m[36m(func pid=15258)[0m rmse_per_class: [0.239, 0.272, 0.05, 0.362, 0.253, 0.169, 0.233, 0.13, 0.152, 0.087]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18083350360393524
[2m[36m(func pid=15692)[0m mae:  0.10733406245708466
[2m[36m(func pid=15692)[0m rmse_per_class: [0.115, 0.288, 0.069, 0.373, 0.05, 0.17, 0.389, 0.131, 0.133, 0.091]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.1786555051803589
[2m[36m(func pid=14462)[0m mae:  0.12966004014015198
[2m[36m(func pid=14462)[0m rmse_per_class: [0.123, 0.266, 0.131, 0.346, 0.075, 0.188, 0.271, 0.137, 0.154, 0.096]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5155 | Steps: 2 | Val loss: 0.3675 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4084 | Steps: 2 | Val loss: 0.3715 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4827 | Steps: 2 | Val loss: 0.4543 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4595 | Steps: 2 | Val loss: 0.3387 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=14835)[0m rmse: 0.18286621570587158
[2m[36m(func pid=14835)[0m mae:  0.12635096907615662
[2m[36m(func pid=14835)[0m rmse_per_class: [0.19, 0.249, 0.097, 0.364, 0.056, 0.188, 0.254, 0.154, 0.181, 0.095]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:06:49 (running for 00:05:55.69)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.46  |  0.179 |                   58 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.516 |  0.183 |                   59 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.408 |  0.197 |                   59 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.475 |  0.181 |                   57 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.19683481752872467
[2m[36m(func pid=15258)[0m mae:  0.12911534309387207
[2m[36m(func pid=15258)[0m rmse_per_class: [0.169, 0.268, 0.051, 0.368, 0.287, 0.177, 0.239, 0.117, 0.206, 0.087]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1789853870868683
[2m[36m(func pid=15692)[0m mae:  0.10910667479038239
[2m[36m(func pid=15692)[0m rmse_per_class: [0.101, 0.273, 0.052, 0.383, 0.053, 0.16, 0.377, 0.135, 0.165, 0.09]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17866024374961853
[2m[36m(func pid=14462)[0m mae:  0.1295779049396515
[2m[36m(func pid=14462)[0m rmse_per_class: [0.123, 0.266, 0.132, 0.347, 0.074, 0.188, 0.271, 0.137, 0.154, 0.096]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5101 | Steps: 2 | Val loss: 0.3613 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4026 | Steps: 2 | Val loss: 0.3856 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4757 | Steps: 2 | Val loss: 0.4420 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4627 | Steps: 2 | Val loss: 0.3398 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=14835)[0m rmse: 0.1807907521724701
[2m[36m(func pid=14835)[0m mae:  0.12535656988620758
[2m[36m(func pid=14835)[0m rmse_per_class: [0.178, 0.247, 0.093, 0.362, 0.056, 0.186, 0.25, 0.153, 0.187, 0.095]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:06:54 (running for 00:06:00.73)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.459 |  0.179 |                   59 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.51  |  0.181 |                   60 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.403 |  0.202 |                   60 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.483 |  0.179 |                   58 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.20214934647083282
[2m[36m(func pid=15258)[0m mae:  0.133795365691185
[2m[36m(func pid=15258)[0m rmse_per_class: [0.118, 0.262, 0.051, 0.373, 0.297, 0.188, 0.25, 0.105, 0.282, 0.095]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18202751874923706
[2m[36m(func pid=15692)[0m mae:  0.10971721261739731
[2m[36m(func pid=15692)[0m rmse_per_class: [0.103, 0.286, 0.051, 0.381, 0.073, 0.181, 0.238, 0.133, 0.286, 0.089]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17866891622543335
[2m[36m(func pid=14462)[0m mae:  0.12950582802295685
[2m[36m(func pid=14462)[0m rmse_per_class: [0.123, 0.266, 0.132, 0.347, 0.073, 0.188, 0.27, 0.137, 0.154, 0.096]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.5028 | Steps: 2 | Val loss: 0.3544 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4233 | Steps: 2 | Val loss: 0.3994 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4922 | Steps: 2 | Val loss: 0.4208 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4651 | Steps: 2 | Val loss: 0.3402 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=14835)[0m rmse: 0.17850802838802338
[2m[36m(func pid=14835)[0m mae:  0.12422072887420654
[2m[36m(func pid=14835)[0m rmse_per_class: [0.163, 0.246, 0.087, 0.359, 0.056, 0.184, 0.248, 0.153, 0.193, 0.095]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.207384392619133
[2m[36m(func pid=15258)[0m mae:  0.13807933032512665
[2m[36m(func pid=15258)[0m rmse_per_class: [0.101, 0.254, 0.051, 0.376, 0.287, 0.199, 0.269, 0.1, 0.321, 0.117]
== Status ==
Current time: 2024-01-07 05:06:59 (running for 00:06:05.80)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.463 |  0.179 |                   60 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.503 |  0.179 |                   61 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.423 |  0.207 |                   61 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.476 |  0.182 |                   59 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1938566267490387
[2m[36m(func pid=15692)[0m mae:  0.11876745522022247
[2m[36m(func pid=15692)[0m rmse_per_class: [0.098, 0.253, 0.051, 0.361, 0.11, 0.311, 0.275, 0.128, 0.266, 0.086]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17863276600837708
[2m[36m(func pid=14462)[0m mae:  0.12942558526992798
[2m[36m(func pid=14462)[0m rmse_per_class: [0.124, 0.266, 0.132, 0.348, 0.072, 0.188, 0.27, 0.137, 0.155, 0.095]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4948 | Steps: 2 | Val loss: 0.3478 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4206 | Steps: 2 | Val loss: 0.3971 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4257 | Steps: 2 | Val loss: 0.4096 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4635 | Steps: 2 | Val loss: 0.3417 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=14835)[0m rmse: 0.17634239792823792
[2m[36m(func pid=14835)[0m mae:  0.12315791845321655
[2m[36m(func pid=14835)[0m rmse_per_class: [0.149, 0.246, 0.081, 0.355, 0.056, 0.182, 0.246, 0.153, 0.199, 0.095]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:07:04 (running for 00:06:10.80)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.465 |  0.179 |                   61 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.495 |  0.176 |                   62 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.423 |  0.207 |                   61 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.421 |  0.189 |                   61 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15692)[0m rmse: 0.18912415206432343
[2m[36m(func pid=15692)[0m mae:  0.11348821967840195
[2m[36m(func pid=15692)[0m rmse_per_class: [0.194, 0.206, 0.046, 0.299, 0.152, 0.317, 0.302, 0.119, 0.141, 0.115]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.2095886766910553
[2m[36m(func pid=15258)[0m mae:  0.14041635394096375
[2m[36m(func pid=15258)[0m rmse_per_class: [0.099, 0.244, 0.052, 0.378, 0.258, 0.207, 0.285, 0.109, 0.31, 0.154]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17867040634155273
[2m[36m(func pid=14462)[0m mae:  0.12937228381633759
[2m[36m(func pid=14462)[0m rmse_per_class: [0.124, 0.266, 0.133, 0.348, 0.072, 0.188, 0.269, 0.137, 0.155, 0.095]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4943 | Steps: 2 | Val loss: 0.3418 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4269 | Steps: 2 | Val loss: 0.4145 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4060 | Steps: 2 | Val loss: 0.4248 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4668 | Steps: 2 | Val loss: 0.3422 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=14835)[0m rmse: 0.1744132936000824
[2m[36m(func pid=14835)[0m mae:  0.12222901731729507
[2m[36m(func pid=14835)[0m rmse_per_class: [0.135, 0.247, 0.075, 0.351, 0.056, 0.18, 0.246, 0.153, 0.205, 0.095]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:07:09 (running for 00:06:15.96)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.463 |  0.179 |                   62 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.494 |  0.174 |                   63 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.427 |  0.209 |                   63 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.421 |  0.189 |                   61 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.2086396962404251
[2m[36m(func pid=15258)[0m mae:  0.1406390219926834
[2m[36m(func pid=15258)[0m rmse_per_class: [0.099, 0.233, 0.057, 0.378, 0.222, 0.212, 0.297, 0.129, 0.263, 0.198]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18281987309455872
[2m[36m(func pid=15692)[0m mae:  0.10697650909423828
[2m[36m(func pid=15692)[0m rmse_per_class: [0.208, 0.221, 0.043, 0.281, 0.194, 0.182, 0.287, 0.114, 0.134, 0.163]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17862549424171448
[2m[36m(func pid=14462)[0m mae:  0.12929269671440125
[2m[36m(func pid=14462)[0m rmse_per_class: [0.124, 0.266, 0.133, 0.348, 0.071, 0.188, 0.269, 0.137, 0.155, 0.095]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4823 | Steps: 2 | Val loss: 0.3352 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4544 | Steps: 2 | Val loss: 0.4165 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4401 | Steps: 2 | Val loss: 0.4143 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4673 | Steps: 2 | Val loss: 0.3424 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=14835)[0m rmse: 0.1723257303237915
[2m[36m(func pid=14835)[0m mae:  0.12112487852573395
[2m[36m(func pid=14835)[0m rmse_per_class: [0.122, 0.249, 0.068, 0.347, 0.056, 0.178, 0.246, 0.153, 0.209, 0.095]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18105857074260712
[2m[36m(func pid=15692)[0m mae:  0.10340024530887604
[2m[36m(func pid=15692)[0m rmse_per_class: [0.168, 0.238, 0.043, 0.312, 0.229, 0.179, 0.245, 0.126, 0.137, 0.134]
[2m[36m(func pid=15692)[0m 
== Status ==
Current time: 2024-01-07 05:07:15 (running for 00:06:21.27)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.467 |  0.179 |                   63 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.482 |  0.172 |                   64 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.44  |  0.205 |                   64 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.454 |  0.181 |                   63 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.20547504723072052
[2m[36m(func pid=15258)[0m mae:  0.13949470221996307
[2m[36m(func pid=15258)[0m rmse_per_class: [0.099, 0.226, 0.059, 0.377, 0.186, 0.214, 0.304, 0.15, 0.202, 0.236]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17856010794639587
[2m[36m(func pid=14462)[0m mae:  0.12920108437538147
[2m[36m(func pid=14462)[0m rmse_per_class: [0.124, 0.266, 0.133, 0.348, 0.071, 0.188, 0.268, 0.137, 0.155, 0.095]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4793 | Steps: 2 | Val loss: 0.3313 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4351 | Steps: 2 | Val loss: 0.3975 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4388 | Steps: 2 | Val loss: 0.4075 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4704 | Steps: 2 | Val loss: 0.3429 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=14835)[0m rmse: 0.17111429572105408
[2m[36m(func pid=14835)[0m mae:  0.12053493410348892
[2m[36m(func pid=14835)[0m rmse_per_class: [0.112, 0.252, 0.064, 0.342, 0.056, 0.177, 0.248, 0.152, 0.213, 0.094]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1797962486743927
[2m[36m(func pid=15692)[0m mae:  0.10159169137477875
[2m[36m(func pid=15692)[0m rmse_per_class: [0.119, 0.234, 0.058, 0.293, 0.244, 0.203, 0.264, 0.15, 0.136, 0.098]
[2m[36m(func pid=15692)[0m 
== Status ==
Current time: 2024-01-07 05:07:20 (running for 00:06:26.43)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.467 |  0.179 |                   64 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.479 |  0.171 |                   65 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.439 |  0.201 |                   65 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.435 |  0.18  |                   64 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.2013894021511078
[2m[36m(func pid=15258)[0m mae:  0.13700853288173676
[2m[36m(func pid=15258)[0m rmse_per_class: [0.099, 0.231, 0.06, 0.375, 0.15, 0.215, 0.306, 0.172, 0.159, 0.246]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.1784876585006714
[2m[36m(func pid=14462)[0m mae:  0.12908941507339478
[2m[36m(func pid=14462)[0m rmse_per_class: [0.124, 0.266, 0.133, 0.348, 0.07, 0.188, 0.268, 0.138, 0.155, 0.095]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4700 | Steps: 2 | Val loss: 0.3273 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4359 | Steps: 2 | Val loss: 0.3981 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4401 | Steps: 2 | Val loss: 0.3934 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=14835)[0m rmse: 0.16994228959083557
[2m[36m(func pid=14835)[0m mae:  0.11984950304031372
[2m[36m(func pid=14835)[0m rmse_per_class: [0.104, 0.257, 0.059, 0.337, 0.056, 0.176, 0.251, 0.152, 0.214, 0.094]
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4728 | Steps: 2 | Val loss: 0.3442 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18622319400310516
[2m[36m(func pid=15692)[0m mae:  0.10475800931453705
[2m[36m(func pid=15692)[0m rmse_per_class: [0.109, 0.21, 0.079, 0.293, 0.25, 0.203, 0.292, 0.207, 0.132, 0.089]
[2m[36m(func pid=15692)[0m 
== Status ==
Current time: 2024-01-07 05:07:25 (running for 00:06:31.50)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.47  |  0.178 |                   65 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.47  |  0.17  |                   66 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.44  |  0.197 |                   66 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.436 |  0.186 |                   65 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.19703011214733124
[2m[36m(func pid=15258)[0m mae:  0.13351401686668396
[2m[36m(func pid=15258)[0m rmse_per_class: [0.097, 0.251, 0.058, 0.37, 0.122, 0.215, 0.305, 0.187, 0.138, 0.226]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17851297557353973
[2m[36m(func pid=14462)[0m mae:  0.1290242224931717
[2m[36m(func pid=14462)[0m rmse_per_class: [0.125, 0.266, 0.134, 0.349, 0.069, 0.188, 0.268, 0.138, 0.155, 0.095]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4616 | Steps: 2 | Val loss: 0.3245 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4138 | Steps: 2 | Val loss: 0.4096 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4449 | Steps: 2 | Val loss: 0.3727 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4704 | Steps: 2 | Val loss: 0.3455 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=14835)[0m rmse: 0.16918697953224182
[2m[36m(func pid=14835)[0m mae:  0.11931638419628143
[2m[36m(func pid=14835)[0m rmse_per_class: [0.099, 0.261, 0.055, 0.332, 0.056, 0.175, 0.255, 0.152, 0.214, 0.094]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.19674795866012573
[2m[36m(func pid=15692)[0m mae:  0.11202790588140488
[2m[36m(func pid=15692)[0m rmse_per_class: [0.098, 0.282, 0.069, 0.328, 0.22, 0.179, 0.263, 0.224, 0.215, 0.089]
[2m[36m(func pid=15692)[0m 
== Status ==
Current time: 2024-01-07 05:07:30 (running for 00:06:36.72)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.473 |  0.179 |                   66 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.462 |  0.169 |                   67 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.445 |  0.192 |                   67 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.414 |  0.197 |                   66 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.19157971441745758
[2m[36m(func pid=15258)[0m mae:  0.12862274050712585
[2m[36m(func pid=15258)[0m rmse_per_class: [0.097, 0.275, 0.059, 0.36, 0.098, 0.213, 0.301, 0.193, 0.133, 0.187]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17851504683494568
[2m[36m(func pid=14462)[0m mae:  0.12894302606582642
[2m[36m(func pid=14462)[0m rmse_per_class: [0.125, 0.266, 0.135, 0.349, 0.069, 0.188, 0.267, 0.138, 0.155, 0.094]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4681 | Steps: 2 | Val loss: 0.3223 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4579 | Steps: 2 | Val loss: 0.4232 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4147 | Steps: 2 | Val loss: 0.3469 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4719 | Steps: 2 | Val loss: 0.3473 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=14835)[0m rmse: 0.16860303282737732
[2m[36m(func pid=14835)[0m mae:  0.11885029077529907
[2m[36m(func pid=14835)[0m rmse_per_class: [0.097, 0.265, 0.051, 0.326, 0.056, 0.174, 0.259, 0.151, 0.213, 0.094]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.1991942673921585
[2m[36m(func pid=15692)[0m mae:  0.11802603304386139
[2m[36m(func pid=15692)[0m rmse_per_class: [0.096, 0.313, 0.047, 0.358, 0.157, 0.169, 0.287, 0.158, 0.316, 0.09]
[2m[36m(func pid=15692)[0m 
== Status ==
Current time: 2024-01-07 05:07:35 (running for 00:06:41.81)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.47  |  0.179 |                   67 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.468 |  0.169 |                   68 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.415 |  0.184 |                   68 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.458 |  0.199 |                   67 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.18401677906513214
[2m[36m(func pid=15258)[0m mae:  0.12213137000799179
[2m[36m(func pid=15258)[0m rmse_per_class: [0.105, 0.285, 0.06, 0.345, 0.079, 0.209, 0.291, 0.192, 0.131, 0.144]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17858974635601044
[2m[36m(func pid=14462)[0m mae:  0.1289088875055313
[2m[36m(func pid=14462)[0m rmse_per_class: [0.126, 0.266, 0.136, 0.349, 0.068, 0.188, 0.267, 0.138, 0.155, 0.094]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4661 | Steps: 2 | Val loss: 0.3208 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4298 | Steps: 2 | Val loss: 0.4218 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4755 | Steps: 2 | Val loss: 0.3478 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3972 | Steps: 2 | Val loss: 0.3202 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=14835)[0m rmse: 0.16813386976718903
[2m[36m(func pid=14835)[0m mae:  0.1183987408876419
[2m[36m(func pid=14835)[0m rmse_per_class: [0.096, 0.268, 0.048, 0.321, 0.056, 0.174, 0.264, 0.151, 0.21, 0.093]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.18698982894420624
[2m[36m(func pid=15692)[0m mae:  0.11481499671936035
[2m[36m(func pid=15692)[0m rmse_per_class: [0.101, 0.281, 0.042, 0.362, 0.101, 0.207, 0.284, 0.11, 0.291, 0.091]
[2m[36m(func pid=15692)[0m 
== Status ==
Current time: 2024-01-07 05:07:40 (running for 00:06:46.91)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.472 |  0.179 |                   68 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.466 |  0.168 |                   69 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.397 |  0.174 |                   69 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.43  |  0.187 |                   68 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.1742282509803772
[2m[36m(func pid=15258)[0m mae:  0.11442909389734268
[2m[36m(func pid=15258)[0m rmse_per_class: [0.132, 0.266, 0.057, 0.322, 0.065, 0.202, 0.279, 0.176, 0.131, 0.111]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14462)[0m rmse: 0.17857341468334198
[2m[36m(func pid=14462)[0m mae:  0.1288459748029709
[2m[36m(func pid=14462)[0m rmse_per_class: [0.126, 0.266, 0.136, 0.35, 0.067, 0.188, 0.267, 0.138, 0.155, 0.094]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4588 | Steps: 2 | Val loss: 0.3190 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4159 | Steps: 2 | Val loss: 0.3772 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3849 | Steps: 2 | Val loss: 0.2988 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4811 | Steps: 2 | Val loss: 0.3487 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=14835)[0m rmse: 0.16746047139167786
[2m[36m(func pid=14835)[0m mae:  0.11779864132404327
[2m[36m(func pid=14835)[0m rmse_per_class: [0.096, 0.271, 0.046, 0.315, 0.056, 0.174, 0.269, 0.15, 0.204, 0.093]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15692)[0m rmse: 0.16976803541183472
[2m[36m(func pid=15692)[0m mae:  0.10153909027576447
[2m[36m(func pid=15692)[0m rmse_per_class: [0.177, 0.233, 0.046, 0.341, 0.074, 0.182, 0.239, 0.108, 0.206, 0.093]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.16454623639583588
[2m[36m(func pid=15258)[0m mae:  0.10687203705310822
[2m[36m(func pid=15258)[0m rmse_per_class: [0.176, 0.227, 0.053, 0.295, 0.057, 0.195, 0.264, 0.154, 0.132, 0.093]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:07:46 (running for 00:06:52.34)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.179 |                   70 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.459 |  0.167 |                   70 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.385 |  0.165 |                   70 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.416 |  0.17  |                   69 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.17857132852077484
[2m[36m(func pid=14462)[0m mae:  0.1287793070077896
[2m[36m(func pid=14462)[0m rmse_per_class: [0.126, 0.266, 0.136, 0.35, 0.067, 0.188, 0.266, 0.138, 0.155, 0.094]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4648 | Steps: 2 | Val loss: 0.3177 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4371 | Steps: 2 | Val loss: 0.3584 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3504 | Steps: 2 | Val loss: 0.2882 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=14835)[0m rmse: 0.16682347655296326
[2m[36m(func pid=14835)[0m mae:  0.11722391843795776
[2m[36m(func pid=14835)[0m rmse_per_class: [0.097, 0.272, 0.044, 0.31, 0.056, 0.175, 0.274, 0.149, 0.198, 0.093]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4786 | Steps: 2 | Val loss: 0.3492 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=15692)[0m rmse: 0.15926013886928558
[2m[36m(func pid=15692)[0m mae:  0.09268645942211151
[2m[36m(func pid=15692)[0m rmse_per_class: [0.229, 0.195, 0.043, 0.293, 0.06, 0.153, 0.27, 0.116, 0.136, 0.096]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.15781983733177185
[2m[36m(func pid=15258)[0m mae:  0.10184957087039948
[2m[36m(func pid=15258)[0m rmse_per_class: [0.213, 0.206, 0.05, 0.277, 0.052, 0.185, 0.252, 0.123, 0.133, 0.087]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:07:51 (running for 00:06:57.60)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.479 |  0.179 |                   71 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.465 |  0.167 |                   71 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.35  |  0.158 |                   71 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.437 |  0.159 |                   70 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.17855016887187958
[2m[36m(func pid=14462)[0m mae:  0.12872254848480225
[2m[36m(func pid=14462)[0m rmse_per_class: [0.127, 0.266, 0.136, 0.35, 0.066, 0.188, 0.266, 0.138, 0.155, 0.094]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4667 | Steps: 2 | Val loss: 0.3163 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4172 | Steps: 2 | Val loss: 0.3594 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3636 | Steps: 2 | Val loss: 0.2867 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=14835)[0m rmse: 0.16604375839233398
[2m[36m(func pid=14835)[0m mae:  0.11653146892786026
[2m[36m(func pid=14835)[0m rmse_per_class: [0.098, 0.272, 0.043, 0.305, 0.056, 0.176, 0.278, 0.148, 0.191, 0.092]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4792 | Steps: 2 | Val loss: 0.3497 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=15692)[0m rmse: 0.15838398039340973
[2m[36m(func pid=15692)[0m mae:  0.09137843549251556
[2m[36m(func pid=15692)[0m rmse_per_class: [0.187, 0.217, 0.04, 0.338, 0.056, 0.151, 0.237, 0.121, 0.134, 0.101]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.1548057347536087
[2m[36m(func pid=15258)[0m mae:  0.09948916733264923
[2m[36m(func pid=15258)[0m rmse_per_class: [0.223, 0.21, 0.046, 0.276, 0.051, 0.175, 0.243, 0.104, 0.133, 0.086]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4624 | Steps: 2 | Val loss: 0.3147 | Batch size: 32 | lr: 0.001 | Duration: 2.62s
== Status ==
Current time: 2024-01-07 05:07:56 (running for 00:07:02.86)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.479 |  0.178 |                   72 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.467 |  0.166 |                   72 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.364 |  0.155 |                   72 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.417 |  0.158 |                   71 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.178484708070755
[2m[36m(func pid=14462)[0m mae:  0.1286146342754364
[2m[36m(func pid=14462)[0m rmse_per_class: [0.127, 0.266, 0.136, 0.35, 0.066, 0.188, 0.266, 0.138, 0.155, 0.094]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4276 | Steps: 2 | Val loss: 0.3936 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=14835)[0m rmse: 0.16511264443397522
[2m[36m(func pid=14835)[0m mae:  0.11574549973011017
[2m[36m(func pid=14835)[0m rmse_per_class: [0.099, 0.27, 0.043, 0.301, 0.056, 0.178, 0.282, 0.147, 0.183, 0.092]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3764 | Steps: 2 | Val loss: 0.2889 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4787 | Steps: 2 | Val loss: 0.3508 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=15692)[0m rmse: 0.17409946024417877
[2m[36m(func pid=15692)[0m mae:  0.10139551013708115
[2m[36m(func pid=15692)[0m rmse_per_class: [0.105, 0.235, 0.093, 0.464, 0.057, 0.153, 0.258, 0.124, 0.136, 0.115]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.15419289469718933
[2m[36m(func pid=15258)[0m mae:  0.09848509728908539
[2m[36m(func pid=15258)[0m rmse_per_class: [0.211, 0.223, 0.044, 0.285, 0.052, 0.166, 0.237, 0.104, 0.133, 0.088]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4593 | Steps: 2 | Val loss: 0.3135 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
[2m[36m(func pid=14462)[0m rmse: 0.17849501967430115== Status ==
Current time: 2024-01-07 05:08:01 (running for 00:07:08.03)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.479 |  0.178 |                   73 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.462 |  0.165 |                   73 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.376 |  0.154 |                   73 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.428 |  0.174 |                   72 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=14462)[0m mae:  0.12855012714862823
[2m[36m(func pid=14462)[0m rmse_per_class: [0.128, 0.266, 0.136, 0.35, 0.065, 0.188, 0.266, 0.139, 0.155, 0.094]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4701 | Steps: 2 | Val loss: 0.3853 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=14835)[0m rmse: 0.16431382298469543
[2m[36m(func pid=14835)[0m mae:  0.11507076025009155
[2m[36m(func pid=14835)[0m rmse_per_class: [0.1, 0.268, 0.043, 0.298, 0.056, 0.179, 0.286, 0.146, 0.176, 0.092]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3858 | Steps: 2 | Val loss: 0.2884 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4802 | Steps: 2 | Val loss: 0.3514 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=15692)[0m rmse: 0.1779761165380478
[2m[36m(func pid=15692)[0m mae:  0.10071267187595367
[2m[36m(func pid=15692)[0m rmse_per_class: [0.101, 0.228, 0.127, 0.426, 0.056, 0.179, 0.267, 0.118, 0.135, 0.143]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.15316614508628845
[2m[36m(func pid=15258)[0m mae:  0.09726575016975403
[2m[36m(func pid=15258)[0m rmse_per_class: [0.185, 0.234, 0.043, 0.291, 0.052, 0.159, 0.233, 0.112, 0.133, 0.089]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4768 | Steps: 2 | Val loss: 0.3116 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 05:08:07 (running for 00:07:13.15)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | RUNNING  | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.48  |  0.178 |                   74 |
| train_5a6ec_00001 | RUNNING  | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.459 |  0.164 |                   74 |
| train_5a6ec_00002 | RUNNING  | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.386 |  0.153 |                   74 |
| train_5a6ec_00003 | RUNNING  | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.47  |  0.178 |                   73 |
| train_5a6ec_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.17846737802028656
[2m[36m(func pid=14462)[0m mae:  0.12847523391246796
[2m[36m(func pid=14462)[0m rmse_per_class: [0.128, 0.266, 0.136, 0.35, 0.065, 0.188, 0.265, 0.139, 0.155, 0.093]
[2m[36m(func pid=14462)[0m 
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4639 | Steps: 2 | Val loss: 0.3683 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=14835)[0m rmse: 0.16331009566783905
[2m[36m(func pid=14835)[0m mae:  0.11419065296649933
[2m[36m(func pid=14835)[0m rmse_per_class: [0.101, 0.264, 0.043, 0.296, 0.056, 0.182, 0.289, 0.145, 0.167, 0.091]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3949 | Steps: 2 | Val loss: 0.2854 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=14462)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4814 | Steps: 2 | Val loss: 0.3522 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=15692)[0m rmse: 0.1698543280363083
[2m[36m(func pid=15692)[0m mae:  0.09539137780666351
[2m[36m(func pid=15692)[0m rmse_per_class: [0.103, 0.211, 0.134, 0.305, 0.054, 0.212, 0.246, 0.111, 0.145, 0.176]
[2m[36m(func pid=15692)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.15180668234825134
[2m[36m(func pid=15258)[0m mae:  0.09609085321426392
[2m[36m(func pid=15258)[0m rmse_per_class: [0.155, 0.241, 0.044, 0.29, 0.053, 0.161, 0.232, 0.121, 0.132, 0.09]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.4578 | Steps: 2 | Val loss: 0.3103 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 05:08:12 (running for 00:07:18.31)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: -0.1574999988079071
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 3 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.477 |  0.163 |                   75 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.395 |  0.152 |                   75 |
| train_5a6ec_00003 | RUNNING    | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.464 |  0.17  |                   74 |
| train_5a6ec_00004 | PENDING    |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14462)[0m rmse: 0.1784875988960266
[2m[36m(func pid=14462)[0m mae:  0.12842823565006256
[2m[36m(func pid=14462)[0m rmse_per_class: [0.128, 0.266, 0.137, 0.351, 0.064, 0.188, 0.265, 0.139, 0.155, 0.093]
[2m[36m(func pid=15692)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4180 | Steps: 2 | Val loss: 0.3765 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=14835)[0m rmse: 0.1625995934009552
[2m[36m(func pid=14835)[0m mae:  0.11355254799127579
[2m[36m(func pid=14835)[0m rmse_per_class: [0.102, 0.26, 0.043, 0.294, 0.056, 0.184, 0.291, 0.143, 0.16, 0.091]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3906 | Steps: 2 | Val loss: 0.2828 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=15692)[0m rmse: 0.17333891987800598
[2m[36m(func pid=15692)[0m mae:  0.10280364751815796
[2m[36m(func pid=15692)[0m rmse_per_class: [0.105, 0.23, 0.085, 0.338, 0.052, 0.193, 0.247, 0.105, 0.212, 0.167]
[2m[36m(func pid=15258)[0m rmse: 0.15109165012836456
[2m[36m(func pid=15258)[0m mae:  0.0956730991601944
[2m[36m(func pid=15258)[0m rmse_per_class: [0.129, 0.243, 0.047, 0.278, 0.054, 0.179, 0.232, 0.127, 0.131, 0.091]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.4611 | Steps: 2 | Val loss: 0.3088 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=14835)[0m rmse: 0.1619771271944046
[2m[36m(func pid=14835)[0m mae:  0.11293765157461166
[2m[36m(func pid=14835)[0m rmse_per_class: [0.102, 0.257, 0.044, 0.294, 0.056, 0.187, 0.293, 0.141, 0.155, 0.092]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.4073 | Steps: 2 | Val loss: 0.2853 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=32174)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=32174)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=32174)[0m Configuration completed!
[2m[36m(func pid=32174)[0m New optimizer parameters:
[2m[36m(func pid=32174)[0m SGD (
[2m[36m(func pid=32174)[0m Parameter Group 0
[2m[36m(func pid=32174)[0m     dampening: 0
[2m[36m(func pid=32174)[0m     differentiable: False
[2m[36m(func pid=32174)[0m     foreach: None
[2m[36m(func pid=32174)[0m     lr: 0.0001
[2m[36m(func pid=32174)[0m     maximize: False
[2m[36m(func pid=32174)[0m     momentum: 0.9
[2m[36m(func pid=32174)[0m     nesterov: False
[2m[36m(func pid=32174)[0m     weight_decay: 0
[2m[36m(func pid=32174)[0m )
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.1528228223323822
[2m[36m(func pid=15258)[0m mae:  0.09751163423061371
[2m[36m(func pid=15258)[0m rmse_per_class: [0.112, 0.241, 0.05, 0.265, 0.054, 0.214, 0.236, 0.132, 0.134, 0.091]
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.4510 | Steps: 2 | Val loss: 0.3075 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 05:08:19 (running for 00:07:25.40)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.461 |  0.162 |                   77 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.391 |  0.151 |                   76 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=32277)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32277)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=32277)[0m Configuration completed!
[2m[36m(func pid=32277)[0m New optimizer parameters:
[2m[36m(func pid=32277)[0m SGD (
[2m[36m(func pid=32277)[0m Parameter Group 0
[2m[36m(func pid=32277)[0m     dampening: 0
[2m[36m(func pid=32277)[0m     differentiable: False
[2m[36m(func pid=32277)[0m     foreach: None
[2m[36m(func pid=32277)[0m     lr: 0.001
[2m[36m(func pid=32277)[0m     maximize: False
[2m[36m(func pid=32277)[0m     momentum: 0.9
[2m[36m(func pid=32277)[0m     nesterov: False
[2m[36m(func pid=32277)[0m     weight_decay: 0
[2m[36m(func pid=32277)[0m )
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.16148214042186737
[2m[36m(func pid=14835)[0m mae:  0.1124357357621193
[2m[36m(func pid=14835)[0m rmse_per_class: [0.102, 0.253, 0.044, 0.294, 0.056, 0.189, 0.295, 0.139, 0.15, 0.092]
== Status ==
Current time: 2024-01-07 05:08:24 (running for 00:07:30.65)
Memory usage on this node: 23.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.451 |  0.161 |                   78 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.407 |  0.153 |                   77 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |        |        |                      |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0432 | Steps: 2 | Val loss: 0.7189 | Batch size: 32 | lr: 0.0001 | Duration: 4.35s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3922 | Steps: 2 | Val loss: 0.2941 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.4609 | Steps: 2 | Val loss: 0.3061 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0392 | Steps: 2 | Val loss: 0.7130 | Batch size: 32 | lr: 0.001 | Duration: 4.15s
[2m[36m(func pid=32174)[0m rmse: 0.1798747330904007
[2m[36m(func pid=32174)[0m mae:  0.13239610195159912
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.263, 0.097, 0.333, 0.1, 0.191, 0.298, 0.143, 0.141, 0.119]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.15818564593791962
[2m[36m(func pid=15258)[0m mae:  0.10223956406116486
[2m[36m(func pid=15258)[0m rmse_per_class: [0.102, 0.234, 0.052, 0.268, 0.054, 0.249, 0.241, 0.136, 0.156, 0.091]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:08:29 (running for 00:07:35.70)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.461 |  0.161 |                   79 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.392 |  0.158 |                   78 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  1.043 |  0.18  |                    1 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |        |        |                      |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.16111522912979126
[2m[36m(func pid=14835)[0m mae:  0.11201866716146469
[2m[36m(func pid=14835)[0m rmse_per_class: [0.102, 0.251, 0.044, 0.294, 0.056, 0.191, 0.297, 0.137, 0.146, 0.094]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.1799112856388092
[2m[36m(func pid=32277)[0m mae:  0.1324082911014557
[2m[36m(func pid=32277)[0m rmse_per_class: [0.114, 0.263, 0.097, 0.333, 0.1, 0.191, 0.298, 0.143, 0.141, 0.119]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0426 | Steps: 2 | Val loss: 0.7189 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3899 | Steps: 2 | Val loss: 0.3064 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.4476 | Steps: 2 | Val loss: 0.3048 | Batch size: 32 | lr: 0.001 | Duration: 2.60s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0120 | Steps: 2 | Val loss: 0.7047 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=32174)[0m rmse: 0.18037715554237366
[2m[36m(func pid=32174)[0m mae:  0.13284704089164734
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.16716347634792328
[2m[36m(func pid=15258)[0m mae:  0.10843701660633087
[2m[36m(func pid=15258)[0m rmse_per_class: [0.095, 0.225, 0.053, 0.288, 0.054, 0.272, 0.248, 0.138, 0.208, 0.091]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.16090449690818787
[2m[36m(func pid=14835)[0m mae:  0.11173232644796371
[2m[36m(func pid=14835)[0m rmse_per_class: [0.102, 0.248, 0.045, 0.294, 0.056, 0.193, 0.297, 0.135, 0.143, 0.096]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:08:35 (running for 00:07:41.43)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.448 |  0.161 |                   80 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.39  |  0.167 |                   79 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  1.043 |  0.18  |                    2 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  1.012 |  0.18  |                    2 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.18038824200630188
[2m[36m(func pid=32277)[0m mae:  0.13284189999103546
[2m[36m(func pid=32277)[0m rmse_per_class: [0.114, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0338 | Steps: 2 | Val loss: 0.7230 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.4080 | Steps: 2 | Val loss: 0.3197 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.4470 | Steps: 2 | Val loss: 0.3040 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.9562 | Steps: 2 | Val loss: 0.6926 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=32174)[0m rmse: 0.18063585460186005
[2m[36m(func pid=32174)[0m mae:  0.13308003544807434
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.105, 0.191, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17581386864185333
[2m[36m(func pid=15258)[0m mae:  0.11412026733160019
[2m[36m(func pid=15258)[0m rmse_per_class: [0.094, 0.215, 0.053, 0.313, 0.054, 0.271, 0.256, 0.139, 0.272, 0.09]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.1608467698097229
[2m[36m(func pid=14835)[0m mae:  0.11159340292215347
[2m[36m(func pid=14835)[0m rmse_per_class: [0.102, 0.248, 0.044, 0.294, 0.056, 0.195, 0.299, 0.132, 0.14, 0.099]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:08:40 (running for 00:07:46.63)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.447 |  0.161 |                   81 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.408 |  0.176 |                   80 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  1.034 |  0.181 |                    3 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.956 |  0.181 |                    3 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.1805894672870636
[2m[36m(func pid=32277)[0m mae:  0.1330232471227646
[2m[36m(func pid=32277)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.105, 0.191, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.0254 | Steps: 2 | Val loss: 0.7274 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.4077 | Steps: 2 | Val loss: 0.3310 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.4543 | Steps: 2 | Val loss: 0.3033 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8965 | Steps: 2 | Val loss: 0.6732 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=32174)[0m rmse: 0.18077796697616577
[2m[36m(func pid=32174)[0m mae:  0.13321170210838318
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.106, 0.192, 0.302, 0.142, 0.143, 0.115]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.18023023009300232
[2m[36m(func pid=15258)[0m mae:  0.11741934716701508
[2m[36m(func pid=15258)[0m rmse_per_class: [0.094, 0.206, 0.053, 0.333, 0.054, 0.25, 0.262, 0.139, 0.322, 0.089]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.16085410118103027
[2m[36m(func pid=14835)[0m mae:  0.11157733201980591
[2m[36m(func pid=14835)[0m rmse_per_class: [0.101, 0.247, 0.045, 0.293, 0.056, 0.194, 0.299, 0.13, 0.138, 0.104]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:08:45 (running for 00:07:51.72)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.454 |  0.161 |                   82 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.408 |  0.18  |                   81 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  1.025 |  0.181 |                    4 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.897 |  0.181 |                    4 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.18060563504695892
[2m[36m(func pid=32277)[0m mae:  0.13305239379405975
[2m[36m(func pid=32277)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.331, 0.105, 0.192, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.0207 | Steps: 2 | Val loss: 0.7304 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.4116 | Steps: 2 | Val loss: 0.3385 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.4387 | Steps: 2 | Val loss: 0.3032 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8282 | Steps: 2 | Val loss: 0.6468 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=32174)[0m rmse: 0.18085941672325134
[2m[36m(func pid=32174)[0m mae:  0.13328930735588074
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.107, 0.192, 0.303, 0.142, 0.143, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.18038442730903625
[2m[36m(func pid=15258)[0m mae:  0.11803271621465683
[2m[36m(func pid=15258)[0m rmse_per_class: [0.094, 0.199, 0.051, 0.346, 0.054, 0.223, 0.266, 0.138, 0.344, 0.088]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.1611015945672989
[2m[36m(func pid=14835)[0m mae:  0.1118120402097702
[2m[36m(func pid=14835)[0m rmse_per_class: [0.101, 0.248, 0.045, 0.291, 0.056, 0.194, 0.299, 0.129, 0.137, 0.111]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:08:50 (running for 00:07:56.72)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.439 |  0.161 |                   83 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.412 |  0.18  |                   82 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  1.021 |  0.181 |                    5 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.828 |  0.181 |                    5 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.18058189749717712
[2m[36m(func pid=32277)[0m mae:  0.13303740322589874
[2m[36m(func pid=32277)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.331, 0.106, 0.191, 0.301, 0.141, 0.143, 0.114]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.0105 | Steps: 2 | Val loss: 0.7322 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.4441 | Steps: 2 | Val loss: 0.3038 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3985 | Steps: 2 | Val loss: 0.3407 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.7608 | Steps: 2 | Val loss: 0.6153 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=32174)[0m rmse: 0.18093779683113098
[2m[36m(func pid=32174)[0m mae:  0.1333550661802292
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.107, 0.191, 0.303, 0.142, 0.142, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.16185319423675537
[2m[36m(func pid=14835)[0m mae:  0.11242641508579254
[2m[36m(func pid=14835)[0m rmse_per_class: [0.1, 0.25, 0.046, 0.289, 0.056, 0.195, 0.299, 0.128, 0.136, 0.12]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17695912718772888
[2m[36m(func pid=15258)[0m mae:  0.11625629663467407
[2m[36m(func pid=15258)[0m rmse_per_class: [0.096, 0.201, 0.05, 0.354, 0.054, 0.193, 0.265, 0.136, 0.335, 0.086]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.18051131069660187
[2m[36m(func pid=32277)[0m mae:  0.13297222554683685
[2m[36m(func pid=32277)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.331, 0.106, 0.191, 0.301, 0.141, 0.143, 0.113]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.0006 | Steps: 2 | Val loss: 0.7319 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.4306 | Steps: 2 | Val loss: 0.3048 | Batch size: 32 | lr: 0.001 | Duration: 2.60s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3921 | Steps: 2 | Val loss: 0.3399 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7018 | Steps: 2 | Val loss: 0.5817 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=32174)[0m rmse: 0.18100431561470032
[2m[36m(func pid=32174)[0m mae:  0.13340449333190918
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.33, 0.108, 0.191, 0.304, 0.142, 0.142, 0.116]
== Status ==
Current time: 2024-01-07 05:08:59 (running for 00:08:05.16)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.444 |  0.162 |                   84 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.398 |  0.177 |                   83 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  1.001 |  0.181 |                    7 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.761 |  0.181 |                    6 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.16267314553260803
[2m[36m(func pid=14835)[0m mae:  0.11309555917978287
[2m[36m(func pid=14835)[0m rmse_per_class: [0.099, 0.252, 0.046, 0.287, 0.056, 0.193, 0.299, 0.13, 0.135, 0.13]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17262670397758484
[2m[36m(func pid=15258)[0m mae:  0.11348271369934082
[2m[36m(func pid=15258)[0m rmse_per_class: [0.107, 0.221, 0.047, 0.359, 0.054, 0.17, 0.261, 0.132, 0.29, 0.085]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.180434912443161
[2m[36m(func pid=32277)[0m mae:  0.13289856910705566
[2m[36m(func pid=32277)[0m rmse_per_class: [0.114, 0.264, 0.103, 0.331, 0.106, 0.191, 0.301, 0.14, 0.143, 0.113]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.9884 | Steps: 2 | Val loss: 0.7307 | Batch size: 32 | lr: 0.0001 | Duration: 2.63s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.4374 | Steps: 2 | Val loss: 0.3066 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3972 | Steps: 2 | Val loss: 0.3391 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.6501 | Steps: 2 | Val loss: 0.5478 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=32174)[0m rmse: 0.18109068274497986
[2m[36m(func pid=32174)[0m mae:  0.13347609341144562
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.33, 0.108, 0.191, 0.305, 0.143, 0.142, 0.116]
== Status ==
Current time: 2024-01-07 05:09:04 (running for 00:08:10.19)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.431 |  0.163 |                   85 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.392 |  0.173 |                   84 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.988 |  0.181 |                    8 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.702 |  0.18  |                    7 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=32174)[0m 

[2m[36m(func pid=14835)[0m rmse: 0.16384629905223846
[2m[36m(func pid=14835)[0m mae:  0.11393813788890839
[2m[36m(func pid=14835)[0m rmse_per_class: [0.098, 0.254, 0.047, 0.286, 0.056, 0.191, 0.298, 0.133, 0.134, 0.142]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17061832547187805
[2m[36m(func pid=15258)[0m mae:  0.11163990199565887
[2m[36m(func pid=15258)[0m rmse_per_class: [0.134, 0.25, 0.045, 0.362, 0.054, 0.158, 0.255, 0.127, 0.234, 0.087]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.18030063807964325
[2m[36m(func pid=32277)[0m mae:  0.13277165591716766
[2m[36m(func pid=32277)[0m rmse_per_class: [0.114, 0.264, 0.103, 0.331, 0.106, 0.191, 0.3, 0.14, 0.143, 0.112]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.9742 | Steps: 2 | Val loss: 0.7272 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.4286 | Steps: 2 | Val loss: 0.3092 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3746 | Steps: 2 | Val loss: 0.3380 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6090 | Steps: 2 | Val loss: 0.5161 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=32174)[0m rmse: 0.1811244785785675
[2m[36m(func pid=32174)[0m mae:  0.13350223004817963
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=32174)[0m 
== Status ==
Current time: 2024-01-07 05:09:09 (running for 00:08:15.36)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.437 |  0.164 |                   86 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.397 |  0.171 |                   85 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.974 |  0.181 |                    9 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.65  |  0.18  |                    8 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.16572165489196777
[2m[36m(func pid=14835)[0m mae:  0.11527781188488007
[2m[36m(func pid=14835)[0m rmse_per_class: [0.097, 0.257, 0.048, 0.286, 0.056, 0.189, 0.298, 0.138, 0.134, 0.156]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17049407958984375
[2m[36m(func pid=15258)[0m mae:  0.11080221831798553
[2m[36m(func pid=15258)[0m rmse_per_class: [0.165, 0.272, 0.042, 0.364, 0.054, 0.158, 0.249, 0.121, 0.187, 0.094]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.18017491698265076
[2m[36m(func pid=32277)[0m mae:  0.1326534003019333
[2m[36m(func pid=32277)[0m rmse_per_class: [0.114, 0.264, 0.104, 0.331, 0.105, 0.19, 0.3, 0.14, 0.144, 0.111]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.9627 | Steps: 2 | Val loss: 0.7226 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.4301 | Steps: 2 | Val loss: 0.3124 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.4029 | Steps: 2 | Val loss: 0.3405 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5813 | Steps: 2 | Val loss: 0.4875 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 05:09:14 (running for 00:08:20.46)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.429 |  0.166 |                   87 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.375 |  0.17  |                   86 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.963 |  0.181 |                   10 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.609 |  0.18  |                    9 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32174)[0m rmse: 0.18115568161010742
[2m[36m(func pid=32174)[0m mae:  0.13352470099925995
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.191, 0.306, 0.143, 0.142, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.16797445714473724
[2m[36m(func pid=14835)[0m mae:  0.11676917970180511
[2m[36m(func pid=14835)[0m rmse_per_class: [0.096, 0.259, 0.049, 0.287, 0.056, 0.186, 0.297, 0.146, 0.133, 0.17]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.1723901629447937
[2m[36m(func pid=15258)[0m mae:  0.11124523729085922
[2m[36m(func pid=15258)[0m rmse_per_class: [0.185, 0.288, 0.04, 0.364, 0.054, 0.165, 0.246, 0.116, 0.155, 0.11]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17999514937400818
[2m[36m(func pid=32277)[0m mae:  0.13249149918556213
[2m[36m(func pid=32277)[0m rmse_per_class: [0.114, 0.264, 0.105, 0.331, 0.105, 0.19, 0.299, 0.139, 0.144, 0.11]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.9509 | Steps: 2 | Val loss: 0.7173 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.4328 | Steps: 2 | Val loss: 0.3162 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3714 | Steps: 2 | Val loss: 0.3387 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5467 | Steps: 2 | Val loss: 0.4617 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 05:09:19 (running for 00:08:25.57)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.43  |  0.168 |                   88 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.403 |  0.172 |                   87 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.951 |  0.181 |                   11 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.581 |  0.18  |                   10 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32174)[0m rmse: 0.18115945160388947
[2m[36m(func pid=32174)[0m mae:  0.13352450728416443
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.191, 0.306, 0.143, 0.142, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.17063821852207184
[2m[36m(func pid=14835)[0m mae:  0.11845275014638901
[2m[36m(func pid=14835)[0m rmse_per_class: [0.096, 0.262, 0.049, 0.291, 0.056, 0.183, 0.296, 0.155, 0.133, 0.184]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17324750125408173
[2m[36m(func pid=15258)[0m mae:  0.11129631847143173
[2m[36m(func pid=15258)[0m rmse_per_class: [0.194, 0.281, 0.04, 0.363, 0.054, 0.174, 0.24, 0.109, 0.141, 0.136]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.1797996461391449
[2m[36m(func pid=32277)[0m mae:  0.1323150098323822
[2m[36m(func pid=32277)[0m rmse_per_class: [0.114, 0.264, 0.105, 0.331, 0.104, 0.189, 0.298, 0.139, 0.144, 0.109]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.9377 | Steps: 2 | Val loss: 0.7114 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.4291 | Steps: 2 | Val loss: 0.3207 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3850 | Steps: 2 | Val loss: 0.3335 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5265 | Steps: 2 | Val loss: 0.4397 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=32174)[0m rmse: 0.18114694952964783
[2m[36m(func pid=32174)[0m mae:  0.1335136592388153
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=32174)[0m 
== Status ==
Current time: 2024-01-07 05:09:25 (running for 00:08:31.38)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.429 |  0.174 |                   90 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.371 |  0.173 |                   88 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.938 |  0.181 |                   12 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.547 |  0.18  |                   11 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.17382265627384186
[2m[36m(func pid=14835)[0m mae:  0.12043585628271103
[2m[36m(func pid=14835)[0m rmse_per_class: [0.097, 0.264, 0.05, 0.296, 0.056, 0.181, 0.295, 0.166, 0.133, 0.199]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17304065823554993
[2m[36m(func pid=15258)[0m mae:  0.1109832301735878
[2m[36m(func pid=15258)[0m rmse_per_class: [0.189, 0.27, 0.041, 0.357, 0.054, 0.182, 0.236, 0.104, 0.135, 0.163]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17960955202579498
[2m[36m(func pid=32277)[0m mae:  0.13215214014053345
[2m[36m(func pid=32277)[0m rmse_per_class: [0.115, 0.264, 0.106, 0.331, 0.103, 0.189, 0.297, 0.139, 0.145, 0.108]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.9261 | Steps: 2 | Val loss: 0.7052 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.4347 | Steps: 2 | Val loss: 0.3258 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3798 | Steps: 2 | Val loss: 0.3234 | Batch size: 32 | lr: 0.01 | Duration: 2.61s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5091 | Steps: 2 | Val loss: 0.4212 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=32174)[0m rmse: 0.18114212155342102
[2m[36m(func pid=32174)[0m mae:  0.13351094722747803
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.263, 0.098, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=32174)[0m 
== Status ==
Current time: 2024-01-07 05:09:30 (running for 00:08:36.50)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.435 |  0.177 |                   91 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.385 |  0.173 |                   89 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.926 |  0.181 |                   13 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.526 |  0.18  |                   12 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=14835)[0m rmse: 0.1772840917110443
[2m[36m(func pid=14835)[0m mae:  0.12240008264780045
[2m[36m(func pid=14835)[0m rmse_per_class: [0.099, 0.267, 0.051, 0.303, 0.056, 0.179, 0.295, 0.176, 0.133, 0.214]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.17065031826496124
[2m[36m(func pid=15258)[0m mae:  0.10945330560207367
[2m[36m(func pid=15258)[0m rmse_per_class: [0.171, 0.25, 0.042, 0.348, 0.054, 0.188, 0.23, 0.099, 0.133, 0.193]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17940577864646912
[2m[36m(func pid=32277)[0m mae:  0.13197976350784302
[2m[36m(func pid=32277)[0m rmse_per_class: [0.115, 0.264, 0.106, 0.332, 0.101, 0.189, 0.296, 0.139, 0.145, 0.108]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.9150 | Steps: 2 | Val loss: 0.6986 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.4345 | Steps: 2 | Val loss: 0.3302 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3831 | Steps: 2 | Val loss: 0.3099 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4963 | Steps: 2 | Val loss: 0.4057 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=32174)[0m rmse: 0.18113042414188385
[2m[36m(func pid=32174)[0m mae:  0.133501335978508
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.263, 0.098, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.1804126799106598
[2m[36m(func pid=14835)[0m mae:  0.12427319586277008
[2m[36m(func pid=14835)[0m rmse_per_class: [0.103, 0.269, 0.051, 0.31, 0.056, 0.177, 0.294, 0.186, 0.133, 0.225]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:09:35 (running for 00:08:41.57)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.434 |  0.18  |                   92 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.383 |  0.166 |                   91 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.915 |  0.181 |                   14 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.509 |  0.179 |                   13 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.16630662977695465
[2m[36m(func pid=15258)[0m mae:  0.10691331326961517
[2m[36m(func pid=15258)[0m rmse_per_class: [0.147, 0.228, 0.042, 0.332, 0.053, 0.192, 0.225, 0.096, 0.132, 0.215]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17918772995471954
[2m[36m(func pid=32277)[0m mae:  0.1317976415157318
[2m[36m(func pid=32277)[0m rmse_per_class: [0.115, 0.264, 0.107, 0.332, 0.1, 0.188, 0.295, 0.138, 0.146, 0.107]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.9016 | Steps: 2 | Val loss: 0.6920 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.4321 | Steps: 2 | Val loss: 0.3348 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3758 | Steps: 2 | Val loss: 0.2948 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4843 | Steps: 2 | Val loss: 0.3927 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=32174)[0m rmse: 0.18110664188861847
[2m[36m(func pid=32174)[0m mae:  0.13348211348056793
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.263, 0.097, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.1835511475801468
[2m[36m(func pid=14835)[0m mae:  0.12614431977272034
[2m[36m(func pid=14835)[0m rmse_per_class: [0.107, 0.27, 0.051, 0.317, 0.056, 0.177, 0.293, 0.197, 0.133, 0.235]
[2m[36m(func pid=14835)[0m 
== Status ==
Current time: 2024-01-07 05:09:40 (running for 00:08:46.74)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.432 |  0.184 |                   93 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.376 |  0.16  |                   92 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.902 |  0.181 |                   15 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.496 |  0.179 |                   14 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=15258)[0m rmse: 0.15971913933753967
[2m[36m(func pid=15258)[0m mae:  0.10287197679281235
[2m[36m(func pid=15258)[0m rmse_per_class: [0.123, 0.213, 0.044, 0.307, 0.053, 0.193, 0.223, 0.097, 0.132, 0.214]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17897439002990723
[2m[36m(func pid=32277)[0m mae:  0.131618469953537
[2m[36m(func pid=32277)[0m rmse_per_class: [0.115, 0.264, 0.107, 0.332, 0.098, 0.188, 0.294, 0.138, 0.146, 0.107]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8900 | Steps: 2 | Val loss: 0.6853 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.4436 | Steps: 2 | Val loss: 0.3393 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3655 | Steps: 2 | Val loss: 0.2816 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4652 | Steps: 2 | Val loss: 0.3811 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=32174)[0m rmse: 0.1811019480228424
[2m[36m(func pid=32174)[0m mae:  0.13347506523132324
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.097, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=14835)[0m rmse: 0.18653705716133118
[2m[36m(func pid=14835)[0m mae:  0.12788811326026917
[2m[36m(func pid=14835)[0m rmse_per_class: [0.113, 0.272, 0.05, 0.325, 0.056, 0.177, 0.291, 0.206, 0.133, 0.242]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.1524425745010376
[2m[36m(func pid=15258)[0m mae:  0.09807248413562775
[2m[36m(func pid=15258)[0m rmse_per_class: [0.103, 0.203, 0.048, 0.277, 0.052, 0.191, 0.224, 0.103, 0.131, 0.192]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:09:46 (running for 00:08:52.75)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.444 |  0.187 |                   94 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.365 |  0.152 |                   93 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.89  |  0.181 |                   16 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.465 |  0.179 |                   16 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.1787078082561493
[2m[36m(func pid=32277)[0m mae:  0.13139842450618744
[2m[36m(func pid=32277)[0m rmse_per_class: [0.116, 0.264, 0.107, 0.333, 0.097, 0.188, 0.292, 0.138, 0.146, 0.107]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8779 | Steps: 2 | Val loss: 0.6783 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.4323 | Steps: 2 | Val loss: 0.3433 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3634 | Steps: 2 | Val loss: 0.2735 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4716 | Steps: 2 | Val loss: 0.3726 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=14835)[0m rmse: 0.18917033076286316
[2m[36m(func pid=14835)[0m mae:  0.12940213084220886
[2m[36m(func pid=14835)[0m rmse_per_class: [0.118, 0.273, 0.05, 0.331, 0.056, 0.178, 0.29, 0.215, 0.133, 0.248]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18108907341957092
[2m[36m(func pid=32174)[0m mae:  0.13346610963344574
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.097, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.14740268886089325
[2m[36m(func pid=15258)[0m mae:  0.0942075327038765
[2m[36m(func pid=15258)[0m rmse_per_class: [0.093, 0.201, 0.055, 0.259, 0.051, 0.186, 0.229, 0.111, 0.131, 0.158]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:09:51 (running for 00:08:58.01)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.432 |  0.189 |                   95 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.363 |  0.147 |                   94 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.878 |  0.181 |                   17 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.472 |  0.178 |                   17 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.17847135663032532
[2m[36m(func pid=32277)[0m mae:  0.1312026083469391
[2m[36m(func pid=32277)[0m rmse_per_class: [0.116, 0.264, 0.107, 0.333, 0.095, 0.188, 0.291, 0.137, 0.146, 0.106]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.4448 | Steps: 2 | Val loss: 0.3477 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8666 | Steps: 2 | Val loss: 0.6714 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3610 | Steps: 2 | Val loss: 0.2720 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4650 | Steps: 2 | Val loss: 0.3654 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=14835)[0m rmse: 0.19184546172618866
[2m[36m(func pid=14835)[0m mae:  0.1308322250843048
[2m[36m(func pid=14835)[0m rmse_per_class: [0.125, 0.275, 0.05, 0.337, 0.056, 0.179, 0.289, 0.222, 0.133, 0.253]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18106502294540405
[2m[36m(func pid=32174)[0m mae:  0.1334521770477295
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.097, 0.329, 0.108, 0.19, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.14662441611289978
[2m[36m(func pid=15258)[0m mae:  0.0929265171289444
[2m[36m(func pid=15258)[0m rmse_per_class: [0.09, 0.206, 0.059, 0.265, 0.05, 0.181, 0.237, 0.12, 0.132, 0.126]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:09:57 (running for 00:09:03.54)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.445 |  0.192 |                   96 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.361 |  0.147 |                   95 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.867 |  0.181 |                   18 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.465 |  0.178 |                   18 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.17822952568531036
[2m[36m(func pid=32277)[0m mae:  0.1310015767812729
[2m[36m(func pid=32277)[0m rmse_per_class: [0.117, 0.264, 0.107, 0.333, 0.093, 0.188, 0.29, 0.137, 0.146, 0.106]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.4323 | Steps: 2 | Val loss: 0.3505 | Batch size: 32 | lr: 0.001 | Duration: 2.60s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8556 | Steps: 2 | Val loss: 0.6643 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3654 | Steps: 2 | Val loss: 0.2755 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=14835)[0m rmse: 0.1934974491596222
[2m[36m(func pid=14835)[0m mae:  0.1317981779575348
[2m[36m(func pid=14835)[0m rmse_per_class: [0.131, 0.275, 0.049, 0.343, 0.056, 0.181, 0.286, 0.228, 0.133, 0.254]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4584 | Steps: 2 | Val loss: 0.3595 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=32174)[0m rmse: 0.18105652928352356
[2m[36m(func pid=32174)[0m mae:  0.13344551622867584
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.108, 0.19, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.14847001433372498
[2m[36m(func pid=15258)[0m mae:  0.09370505064725876
[2m[36m(func pid=15258)[0m rmse_per_class: [0.091, 0.214, 0.06, 0.278, 0.05, 0.174, 0.248, 0.131, 0.134, 0.103]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:10:02 (running for 00:09:08.61)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.432 |  0.193 |                   97 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.365 |  0.148 |                   96 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.856 |  0.181 |                   19 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.458 |  0.178 |                   19 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.17808403074741364
[2m[36m(func pid=32277)[0m mae:  0.13086716830730438
[2m[36m(func pid=32277)[0m rmse_per_class: [0.117, 0.264, 0.108, 0.334, 0.092, 0.188, 0.289, 0.137, 0.146, 0.106]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.4404 | Steps: 2 | Val loss: 0.3526 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8455 | Steps: 2 | Val loss: 0.6576 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3550 | Steps: 2 | Val loss: 0.2798 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=14835)[0m rmse: 0.19470719993114471
[2m[36m(func pid=14835)[0m mae:  0.1324836164712906
[2m[36m(func pid=14835)[0m rmse_per_class: [0.135, 0.276, 0.048, 0.347, 0.056, 0.182, 0.284, 0.233, 0.133, 0.253]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4553 | Steps: 2 | Val loss: 0.3547 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=32174)[0m rmse: 0.181078240275383
[2m[36m(func pid=32174)[0m mae:  0.13345706462860107
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.108, 0.19, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.15027379989624023
[2m[36m(func pid=15258)[0m mae:  0.0954064354300499
[2m[36m(func pid=15258)[0m rmse_per_class: [0.092, 0.221, 0.057, 0.283, 0.052, 0.166, 0.26, 0.136, 0.142, 0.093]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:10:07 (running for 00:09:13.88)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.44  |  0.195 |                   98 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.355 |  0.15  |                   97 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.846 |  0.181 |                   20 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.455 |  0.178 |                   20 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.17795430123806
[2m[36m(func pid=32277)[0m mae:  0.13074246048927307
[2m[36m(func pid=32277)[0m rmse_per_class: [0.118, 0.264, 0.108, 0.334, 0.091, 0.188, 0.288, 0.137, 0.146, 0.106]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.4256 | Steps: 2 | Val loss: 0.3534 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8327 | Steps: 2 | Val loss: 0.6499 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3865 | Steps: 2 | Val loss: 0.2830 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=14835)[0m rmse: 0.19531628489494324
[2m[36m(func pid=14835)[0m mae:  0.1327521651983261
[2m[36m(func pid=14835)[0m rmse_per_class: [0.141, 0.276, 0.048, 0.35, 0.056, 0.184, 0.281, 0.235, 0.133, 0.25]
[2m[36m(func pid=14835)[0m 
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4524 | Steps: 2 | Val loss: 0.3505 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=32174)[0m rmse: 0.18107810616493225
[2m[36m(func pid=32174)[0m mae:  0.1334526389837265
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.108, 0.19, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.15157762169837952
[2m[36m(func pid=15258)[0m mae:  0.0970211923122406
[2m[36m(func pid=15258)[0m rmse_per_class: [0.092, 0.224, 0.059, 0.277, 0.057, 0.16, 0.272, 0.132, 0.154, 0.088]
[2m[36m(func pid=15258)[0m 
== Status ==
Current time: 2024-01-07 05:10:13 (running for 00:09:19.28)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00001 | RUNNING    | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.426 |  0.195 |                   99 |
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.387 |  0.152 |                   98 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.833 |  0.181 |                   21 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.452 |  0.178 |                   21 |
| train_5a6ec_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.17776957154273987
[2m[36m(func pid=32277)[0m mae:  0.1305839866399765
[2m[36m(func pid=32277)[0m rmse_per_class: [0.118, 0.264, 0.108, 0.334, 0.089, 0.188, 0.288, 0.137, 0.146, 0.106]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=14835)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.4408 | Steps: 2 | Val loss: 0.3539 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8238 | Steps: 2 | Val loss: 0.6429 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3589 | Steps: 2 | Val loss: 0.2856 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=14835)[0m rmse: 0.1956302523612976
[2m[36m(func pid=14835)[0m mae:  0.132843017578125
[2m[36m(func pid=14835)[0m rmse_per_class: [0.145, 0.276, 0.048, 0.353, 0.056, 0.185, 0.279, 0.236, 0.133, 0.246]
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4495 | Steps: 2 | Val loss: 0.3470 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=32174)[0m rmse: 0.1810883730649948
[2m[36m(func pid=32174)[0m mae:  0.13345013558864594
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.107, 0.191, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=15258)[0m rmse: 0.15278610587120056
[2m[36m(func pid=15258)[0m mae:  0.09904120862483978
[2m[36m(func pid=15258)[0m rmse_per_class: [0.091, 0.226, 0.054, 0.262, 0.065, 0.16, 0.28, 0.131, 0.173, 0.086]
[2m[36m(func pid=15258)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17758329212665558
[2m[36m(func pid=32277)[0m mae:  0.13041658699512482
[2m[36m(func pid=32277)[0m rmse_per_class: [0.118, 0.264, 0.109, 0.334, 0.087, 0.188, 0.287, 0.137, 0.146, 0.105]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8179 | Steps: 2 | Val loss: 0.6361 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=15258)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3576 | Steps: 2 | Val loss: 0.2894 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4452 | Steps: 2 | Val loss: 0.3442 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=32174)[0m rmse: 0.1810760498046875
[2m[36m(func pid=32174)[0m mae:  0.13343855738639832
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.107, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=15258)[0m rmse: 0.15581989288330078
[2m[36m(func pid=15258)[0m mae:  0.10183541476726532
[2m[36m(func pid=15258)[0m rmse_per_class: [0.093, 0.226, 0.048, 0.258, 0.079, 0.171, 0.285, 0.124, 0.19, 0.085]
[2m[36m(func pid=32277)[0m rmse: 0.1774386614561081
[2m[36m(func pid=32277)[0m mae:  0.13028572499752045
[2m[36m(func pid=32277)[0m rmse_per_class: [0.119, 0.264, 0.109, 0.334, 0.086, 0.188, 0.286, 0.137, 0.146, 0.105]
== Status ==
Current time: 2024-01-07 05:10:18 (running for 00:09:24.50)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00002 | RUNNING    | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.359 |  0.153 |                   99 |
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.824 |  0.181 |                   22 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.45  |  0.178 |                   22 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=37799)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=37799)[0m Configuration completed!
[2m[36m(func pid=37799)[0m New optimizer parameters:
[2m[36m(func pid=37799)[0m SGD (
[2m[36m(func pid=37799)[0m Parameter Group 0
[2m[36m(func pid=37799)[0m     dampening: 0
[2m[36m(func pid=37799)[0m     differentiable: False
[2m[36m(func pid=37799)[0m     foreach: None
[2m[36m(func pid=37799)[0m     lr: 0.01
[2m[36m(func pid=37799)[0m     maximize: False
[2m[36m(func pid=37799)[0m     momentum: 0.9
[2m[36m(func pid=37799)[0m     nesterov: False
[2m[36m(func pid=37799)[0m     weight_decay: 0
[2m[36m(func pid=37799)[0m )
[2m[36m(func pid=37799)[0m 
== Status ==
Current time: 2024-01-07 05:10:24 (running for 00:09:30.69)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 3 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.824 |  0.181 |                   22 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.45  |  0.178 |                   22 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.8052 | Steps: 2 | Val loss: 0.6296 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4467 | Steps: 2 | Val loss: 0.3418 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0069 | Steps: 2 | Val loss: 0.6621 | Batch size: 32 | lr: 0.01 | Duration: 4.27s
[2m[36m(func pid=32174)[0m rmse: 0.18108849227428436
[2m[36m(func pid=32174)[0m mae:  0.13344526290893555
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.107, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17728778719902039
[2m[36m(func pid=32277)[0m mae:  0.1301446259021759
[2m[36m(func pid=32277)[0m rmse_per_class: [0.119, 0.263, 0.109, 0.335, 0.085, 0.188, 0.286, 0.137, 0.146, 0.105]
[2m[36m(func pid=37799)[0m rmse: 0.17979583144187927
[2m[36m(func pid=37799)[0m mae:  0.13229981064796448
[2m[36m(func pid=37799)[0m rmse_per_class: [0.115, 0.263, 0.098, 0.334, 0.099, 0.191, 0.297, 0.143, 0.141, 0.118]
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7962 | Steps: 2 | Val loss: 0.6231 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 05:10:29 (running for 00:09:36.07)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.805 |  0.181 |                   24 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.445 |  0.177 |                   23 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |        |        |                      |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=38408)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=38408)[0m Configuration completed!
[2m[36m(func pid=38408)[0m New optimizer parameters:
[2m[36m(func pid=38408)[0m SGD (
[2m[36m(func pid=38408)[0m Parameter Group 0
[2m[36m(func pid=38408)[0m     dampening: 0
[2m[36m(func pid=38408)[0m     differentiable: False
[2m[36m(func pid=38408)[0m     foreach: None
[2m[36m(func pid=38408)[0m     lr: 0.1
[2m[36m(func pid=38408)[0m     maximize: False
[2m[36m(func pid=38408)[0m     momentum: 0.9
[2m[36m(func pid=38408)[0m     nesterov: False
[2m[36m(func pid=38408)[0m     weight_decay: 0
[2m[36m(func pid=38408)[0m )
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:10:35 (running for 00:09:41.42)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.796 |  0.181 |                   25 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.447 |  0.177 |                   24 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  1.007 |  0.18  |                    1 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |        |        |                      |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32174)[0m rmse: 0.18106546998023987
[2m[36m(func pid=32174)[0m mae:  0.13342365622520447
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.107, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.7765 | Steps: 2 | Val loss: 0.5981 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4448 | Steps: 2 | Val loss: 0.3398 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.7996 | Steps: 2 | Val loss: 0.4377 | Batch size: 32 | lr: 0.1 | Duration: 4.35s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7900 | Steps: 2 | Val loss: 0.6167 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=37799)[0m rmse: 0.1800341010093689
[2m[36m(func pid=37799)[0m mae:  0.1325012892484665
[2m[36m(func pid=37799)[0m rmse_per_class: [0.114, 0.264, 0.104, 0.334, 0.101, 0.191, 0.294, 0.141, 0.144, 0.113]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17714542150497437
[2m[36m(func pid=32277)[0m mae:  0.13001689314842224
[2m[36m(func pid=32277)[0m rmse_per_class: [0.119, 0.263, 0.11, 0.335, 0.083, 0.188, 0.285, 0.136, 0.146, 0.105]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.1794128715991974
[2m[36m(func pid=38408)[0m mae:  0.1313192993402481
[2m[36m(func pid=38408)[0m rmse_per_class: [0.12, 0.264, 0.118, 0.342, 0.088, 0.191, 0.283, 0.139, 0.144, 0.106]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:10:40 (running for 00:09:46.52)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.79  |  0.181 |                   26 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.445 |  0.177 |                   25 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.777 |  0.18  |                    2 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.8   |  0.179 |                    1 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32174)[0m rmse: 0.1810571551322937
[2m[36m(func pid=32174)[0m mae:  0.13341578841209412
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.106, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5582 | Steps: 2 | Val loss: 0.5109 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4409 | Steps: 2 | Val loss: 0.3381 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.4888 | Steps: 2 | Val loss: 0.3665 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7756 | Steps: 2 | Val loss: 0.6103 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=37799)[0m rmse: 0.1798572540283203
[2m[36m(func pid=37799)[0m mae:  0.1323021948337555
[2m[36m(func pid=37799)[0m rmse_per_class: [0.114, 0.264, 0.107, 0.335, 0.1, 0.191, 0.293, 0.139, 0.145, 0.11]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17701993882656097
[2m[36m(func pid=32277)[0m mae:  0.12988969683647156
[2m[36m(func pid=32277)[0m rmse_per_class: [0.12, 0.263, 0.11, 0.335, 0.082, 0.188, 0.284, 0.136, 0.146, 0.105]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.18005621433258057
[2m[36m(func pid=38408)[0m mae:  0.1308235377073288
[2m[36m(func pid=38408)[0m rmse_per_class: [0.12, 0.265, 0.139, 0.346, 0.078, 0.189, 0.275, 0.139, 0.149, 0.099]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:10:45 (running for 00:09:51.71)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.776 |  0.181 |                   27 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.441 |  0.177 |                   26 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.558 |  0.18  |                    3 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.489 |  0.18  |                    2 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32174)[0m rmse: 0.1810445487499237
[2m[36m(func pid=32174)[0m mae:  0.13340364396572113
[2m[36m(func pid=32174)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.106, 0.191, 0.305, 0.146, 0.142, 0.119]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4620 | Steps: 2 | Val loss: 0.4287 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4440 | Steps: 2 | Val loss: 0.3368 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.6339 | Steps: 2 | Val loss: 0.3300 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.7679 | Steps: 2 | Val loss: 0.6039 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=32277)[0m rmse: 0.17692027986049652
[2m[36m(func pid=32277)[0m mae:  0.1297948658466339
[2m[36m(func pid=32277)[0m rmse_per_class: [0.12, 0.263, 0.11, 0.335, 0.081, 0.188, 0.284, 0.136, 0.146, 0.105]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.17932724952697754
[2m[36m(func pid=37799)[0m mae:  0.13175848126411438
[2m[36m(func pid=37799)[0m rmse_per_class: [0.115, 0.264, 0.111, 0.336, 0.097, 0.19, 0.289, 0.137, 0.146, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.17879313230514526
[2m[36m(func pid=38408)[0m mae:  0.12823544442653656
[2m[36m(func pid=38408)[0m rmse_per_class: [0.126, 0.265, 0.147, 0.351, 0.062, 0.188, 0.264, 0.14, 0.153, 0.093]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:10:50 (running for 00:09:56.93)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.768 |  0.181 |                   28 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.444 |  0.177 |                   27 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.462 |  0.179 |                    4 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.634 |  0.179 |                    3 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32174)[0m rmse: 0.1810227781534195
[2m[36m(func pid=32174)[0m mae:  0.13338661193847656
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.106, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4454 | Steps: 2 | Val loss: 0.3724 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4353 | Steps: 2 | Val loss: 0.3356 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.7051 | Steps: 2 | Val loss: 0.3256 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7619 | Steps: 2 | Val loss: 0.5980 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=37799)[0m rmse: 0.17846640944480896
[2m[36m(func pid=37799)[0m mae:  0.13091804087162018
[2m[36m(func pid=37799)[0m rmse_per_class: [0.116, 0.264, 0.114, 0.337, 0.092, 0.19, 0.285, 0.136, 0.147, 0.104]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17687949538230896
[2m[36m(func pid=32277)[0m mae:  0.12974491715431213
[2m[36m(func pid=32277)[0m rmse_per_class: [0.121, 0.263, 0.111, 0.335, 0.08, 0.189, 0.284, 0.136, 0.146, 0.104]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.1779555380344391
[2m[36m(func pid=38408)[0m mae:  0.12559589743614197
[2m[36m(func pid=38408)[0m rmse_per_class: [0.134, 0.262, 0.134, 0.356, 0.055, 0.187, 0.262, 0.143, 0.157, 0.091]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18100731074810028
[2m[36m(func pid=32174)[0m mae:  0.13337400555610657
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.106, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=32174)[0m 
== Status ==
Current time: 2024-01-07 05:10:55 (running for 00:10:02.04)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.762 |  0.181 |                   29 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.435 |  0.177 |                   28 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.445 |  0.178 |                    5 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.705 |  0.178 |                    4 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4563 | Steps: 2 | Val loss: 0.3400 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4359 | Steps: 2 | Val loss: 0.3345 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.6087 | Steps: 2 | Val loss: 0.3274 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.7543 | Steps: 2 | Val loss: 0.5921 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=37799)[0m rmse: 0.1773814707994461
[2m[36m(func pid=37799)[0m mae:  0.12984588742256165
[2m[36m(func pid=37799)[0m rmse_per_class: [0.117, 0.264, 0.116, 0.338, 0.085, 0.189, 0.28, 0.135, 0.148, 0.101]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.1768217384815216
[2m[36m(func pid=32277)[0m mae:  0.12968717515468597
[2m[36m(func pid=32277)[0m rmse_per_class: [0.121, 0.263, 0.111, 0.335, 0.079, 0.189, 0.284, 0.136, 0.146, 0.104]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.17449751496315002
[2m[36m(func pid=38408)[0m mae:  0.12340816110372543
[2m[36m(func pid=38408)[0m rmse_per_class: [0.147, 0.253, 0.087, 0.356, 0.055, 0.185, 0.258, 0.145, 0.166, 0.092]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:11:00 (running for 00:10:07.10)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.754 |  0.181 |                   30 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.436 |  0.177 |                   29 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.456 |  0.177 |                    6 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.609 |  0.174 |                    5 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32174)[0m rmse: 0.1809782087802887
[2m[36m(func pid=32174)[0m mae:  0.1333518922328949
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.106, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4335 | Steps: 2 | Val loss: 0.3335 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4728 | Steps: 2 | Val loss: 0.3243 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5614 | Steps: 2 | Val loss: 0.3297 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.7482 | Steps: 2 | Val loss: 0.5868 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=37799)[0m rmse: 0.17636416852474213
[2m[36m(func pid=37799)[0m mae:  0.1287720650434494
[2m[36m(func pid=37799)[0m rmse_per_class: [0.119, 0.264, 0.117, 0.34, 0.078, 0.188, 0.275, 0.135, 0.148, 0.099]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17676471173763275
[2m[36m(func pid=32277)[0m mae:  0.12959909439086914
[2m[36m(func pid=32277)[0m rmse_per_class: [0.122, 0.263, 0.112, 0.335, 0.078, 0.189, 0.283, 0.136, 0.146, 0.104]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.167589470744133
[2m[36m(func pid=38408)[0m mae:  0.11970994621515274
[2m[36m(func pid=38408)[0m rmse_per_class: [0.116, 0.258, 0.05, 0.339, 0.056, 0.181, 0.26, 0.144, 0.18, 0.092]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18095120787620544
[2m[36m(func pid=32174)[0m mae:  0.1333307921886444
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.304, 0.145, 0.142, 0.119]
[2m[36m(func pid=32174)[0m 
== Status ==
Current time: 2024-01-07 05:11:06 (running for 00:10:12.27)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.748 |  0.181 |                   31 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.434 |  0.177 |                   30 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.473 |  0.176 |                    7 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.561 |  0.168 |                    6 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4857 | Steps: 2 | Val loss: 0.3183 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4358 | Steps: 2 | Val loss: 0.3327 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5057 | Steps: 2 | Val loss: 0.3234 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7397 | Steps: 2 | Val loss: 0.5814 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=37799)[0m rmse: 0.17547723650932312
[2m[36m(func pid=37799)[0m mae:  0.12777310609817505
[2m[36m(func pid=37799)[0m rmse_per_class: [0.121, 0.263, 0.118, 0.34, 0.072, 0.187, 0.272, 0.136, 0.148, 0.097]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.176661878824234
[2m[36m(func pid=32277)[0m mae:  0.12951865792274475
[2m[36m(func pid=32277)[0m rmse_per_class: [0.122, 0.263, 0.111, 0.335, 0.078, 0.189, 0.283, 0.136, 0.146, 0.104]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.16207773983478546
[2m[36m(func pid=38408)[0m mae:  0.11422902345657349
[2m[36m(func pid=38408)[0m rmse_per_class: [0.099, 0.253, 0.046, 0.301, 0.056, 0.194, 0.284, 0.135, 0.161, 0.091]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:11:11 (running for 00:10:17.42)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.74  |  0.181 |                   32 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.436 |  0.177 |                   31 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.486 |  0.175 |                    8 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.506 |  0.162 |                    7 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32174)[0m rmse: 0.18092958629131317
[2m[36m(func pid=32174)[0m mae:  0.13331547379493713
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4887 | Steps: 2 | Val loss: 0.3163 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4340 | Steps: 2 | Val loss: 0.3320 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4565 | Steps: 2 | Val loss: 0.3217 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7314 | Steps: 2 | Val loss: 0.5766 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=37799)[0m rmse: 0.17457982897758484
[2m[36m(func pid=37799)[0m mae:  0.1267622411251068
[2m[36m(func pid=37799)[0m rmse_per_class: [0.124, 0.262, 0.117, 0.341, 0.066, 0.187, 0.27, 0.136, 0.148, 0.096]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.1766006052494049
[2m[36m(func pid=32277)[0m mae:  0.12945345044136047
[2m[36m(func pid=32277)[0m rmse_per_class: [0.123, 0.263, 0.111, 0.335, 0.077, 0.189, 0.283, 0.136, 0.146, 0.104]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.16344447433948517
[2m[36m(func pid=38408)[0m mae:  0.11347977817058563
[2m[36m(func pid=38408)[0m rmse_per_class: [0.103, 0.259, 0.047, 0.294, 0.056, 0.209, 0.298, 0.134, 0.136, 0.099]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:11:16 (running for 00:10:22.45)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.731 |  0.181 |                   33 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.434 |  0.177 |                   32 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.489 |  0.175 |                    9 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.457 |  0.163 |                    8 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32174)[0m rmse: 0.18092669546604156
[2m[36m(func pid=32174)[0m mae:  0.13331253826618195
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4868 | Steps: 2 | Val loss: 0.3154 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4322 | Steps: 2 | Val loss: 0.3313 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4568 | Steps: 2 | Val loss: 0.3348 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.7289 | Steps: 2 | Val loss: 0.5711 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=37799)[0m rmse: 0.17380698025226593
[2m[36m(func pid=37799)[0m mae:  0.12582316994667053
[2m[36m(func pid=37799)[0m rmse_per_class: [0.126, 0.26, 0.115, 0.341, 0.062, 0.186, 0.268, 0.137, 0.148, 0.095]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17650075256824493
[2m[36m(func pid=32277)[0m mae:  0.12938207387924194
[2m[36m(func pid=32277)[0m rmse_per_class: [0.123, 0.263, 0.111, 0.335, 0.076, 0.189, 0.283, 0.136, 0.146, 0.104]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.17837414145469666
[2m[36m(func pid=38408)[0m mae:  0.12367689609527588
[2m[36m(func pid=38408)[0m rmse_per_class: [0.1, 0.274, 0.047, 0.308, 0.056, 0.186, 0.294, 0.213, 0.133, 0.173]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18089313805103302
[2m[36m(func pid=32174)[0m mae:  0.13328664004802704
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4330 | Steps: 2 | Val loss: 0.3307 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4804 | Steps: 2 | Val loss: 0.3147 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4484 | Steps: 2 | Val loss: 0.3455 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7136 | Steps: 2 | Val loss: 0.5660 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 05:11:24 (running for 00:10:30.54)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.729 |  0.181 |                   34 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.432 |  0.177 |                   33 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.48  |  0.173 |                   11 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.457 |  0.178 |                    9 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.17342780530452728
[2m[36m(func pid=37799)[0m mae:  0.1252218335866928
[2m[36m(func pid=37799)[0m rmse_per_class: [0.13, 0.259, 0.114, 0.341, 0.059, 0.186, 0.266, 0.137, 0.148, 0.094]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17642956972122192
[2m[36m(func pid=32277)[0m mae:  0.1293252408504486
[2m[36m(func pid=32277)[0m rmse_per_class: [0.124, 0.262, 0.11, 0.335, 0.075, 0.189, 0.282, 0.136, 0.146, 0.104]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.18689841032028198
[2m[36m(func pid=38408)[0m mae:  0.12987391650676727
[2m[36m(func pid=38408)[0m rmse_per_class: [0.101, 0.273, 0.045, 0.35, 0.055, 0.181, 0.276, 0.207, 0.133, 0.248]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18088829517364502
[2m[36m(func pid=32174)[0m mae:  0.13327883183956146
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4667 | Steps: 2 | Val loss: 0.3136 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4349 | Steps: 2 | Val loss: 0.3304 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4481 | Steps: 2 | Val loss: 0.3186 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.7117 | Steps: 2 | Val loss: 0.5613 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 05:11:29 (running for 00:10:35.83)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.714 |  0.181 |                   35 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.435 |  0.176 |                   35 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.48  |  0.173 |                   11 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.448 |  0.187 |                   10 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.17639558017253876
[2m[36m(func pid=32277)[0m mae:  0.12928013503551483
[2m[36m(func pid=32277)[0m rmse_per_class: [0.124, 0.262, 0.11, 0.335, 0.075, 0.189, 0.282, 0.136, 0.146, 0.104]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.17328952252864838
[2m[36m(func pid=37799)[0m mae:  0.12486201524734497
[2m[36m(func pid=37799)[0m rmse_per_class: [0.134, 0.258, 0.114, 0.34, 0.057, 0.185, 0.265, 0.137, 0.149, 0.094]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.1738644242286682
[2m[36m(func pid=38408)[0m mae:  0.12326488643884659
[2m[36m(func pid=38408)[0m rmse_per_class: [0.104, 0.247, 0.044, 0.36, 0.054, 0.184, 0.259, 0.135, 0.15, 0.201]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18086376786231995
[2m[36m(func pid=32174)[0m mae:  0.13325396180152893
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4361 | Steps: 2 | Val loss: 0.3302 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4519 | Steps: 2 | Val loss: 0.3123 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4136 | Steps: 2 | Val loss: 0.3011 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7110 | Steps: 2 | Val loss: 0.5566 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=32277)[0m rmse: 0.17637379467487335
[2m[36m(func pid=32277)[0m mae:  0.12923552095890045
[2m[36m(func pid=32277)[0m rmse_per_class: [0.125, 0.262, 0.111, 0.335, 0.074, 0.189, 0.282, 0.136, 0.146, 0.104]
== Status ==
Current time: 2024-01-07 05:11:34 (running for 00:10:40.97)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.712 |  0.181 |                   36 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.436 |  0.176 |                   36 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.467 |  0.173 |                   12 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.448 |  0.174 |                   11 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.17333444952964783
[2m[36m(func pid=37799)[0m mae:  0.12465611845254898
[2m[36m(func pid=37799)[0m rmse_per_class: [0.138, 0.257, 0.114, 0.34, 0.056, 0.185, 0.264, 0.138, 0.149, 0.094]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.1644640862941742
[2m[36m(func pid=38408)[0m mae:  0.11713504791259766
[2m[36m(func pid=38408)[0m rmse_per_class: [0.096, 0.25, 0.045, 0.351, 0.057, 0.179, 0.257, 0.121, 0.173, 0.117]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18082860112190247
[2m[36m(func pid=32174)[0m mae:  0.13322857022285461
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4333 | Steps: 2 | Val loss: 0.3297 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4358 | Steps: 2 | Val loss: 0.3104 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3982 | Steps: 2 | Val loss: 0.2947 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.7006 | Steps: 2 | Val loss: 0.5523 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 05:11:39 (running for 00:10:46.07)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.711 |  0.181 |                   37 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.433 |  0.176 |                   37 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.452 |  0.173 |                   13 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.414 |  0.164 |                   12 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.1763247549533844
[2m[36m(func pid=32277)[0m mae:  0.12916609644889832
[2m[36m(func pid=32277)[0m rmse_per_class: [0.125, 0.262, 0.111, 0.335, 0.074, 0.189, 0.282, 0.136, 0.146, 0.104]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.1729746013879776
[2m[36m(func pid=37799)[0m mae:  0.124272920191288
[2m[36m(func pid=37799)[0m rmse_per_class: [0.14, 0.256, 0.112, 0.338, 0.055, 0.185, 0.263, 0.137, 0.149, 0.094]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.16495028138160706
[2m[36m(func pid=38408)[0m mae:  0.1148800477385521
[2m[36m(func pid=38408)[0m rmse_per_class: [0.098, 0.287, 0.046, 0.314, 0.084, 0.174, 0.277, 0.129, 0.146, 0.093]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18080267310142517
[2m[36m(func pid=32174)[0m mae:  0.13320598006248474
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4300 | Steps: 2 | Val loss: 0.3294 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4222 | Steps: 2 | Val loss: 0.3088 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3932 | Steps: 2 | Val loss: 0.2879 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6943 | Steps: 2 | Val loss: 0.5478 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=37799)[0m rmse: 0.17248114943504333
[2m[36m(func pid=37799)[0m mae:  0.12386993318796158
[2m[36m(func pid=37799)[0m rmse_per_class: [0.142, 0.255, 0.109, 0.336, 0.055, 0.184, 0.264, 0.137, 0.15, 0.094]
== Status ==
Current time: 2024-01-07 05:11:45 (running for 00:10:51.21)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.701 |  0.181 |                   38 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.433 |  0.176 |                   37 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.422 |  0.172 |                   15 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.398 |  0.165 |                   13 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17631657421588898
[2m[36m(func pid=32277)[0m mae:  0.12911126017570496
[2m[36m(func pid=32277)[0m rmse_per_class: [0.126, 0.262, 0.112, 0.335, 0.073, 0.189, 0.282, 0.136, 0.146, 0.103]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.1612720489501953
[2m[36m(func pid=38408)[0m mae:  0.11074928939342499
[2m[36m(func pid=38408)[0m rmse_per_class: [0.095, 0.232, 0.047, 0.285, 0.143, 0.176, 0.281, 0.129, 0.134, 0.09]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18077455461025238
[2m[36m(func pid=32174)[0m mae:  0.13318586349487305
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4161 | Steps: 2 | Val loss: 0.3075 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4306 | Steps: 2 | Val loss: 0.3288 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3797 | Steps: 2 | Val loss: 0.2984 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6900 | Steps: 2 | Val loss: 0.5437 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 05:11:50 (running for 00:10:56.24)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.694 |  0.181 |                   39 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.43  |  0.176 |                   38 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.416 |  0.172 |                   16 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.393 |  0.161 |                   14 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.17151734232902527
[2m[36m(func pid=37799)[0m mae:  0.1232357993721962
[2m[36m(func pid=37799)[0m rmse_per_class: [0.14, 0.255, 0.104, 0.332, 0.055, 0.184, 0.265, 0.136, 0.151, 0.094]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17624565958976746
[2m[36m(func pid=32277)[0m mae:  0.12901851534843445
[2m[36m(func pid=32277)[0m rmse_per_class: [0.126, 0.262, 0.112, 0.335, 0.072, 0.189, 0.282, 0.136, 0.146, 0.103]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.16873447597026825
[2m[36m(func pid=38408)[0m mae:  0.1149485856294632
[2m[36m(func pid=38408)[0m rmse_per_class: [0.11, 0.245, 0.047, 0.301, 0.187, 0.182, 0.271, 0.12, 0.133, 0.092]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18075522780418396
[2m[36m(func pid=32174)[0m mae:  0.13317352533340454
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4298 | Steps: 2 | Val loss: 0.3283 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4173 | Steps: 2 | Val loss: 0.3064 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3813 | Steps: 2 | Val loss: 0.3093 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6815 | Steps: 2 | Val loss: 0.5403 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 05:11:55 (running for 00:11:01.56)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.69  |  0.181 |                   40 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.431 |  0.176 |                   39 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.417 |  0.17  |                   17 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.38  |  0.169 |                   15 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.17024730145931244
[2m[36m(func pid=37799)[0m mae:  0.1225116029381752
[2m[36m(func pid=37799)[0m rmse_per_class: [0.135, 0.253, 0.097, 0.33, 0.055, 0.183, 0.267, 0.135, 0.153, 0.095]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17613710463047028
[2m[36m(func pid=32277)[0m mae:  0.12892022728919983
[2m[36m(func pid=32277)[0m rmse_per_class: [0.126, 0.262, 0.112, 0.335, 0.072, 0.189, 0.281, 0.136, 0.146, 0.103]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.17221708595752716
[2m[36m(func pid=38408)[0m mae:  0.11943839490413666
[2m[36m(func pid=38408)[0m rmse_per_class: [0.113, 0.247, 0.045, 0.336, 0.183, 0.175, 0.269, 0.114, 0.137, 0.103]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18074913322925568
[2m[36m(func pid=32174)[0m mae:  0.13316501677036285
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.144, 0.143, 0.117]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3695 | Steps: 2 | Val loss: 0.3094 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4054 | Steps: 2 | Val loss: 0.3047 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4289 | Steps: 2 | Val loss: 0.3279 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6730 | Steps: 2 | Val loss: 0.5359 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 05:12:00 (running for 00:11:06.91)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.681 |  0.181 |                   41 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.43  |  0.176 |                   40 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.405 |  0.168 |                   18 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.381 |  0.172 |                   16 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.1683836728334427
[2m[36m(func pid=37799)[0m mae:  0.1214376837015152
[2m[36m(func pid=37799)[0m rmse_per_class: [0.126, 0.253, 0.088, 0.326, 0.054, 0.182, 0.268, 0.134, 0.155, 0.097]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.17180117964744568
[2m[36m(func pid=38408)[0m mae:  0.1210334300994873
[2m[36m(func pid=38408)[0m rmse_per_class: [0.095, 0.233, 0.05, 0.344, 0.128, 0.175, 0.275, 0.129, 0.151, 0.14]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17605218291282654
[2m[36m(func pid=32277)[0m mae:  0.12883606553077698
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.112, 0.335, 0.071, 0.189, 0.281, 0.136, 0.146, 0.103]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.1807234138250351
[2m[36m(func pid=32174)[0m mae:  0.13314051926136017
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.144, 0.143, 0.117]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4046 | Steps: 2 | Val loss: 0.3026 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3716 | Steps: 2 | Val loss: 0.2980 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4271 | Steps: 2 | Val loss: 0.3273 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6741 | Steps: 2 | Val loss: 0.5323 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 05:12:06 (running for 00:11:12.15)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.673 |  0.181 |                   42 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.429 |  0.176 |                   41 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.405 |  0.168 |                   18 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.372 |  0.165 |                   18 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=38408)[0m rmse: 0.16473551094532013
[2m[36m(func pid=38408)[0m mae:  0.11689314991235733
[2m[36m(func pid=38408)[0m rmse_per_class: [0.095, 0.232, 0.046, 0.327, 0.08, 0.179, 0.281, 0.118, 0.143, 0.147]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16651904582977295
[2m[36m(func pid=37799)[0m mae:  0.12031857669353485
[2m[36m(func pid=37799)[0m rmse_per_class: [0.118, 0.252, 0.079, 0.322, 0.054, 0.181, 0.27, 0.133, 0.157, 0.098]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.1759105622768402
[2m[36m(func pid=32277)[0m mae:  0.12871399521827698
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.111, 0.335, 0.071, 0.189, 0.281, 0.136, 0.146, 0.103]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.180704265832901
[2m[36m(func pid=32174)[0m mae:  0.13312456011772156
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.144, 0.143, 0.117]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3564 | Steps: 2 | Val loss: 0.2772 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3973 | Steps: 2 | Val loss: 0.3014 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4264 | Steps: 2 | Val loss: 0.3269 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.6686 | Steps: 2 | Val loss: 0.5287 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 05:12:11 (running for 00:11:17.30)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.674 |  0.181 |                   43 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.427 |  0.176 |                   42 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.405 |  0.167 |                   19 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.356 |  0.153 |                   19 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=38408)[0m rmse: 0.15252678096294403
[2m[36m(func pid=38408)[0m mae:  0.10731923580169678
[2m[36m(func pid=38408)[0m rmse_per_class: [0.1, 0.23, 0.044, 0.292, 0.064, 0.176, 0.269, 0.108, 0.137, 0.106]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16504450142383575
[2m[36m(func pid=37799)[0m mae:  0.11954597383737564
[2m[36m(func pid=37799)[0m rmse_per_class: [0.111, 0.252, 0.069, 0.319, 0.054, 0.18, 0.272, 0.133, 0.159, 0.101]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17582257091999054
[2m[36m(func pid=32277)[0m mae:  0.1286364644765854
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.111, 0.334, 0.071, 0.189, 0.281, 0.136, 0.146, 0.103]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.1806698590517044
[2m[36m(func pid=32174)[0m mae:  0.13309648633003235
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.104, 0.191, 0.303, 0.144, 0.143, 0.117]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3562 | Steps: 2 | Val loss: 0.2688 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4034 | Steps: 2 | Val loss: 0.3008 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4276 | Steps: 2 | Val loss: 0.3265 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6694 | Steps: 2 | Val loss: 0.5255 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 05:12:16 (running for 00:11:22.38)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.669 |  0.181 |                   44 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.426 |  0.176 |                   43 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.397 |  0.165 |                   20 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.356 |  0.149 |                   20 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=38408)[0m rmse: 0.1494661271572113
[2m[36m(func pid=38408)[0m mae:  0.1031491607427597
[2m[36m(func pid=38408)[0m rmse_per_class: [0.124, 0.224, 0.047, 0.278, 0.059, 0.169, 0.253, 0.11, 0.139, 0.092]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16430698335170746
[2m[36m(func pid=37799)[0m mae:  0.11917610466480255
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.253, 0.063, 0.316, 0.054, 0.18, 0.275, 0.132, 0.16, 0.104]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.1757417917251587
[2m[36m(func pid=32277)[0m mae:  0.12854750454425812
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.111, 0.334, 0.07, 0.189, 0.28, 0.136, 0.146, 0.103]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18064185976982117
[2m[36m(func pid=32174)[0m mae:  0.13307996094226837
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.104, 0.191, 0.303, 0.144, 0.143, 0.117]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3555 | Steps: 2 | Val loss: 0.2712 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3937 | Steps: 2 | Val loss: 0.3004 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4276 | Steps: 2 | Val loss: 0.3261 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.6574 | Steps: 2 | Val loss: 0.5220 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 05:12:21 (running for 00:11:27.54)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.669 |  0.181 |                   45 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.428 |  0.176 |                   44 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.403 |  0.164 |                   21 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.355 |  0.15  |                   21 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=38408)[0m rmse: 0.1501462161540985
[2m[36m(func pid=38408)[0m mae:  0.10341417789459229
[2m[36m(func pid=38408)[0m rmse_per_class: [0.116, 0.232, 0.046, 0.287, 0.059, 0.169, 0.244, 0.11, 0.147, 0.091]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16378693282604218
[2m[36m(func pid=37799)[0m mae:  0.11897361278533936
[2m[36m(func pid=37799)[0m rmse_per_class: [0.103, 0.253, 0.057, 0.316, 0.054, 0.179, 0.276, 0.133, 0.159, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17566505074501038
[2m[36m(func pid=32277)[0m mae:  0.12846465408802032
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.111, 0.334, 0.07, 0.189, 0.28, 0.136, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18061932921409607
[2m[36m(func pid=32174)[0m mae:  0.1330605000257492
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.104, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3476 | Steps: 2 | Val loss: 0.2808 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3923 | Steps: 2 | Val loss: 0.3003 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4247 | Steps: 2 | Val loss: 0.3258 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.6568 | Steps: 2 | Val loss: 0.5185 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=38408)[0m rmse: 0.1551407277584076
[2m[36m(func pid=38408)[0m mae:  0.10780604183673859
[2m[36m(func pid=38408)[0m rmse_per_class: [0.112, 0.223, 0.048, 0.318, 0.062, 0.174, 0.251, 0.107, 0.166, 0.091]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:12:26 (running for 00:11:32.76)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.657 |  0.181 |                   46 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.428 |  0.176 |                   45 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.392 |  0.164 |                   23 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.348 |  0.155 |                   22 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16391737759113312
[2m[36m(func pid=37799)[0m mae:  0.11909060180187225
[2m[36m(func pid=37799)[0m rmse_per_class: [0.101, 0.253, 0.055, 0.317, 0.055, 0.179, 0.277, 0.134, 0.157, 0.111]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17561334371566772
[2m[36m(func pid=32277)[0m mae:  0.128388449549675
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.111, 0.334, 0.07, 0.189, 0.28, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18058441579341888
[2m[36m(func pid=32174)[0m mae:  0.1330341100692749
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3534 | Steps: 2 | Val loss: 0.2843 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3912 | Steps: 2 | Val loss: 0.3004 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4250 | Steps: 2 | Val loss: 0.3254 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.6452 | Steps: 2 | Val loss: 0.5147 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=38408)[0m rmse: 0.15631014108657837
[2m[36m(func pid=38408)[0m mae:  0.1087532788515091
[2m[36m(func pid=38408)[0m rmse_per_class: [0.112, 0.216, 0.056, 0.332, 0.067, 0.172, 0.257, 0.105, 0.151, 0.095]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:12:31 (running for 00:11:37.82)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.657 |  0.181 |                   47 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.425 |  0.176 |                   46 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.391 |  0.164 |                   24 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.353 |  0.156 |                   23 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.1643327921628952
[2m[36m(func pid=37799)[0m mae:  0.1193065494298935
[2m[36m(func pid=37799)[0m rmse_per_class: [0.101, 0.253, 0.055, 0.318, 0.055, 0.179, 0.277, 0.136, 0.156, 0.114]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17552825808525085
[2m[36m(func pid=32277)[0m mae:  0.12832194566726685
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.111, 0.334, 0.069, 0.189, 0.28, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18054771423339844
[2m[36m(func pid=32174)[0m mae:  0.13300320506095886
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3388 | Steps: 2 | Val loss: 0.2769 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3907 | Steps: 2 | Val loss: 0.3007 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4269 | Steps: 2 | Val loss: 0.3251 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.6462 | Steps: 2 | Val loss: 0.5115 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=38408)[0m rmse: 0.15284809470176697
[2m[36m(func pid=38408)[0m mae:  0.10642411559820175
[2m[36m(func pid=38408)[0m rmse_per_class: [0.107, 0.218, 0.046, 0.313, 0.075, 0.167, 0.259, 0.105, 0.139, 0.1]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:12:36 (running for 00:11:42.89)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.645 |  0.181 |                   48 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.425 |  0.176 |                   47 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.391 |  0.165 |                   25 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.339 |  0.153 |                   24 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16495363414287567
[2m[36m(func pid=37799)[0m mae:  0.11966820061206818
[2m[36m(func pid=37799)[0m rmse_per_class: [0.101, 0.254, 0.055, 0.32, 0.055, 0.179, 0.277, 0.137, 0.155, 0.116]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17553921043872833
[2m[36m(func pid=32277)[0m mae:  0.12827914953231812
[2m[36m(func pid=32277)[0m rmse_per_class: [0.128, 0.261, 0.112, 0.334, 0.069, 0.189, 0.28, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18051683902740479
[2m[36m(func pid=32174)[0m mae:  0.1329774707555771
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3451 | Steps: 2 | Val loss: 0.2740 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3877 | Steps: 2 | Val loss: 0.3007 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4246 | Steps: 2 | Val loss: 0.3247 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.6417 | Steps: 2 | Val loss: 0.5084 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=38408)[0m rmse: 0.15212897956371307
[2m[36m(func pid=38408)[0m mae:  0.10557346045970917
[2m[36m(func pid=38408)[0m rmse_per_class: [0.098, 0.219, 0.042, 0.291, 0.091, 0.17, 0.262, 0.103, 0.135, 0.109]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:12:42 (running for 00:11:48.13)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.646 |  0.181 |                   49 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.427 |  0.176 |                   48 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.388 |  0.166 |                   26 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.345 |  0.152 |                   25 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16551131010055542
[2m[36m(func pid=37799)[0m mae:  0.11992254108190536
[2m[36m(func pid=37799)[0m rmse_per_class: [0.102, 0.254, 0.058, 0.322, 0.056, 0.179, 0.276, 0.138, 0.153, 0.118]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17545124888420105
[2m[36m(func pid=32277)[0m mae:  0.12818555533885956
[2m[36m(func pid=32277)[0m rmse_per_class: [0.128, 0.261, 0.112, 0.334, 0.069, 0.189, 0.28, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.1804695427417755
[2m[36m(func pid=32174)[0m mae:  0.1329437792301178
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3335 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3867 | Steps: 2 | Val loss: 0.3007 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4220 | Steps: 2 | Val loss: 0.3241 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=38408)[0m rmse: 0.15303343534469604
[2m[36m(func pid=38408)[0m mae:  0.10546894371509552
[2m[36m(func pid=38408)[0m rmse_per_class: [0.102, 0.22, 0.043, 0.285, 0.109, 0.172, 0.255, 0.101, 0.14, 0.103]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6387 | Steps: 2 | Val loss: 0.5054 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 05:12:47 (running for 00:11:53.50)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.642 |  0.18  |                   50 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.425 |  0.175 |                   49 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.387 |  0.166 |                   27 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.334 |  0.153 |                   26 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16604359447956085
[2m[36m(func pid=37799)[0m mae:  0.1201171725988388
[2m[36m(func pid=37799)[0m rmse_per_class: [0.104, 0.254, 0.062, 0.323, 0.056, 0.179, 0.275, 0.137, 0.151, 0.119]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17525488138198853
[2m[36m(func pid=32277)[0m mae:  0.12805387377738953
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.111, 0.333, 0.069, 0.188, 0.28, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18046577274799347
[2m[36m(func pid=32174)[0m mae:  0.13294073939323425
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3341 | Steps: 2 | Val loss: 0.2741 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4218 | Steps: 2 | Val loss: 0.3238 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3842 | Steps: 2 | Val loss: 0.3006 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=38408)[0m rmse: 0.15460321307182312
[2m[36m(func pid=38408)[0m mae:  0.10583461821079254
[2m[36m(func pid=38408)[0m rmse_per_class: [0.109, 0.223, 0.044, 0.29, 0.115, 0.169, 0.247, 0.102, 0.147, 0.1]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6338 | Steps: 2 | Val loss: 0.5025 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 05:12:52 (running for 00:11:58.79)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.639 |  0.18  |                   51 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.422 |  0.175 |                   50 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.384 |  0.167 |                   28 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.334 |  0.155 |                   27 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.17521126568317413
[2m[36m(func pid=32277)[0m mae:  0.12799005210399628
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.111, 0.333, 0.068, 0.188, 0.279, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16676218807697296
[2m[36m(func pid=37799)[0m mae:  0.12031185626983643
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.254, 0.069, 0.324, 0.057, 0.179, 0.273, 0.137, 0.149, 0.119]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18043620884418488
[2m[36m(func pid=32174)[0m mae:  0.13292020559310913
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.143, 0.143, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3346 | Steps: 2 | Val loss: 0.2756 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4219 | Steps: 2 | Val loss: 0.3235 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3854 | Steps: 2 | Val loss: 0.3005 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=38408)[0m rmse: 0.1549346148967743
[2m[36m(func pid=38408)[0m mae:  0.10599076747894287
[2m[36m(func pid=38408)[0m rmse_per_class: [0.113, 0.222, 0.044, 0.302, 0.104, 0.167, 0.245, 0.103, 0.15, 0.1]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.6304 | Steps: 2 | Val loss: 0.4993 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 05:12:57 (running for 00:12:03.90)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.634 |  0.18  |                   52 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.422 |  0.175 |                   51 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.385 |  0.167 |                   29 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.335 |  0.155 |                   28 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16723677515983582
[2m[36m(func pid=37799)[0m mae:  0.12036623805761337
[2m[36m(func pid=37799)[0m rmse_per_class: [0.107, 0.254, 0.074, 0.325, 0.058, 0.18, 0.272, 0.135, 0.148, 0.119]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17514468729496002
[2m[36m(func pid=32277)[0m mae:  0.12793183326721191
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.111, 0.333, 0.068, 0.188, 0.279, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.1803922951221466
[2m[36m(func pid=32174)[0m mae:  0.1328888237476349
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.143, 0.143, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3296 | Steps: 2 | Val loss: 0.2736 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4178 | Steps: 2 | Val loss: 0.3230 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3814 | Steps: 2 | Val loss: 0.2997 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=38408)[0m rmse: 0.15296286344528198
[2m[36m(func pid=38408)[0m mae:  0.10456068813800812
[2m[36m(func pid=38408)[0m rmse_per_class: [0.123, 0.213, 0.044, 0.304, 0.088, 0.166, 0.251, 0.102, 0.141, 0.097]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.6207 | Steps: 2 | Val loss: 0.4969 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 05:13:02 (running for 00:12:08.93)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.63  |  0.18  |                   53 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.422 |  0.175 |                   52 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.381 |  0.167 |                   30 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.33  |  0.153 |                   29 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16707322001457214
[2m[36m(func pid=37799)[0m mae:  0.12023041397333145
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.254, 0.074, 0.324, 0.059, 0.18, 0.271, 0.134, 0.149, 0.119]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17501351237297058
[2m[36m(func pid=32277)[0m mae:  0.127841979265213
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.11, 0.333, 0.068, 0.188, 0.279, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18038369715213776
[2m[36m(func pid=32174)[0m mae:  0.13287875056266785
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3351 | Steps: 2 | Val loss: 0.2692 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3808 | Steps: 2 | Val loss: 0.2993 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4195 | Steps: 2 | Val loss: 0.3227 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=38408)[0m rmse: 0.1482972800731659
[2m[36m(func pid=38408)[0m mae:  0.1019650474190712
[2m[36m(func pid=38408)[0m rmse_per_class: [0.1, 0.217, 0.043, 0.292, 0.076, 0.164, 0.258, 0.099, 0.137, 0.097]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.6164 | Steps: 2 | Val loss: 0.4935 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 05:13:07 (running for 00:12:14.03)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.621 |  0.18  |                   54 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.419 |  0.175 |                   54 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.381 |  0.167 |                   30 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.335 |  0.148 |                   30 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.1749458611011505
[2m[36m(func pid=32277)[0m mae:  0.127772718667984
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.11, 0.332, 0.068, 0.188, 0.279, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16708265244960785
[2m[36m(func pid=37799)[0m mae:  0.12011142820119858
[2m[36m(func pid=37799)[0m rmse_per_class: [0.107, 0.253, 0.076, 0.324, 0.061, 0.18, 0.269, 0.133, 0.149, 0.118]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18036368489265442
[2m[36m(func pid=32174)[0m mae:  0.1328597068786621
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3268 | Steps: 2 | Val loss: 0.2672 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4220 | Steps: 2 | Val loss: 0.3227 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3774 | Steps: 2 | Val loss: 0.2986 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=38408)[0m rmse: 0.14724640548229218
[2m[36m(func pid=38408)[0m mae:  0.10192947089672089
[2m[36m(func pid=38408)[0m rmse_per_class: [0.094, 0.211, 0.043, 0.282, 0.071, 0.166, 0.255, 0.099, 0.152, 0.099]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.6193 | Steps: 2 | Val loss: 0.4916 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 05:13:13 (running for 00:12:19.15)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.616 |  0.18  |                   55 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.422 |  0.175 |                   55 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.381 |  0.167 |                   31 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.327 |  0.147 |                   31 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.1749972403049469
[2m[36m(func pid=32277)[0m mae:  0.12774500250816345
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.111, 0.332, 0.067, 0.188, 0.279, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16655519604682922
[2m[36m(func pid=37799)[0m mae:  0.11979849636554718
[2m[36m(func pid=37799)[0m rmse_per_class: [0.108, 0.253, 0.073, 0.324, 0.062, 0.18, 0.268, 0.132, 0.149, 0.116]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.1803523302078247
[2m[36m(func pid=32174)[0m mae:  0.13284888863563538
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.103, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3421 | Steps: 2 | Val loss: 0.2688 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4204 | Steps: 2 | Val loss: 0.3223 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3756 | Steps: 2 | Val loss: 0.2977 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=38408)[0m rmse: 0.14902296662330627
[2m[36m(func pid=38408)[0m mae:  0.10294608026742935
[2m[36m(func pid=38408)[0m rmse_per_class: [0.105, 0.213, 0.044, 0.285, 0.069, 0.168, 0.25, 0.1, 0.154, 0.103]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6193 | Steps: 2 | Val loss: 0.4888 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=32277)[0m rmse: 0.17492464184761047
[2m[36m(func pid=32277)[0m mae:  0.1276620328426361
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.111, 0.332, 0.067, 0.188, 0.279, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
== Status ==
Current time: 2024-01-07 05:13:18 (running for 00:12:24.39)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.619 |  0.18  |                   56 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.42  |  0.175 |                   56 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.376 |  0.166 |                   33 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.342 |  0.149 |                   32 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16594427824020386
[2m[36m(func pid=37799)[0m mae:  0.11943133175373077
[2m[36m(func pid=37799)[0m rmse_per_class: [0.109, 0.252, 0.069, 0.324, 0.064, 0.18, 0.266, 0.131, 0.15, 0.114]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.1803068220615387
[2m[36m(func pid=32174)[0m mae:  0.1328153759241104
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3272 | Steps: 2 | Val loss: 0.2670 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4164 | Steps: 2 | Val loss: 0.3219 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3777 | Steps: 2 | Val loss: 0.2969 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.6131 | Steps: 2 | Val loss: 0.4860 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=38408)[0m rmse: 0.14859937131404877
[2m[36m(func pid=38408)[0m mae:  0.10107475519180298
[2m[36m(func pid=38408)[0m rmse_per_class: [0.117, 0.215, 0.05, 0.28, 0.074, 0.165, 0.247, 0.099, 0.138, 0.1]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:13:23 (running for 00:12:29.43)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.619 |  0.18  |                   57 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.416 |  0.175 |                   57 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.376 |  0.166 |                   33 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.327 |  0.149 |                   33 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.1748294234275818
[2m[36m(func pid=32277)[0m mae:  0.12759681046009064
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.111, 0.332, 0.067, 0.188, 0.279, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16542686522006989
[2m[36m(func pid=37799)[0m mae:  0.1190657839179039
[2m[36m(func pid=37799)[0m rmse_per_class: [0.109, 0.252, 0.066, 0.323, 0.065, 0.18, 0.265, 0.131, 0.151, 0.112]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18026800453662872
[2m[36m(func pid=32174)[0m mae:  0.13278751075267792
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3443 | Steps: 2 | Val loss: 0.2696 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4194 | Steps: 2 | Val loss: 0.3217 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3768 | Steps: 2 | Val loss: 0.2961 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=38408)[0m rmse: 0.1506279855966568
[2m[36m(func pid=38408)[0m mae:  0.10211149603128433
[2m[36m(func pid=38408)[0m rmse_per_class: [0.117, 0.212, 0.05, 0.29, 0.086, 0.165, 0.252, 0.098, 0.136, 0.101]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:13:28 (running for 00:12:34.58)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.613 |  0.18  |                   58 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.419 |  0.175 |                   58 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.378 |  0.165 |                   34 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.344 |  0.151 |                   34 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6109 | Steps: 2 | Val loss: 0.4838 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=32277)[0m rmse: 0.1747729778289795
[2m[36m(func pid=32277)[0m mae:  0.12754479050636292
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.111, 0.332, 0.067, 0.188, 0.279, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16483944654464722
[2m[36m(func pid=37799)[0m mae:  0.11866284906864166
[2m[36m(func pid=37799)[0m rmse_per_class: [0.109, 0.252, 0.063, 0.323, 0.066, 0.18, 0.264, 0.13, 0.151, 0.11]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m rmse: 0.18024680018424988
[2m[36m(func pid=32174)[0m mae:  0.1327691674232483
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3288 | Steps: 2 | Val loss: 0.2765 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4179 | Steps: 2 | Val loss: 0.3214 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3784 | Steps: 2 | Val loss: 0.2951 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=38408)[0m rmse: 0.15411841869354248
[2m[36m(func pid=38408)[0m mae:  0.1052965372800827
[2m[36m(func pid=38408)[0m rmse_per_class: [0.102, 0.214, 0.048, 0.307, 0.1, 0.165, 0.259, 0.097, 0.142, 0.107]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:13:33 (running for 00:12:39.74)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.611 |  0.18  |                   59 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.418 |  0.175 |                   59 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.377 |  0.165 |                   35 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.329 |  0.154 |                   35 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6046 | Steps: 2 | Val loss: 0.4815 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=32277)[0m rmse: 0.17472440004348755
[2m[36m(func pid=32277)[0m mae:  0.12748365104198456
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.261, 0.111, 0.332, 0.067, 0.188, 0.279, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16408105194568634
[2m[36m(func pid=37799)[0m mae:  0.11819237470626831
[2m[36m(func pid=37799)[0m rmse_per_class: [0.107, 0.251, 0.059, 0.323, 0.067, 0.18, 0.263, 0.13, 0.151, 0.109]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3204 | Steps: 2 | Val loss: 0.2778 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=32174)[0m rmse: 0.1802116334438324
[2m[36m(func pid=32174)[0m mae:  0.13274426758289337
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4212 | Steps: 2 | Val loss: 0.3215 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3743 | Steps: 2 | Val loss: 0.2946 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=38408)[0m rmse: 0.1553952544927597
[2m[36m(func pid=38408)[0m mae:  0.10611168295145035
[2m[36m(func pid=38408)[0m rmse_per_class: [0.115, 0.212, 0.041, 0.313, 0.106, 0.165, 0.254, 0.098, 0.15, 0.1]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:13:38 (running for 00:12:44.97)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.605 |  0.18  |                   60 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.418 |  0.175 |                   59 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.374 |  0.164 |                   37 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.32  |  0.155 |                   36 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16374985873699188
[2m[36m(func pid=37799)[0m mae:  0.11792083084583282
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.251, 0.058, 0.322, 0.068, 0.18, 0.263, 0.13, 0.151, 0.108]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17478826642036438
[2m[36m(func pid=32277)[0m mae:  0.12746870517730713
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.26, 0.112, 0.332, 0.067, 0.188, 0.279, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6002 | Steps: 2 | Val loss: 0.4795 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3235 | Steps: 2 | Val loss: 0.2730 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=32174)[0m rmse: 0.18021240830421448
[2m[36m(func pid=32174)[0m mae:  0.13274237513542175
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4187 | Steps: 2 | Val loss: 0.3212 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3726 | Steps: 2 | Val loss: 0.2939 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=38408)[0m rmse: 0.1534673422574997
[2m[36m(func pid=38408)[0m mae:  0.10400334745645523
[2m[36m(func pid=38408)[0m rmse_per_class: [0.117, 0.212, 0.043, 0.297, 0.109, 0.166, 0.248, 0.097, 0.15, 0.095]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:13:43 (running for 00:12:50.12)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.6   |  0.18  |                   61 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.419 |  0.175 |                   61 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.374 |  0.164 |                   37 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.324 |  0.153 |                   37 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.1747162789106369
[2m[36m(func pid=32277)[0m mae:  0.12740111351013184
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.26, 0.112, 0.332, 0.066, 0.188, 0.278, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16341088712215424
[2m[36m(func pid=37799)[0m mae:  0.11768309772014618
[2m[36m(func pid=37799)[0m rmse_per_class: [0.105, 0.25, 0.058, 0.322, 0.069, 0.179, 0.264, 0.13, 0.15, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.6012 | Steps: 2 | Val loss: 0.4772 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3221 | Steps: 2 | Val loss: 0.2690 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=32174)[0m rmse: 0.18018755316734314
[2m[36m(func pid=32174)[0m mae:  0.1327233761548996
[2m[36m(func pid=32174)[0m rmse_per_class: [0.115, 0.264, 0.099, 0.33, 0.102, 0.191, 0.3, 0.143, 0.143, 0.115]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4172 | Steps: 2 | Val loss: 0.3210 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3728 | Steps: 2 | Val loss: 0.2938 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=38408)[0m rmse: 0.1506805717945099
[2m[36m(func pid=38408)[0m mae:  0.10181272029876709
[2m[36m(func pid=38408)[0m rmse_per_class: [0.112, 0.211, 0.047, 0.283, 0.104, 0.169, 0.246, 0.097, 0.143, 0.096]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:13:49 (running for 00:12:55.17)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.601 |  0.18  |                   62 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.419 |  0.175 |                   61 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.373 |  0.163 |                   39 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.322 |  0.151 |                   38 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.17473800480365753
[2m[36m(func pid=32277)[0m mae:  0.12736988067626953
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.26, 0.112, 0.332, 0.066, 0.188, 0.278, 0.135, 0.146, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16339810192584991
[2m[36m(func pid=37799)[0m mae:  0.11760751903057098
[2m[36m(func pid=37799)[0m rmse_per_class: [0.105, 0.25, 0.059, 0.321, 0.069, 0.179, 0.264, 0.13, 0.15, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5956 | Steps: 2 | Val loss: 0.4737 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3247 | Steps: 2 | Val loss: 0.2674 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=32174)[0m rmse: 0.18012799322605133
[2m[36m(func pid=32174)[0m mae:  0.13267840445041656
[2m[36m(func pid=32174)[0m rmse_per_class: [0.115, 0.264, 0.099, 0.33, 0.102, 0.191, 0.3, 0.143, 0.143, 0.115]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3732 | Steps: 2 | Val loss: 0.2935 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4151 | Steps: 2 | Val loss: 0.3207 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=38408)[0m rmse: 0.1491589993238449
[2m[36m(func pid=38408)[0m mae:  0.10098975896835327
[2m[36m(func pid=38408)[0m rmse_per_class: [0.111, 0.21, 0.047, 0.278, 0.091, 0.171, 0.244, 0.097, 0.143, 0.099]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:13:54 (running for 00:13:00.29)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.596 |  0.18  |                   63 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.417 |  0.175 |                   62 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.373 |  0.163 |                   40 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.325 |  0.149 |                   39 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16334733366966248
[2m[36m(func pid=37799)[0m mae:  0.11757199466228485
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.25, 0.059, 0.32, 0.069, 0.179, 0.265, 0.129, 0.15, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5959 | Steps: 2 | Val loss: 0.4715 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=32277)[0m rmse: 0.17468565702438354
[2m[36m(func pid=32277)[0m mae:  0.12731146812438965
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.26, 0.112, 0.332, 0.066, 0.188, 0.278, 0.135, 0.147, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3260 | Steps: 2 | Val loss: 0.2693 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=32174)[0m rmse: 0.1801043450832367
[2m[36m(func pid=32174)[0m mae:  0.13266441226005554
[2m[36m(func pid=32174)[0m rmse_per_class: [0.115, 0.264, 0.099, 0.33, 0.101, 0.191, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3743 | Steps: 2 | Val loss: 0.2932 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4178 | Steps: 2 | Val loss: 0.3205 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=38408)[0m rmse: 0.14988675713539124
[2m[36m(func pid=38408)[0m mae:  0.10209678113460541
[2m[36m(func pid=38408)[0m rmse_per_class: [0.119, 0.209, 0.042, 0.294, 0.079, 0.166, 0.243, 0.098, 0.149, 0.099]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:13:59 (running for 00:13:05.30)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.596 |  0.18  |                   64 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.415 |  0.175 |                   63 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.374 |  0.163 |                   41 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.326 |  0.15  |                   40 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16323432326316833
[2m[36m(func pid=37799)[0m mae:  0.11753978580236435
[2m[36m(func pid=37799)[0m rmse_per_class: [0.104, 0.25, 0.058, 0.319, 0.069, 0.179, 0.266, 0.129, 0.151, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5920 | Steps: 2 | Val loss: 0.4689 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=32277)[0m rmse: 0.17465156316757202
[2m[36m(func pid=32277)[0m mae:  0.12724724411964417
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.26, 0.112, 0.332, 0.066, 0.188, 0.278, 0.135, 0.147, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3229 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=32174)[0m rmse: 0.18004979193210602
[2m[36m(func pid=32174)[0m mae:  0.13262459635734558
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.101, 0.191, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3708 | Steps: 2 | Val loss: 0.2932 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4159 | Steps: 2 | Val loss: 0.3204 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=38408)[0m rmse: 0.1510469764471054
[2m[36m(func pid=38408)[0m mae:  0.10339231789112091
[2m[36m(func pid=38408)[0m rmse_per_class: [0.117, 0.208, 0.041, 0.313, 0.071, 0.163, 0.244, 0.099, 0.159, 0.096]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:14:04 (running for 00:13:10.53)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.592 |  0.18  |                   65 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.418 |  0.175 |                   64 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.371 |  0.163 |                   42 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.323 |  0.151 |                   41 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16327641904354095
[2m[36m(func pid=37799)[0m mae:  0.11755441129207611
[2m[36m(func pid=37799)[0m rmse_per_class: [0.104, 0.249, 0.059, 0.319, 0.069, 0.179, 0.266, 0.129, 0.151, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.5935 | Steps: 2 | Val loss: 0.4669 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=32277)[0m rmse: 0.17463816702365875
[2m[36m(func pid=32277)[0m mae:  0.1272214651107788
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.26, 0.112, 0.332, 0.066, 0.188, 0.278, 0.135, 0.147, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3164 | Steps: 2 | Val loss: 0.2704 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=32174)[0m rmse: 0.18002235889434814
[2m[36m(func pid=32174)[0m mae:  0.1325986087322235
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.101, 0.19, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3723 | Steps: 2 | Val loss: 0.2929 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4147 | Steps: 2 | Val loss: 0.3202 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=38408)[0m rmse: 0.14895106852054596
[2m[36m(func pid=38408)[0m mae:  0.10230646282434464
[2m[36m(func pid=38408)[0m rmse_per_class: [0.108, 0.207, 0.041, 0.3, 0.066, 0.162, 0.252, 0.097, 0.162, 0.094]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.5887 | Steps: 2 | Val loss: 0.4653 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=37799)[0m rmse: 0.16318240761756897
[2m[36m(func pid=37799)[0m mae:  0.11742638051509857
[2m[36m(func pid=37799)[0m rmse_per_class: [0.104, 0.249, 0.06, 0.319, 0.069, 0.178, 0.266, 0.129, 0.151, 0.107]
[2m[36m(func pid=37799)[0m 
== Status ==
Current time: 2024-01-07 05:14:09 (running for 00:13:15.70)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.594 |  0.18  |                   66 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.416 |  0.175 |                   65 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.372 |  0.163 |                   43 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.316 |  0.149 |                   42 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32277)[0m rmse: 0.17457513511180878
[2m[36m(func pid=32277)[0m mae:  0.1271965205669403
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.26, 0.112, 0.332, 0.066, 0.188, 0.278, 0.135, 0.147, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3194 | Steps: 2 | Val loss: 0.2662 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=32174)[0m rmse: 0.18002143502235413
[2m[36m(func pid=32174)[0m mae:  0.13259512186050415
[2m[36m(func pid=32174)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.101, 0.19, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3708 | Steps: 2 | Val loss: 0.2924 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4128 | Steps: 2 | Val loss: 0.3196 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=38408)[0m rmse: 0.1464807540178299
[2m[36m(func pid=38408)[0m mae:  0.09953784942626953
[2m[36m(func pid=38408)[0m rmse_per_class: [0.113, 0.207, 0.046, 0.287, 0.066, 0.161, 0.251, 0.096, 0.142, 0.094]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:14:14 (running for 00:13:20.81)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.589 |  0.18  |                   67 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.415 |  0.175 |                   66 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.371 |  0.163 |                   44 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.319 |  0.146 |                   43 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16294273734092712
[2m[36m(func pid=37799)[0m mae:  0.1171819344162941
[2m[36m(func pid=37799)[0m rmse_per_class: [0.103, 0.249, 0.06, 0.318, 0.068, 0.178, 0.266, 0.129, 0.15, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.5869 | Steps: 2 | Val loss: 0.4630 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=32277)[0m rmse: 0.17437316477298737
[2m[36m(func pid=32277)[0m mae:  0.12708456814289093
[2m[36m(func pid=32277)[0m rmse_per_class: [0.126, 0.26, 0.111, 0.332, 0.066, 0.188, 0.278, 0.135, 0.147, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3205 | Steps: 2 | Val loss: 0.2648 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=32174)[0m rmse: 0.17999453842639923
[2m[36m(func pid=32174)[0m mae:  0.13257566094398499
[2m[36m(func pid=32174)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.101, 0.19, 0.3, 0.142, 0.144, 0.115]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3715 | Steps: 2 | Val loss: 0.2919 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4144 | Steps: 2 | Val loss: 0.3194 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=38408)[0m rmse: 0.1463290899991989
[2m[36m(func pid=38408)[0m mae:  0.09907041490077972
[2m[36m(func pid=38408)[0m rmse_per_class: [0.116, 0.209, 0.048, 0.273, 0.068, 0.163, 0.249, 0.099, 0.141, 0.097]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:14:19 (running for 00:13:25.94)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.587 |  0.18  |                   68 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   67 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.372 |  0.163 |                   45 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.321 |  0.146 |                   44 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.1626056581735611
[2m[36m(func pid=37799)[0m mae:  0.11685548722743988
[2m[36m(func pid=37799)[0m rmse_per_class: [0.103, 0.248, 0.06, 0.318, 0.068, 0.178, 0.266, 0.129, 0.149, 0.108]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.5705 | Steps: 2 | Val loss: 0.4599 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=32277)[0m rmse: 0.17430154979228973
[2m[36m(func pid=32277)[0m mae:  0.12702614068984985
[2m[36m(func pid=32277)[0m rmse_per_class: [0.125, 0.26, 0.111, 0.332, 0.065, 0.188, 0.277, 0.135, 0.147, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3135 | Steps: 2 | Val loss: 0.2655 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=32174)[0m rmse: 0.17996546626091003
[2m[36m(func pid=32174)[0m mae:  0.13255123794078827
[2m[36m(func pid=32174)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3727 | Steps: 2 | Val loss: 0.2915 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4144 | Steps: 2 | Val loss: 0.3194 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=38408)[0m rmse: 0.14691533148288727
[2m[36m(func pid=38408)[0m mae:  0.09994306415319443
[2m[36m(func pid=38408)[0m rmse_per_class: [0.111, 0.209, 0.042, 0.276, 0.073, 0.164, 0.246, 0.101, 0.144, 0.103]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:14:25 (running for 00:13:31.29)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.571 |  0.18  |                   69 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.414 |  0.174 |                   68 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.373 |  0.162 |                   46 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.313 |  0.147 |                   45 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16238734126091003
[2m[36m(func pid=37799)[0m mae:  0.11666212975978851
[2m[36m(func pid=37799)[0m rmse_per_class: [0.103, 0.249, 0.058, 0.317, 0.068, 0.178, 0.265, 0.128, 0.148, 0.11]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.5778 | Steps: 2 | Val loss: 0.4591 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=32277)[0m rmse: 0.17439071834087372
[2m[36m(func pid=32277)[0m mae:  0.12704399228096008
[2m[36m(func pid=32277)[0m rmse_per_class: [0.126, 0.26, 0.111, 0.332, 0.065, 0.188, 0.277, 0.135, 0.147, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3241 | Steps: 2 | Val loss: 0.2698 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=32174)[0m rmse: 0.1799854040145874
[2m[36m(func pid=32174)[0m mae:  0.13255977630615234
[2m[36m(func pid=32174)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.101, 0.19, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3712 | Steps: 2 | Val loss: 0.2914 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4125 | Steps: 2 | Val loss: 0.3192 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=38408)[0m rmse: 0.1496216356754303
[2m[36m(func pid=38408)[0m mae:  0.10186094045639038
[2m[36m(func pid=38408)[0m rmse_per_class: [0.109, 0.209, 0.041, 0.294, 0.084, 0.164, 0.244, 0.098, 0.15, 0.104]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:14:30 (running for 00:13:36.44)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.578 |  0.18  |                   70 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.414 |  0.174 |                   69 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.371 |  0.162 |                   47 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.324 |  0.15  |                   46 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.1622941493988037
[2m[36m(func pid=37799)[0m mae:  0.11664865911006927
[2m[36m(func pid=37799)[0m rmse_per_class: [0.104, 0.248, 0.056, 0.318, 0.068, 0.178, 0.264, 0.128, 0.147, 0.111]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.5762 | Steps: 2 | Val loss: 0.4574 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=32277)[0m rmse: 0.1744222193956375
[2m[36m(func pid=32277)[0m mae:  0.12702450156211853
[2m[36m(func pid=32277)[0m rmse_per_class: [0.126, 0.26, 0.112, 0.332, 0.065, 0.188, 0.277, 0.135, 0.147, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3094 | Steps: 2 | Val loss: 0.2779 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=32174)[0m rmse: 0.17994554340839386
[2m[36m(func pid=32174)[0m mae:  0.13252875208854675
[2m[36m(func pid=32174)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.101, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3674 | Steps: 2 | Val loss: 0.2916 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4122 | Steps: 2 | Val loss: 0.3190 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=38408)[0m rmse: 0.15494461357593536
[2m[36m(func pid=38408)[0m mae:  0.10542663186788559
[2m[36m(func pid=38408)[0m rmse_per_class: [0.123, 0.209, 0.042, 0.314, 0.092, 0.165, 0.247, 0.096, 0.164, 0.098]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:14:35 (running for 00:13:41.74)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.576 |  0.18  |                   71 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   70 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.367 |  0.163 |                   48 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.309 |  0.155 |                   47 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16256104409694672
[2m[36m(func pid=37799)[0m mae:  0.11674080789089203
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.249, 0.056, 0.318, 0.067, 0.178, 0.265, 0.128, 0.147, 0.113]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.5649 | Steps: 2 | Val loss: 0.4562 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=32277)[0m rmse: 0.17437653243541718
[2m[36m(func pid=32277)[0m mae:  0.1269875466823578
[2m[36m(func pid=32277)[0m rmse_per_class: [0.126, 0.26, 0.111, 0.332, 0.065, 0.188, 0.277, 0.135, 0.147, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3123 | Steps: 2 | Val loss: 0.2772 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=32174)[0m rmse: 0.17994388937950134
[2m[36m(func pid=32174)[0m mae:  0.13252444565296173
[2m[36m(func pid=32174)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.101, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3683 | Steps: 2 | Val loss: 0.2914 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4141 | Steps: 2 | Val loss: 0.3190 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=38408)[0m rmse: 0.1553710550069809
[2m[36m(func pid=38408)[0m mae:  0.10504579544067383
[2m[36m(func pid=38408)[0m rmse_per_class: [0.13, 0.209, 0.043, 0.308, 0.099, 0.164, 0.253, 0.098, 0.159, 0.091]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.5671 | Steps: 2 | Val loss: 0.4538 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 05:14:40 (running for 00:13:47.05)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.565 |  0.18  |                   72 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.412 |  0.174 |                   71 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.368 |  0.163 |                   49 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.312 |  0.155 |                   48 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16258300840854645
[2m[36m(func pid=37799)[0m mae:  0.11678425222635269
[2m[36m(func pid=37799)[0m rmse_per_class: [0.108, 0.248, 0.055, 0.317, 0.067, 0.178, 0.265, 0.127, 0.146, 0.115]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.1743984967470169
[2m[36m(func pid=32277)[0m mae:  0.1269807070493698
[2m[36m(func pid=32277)[0m rmse_per_class: [0.127, 0.26, 0.112, 0.332, 0.065, 0.188, 0.277, 0.135, 0.147, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3101 | Steps: 2 | Val loss: 0.2694 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=32174)[0m rmse: 0.1799280345439911
[2m[36m(func pid=32174)[0m mae:  0.13251568377017975
[2m[36m(func pid=32174)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.101, 0.19, 0.299, 0.142, 0.144, 0.114]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3726 | Steps: 2 | Val loss: 0.2915 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4102 | Steps: 2 | Val loss: 0.3187 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=38408)[0m rmse: 0.15070989727973938
[2m[36m(func pid=38408)[0m mae:  0.10110950469970703
[2m[36m(func pid=38408)[0m rmse_per_class: [0.12, 0.21, 0.048, 0.289, 0.096, 0.162, 0.253, 0.097, 0.143, 0.09]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5745 | Steps: 2 | Val loss: 0.4521 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 05:14:46 (running for 00:13:52.29)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   73 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.414 |  0.174 |                   72 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.373 |  0.163 |                   50 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.31  |  0.151 |                   49 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16270339488983154
[2m[36m(func pid=37799)[0m mae:  0.1168048158288002
[2m[36m(func pid=37799)[0m rmse_per_class: [0.111, 0.249, 0.054, 0.316, 0.066, 0.178, 0.265, 0.127, 0.146, 0.115]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17431879043579102
[2m[36m(func pid=32277)[0m mae:  0.1269267201423645
[2m[36m(func pid=32277)[0m rmse_per_class: [0.126, 0.26, 0.111, 0.332, 0.065, 0.187, 0.277, 0.135, 0.147, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3153 | Steps: 2 | Val loss: 0.2637 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=32174)[0m rmse: 0.17988701164722443
[2m[36m(func pid=32174)[0m mae:  0.13248327374458313
[2m[36m(func pid=32174)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.299, 0.142, 0.144, 0.114]
[2m[36m(func pid=32174)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3684 | Steps: 2 | Val loss: 0.2912 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4090 | Steps: 2 | Val loss: 0.3183 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=38408)[0m rmse: 0.14659586548805237
[2m[36m(func pid=38408)[0m mae:  0.09777425229549408
[2m[36m(func pid=38408)[0m rmse_per_class: [0.108, 0.21, 0.05, 0.274, 0.09, 0.161, 0.25, 0.096, 0.135, 0.091]
[2m[36m(func pid=38408)[0m 
== Status ==
Current time: 2024-01-07 05:14:51 (running for 00:13:57.32)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1602500006556511
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00004 | RUNNING    | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.574 |  0.18  |                   74 |
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.41  |  0.174 |                   73 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.368 |  0.163 |                   51 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.315 |  0.147 |                   50 |
| train_5a6ec_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=32174)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.5669 | Steps: 2 | Val loss: 0.4507 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=37799)[0m rmse: 0.16255494952201843
[2m[36m(func pid=37799)[0m mae:  0.11672697216272354
[2m[36m(func pid=37799)[0m rmse_per_class: [0.11, 0.248, 0.054, 0.316, 0.066, 0.178, 0.265, 0.127, 0.147, 0.116]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.17416921257972717
[2m[36m(func pid=32277)[0m mae:  0.12684786319732666
[2m[36m(func pid=32277)[0m rmse_per_class: [0.126, 0.26, 0.11, 0.332, 0.065, 0.187, 0.277, 0.135, 0.148, 0.102]
[2m[36m(func pid=32277)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3153 | Steps: 2 | Val loss: 0.2620 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=32174)[0m rmse: 0.17988494038581848
[2m[36m(func pid=32174)[0m mae:  0.13247904181480408
[2m[36m(func pid=32174)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.299, 0.142, 0.144, 0.114]
[2m[36m(func pid=32277)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4129 | Steps: 2 | Val loss: 0.3184 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3694 | Steps: 2 | Val loss: 0.2910 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=38408)[0m rmse: 0.14505235850811005
[2m[36m(func pid=38408)[0m mae:  0.09699765592813492
[2m[36m(func pid=38408)[0m rmse_per_class: [0.108, 0.21, 0.042, 0.273, 0.087, 0.161, 0.244, 0.096, 0.134, 0.095]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=32277)[0m rmse: 0.1742134988307953
[2m[36m(func pid=32277)[0m mae:  0.12685398757457733
[2m[36m(func pid=32277)[0m rmse_per_class: [0.126, 0.26, 0.11, 0.332, 0.065, 0.187, 0.277, 0.135, 0.148, 0.102]
[2m[36m(func pid=37799)[0m rmse: 0.1622791886329651
[2m[36m(func pid=37799)[0m mae:  0.11664444208145142
[2m[36m(func pid=37799)[0m rmse_per_class: [0.108, 0.248, 0.052, 0.316, 0.066, 0.177, 0.265, 0.127, 0.147, 0.117]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3091 | Steps: 2 | Val loss: 0.2662 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3676 | Steps: 2 | Val loss: 0.2910 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=38408)[0m rmse: 0.14835378527641296
[2m[36m(func pid=38408)[0m mae:  0.10004828870296478
[2m[36m(func pid=38408)[0m rmse_per_class: [0.117, 0.21, 0.041, 0.282, 0.086, 0.164, 0.242, 0.096, 0.144, 0.101]
== Status ==
Current time: 2024-01-07 05:14:56 (running for 00:14:02.81)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: -0.16300000250339508
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.409 |  0.174 |                   74 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.369 |  0.162 |                   52 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.315 |  0.145 |                   51 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.162206768989563
[2m[36m(func pid=37799)[0m mae:  0.1165991798043251
[2m[36m(func pid=37799)[0m rmse_per_class: [0.108, 0.247, 0.052, 0.317, 0.065, 0.177, 0.265, 0.127, 0.147, 0.116]
== Status ==
Current time: 2024-01-07 05:15:01 (running for 00:14:07.96)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: -0.16300000250339508
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00005 | RUNNING    | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.409 |  0.174 |                   74 |
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.369 |  0.162 |                   52 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.309 |  0.148 |                   52 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=49621)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=49621)[0m Configuration completed!
[2m[36m(func pid=49621)[0m New optimizer parameters:
[2m[36m(func pid=49621)[0m SGD (
[2m[36m(func pid=49621)[0m Parameter Group 0
[2m[36m(func pid=49621)[0m     dampening: 0
[2m[36m(func pid=49621)[0m     differentiable: False
[2m[36m(func pid=49621)[0m     foreach: None
[2m[36m(func pid=49621)[0m     lr: 0.0001
[2m[36m(func pid=49621)[0m     maximize: False
[2m[36m(func pid=49621)[0m     momentum: 0.99
[2m[36m(func pid=49621)[0m     nesterov: False
[2m[36m(func pid=49621)[0m     weight_decay: 0.0001
[2m[36m(func pid=49621)[0m )
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3694 | Steps: 2 | Val loss: 0.2908 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3095 | Steps: 2 | Val loss: 0.2765 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0458 | Steps: 2 | Val loss: 0.7188 | Batch size: 32 | lr: 0.0001 | Duration: 4.52s
[2m[36m(func pid=37799)[0m rmse: 0.16196686029434204
[2m[36m(func pid=37799)[0m mae:  0.1164366751909256
[2m[36m(func pid=37799)[0m rmse_per_class: [0.107, 0.247, 0.052, 0.318, 0.065, 0.177, 0.264, 0.127, 0.148, 0.114]
[2m[36m(func pid=38408)[0m rmse: 0.15489938855171204
[2m[36m(func pid=38408)[0m mae:  0.10525836050510406
[2m[36m(func pid=38408)[0m rmse_per_class: [0.134, 0.208, 0.041, 0.303, 0.083, 0.166, 0.25, 0.095, 0.164, 0.106]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17988383769989014
[2m[36m(func pid=49621)[0m mae:  0.13240793347358704
[2m[36m(func pid=49621)[0m rmse_per_class: [0.114, 0.263, 0.097, 0.333, 0.1, 0.191, 0.298, 0.143, 0.141, 0.119]
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3044 | Steps: 2 | Val loss: 0.2738 | Batch size: 32 | lr: 0.1 | Duration: 2.61s
== Status ==
Current time: 2024-01-07 05:15:07 (running for 00:14:13.30)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.368 |  0.162 |                   53 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.309 |  0.155 |                   53 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50153)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=50153)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=50153)[0m Configuration completed!
[2m[36m(func pid=50153)[0m New optimizer parameters:
[2m[36m(func pid=50153)[0m SGD (
[2m[36m(func pid=50153)[0m Parameter Group 0
[2m[36m(func pid=50153)[0m     dampening: 0
[2m[36m(func pid=50153)[0m     differentiable: False
[2m[36m(func pid=50153)[0m     foreach: None
[2m[36m(func pid=50153)[0m     lr: 0.001
[2m[36m(func pid=50153)[0m     maximize: False
[2m[36m(func pid=50153)[0m     momentum: 0.99
[2m[36m(func pid=50153)[0m     nesterov: False
[2m[36m(func pid=50153)[0m     weight_decay: 0.0001
[2m[36m(func pid=50153)[0m )
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.15219208598136902
[2m[36m(func pid=38408)[0m mae:  0.1032065600156784
[2m[36m(func pid=38408)[0m rmse_per_class: [0.122, 0.207, 0.042, 0.302, 0.083, 0.163, 0.248, 0.095, 0.156, 0.104]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3665 | Steps: 2 | Val loss: 0.2905 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0422 | Steps: 2 | Val loss: 0.7187 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0396 | Steps: 2 | Val loss: 0.7131 | Batch size: 32 | lr: 0.001 | Duration: 4.47s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3057 | Steps: 2 | Val loss: 0.2702 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 05:15:15 (running for 00:14:21.48)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.367 |  0.162 |                   55 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.304 |  0.152 |                   54 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  1.046 |  0.18  |                    1 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16176077723503113
[2m[36m(func pid=37799)[0m mae:  0.11618313938379288
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.247, 0.053, 0.318, 0.065, 0.177, 0.264, 0.127, 0.148, 0.113]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1804002970457077
[2m[36m(func pid=49621)[0m mae:  0.13285335898399353
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.17989341914653778
[2m[36m(func pid=50153)[0m mae:  0.13241145014762878
[2m[36m(func pid=50153)[0m rmse_per_class: [0.115, 0.263, 0.097, 0.333, 0.1, 0.191, 0.298, 0.143, 0.141, 0.119]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14993436634540558
[2m[36m(func pid=38408)[0m mae:  0.10110459476709366
[2m[36m(func pid=38408)[0m rmse_per_class: [0.126, 0.206, 0.041, 0.296, 0.08, 0.161, 0.248, 0.095, 0.147, 0.098]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3667 | Steps: 2 | Val loss: 0.2902 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0358 | Steps: 2 | Val loss: 0.7225 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3247 | Steps: 2 | Val loss: 0.2655 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0089 | Steps: 2 | Val loss: 0.7038 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 05:15:20 (running for 00:14:26.66)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.367 |  0.162 |                   56 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.306 |  0.15  |                   55 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  1.042 |  0.18  |                    2 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  1.04  |  0.18  |                    1 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16158103942871094
[2m[36m(func pid=37799)[0m mae:  0.11603764444589615
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.246, 0.054, 0.318, 0.065, 0.177, 0.264, 0.126, 0.149, 0.112]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1806446611881256
[2m[36m(func pid=49621)[0m mae:  0.13307654857635498
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.105, 0.191, 0.3, 0.142, 0.143, 0.116]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14736495912075043
[2m[36m(func pid=38408)[0m mae:  0.09907034784555435
[2m[36m(func pid=38408)[0m rmse_per_class: [0.121, 0.206, 0.041, 0.284, 0.082, 0.16, 0.248, 0.095, 0.145, 0.091]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.1803485006093979
[2m[36m(func pid=50153)[0m mae:  0.13281533122062683
[2m[36m(func pid=50153)[0m rmse_per_class: [0.114, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3669 | Steps: 2 | Val loss: 0.2896 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.0264 | Steps: 2 | Val loss: 0.7261 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3116 | Steps: 2 | Val loss: 0.2653 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.9525 | Steps: 2 | Val loss: 0.6884 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 05:15:25 (running for 00:14:31.88)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.367 |  0.161 |                   57 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.325 |  0.147 |                   56 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  1.036 |  0.181 |                    3 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  1.009 |  0.18  |                    2 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.1611328423023224
[2m[36m(func pid=37799)[0m mae:  0.11573150008916855
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.245, 0.053, 0.318, 0.065, 0.177, 0.263, 0.126, 0.148, 0.111]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1807682365179062
[2m[36m(func pid=49621)[0m mae:  0.13319949805736542
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.331, 0.106, 0.192, 0.302, 0.142, 0.143, 0.115]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14750808477401733
[2m[36m(func pid=38408)[0m mae:  0.09896635264158249
[2m[36m(func pid=38408)[0m rmse_per_class: [0.109, 0.207, 0.052, 0.281, 0.087, 0.16, 0.246, 0.096, 0.149, 0.088]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18055205047130585
[2m[36m(func pid=50153)[0m mae:  0.132997065782547
[2m[36m(func pid=50153)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.104, 0.191, 0.299, 0.142, 0.143, 0.115]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3687 | Steps: 2 | Val loss: 0.2890 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.0138 | Steps: 2 | Val loss: 0.7276 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3114 | Steps: 2 | Val loss: 0.2655 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8732 | Steps: 2 | Val loss: 0.6619 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 05:15:30 (running for 00:14:36.90)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.367 |  0.161 |                   57 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.312 |  0.148 |                   57 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  1.014 |  0.181 |                    5 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.953 |  0.181 |                    3 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16070492565631866
[2m[36m(func pid=37799)[0m mae:  0.11536884307861328
[2m[36m(func pid=37799)[0m rmse_per_class: [0.105, 0.245, 0.053, 0.318, 0.065, 0.176, 0.262, 0.126, 0.148, 0.11]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.18085356056690216
[2m[36m(func pid=49621)[0m mae:  0.13328124582767487
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.107, 0.192, 0.303, 0.142, 0.143, 0.116]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18059757351875305
[2m[36m(func pid=50153)[0m mae:  0.1330423653125763
[2m[36m(func pid=50153)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.332, 0.105, 0.191, 0.3, 0.141, 0.143, 0.114]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.1479092240333557
[2m[36m(func pid=38408)[0m mae:  0.09867078065872192
[2m[36m(func pid=38408)[0m rmse_per_class: [0.098, 0.206, 0.065, 0.285, 0.089, 0.161, 0.243, 0.096, 0.146, 0.09]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.9963 | Steps: 2 | Val loss: 0.7274 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3676 | Steps: 2 | Val loss: 0.2887 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3041 | Steps: 2 | Val loss: 0.2684 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.7851 | Steps: 2 | Val loss: 0.6244 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 05:15:36 (running for 00:14:42.13)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.369 |  0.161 |                   58 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.311 |  0.148 |                   58 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.996 |  0.181 |                    6 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.873 |  0.181 |                    4 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.18093232810497284
[2m[36m(func pid=49621)[0m mae:  0.13334763050079346
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.107, 0.191, 0.303, 0.142, 0.142, 0.116]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16057942807674408
[2m[36m(func pid=37799)[0m mae:  0.11522893607616425
[2m[36m(func pid=37799)[0m rmse_per_class: [0.104, 0.245, 0.053, 0.318, 0.065, 0.176, 0.262, 0.125, 0.148, 0.11]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.1484716236591339
[2m[36m(func pid=38408)[0m mae:  0.09980733692646027
[2m[36m(func pid=38408)[0m rmse_per_class: [0.104, 0.206, 0.045, 0.293, 0.093, 0.162, 0.244, 0.098, 0.146, 0.095]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18058349192142487
[2m[36m(func pid=50153)[0m mae:  0.13302293419837952
[2m[36m(func pid=50153)[0m rmse_per_class: [0.113, 0.264, 0.103, 0.332, 0.105, 0.191, 0.3, 0.141, 0.143, 0.114]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.9825 | Steps: 2 | Val loss: 0.7238 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3687 | Steps: 2 | Val loss: 0.2886 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.6975 | Steps: 2 | Val loss: 0.5780 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3094 | Steps: 2 | Val loss: 0.2731 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 05:15:41 (running for 00:14:47.42)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.369 |  0.16  |                   60 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.304 |  0.148 |                   59 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.996 |  0.181 |                    6 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.785 |  0.181 |                    5 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.1604471653699875
[2m[36m(func pid=37799)[0m mae:  0.11520850658416748
[2m[36m(func pid=37799)[0m rmse_per_class: [0.103, 0.244, 0.052, 0.318, 0.066, 0.176, 0.262, 0.125, 0.148, 0.11]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.18099485337734222
[2m[36m(func pid=49621)[0m mae:  0.13340207934379578
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.108, 0.191, 0.304, 0.142, 0.142, 0.116]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18045766651630402
[2m[36m(func pid=50153)[0m mae:  0.13289891183376312
[2m[36m(func pid=50153)[0m rmse_per_class: [0.114, 0.264, 0.104, 0.331, 0.105, 0.191, 0.3, 0.14, 0.143, 0.113]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.1532628983259201
[2m[36m(func pid=38408)[0m mae:  0.10247582197189331
[2m[36m(func pid=38408)[0m rmse_per_class: [0.14, 0.205, 0.041, 0.3, 0.093, 0.165, 0.248, 0.096, 0.145, 0.1]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3697 | Steps: 2 | Val loss: 0.2887 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.9625 | Steps: 2 | Val loss: 0.7168 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.6254 | Steps: 2 | Val loss: 0.5280 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 05:15:46 (running for 00:14:52.57)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.369 |  0.16  |                   60 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.309 |  0.153 |                   60 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.963 |  0.181 |                    8 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.697 |  0.18  |                    6 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3228 | Steps: 2 | Val loss: 0.2747 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=49621)[0m rmse: 0.1810101568698883
[2m[36m(func pid=49621)[0m mae:  0.13341297209262848
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.33, 0.108, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16062185168266296
[2m[36m(func pid=37799)[0m mae:  0.11527121067047119
[2m[36m(func pid=37799)[0m rmse_per_class: [0.104, 0.244, 0.053, 0.318, 0.066, 0.176, 0.263, 0.124, 0.148, 0.11]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18027234077453613
[2m[36m(func pid=50153)[0m mae:  0.1327112466096878
[2m[36m(func pid=50153)[0m rmse_per_class: [0.114, 0.264, 0.105, 0.332, 0.105, 0.19, 0.299, 0.139, 0.144, 0.111]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.1546122431755066
[2m[36m(func pid=38408)[0m mae:  0.10370011627674103
[2m[36m(func pid=38408)[0m rmse_per_class: [0.142, 0.205, 0.041, 0.301, 0.096, 0.164, 0.25, 0.095, 0.154, 0.098]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.9423 | Steps: 2 | Val loss: 0.7072 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3693 | Steps: 2 | Val loss: 0.2886 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3099 | Steps: 2 | Val loss: 0.2689 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.5628 | Steps: 2 | Val loss: 0.4790 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 05:15:51 (running for 00:14:57.76)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.37  |  0.161 |                   61 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.323 |  0.155 |                   61 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.942 |  0.181 |                    9 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.625 |  0.18  |                    7 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.18101796507835388
[2m[36m(func pid=49621)[0m mae:  0.13341554999351501
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.329, 0.108, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16057059168815613
[2m[36m(func pid=37799)[0m mae:  0.11525305360555649
[2m[36m(func pid=37799)[0m rmse_per_class: [0.104, 0.244, 0.053, 0.318, 0.066, 0.176, 0.263, 0.124, 0.148, 0.11]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.1800771802663803
[2m[36m(func pid=50153)[0m mae:  0.1324910819530487
[2m[36m(func pid=50153)[0m rmse_per_class: [0.114, 0.264, 0.107, 0.332, 0.104, 0.19, 0.297, 0.139, 0.144, 0.109]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14964145421981812
[2m[36m(func pid=38408)[0m mae:  0.1009063571691513
[2m[36m(func pid=38408)[0m rmse_per_class: [0.104, 0.207, 0.042, 0.293, 0.099, 0.161, 0.247, 0.096, 0.154, 0.094]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.9176 | Steps: 2 | Val loss: 0.6964 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3628 | Steps: 2 | Val loss: 0.2888 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3064 | Steps: 2 | Val loss: 0.2659 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.5142 | Steps: 2 | Val loss: 0.4343 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 05:15:56 (running for 00:15:02.85)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.369 |  0.161 |                   62 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.31  |  0.15  |                   62 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.918 |  0.181 |                   10 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.563 |  0.18  |                    8 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.18103566765785217
[2m[36m(func pid=49621)[0m mae:  0.13342177867889404
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.329, 0.109, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.16078683733940125
[2m[36m(func pid=37799)[0m mae:  0.11543434858322144
[2m[36m(func pid=37799)[0m rmse_per_class: [0.105, 0.243, 0.053, 0.318, 0.067, 0.176, 0.263, 0.124, 0.149, 0.11]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14874278008937836
[2m[36m(func pid=38408)[0m mae:  0.09840569645166397
[2m[36m(func pid=38408)[0m rmse_per_class: [0.092, 0.207, 0.078, 0.272, 0.096, 0.16, 0.249, 0.096, 0.143, 0.094]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.1798396110534668
[2m[36m(func pid=50153)[0m mae:  0.13221654295921326
[2m[36m(func pid=50153)[0m rmse_per_class: [0.115, 0.264, 0.11, 0.333, 0.103, 0.189, 0.295, 0.138, 0.145, 0.107]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8960 | Steps: 2 | Val loss: 0.6838 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3643 | Steps: 2 | Val loss: 0.2887 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3194 | Steps: 2 | Val loss: 0.2648 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4873 | Steps: 2 | Val loss: 0.3974 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 05:16:01 (running for 00:15:07.88)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.363 |  0.161 |                   63 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.306 |  0.149 |                   63 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.896 |  0.181 |                   11 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.514 |  0.18  |                    9 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.1810419261455536
[2m[36m(func pid=49621)[0m mae:  0.13341526687145233
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.1607709676027298
[2m[36m(func pid=37799)[0m mae:  0.11543004214763641
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.243, 0.053, 0.317, 0.067, 0.176, 0.264, 0.123, 0.148, 0.11]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.146818146109581
[2m[36m(func pid=38408)[0m mae:  0.09743276983499527
[2m[36m(func pid=38408)[0m rmse_per_class: [0.096, 0.206, 0.067, 0.268, 0.093, 0.16, 0.254, 0.095, 0.139, 0.091]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.17954596877098083
[2m[36m(func pid=50153)[0m mae:  0.13187173008918762
[2m[36m(func pid=50153)[0m rmse_per_class: [0.115, 0.264, 0.113, 0.334, 0.101, 0.189, 0.292, 0.137, 0.146, 0.105]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3681 | Steps: 2 | Val loss: 0.2886 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8650 | Steps: 2 | Val loss: 0.6694 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3199 | Steps: 2 | Val loss: 0.2713 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4592 | Steps: 2 | Val loss: 0.3680 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 05:16:06 (running for 00:15:13.12)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.364 |  0.161 |                   64 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.319 |  0.147 |                   64 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.865 |  0.181 |                   12 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.487 |  0.18  |                   10 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16059836745262146
[2m[36m(func pid=37799)[0m mae:  0.11534182727336884
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.243, 0.051, 0.318, 0.067, 0.176, 0.264, 0.123, 0.147, 0.111]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.18101298809051514
[2m[36m(func pid=49621)[0m mae:  0.1333875060081482
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.15173503756523132
[2m[36m(func pid=38408)[0m mae:  0.1012338176369667
[2m[36m(func pid=38408)[0m rmse_per_class: [0.135, 0.205, 0.043, 0.29, 0.098, 0.16, 0.254, 0.094, 0.144, 0.094]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.17926788330078125
[2m[36m(func pid=50153)[0m mae:  0.13149820268154144
[2m[36m(func pid=50153)[0m rmse_per_class: [0.116, 0.264, 0.116, 0.336, 0.098, 0.188, 0.289, 0.136, 0.147, 0.103]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3646 | Steps: 2 | Val loss: 0.2885 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8441 | Steps: 2 | Val loss: 0.6533 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3047 | Steps: 2 | Val loss: 0.2811 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4499 | Steps: 2 | Val loss: 0.3470 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 05:16:11 (running for 00:15:18.13)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.365 |  0.161 |                   66 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.32  |  0.152 |                   65 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.865 |  0.181 |                   12 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.459 |  0.179 |                   11 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16064779460430145
[2m[36m(func pid=37799)[0m mae:  0.1152210608124733
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.242, 0.053, 0.318, 0.068, 0.176, 0.263, 0.123, 0.147, 0.11]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1809874176979065
[2m[36m(func pid=49621)[0m mae:  0.13336411118507385
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.15668520331382751
[2m[36m(func pid=38408)[0m mae:  0.10486791282892227
[2m[36m(func pid=38408)[0m rmse_per_class: [0.151, 0.205, 0.04, 0.321, 0.096, 0.16, 0.25, 0.095, 0.153, 0.097]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.1789996325969696
[2m[36m(func pid=50153)[0m mae:  0.13109169900417328
[2m[36m(func pid=50153)[0m rmse_per_class: [0.117, 0.264, 0.119, 0.338, 0.095, 0.188, 0.285, 0.136, 0.148, 0.101]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3641 | Steps: 2 | Val loss: 0.2883 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8188 | Steps: 2 | Val loss: 0.6367 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3030 | Steps: 2 | Val loss: 0.2767 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=37799)[0m rmse: 0.16054269671440125
[2m[36m(func pid=37799)[0m mae:  0.1150495782494545
[2m[36m(func pid=37799)[0m rmse_per_class: [0.107, 0.242, 0.053, 0.318, 0.067, 0.176, 0.263, 0.123, 0.146, 0.109]== Status ==
Current time: 2024-01-07 05:16:17 (running for 00:15:23.25)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.364 |  0.161 |                   67 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.305 |  0.157 |                   66 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.844 |  0.181 |                   13 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.45  |  0.179 |                   12 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1809300035238266
[2m[36m(func pid=49621)[0m mae:  0.13331548869609833
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.108, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4477 | Steps: 2 | Val loss: 0.3338 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=38408)[0m rmse: 0.15131518244743347
[2m[36m(func pid=38408)[0m mae:  0.10137452930212021
[2m[36m(func pid=38408)[0m rmse_per_class: [0.118, 0.205, 0.041, 0.322, 0.092, 0.16, 0.237, 0.095, 0.147, 0.095]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.17878936231136322
[2m[36m(func pid=50153)[0m mae:  0.13068929314613342
[2m[36m(func pid=50153)[0m rmse_per_class: [0.118, 0.265, 0.122, 0.34, 0.091, 0.187, 0.282, 0.136, 0.149, 0.099]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3608 | Steps: 2 | Val loss: 0.2876 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7914 | Steps: 2 | Val loss: 0.6195 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3004 | Steps: 2 | Val loss: 0.2671 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 05:16:22 (running for 00:15:28.38)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.361 |  0.16  |                   68 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.303 |  0.151 |                   67 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.819 |  0.181 |                   14 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.448 |  0.179 |                   13 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.16011235117912292
[2m[36m(func pid=37799)[0m mae:  0.11474025249481201
[2m[36m(func pid=37799)[0m rmse_per_class: [0.108, 0.242, 0.052, 0.317, 0.067, 0.176, 0.262, 0.123, 0.146, 0.109]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4513 | Steps: 2 | Val loss: 0.3275 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=49621)[0m rmse: 0.18086257576942444
[2m[36m(func pid=49621)[0m mae:  0.13325458765029907
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.108, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.1463114321231842
[2m[36m(func pid=38408)[0m mae:  0.09751008450984955
[2m[36m(func pid=38408)[0m rmse_per_class: [0.101, 0.203, 0.049, 0.301, 0.086, 0.159, 0.234, 0.096, 0.141, 0.093]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.17862263321876526
[2m[36m(func pid=50153)[0m mae:  0.1302788406610489
[2m[36m(func pid=50153)[0m rmse_per_class: [0.119, 0.265, 0.125, 0.342, 0.087, 0.186, 0.278, 0.136, 0.15, 0.097]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3643 | Steps: 2 | Val loss: 0.2870 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7641 | Steps: 2 | Val loss: 0.6015 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2996 | Steps: 2 | Val loss: 0.2629 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4537 | Steps: 2 | Val loss: 0.3263 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 05:16:27 (running for 00:15:33.75)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.364 |  0.16  |                   69 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.3   |  0.146 |                   68 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.791 |  0.181 |                   15 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.451 |  0.179 |                   14 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15971536934375763
[2m[36m(func pid=37799)[0m mae:  0.11443110555410385
[2m[36m(func pid=37799)[0m rmse_per_class: [0.107, 0.242, 0.051, 0.317, 0.067, 0.176, 0.261, 0.123, 0.146, 0.108]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.18078351020812988
[2m[36m(func pid=49621)[0m mae:  0.13318952918052673
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.108, 0.19, 0.304, 0.143, 0.142, 0.116]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.17852255702018738
[2m[36m(func pid=50153)[0m mae:  0.12987308204174042
[2m[36m(func pid=50153)[0m rmse_per_class: [0.12, 0.266, 0.129, 0.344, 0.083, 0.186, 0.275, 0.137, 0.151, 0.096]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14546582102775574
[2m[36m(func pid=38408)[0m mae:  0.09702998399734497
[2m[36m(func pid=38408)[0m rmse_per_class: [0.116, 0.204, 0.048, 0.273, 0.079, 0.158, 0.249, 0.095, 0.141, 0.091]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3636 | Steps: 2 | Val loss: 0.2866 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7407 | Steps: 2 | Val loss: 0.5835 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3138 | Steps: 2 | Val loss: 0.2658 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4681 | Steps: 2 | Val loss: 0.3295 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 05:16:32 (running for 00:15:38.93)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.364 |  0.16  |                   69 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.3   |  0.145 |                   69 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.741 |  0.181 |                   17 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.454 |  0.179 |                   15 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.1807166188955307
[2m[36m(func pid=49621)[0m mae:  0.13312941789627075
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.107, 0.19, 0.304, 0.143, 0.143, 0.115]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.15946067869663239
[2m[36m(func pid=37799)[0m mae:  0.11420432478189468
[2m[36m(func pid=37799)[0m rmse_per_class: [0.107, 0.242, 0.05, 0.315, 0.067, 0.176, 0.261, 0.123, 0.147, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.1784728467464447
[2m[36m(func pid=50153)[0m mae:  0.12945431470870972
[2m[36m(func pid=50153)[0m rmse_per_class: [0.121, 0.266, 0.131, 0.346, 0.078, 0.186, 0.272, 0.137, 0.152, 0.094]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14594605565071106
[2m[36m(func pid=38408)[0m mae:  0.09803201258182526
[2m[36m(func pid=38408)[0m rmse_per_class: [0.12, 0.203, 0.043, 0.268, 0.077, 0.158, 0.262, 0.095, 0.141, 0.092]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.7173 | Steps: 2 | Val loss: 0.5651 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3640 | Steps: 2 | Val loss: 0.2859 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4825 | Steps: 2 | Val loss: 0.3360 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 05:16:38 (running for 00:15:44.17)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.364 |  0.159 |                   70 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.314 |  0.146 |                   70 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.717 |  0.181 |                   18 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.468 |  0.178 |                   16 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3027 | Steps: 2 | Val loss: 0.2667 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=49621)[0m rmse: 0.1806303709745407
[2m[36m(func pid=49621)[0m mae:  0.13306260108947754
[2m[36m(func pid=49621)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.107, 0.19, 0.303, 0.143, 0.143, 0.115]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.1591077595949173
[2m[36m(func pid=37799)[0m mae:  0.11394017934799194
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.242, 0.05, 0.313, 0.066, 0.176, 0.261, 0.123, 0.147, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.17856556177139282
[2m[36m(func pid=50153)[0m mae:  0.12909837067127228
[2m[36m(func pid=50153)[0m rmse_per_class: [0.122, 0.267, 0.134, 0.349, 0.074, 0.185, 0.269, 0.138, 0.153, 0.093]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14795032143592834
[2m[36m(func pid=38408)[0m mae:  0.0990862026810646
[2m[36m(func pid=38408)[0m rmse_per_class: [0.125, 0.203, 0.046, 0.28, 0.081, 0.158, 0.253, 0.095, 0.146, 0.092]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6931 | Steps: 2 | Val loss: 0.5471 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3632 | Steps: 2 | Val loss: 0.2858 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3112 | Steps: 2 | Val loss: 0.2684 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4957 | Steps: 2 | Val loss: 0.3448 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=49621)[0m rmse: 0.1805599480867386
== Status ==
Current time: 2024-01-07 05:16:43 (running for 00:15:49.45)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.364 |  0.159 |                   71 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.303 |  0.148 |                   71 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.693 |  0.181 |                   19 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.482 |  0.179 |                   17 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=49621)[0m mae:  0.13300654292106628

[2m[36m(func pid=49621)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.329, 0.106, 0.19, 0.303, 0.143, 0.143, 0.115]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.15907831490039825
[2m[36m(func pid=37799)[0m mae:  0.11386438459157944
[2m[36m(func pid=37799)[0m rmse_per_class: [0.104, 0.242, 0.051, 0.313, 0.067, 0.176, 0.261, 0.122, 0.147, 0.108]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14854583144187927
[2m[36m(func pid=38408)[0m mae:  0.09879171848297119
[2m[36m(func pid=38408)[0m rmse_per_class: [0.126, 0.203, 0.042, 0.299, 0.088, 0.159, 0.238, 0.095, 0.143, 0.092]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.17876741290092468
[2m[36m(func pid=50153)[0m mae:  0.12876439094543457
[2m[36m(func pid=50153)[0m rmse_per_class: [0.124, 0.267, 0.137, 0.351, 0.071, 0.185, 0.268, 0.139, 0.153, 0.093]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3592 | Steps: 2 | Val loss: 0.2857 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6680 | Steps: 2 | Val loss: 0.5296 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3073 | Steps: 2 | Val loss: 0.2679 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.5128 | Steps: 2 | Val loss: 0.3551 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 05:16:48 (running for 00:15:54.66)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.363 |  0.159 |                   72 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.311 |  0.149 |                   72 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.668 |  0.18  |                   20 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.496 |  0.179 |                   18 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.1589416265487671
[2m[36m(func pid=37799)[0m mae:  0.113807812333107
[2m[36m(func pid=37799)[0m rmse_per_class: [0.103, 0.241, 0.05, 0.313, 0.067, 0.176, 0.261, 0.122, 0.147, 0.109]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.18049731850624084
[2m[36m(func pid=49621)[0m mae:  0.13295134902000427
[2m[36m(func pid=49621)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.329, 0.106, 0.19, 0.302, 0.143, 0.143, 0.115]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.17908427119255066
[2m[36m(func pid=50153)[0m mae:  0.12847423553466797
[2m[36m(func pid=50153)[0m rmse_per_class: [0.125, 0.267, 0.14, 0.353, 0.067, 0.185, 0.267, 0.14, 0.154, 0.092]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.1481333076953888
[2m[36m(func pid=38408)[0m mae:  0.09822113811969757
[2m[36m(func pid=38408)[0m rmse_per_class: [0.121, 0.204, 0.043, 0.296, 0.093, 0.16, 0.238, 0.094, 0.138, 0.096]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3604 | Steps: 2 | Val loss: 0.2858 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6405 | Steps: 2 | Val loss: 0.5127 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5239 | Steps: 2 | Val loss: 0.3672 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3061 | Steps: 2 | Val loss: 0.2648 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 05:16:53 (running for 00:15:59.79)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1655000001192093
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.36  |  0.159 |                   74 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.307 |  0.148 |                   73 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.668 |  0.18  |                   20 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.513 |  0.179 |                   19 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15903829038143158
[2m[36m(func pid=37799)[0m mae:  0.11382690817117691
[2m[36m(func pid=37799)[0m rmse_per_class: [0.103, 0.242, 0.05, 0.313, 0.067, 0.175, 0.261, 0.122, 0.148, 0.109]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.18046219646930695
[2m[36m(func pid=49621)[0m mae:  0.13291151821613312
[2m[36m(func pid=49621)[0m rmse_per_class: [0.114, 0.264, 0.1, 0.33, 0.105, 0.19, 0.301, 0.143, 0.143, 0.114]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.17963656783103943
[2m[36m(func pid=50153)[0m mae:  0.12832018733024597
[2m[36m(func pid=50153)[0m rmse_per_class: [0.126, 0.268, 0.143, 0.356, 0.064, 0.185, 0.267, 0.141, 0.154, 0.092]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14689037203788757
[2m[36m(func pid=38408)[0m mae:  0.09756603837013245
[2m[36m(func pid=38408)[0m rmse_per_class: [0.116, 0.204, 0.044, 0.283, 0.093, 0.158, 0.245, 0.094, 0.137, 0.094]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6294 | Steps: 2 | Val loss: 0.4963 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3596 | Steps: 2 | Val loss: 0.2856 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3008 | Steps: 2 | Val loss: 0.2653 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5449 | Steps: 2 | Val loss: 0.3791 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 05:16:58 (running for 00:16:05.01)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16099999845027924
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.36  |  0.159 |                   75 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.306 |  0.147 |                   74 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.641 |  0.18  |                   21 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.524 |  0.18  |                   20 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15894940495491028
[2m[36m(func pid=37799)[0m mae:  0.11368238925933838
[2m[36m(func pid=37799)[0m rmse_per_class: [0.104, 0.241, 0.051, 0.313, 0.067, 0.175, 0.26, 0.122, 0.147, 0.109]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1803709864616394
[2m[36m(func pid=49621)[0m mae:  0.13283170759677887
[2m[36m(func pid=49621)[0m rmse_per_class: [0.114, 0.264, 0.1, 0.33, 0.105, 0.19, 0.3, 0.142, 0.144, 0.114]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14699237048625946
[2m[36m(func pid=38408)[0m mae:  0.09788303822278976
[2m[36m(func pid=38408)[0m rmse_per_class: [0.118, 0.204, 0.041, 0.278, 0.096, 0.158, 0.25, 0.095, 0.14, 0.089]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18017195165157318
[2m[36m(func pid=50153)[0m mae:  0.12814906239509583
[2m[36m(func pid=50153)[0m rmse_per_class: [0.127, 0.268, 0.145, 0.358, 0.062, 0.186, 0.268, 0.142, 0.154, 0.091]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3597 | Steps: 2 | Val loss: 0.2855 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.6039 | Steps: 2 | Val loss: 0.4808 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=37799)[0m rmse: 0.15885543823242188
[2m[36m(func pid=37799)[0m mae:  0.11350952088832855
[2m[36m(func pid=37799)[0m rmse_per_class: [0.105, 0.242, 0.051, 0.314, 0.067, 0.175, 0.259, 0.122, 0.146, 0.108]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2973 | Steps: 2 | Val loss: 0.2669 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 05:17:04 (running for 00:16:10.13)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.36  |  0.159 |                   76 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.301 |  0.147 |                   75 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.604 |  0.18  |                   23 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.545 |  0.18  |                   21 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.18030622601509094
[2m[36m(func pid=49621)[0m mae:  0.13277187943458557
[2m[36m(func pid=49621)[0m rmse_per_class: [0.114, 0.265, 0.101, 0.33, 0.104, 0.19, 0.3, 0.142, 0.144, 0.114]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5691 | Steps: 2 | Val loss: 0.3917 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=38408)[0m rmse: 0.148450568318367
[2m[36m(func pid=38408)[0m mae:  0.09842517971992493
[2m[36m(func pid=38408)[0m rmse_per_class: [0.12, 0.204, 0.045, 0.288, 0.099, 0.158, 0.247, 0.095, 0.142, 0.087]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18088576197624207
[2m[36m(func pid=50153)[0m mae:  0.12808667123317719
[2m[36m(func pid=50153)[0m rmse_per_class: [0.128, 0.269, 0.148, 0.36, 0.06, 0.186, 0.27, 0.143, 0.154, 0.091]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3615 | Steps: 2 | Val loss: 0.2854 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5919 | Steps: 2 | Val loss: 0.4657 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 05:17:09 (running for 00:16:15.24)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.361 |  0.159 |                   77 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.297 |  0.148 |                   76 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.604 |  0.18  |                   23 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.569 |  0.181 |                   22 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.1588072031736374
[2m[36m(func pid=37799)[0m mae:  0.11345712095499039
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.241, 0.05, 0.314, 0.067, 0.175, 0.26, 0.122, 0.145, 0.109]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2967 | Steps: 2 | Val loss: 0.2675 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=49621)[0m rmse: 0.1801910400390625
[2m[36m(func pid=49621)[0m mae:  0.1326746642589569
[2m[36m(func pid=49621)[0m rmse_per_class: [0.114, 0.265, 0.102, 0.331, 0.103, 0.19, 0.299, 0.141, 0.144, 0.113]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5766 | Steps: 2 | Val loss: 0.4029 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=38408)[0m rmse: 0.1486385613679886
[2m[36m(func pid=38408)[0m mae:  0.0982527807354927
[2m[36m(func pid=38408)[0m rmse_per_class: [0.116, 0.203, 0.046, 0.293, 0.101, 0.158, 0.244, 0.096, 0.141, 0.087]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.1814519464969635
[2m[36m(func pid=50153)[0m mae:  0.1279478371143341
[2m[36m(func pid=50153)[0m rmse_per_class: [0.129, 0.27, 0.149, 0.361, 0.058, 0.186, 0.272, 0.144, 0.154, 0.091]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3597 | Steps: 2 | Val loss: 0.2856 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5732 | Steps: 2 | Val loss: 0.4516 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2974 | Steps: 2 | Val loss: 0.2687 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 05:17:14 (running for 00:16:20.47)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.361 |  0.159 |                   77 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.297 |  0.149 |                   77 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.573 |  0.18  |                   25 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.577 |  0.181 |                   23 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.1590040922164917
[2m[36m(func pid=37799)[0m mae:  0.11358873546123505
[2m[36m(func pid=37799)[0m rmse_per_class: [0.108, 0.241, 0.049, 0.314, 0.067, 0.175, 0.26, 0.122, 0.145, 0.109]
[2m[36m(func pid=49621)[0m rmse: 0.18007417023181915
[2m[36m(func pid=49621)[0m mae:  0.13257303833961487
[2m[36m(func pid=49621)[0m rmse_per_class: [0.114, 0.265, 0.102, 0.331, 0.103, 0.19, 0.298, 0.141, 0.145, 0.113]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5875 | Steps: 2 | Val loss: 0.4146 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=38408)[0m rmse: 0.14898549020290375
[2m[36m(func pid=38408)[0m mae:  0.09836755692958832
[2m[36m(func pid=38408)[0m rmse_per_class: [0.121, 0.202, 0.049, 0.293, 0.097, 0.159, 0.241, 0.095, 0.143, 0.091]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18226829171180725
[2m[36m(func pid=50153)[0m mae:  0.12797313928604126
[2m[36m(func pid=50153)[0m rmse_per_class: [0.13, 0.27, 0.151, 0.363, 0.057, 0.186, 0.274, 0.145, 0.154, 0.091]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5544 | Steps: 2 | Val loss: 0.4385 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3579 | Steps: 2 | Val loss: 0.2860 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2993 | Steps: 2 | Val loss: 0.2702 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 05:17:19 (running for 00:16:25.79)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.36  |  0.159 |                   78 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.297 |  0.149 |                   78 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.554 |  0.18  |                   26 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.588 |  0.182 |                   24 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.1799662858247757
[2m[36m(func pid=49621)[0m mae:  0.13247643411159515
[2m[36m(func pid=49621)[0m rmse_per_class: [0.115, 0.265, 0.103, 0.331, 0.102, 0.19, 0.297, 0.14, 0.145, 0.112]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=37799)[0m rmse: 0.1592717319726944
[2m[36m(func pid=37799)[0m mae:  0.11372967064380646
[2m[36m(func pid=37799)[0m rmse_per_class: [0.109, 0.241, 0.05, 0.315, 0.067, 0.175, 0.26, 0.121, 0.145, 0.11]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.6037 | Steps: 2 | Val loss: 0.4263 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=38408)[0m rmse: 0.1504189670085907
[2m[36m(func pid=38408)[0m mae:  0.10046728700399399
[2m[36m(func pid=38408)[0m rmse_per_class: [0.126, 0.203, 0.041, 0.29, 0.09, 0.159, 0.244, 0.094, 0.16, 0.097]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18319858610630035
[2m[36m(func pid=50153)[0m mae:  0.1280878484249115
[2m[36m(func pid=50153)[0m rmse_per_class: [0.132, 0.271, 0.154, 0.365, 0.056, 0.187, 0.277, 0.146, 0.154, 0.091]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3635 | Steps: 2 | Val loss: 0.2859 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5473 | Steps: 2 | Val loss: 0.4266 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3065 | Steps: 2 | Val loss: 0.2682 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 05:17:24 (running for 00:16:31.05)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.363 |  0.159 |                   80 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.299 |  0.15  |                   79 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.554 |  0.18  |                   26 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.604 |  0.183 |                   25 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15917959809303284
[2m[36m(func pid=37799)[0m mae:  0.1136108785867691
[2m[36m(func pid=37799)[0m rmse_per_class: [0.108, 0.241, 0.05, 0.316, 0.067, 0.175, 0.259, 0.121, 0.145, 0.11]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1798515021800995
[2m[36m(func pid=49621)[0m mae:  0.13237419724464417
[2m[36m(func pid=49621)[0m rmse_per_class: [0.115, 0.265, 0.104, 0.332, 0.101, 0.19, 0.296, 0.14, 0.145, 0.111]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.6050 | Steps: 2 | Val loss: 0.4357 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=38408)[0m rmse: 0.149069145321846
[2m[36m(func pid=38408)[0m mae:  0.09985808283090591
[2m[36m(func pid=38408)[0m rmse_per_class: [0.118, 0.202, 0.048, 0.282, 0.081, 0.159, 0.247, 0.096, 0.155, 0.103]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18396073579788208
[2m[36m(func pid=50153)[0m mae:  0.12811438739299774
[2m[36m(func pid=50153)[0m rmse_per_class: [0.133, 0.271, 0.155, 0.366, 0.055, 0.188, 0.28, 0.146, 0.154, 0.091]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5352 | Steps: 2 | Val loss: 0.4152 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3583 | Steps: 2 | Val loss: 0.2859 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2959 | Steps: 2 | Val loss: 0.2630 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.6170 | Steps: 2 | Val loss: 0.4452 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 05:17:30 (running for 00:16:36.22)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.358 |  0.159 |                   81 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.306 |  0.149 |                   80 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.547 |  0.18  |                   27 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.605 |  0.184 |                   26 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15922021865844727
[2m[36m(func pid=37799)[0m mae:  0.11366166919469833
[2m[36m(func pid=37799)[0m rmse_per_class: [0.108, 0.241, 0.051, 0.315, 0.067, 0.175, 0.259, 0.121, 0.145, 0.11]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17973853647708893
[2m[36m(func pid=49621)[0m mae:  0.13226251304149628
[2m[36m(func pid=49621)[0m rmse_per_class: [0.115, 0.265, 0.105, 0.332, 0.1, 0.19, 0.295, 0.139, 0.146, 0.11]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14424966275691986
[2m[36m(func pid=38408)[0m mae:  0.09660258144140244
[2m[36m(func pid=38408)[0m rmse_per_class: [0.101, 0.202, 0.047, 0.274, 0.076, 0.157, 0.248, 0.095, 0.141, 0.101]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18488547205924988
[2m[36m(func pid=50153)[0m mae:  0.12827077507972717
[2m[36m(func pid=50153)[0m rmse_per_class: [0.134, 0.271, 0.157, 0.368, 0.055, 0.188, 0.284, 0.147, 0.153, 0.092]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3604 | Steps: 2 | Val loss: 0.2852 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5187 | Steps: 2 | Val loss: 0.4051 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3085 | Steps: 2 | Val loss: 0.2614 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6257 | Steps: 2 | Val loss: 0.4528 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 05:17:35 (running for 00:16:41.49)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.36  |  0.159 |                   82 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.296 |  0.144 |                   81 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.535 |  0.18  |                   28 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.617 |  0.185 |                   27 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15880917012691498
[2m[36m(func pid=37799)[0m mae:  0.11339624971151352
[2m[36m(func pid=37799)[0m rmse_per_class: [0.107, 0.24, 0.05, 0.315, 0.067, 0.175, 0.259, 0.121, 0.145, 0.109]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17965315282344818
[2m[36m(func pid=49621)[0m mae:  0.13216838240623474
[2m[36m(func pid=49621)[0m rmse_per_class: [0.115, 0.265, 0.106, 0.333, 0.099, 0.19, 0.294, 0.139, 0.146, 0.11]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14280986785888672
[2m[36m(func pid=38408)[0m mae:  0.09512355178594589
[2m[36m(func pid=38408)[0m rmse_per_class: [0.102, 0.204, 0.04, 0.278, 0.076, 0.157, 0.246, 0.093, 0.135, 0.097]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18563713133335114
[2m[36m(func pid=50153)[0m mae:  0.1283269077539444
[2m[36m(func pid=50153)[0m rmse_per_class: [0.136, 0.272, 0.158, 0.369, 0.055, 0.189, 0.287, 0.148, 0.153, 0.092]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3672 | Steps: 2 | Val loss: 0.2847 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5096 | Steps: 2 | Val loss: 0.3960 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2916 | Steps: 2 | Val loss: 0.2625 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 05:17:40 (running for 00:16:46.73)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.367 |  0.158 |                   83 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.309 |  0.143 |                   82 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.519 |  0.18  |                   29 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.626 |  0.186 |                   28 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15848979353904724
[2m[36m(func pid=37799)[0m mae:  0.11316460371017456
[2m[36m(func pid=37799)[0m rmse_per_class: [0.105, 0.24, 0.05, 0.314, 0.068, 0.175, 0.259, 0.12, 0.145, 0.108]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.6387 | Steps: 2 | Val loss: 0.4576 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=49621)[0m rmse: 0.17957836389541626
[2m[36m(func pid=49621)[0m mae:  0.13208112120628357
[2m[36m(func pid=49621)[0m rmse_per_class: [0.115, 0.265, 0.107, 0.333, 0.099, 0.19, 0.293, 0.139, 0.146, 0.109]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14425744116306305
[2m[36m(func pid=38408)[0m mae:  0.0952053964138031
[2m[36m(func pid=38408)[0m rmse_per_class: [0.124, 0.2, 0.039, 0.277, 0.08, 0.156, 0.248, 0.094, 0.135, 0.089]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18616865575313568
[2m[36m(func pid=50153)[0m mae:  0.1282850205898285
[2m[36m(func pid=50153)[0m rmse_per_class: [0.137, 0.272, 0.157, 0.37, 0.054, 0.189, 0.29, 0.149, 0.152, 0.092]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3589 | Steps: 2 | Val loss: 0.2847 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5001 | Steps: 2 | Val loss: 0.3876 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2970 | Steps: 2 | Val loss: 0.2669 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.6281 | Steps: 2 | Val loss: 0.4633 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 05:17:45 (running for 00:16:52.01)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.359 |  0.158 |                   84 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.292 |  0.144 |                   83 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.51  |  0.18  |                   30 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.639 |  0.186 |                   29 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15848790109157562
[2m[36m(func pid=37799)[0m mae:  0.11315678060054779
[2m[36m(func pid=37799)[0m rmse_per_class: [0.105, 0.239, 0.051, 0.315, 0.069, 0.175, 0.259, 0.12, 0.146, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17951913177967072
[2m[36m(func pid=49621)[0m mae:  0.132003054022789
[2m[36m(func pid=49621)[0m rmse_per_class: [0.116, 0.265, 0.108, 0.334, 0.098, 0.19, 0.292, 0.138, 0.147, 0.108]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14770212769508362
[2m[36m(func pid=38408)[0m mae:  0.09785060584545135
[2m[36m(func pid=38408)[0m rmse_per_class: [0.125, 0.199, 0.046, 0.285, 0.086, 0.156, 0.248, 0.094, 0.152, 0.086]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.1870039999485016
[2m[36m(func pid=50153)[0m mae:  0.12843915820121765
[2m[36m(func pid=50153)[0m rmse_per_class: [0.138, 0.272, 0.159, 0.371, 0.054, 0.19, 0.293, 0.149, 0.152, 0.092]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4874 | Steps: 2 | Val loss: 0.3800 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3566 | Steps: 2 | Val loss: 0.2847 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2943 | Steps: 2 | Val loss: 0.2703 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 05:17:50 (running for 00:16:57.04)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.357 |  0.158 |                   85 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.297 |  0.148 |                   84 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.5   |  0.18  |                   31 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.628 |  0.187 |                   30 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15848329663276672
[2m[36m(func pid=37799)[0m mae:  0.1131739392876625
[2m[36m(func pid=37799)[0m rmse_per_class: [0.105, 0.239, 0.051, 0.314, 0.069, 0.175, 0.259, 0.12, 0.146, 0.106]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.6480 | Steps: 2 | Val loss: 0.4661 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=49621)[0m rmse: 0.1794697493314743
[2m[36m(func pid=49621)[0m mae:  0.13192275166511536
[2m[36m(func pid=49621)[0m rmse_per_class: [0.116, 0.265, 0.109, 0.334, 0.097, 0.19, 0.291, 0.138, 0.147, 0.108]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.15037789940834045
[2m[36m(func pid=38408)[0m mae:  0.09941364824771881
[2m[36m(func pid=38408)[0m rmse_per_class: [0.121, 0.2, 0.058, 0.292, 0.09, 0.157, 0.244, 0.094, 0.162, 0.086]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18756869435310364
[2m[36m(func pid=50153)[0m mae:  0.1284617930650711
[2m[36m(func pid=50153)[0m rmse_per_class: [0.139, 0.272, 0.158, 0.371, 0.054, 0.19, 0.297, 0.15, 0.151, 0.093]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3662 | Steps: 2 | Val loss: 0.2847 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4805 | Steps: 2 | Val loss: 0.3730 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2973 | Steps: 2 | Val loss: 0.2663 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 05:17:55 (running for 00:17:02.08)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.366 |  0.159 |                   86 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.294 |  0.15  |                   85 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.487 |  0.179 |                   32 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.648 |  0.188 |                   31 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15861256420612335
[2m[36m(func pid=37799)[0m mae:  0.11317578703165054
[2m[36m(func pid=37799)[0m rmse_per_class: [0.105, 0.238, 0.053, 0.314, 0.069, 0.175, 0.259, 0.12, 0.147, 0.106]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.6518 | Steps: 2 | Val loss: 0.4700 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=49621)[0m rmse: 0.17940753698349
[2m[36m(func pid=49621)[0m mae:  0.13182596862316132
[2m[36m(func pid=49621)[0m rmse_per_class: [0.116, 0.265, 0.11, 0.335, 0.096, 0.19, 0.29, 0.137, 0.148, 0.107]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.1470918208360672
[2m[36m(func pid=38408)[0m mae:  0.09731025993824005
[2m[36m(func pid=38408)[0m rmse_per_class: [0.115, 0.2, 0.049, 0.289, 0.09, 0.157, 0.241, 0.094, 0.149, 0.088]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.18836547434329987
[2m[36m(func pid=50153)[0m mae:  0.12861037254333496
[2m[36m(func pid=50153)[0m rmse_per_class: [0.14, 0.273, 0.16, 0.372, 0.054, 0.191, 0.3, 0.15, 0.151, 0.093]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3646 | Steps: 2 | Val loss: 0.2845 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4788 | Steps: 2 | Val loss: 0.3670 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3094 | Steps: 2 | Val loss: 0.2633 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 05:18:01 (running for 00:17:07.23)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.365 |  0.159 |                   87 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.297 |  0.147 |                   86 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.48  |  0.179 |                   33 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.652 |  0.188 |                   32 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15856529772281647
[2m[36m(func pid=37799)[0m mae:  0.11298874765634537
[2m[36m(func pid=37799)[0m rmse_per_class: [0.105, 0.238, 0.055, 0.314, 0.069, 0.175, 0.259, 0.119, 0.147, 0.106]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.6629 | Steps: 2 | Val loss: 0.4717 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=49621)[0m rmse: 0.1793358325958252
[2m[36m(func pid=49621)[0m mae:  0.13172143697738647
[2m[36m(func pid=49621)[0m rmse_per_class: [0.117, 0.265, 0.111, 0.335, 0.095, 0.19, 0.289, 0.137, 0.148, 0.106]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14460104703903198
[2m[36m(func pid=38408)[0m mae:  0.09608035534620285
[2m[36m(func pid=38408)[0m rmse_per_class: [0.104, 0.202, 0.04, 0.283, 0.09, 0.157, 0.244, 0.093, 0.138, 0.094]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3594 | Steps: 2 | Val loss: 0.2841 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=50153)[0m rmse: 0.1889287531375885
[2m[36m(func pid=50153)[0m mae:  0.1286374032497406
[2m[36m(func pid=50153)[0m rmse_per_class: [0.142, 0.273, 0.159, 0.373, 0.055, 0.191, 0.303, 0.151, 0.151, 0.093]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4732 | Steps: 2 | Val loss: 0.3615 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3075 | Steps: 2 | Val loss: 0.2647 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 05:18:06 (running for 00:17:12.37)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.359 |  0.158 |                   88 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.309 |  0.145 |                   87 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.479 |  0.179 |                   34 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.663 |  0.189 |                   33 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15829002857208252
[2m[36m(func pid=37799)[0m mae:  0.11278945207595825
[2m[36m(func pid=37799)[0m rmse_per_class: [0.104, 0.238, 0.055, 0.312, 0.069, 0.175, 0.26, 0.119, 0.147, 0.106]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.6675 | Steps: 2 | Val loss: 0.4765 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=49621)[0m rmse: 0.17927011847496033
[2m[36m(func pid=49621)[0m mae:  0.13162077963352203
[2m[36m(func pid=49621)[0m rmse_per_class: [0.117, 0.265, 0.112, 0.336, 0.094, 0.19, 0.288, 0.137, 0.148, 0.105]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14650827646255493
[2m[36m(func pid=38408)[0m mae:  0.0975007712841034
[2m[36m(func pid=38408)[0m rmse_per_class: [0.109, 0.201, 0.043, 0.278, 0.094, 0.157, 0.249, 0.093, 0.139, 0.101]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3563 | Steps: 2 | Val loss: 0.2835 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=50153)[0m rmse: 0.19006173312664032
[2m[36m(func pid=50153)[0m mae:  0.12899324297904968
[2m[36m(func pid=50153)[0m rmse_per_class: [0.144, 0.273, 0.163, 0.374, 0.055, 0.192, 0.306, 0.151, 0.15, 0.093]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4657 | Steps: 2 | Val loss: 0.3568 | Batch size: 32 | lr: 0.0001 | Duration: 2.63s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3011 | Steps: 2 | Val loss: 0.2715 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 05:18:11 (running for 00:17:17.45)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.356 |  0.158 |                   89 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.307 |  0.147 |                   88 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.473 |  0.179 |                   35 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.667 |  0.19  |                   34 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15783248841762543
[2m[36m(func pid=37799)[0m mae:  0.11259349435567856
[2m[36m(func pid=37799)[0m rmse_per_class: [0.102, 0.238, 0.052, 0.312, 0.069, 0.175, 0.259, 0.119, 0.148, 0.106]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.6485 | Steps: 2 | Val loss: 0.4772 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=49621)[0m rmse: 0.17921850085258484
[2m[36m(func pid=49621)[0m mae:  0.13152727484703064
[2m[36m(func pid=49621)[0m rmse_per_class: [0.117, 0.265, 0.113, 0.337, 0.093, 0.189, 0.287, 0.137, 0.149, 0.105]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.15227942168712616
[2m[36m(func pid=38408)[0m mae:  0.1013016477227211
[2m[36m(func pid=38408)[0m rmse_per_class: [0.13, 0.204, 0.049, 0.283, 0.098, 0.16, 0.252, 0.093, 0.15, 0.103]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3604 | Steps: 2 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=50153)[0m rmse: 0.19084888696670532
[2m[36m(func pid=50153)[0m mae:  0.12922243773937225
[2m[36m(func pid=50153)[0m rmse_per_class: [0.146, 0.273, 0.165, 0.374, 0.055, 0.193, 0.308, 0.151, 0.15, 0.094]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4604 | Steps: 2 | Val loss: 0.3523 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2967 | Steps: 2 | Val loss: 0.2700 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 05:18:16 (running for 00:17:22.61)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.36  |  0.158 |                   90 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.301 |  0.152 |                   89 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.466 |  0.179 |                   36 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.648 |  0.191 |                   35 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15763573348522186
[2m[36m(func pid=37799)[0m mae:  0.11253838241100311
[2m[36m(func pid=37799)[0m rmse_per_class: [0.102, 0.237, 0.05, 0.311, 0.068, 0.175, 0.259, 0.119, 0.149, 0.106]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6592 | Steps: 2 | Val loss: 0.4776 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=49621)[0m rmse: 0.1791418045759201
[2m[36m(func pid=49621)[0m mae:  0.13140957057476044
[2m[36m(func pid=49621)[0m rmse_per_class: [0.118, 0.265, 0.114, 0.337, 0.092, 0.189, 0.286, 0.137, 0.149, 0.104]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.15108028054237366
[2m[36m(func pid=38408)[0m mae:  0.10049577057361603
[2m[36m(func pid=38408)[0m rmse_per_class: [0.123, 0.204, 0.042, 0.289, 0.102, 0.159, 0.245, 0.094, 0.157, 0.096]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.1916341632604599
[2m[36m(func pid=50153)[0m mae:  0.12947267293930054
[2m[36m(func pid=50153)[0m rmse_per_class: [0.148, 0.273, 0.167, 0.375, 0.055, 0.193, 0.309, 0.152, 0.149, 0.094]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3550 | Steps: 2 | Val loss: 0.2831 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4475 | Steps: 2 | Val loss: 0.3488 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2910 | Steps: 2 | Val loss: 0.2647 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
== Status ==
Current time: 2024-01-07 05:18:21 (running for 00:17:27.84)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.355 |  0.157 |                   91 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.297 |  0.151 |                   90 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.46  |  0.179 |                   37 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.659 |  0.192 |                   36 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15747399628162384
[2m[36m(func pid=37799)[0m mae:  0.11253515630960464
[2m[36m(func pid=37799)[0m rmse_per_class: [0.102, 0.237, 0.048, 0.311, 0.069, 0.175, 0.259, 0.119, 0.149, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6441 | Steps: 2 | Val loss: 0.4765 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=49621)[0m rmse: 0.17912128567695618
[2m[36m(func pid=49621)[0m mae:  0.1313430666923523
[2m[36m(func pid=49621)[0m rmse_per_class: [0.118, 0.266, 0.115, 0.337, 0.092, 0.189, 0.285, 0.136, 0.15, 0.104]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14616024494171143
[2m[36m(func pid=38408)[0m mae:  0.09677103906869888
[2m[36m(func pid=38408)[0m rmse_per_class: [0.113, 0.201, 0.039, 0.283, 0.099, 0.157, 0.241, 0.094, 0.145, 0.09]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.19231660664081573
[2m[36m(func pid=50153)[0m mae:  0.12969686090946198
[2m[36m(func pid=50153)[0m rmse_per_class: [0.151, 0.273, 0.169, 0.375, 0.055, 0.194, 0.31, 0.152, 0.149, 0.094]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3540 | Steps: 2 | Val loss: 0.2832 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4520 | Steps: 2 | Val loss: 0.3456 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2986 | Steps: 2 | Val loss: 0.2651 | Batch size: 32 | lr: 0.1 | Duration: 2.60s
== Status ==
Current time: 2024-01-07 05:18:26 (running for 00:17:32.84)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.354 |  0.158 |                   92 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.146 |                   91 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.447 |  0.179 |                   38 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.644 |  0.192 |                   37 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15752723813056946
[2m[36m(func pid=37799)[0m mae:  0.11262331157922745
[2m[36m(func pid=37799)[0m rmse_per_class: [0.102, 0.237, 0.047, 0.311, 0.069, 0.175, 0.259, 0.119, 0.149, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6456 | Steps: 2 | Val loss: 0.4747 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=49621)[0m rmse: 0.17908567190170288
[2m[36m(func pid=49621)[0m mae:  0.1312524825334549
[2m[36m(func pid=49621)[0m rmse_per_class: [0.118, 0.266, 0.116, 0.338, 0.091, 0.189, 0.284, 0.136, 0.15, 0.103]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14604716002941132
[2m[36m(func pid=38408)[0m mae:  0.096788689494133
[2m[36m(func pid=38408)[0m rmse_per_class: [0.115, 0.201, 0.038, 0.284, 0.093, 0.157, 0.244, 0.093, 0.145, 0.089]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.1929660588502884
[2m[36m(func pid=50153)[0m mae:  0.12993067502975464
[2m[36m(func pid=50153)[0m rmse_per_class: [0.154, 0.273, 0.171, 0.376, 0.055, 0.195, 0.311, 0.152, 0.149, 0.094]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3551 | Steps: 2 | Val loss: 0.2832 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4519 | Steps: 2 | Val loss: 0.3430 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2975 | Steps: 2 | Val loss: 0.2677 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 05:18:31 (running for 00:17:37.88)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.355 |  0.157 |                   93 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.299 |  0.146 |                   92 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.452 |  0.179 |                   39 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.646 |  0.193 |                   38 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15749788284301758
[2m[36m(func pid=37799)[0m mae:  0.11254684627056122
[2m[36m(func pid=37799)[0m rmse_per_class: [0.103, 0.237, 0.047, 0.311, 0.069, 0.175, 0.258, 0.119, 0.148, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6354 | Steps: 2 | Val loss: 0.4718 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=49621)[0m rmse: 0.1790580302476883
[2m[36m(func pid=49621)[0m mae:  0.1311708390712738
[2m[36m(func pid=49621)[0m rmse_per_class: [0.118, 0.266, 0.117, 0.338, 0.09, 0.189, 0.283, 0.136, 0.15, 0.102]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14827434718608856
[2m[36m(func pid=38408)[0m mae:  0.09797212481498718
[2m[36m(func pid=38408)[0m rmse_per_class: [0.134, 0.201, 0.045, 0.282, 0.089, 0.156, 0.253, 0.093, 0.141, 0.088]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3549 | Steps: 2 | Val loss: 0.2834 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=50153)[0m rmse: 0.19354148209095
[2m[36m(func pid=50153)[0m mae:  0.1301504224538803
[2m[36m(func pid=50153)[0m rmse_per_class: [0.157, 0.273, 0.172, 0.376, 0.055, 0.195, 0.311, 0.153, 0.149, 0.094]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4485 | Steps: 2 | Val loss: 0.3407 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2965 | Steps: 2 | Val loss: 0.2661 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 05:18:36 (running for 00:17:43.04)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.355 |  0.158 |                   94 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.298 |  0.148 |                   93 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.452 |  0.179 |                   40 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.635 |  0.194 |                   39 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15769881010055542
[2m[36m(func pid=37799)[0m mae:  0.11258568614721298
[2m[36m(func pid=37799)[0m rmse_per_class: [0.105, 0.237, 0.047, 0.312, 0.069, 0.175, 0.258, 0.119, 0.148, 0.107]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6372 | Steps: 2 | Val loss: 0.4702 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=49621)[0m rmse: 0.17903007566928864
[2m[36m(func pid=49621)[0m mae:  0.13109156489372253
[2m[36m(func pid=49621)[0m rmse_per_class: [0.119, 0.266, 0.118, 0.339, 0.089, 0.189, 0.282, 0.136, 0.151, 0.102]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14724302291870117
[2m[36m(func pid=38408)[0m mae:  0.097437784075737
[2m[36m(func pid=38408)[0m rmse_per_class: [0.127, 0.201, 0.044, 0.28, 0.088, 0.156, 0.252, 0.093, 0.141, 0.089]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3565 | Steps: 2 | Val loss: 0.2837 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=50153)[0m rmse: 0.19432775676250458
[2m[36m(func pid=50153)[0m mae:  0.13046471774578094
[2m[36m(func pid=50153)[0m rmse_per_class: [0.16, 0.273, 0.175, 0.377, 0.056, 0.196, 0.311, 0.153, 0.149, 0.095]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4499 | Steps: 2 | Val loss: 0.3389 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2965 | Steps: 2 | Val loss: 0.2628 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 05:18:42 (running for 00:17:48.25)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.357 |  0.158 |                   95 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.296 |  0.147 |                   94 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.448 |  0.179 |                   41 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.637 |  0.194 |                   40 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15793976187705994
[2m[36m(func pid=37799)[0m mae:  0.112665556371212
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.237, 0.049, 0.312, 0.069, 0.175, 0.259, 0.119, 0.147, 0.108]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17904798686504364
[2m[36m(func pid=49621)[0m mae:  0.13103696703910828
[2m[36m(func pid=49621)[0m rmse_per_class: [0.119, 0.266, 0.119, 0.34, 0.088, 0.189, 0.281, 0.136, 0.151, 0.101]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6340 | Steps: 2 | Val loss: 0.4666 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=38408)[0m rmse: 0.14447441697120667
[2m[36m(func pid=38408)[0m mae:  0.09585146605968475
[2m[36m(func pid=38408)[0m rmse_per_class: [0.1, 0.202, 0.041, 0.279, 0.093, 0.157, 0.245, 0.093, 0.143, 0.091]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3561 | Steps: 2 | Val loss: 0.2836 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=50153)[0m rmse: 0.19491994380950928
[2m[36m(func pid=50153)[0m mae:  0.13071759045124054
[2m[36m(func pid=50153)[0m rmse_per_class: [0.164, 0.273, 0.177, 0.377, 0.056, 0.196, 0.311, 0.153, 0.149, 0.095]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4476 | Steps: 2 | Val loss: 0.3373 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2989 | Steps: 2 | Val loss: 0.2639 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 05:18:47 (running for 00:17:53.38)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.356 |  0.158 |                   96 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.297 |  0.144 |                   95 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.45  |  0.179 |                   42 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.634 |  0.195 |                   41 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15793685615062714
[2m[36m(func pid=37799)[0m mae:  0.1125951036810875
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.237, 0.049, 0.312, 0.069, 0.175, 0.258, 0.118, 0.145, 0.109]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17901751399040222
[2m[36m(func pid=49621)[0m mae:  0.1309587061405182
[2m[36m(func pid=49621)[0m rmse_per_class: [0.119, 0.266, 0.12, 0.34, 0.087, 0.189, 0.28, 0.136, 0.151, 0.101]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6066 | Steps: 2 | Val loss: 0.4619 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=38408)[0m rmse: 0.1455027163028717
[2m[36m(func pid=38408)[0m mae:  0.09644521772861481
[2m[36m(func pid=38408)[0m rmse_per_class: [0.102, 0.203, 0.04, 0.281, 0.1, 0.158, 0.243, 0.093, 0.142, 0.094]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3578 | Steps: 2 | Val loss: 0.2834 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=50153)[0m rmse: 0.19532136619091034
[2m[36m(func pid=50153)[0m mae:  0.13093051314353943
[2m[36m(func pid=50153)[0m rmse_per_class: [0.168, 0.273, 0.177, 0.377, 0.056, 0.197, 0.309, 0.153, 0.149, 0.095]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4468 | Steps: 2 | Val loss: 0.3360 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2942 | Steps: 2 | Val loss: 0.2646 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
== Status ==
Current time: 2024-01-07 05:18:52 (running for 00:17:58.63)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.358 |  0.158 |                   97 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.299 |  0.146 |                   96 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.448 |  0.179 |                   43 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.607 |  0.195 |                   42 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.1577901989221573
[2m[36m(func pid=37799)[0m mae:  0.11247893422842026
[2m[36m(func pid=37799)[0m rmse_per_class: [0.105, 0.237, 0.049, 0.312, 0.07, 0.175, 0.258, 0.118, 0.145, 0.109]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1789964884519577
[2m[36m(func pid=49621)[0m mae:  0.1308807134628296
[2m[36m(func pid=49621)[0m rmse_per_class: [0.12, 0.266, 0.121, 0.341, 0.086, 0.189, 0.28, 0.136, 0.152, 0.1]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14719465374946594
[2m[36m(func pid=38408)[0m mae:  0.09689696133136749
[2m[36m(func pid=38408)[0m rmse_per_class: [0.124, 0.204, 0.041, 0.283, 0.098, 0.157, 0.24, 0.094, 0.139, 0.094]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6101 | Steps: 2 | Val loss: 0.4586 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3563 | Steps: 2 | Val loss: 0.2834 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=50153)[0m rmse: 0.1960001289844513
[2m[36m(func pid=50153)[0m mae:  0.13130193948745728
[2m[36m(func pid=50153)[0m rmse_per_class: [0.173, 0.272, 0.18, 0.377, 0.056, 0.197, 0.307, 0.153, 0.149, 0.095]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2942 | Steps: 2 | Val loss: 0.2664 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4472 | Steps: 2 | Val loss: 0.3351 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 05:18:57 (running for 00:18:03.76)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.356 |  0.158 |                   98 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.294 |  0.147 |                   97 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.447 |  0.179 |                   44 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.61  |  0.196 |                   43 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.1578236222267151
[2m[36m(func pid=37799)[0m mae:  0.11241903156042099
[2m[36m(func pid=37799)[0m rmse_per_class: [0.106, 0.236, 0.05, 0.312, 0.07, 0.175, 0.259, 0.118, 0.144, 0.109]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14880183339118958
[2m[36m(func pid=38408)[0m mae:  0.09771421551704407
[2m[36m(func pid=38408)[0m rmse_per_class: [0.135, 0.202, 0.043, 0.283, 0.101, 0.159, 0.24, 0.094, 0.142, 0.09]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1789613664150238
[2m[36m(func pid=49621)[0m mae:  0.13078764081001282
[2m[36m(func pid=49621)[0m rmse_per_class: [0.12, 0.266, 0.122, 0.341, 0.085, 0.189, 0.279, 0.136, 0.152, 0.1]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.6143 | Steps: 2 | Val loss: 0.4543 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3550 | Steps: 2 | Val loss: 0.2834 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=50153)[0m rmse: 0.1964751034975052
[2m[36m(func pid=50153)[0m mae:  0.13155916333198547
[2m[36m(func pid=50153)[0m rmse_per_class: [0.179, 0.272, 0.181, 0.378, 0.056, 0.197, 0.305, 0.154, 0.149, 0.095]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2900 | Steps: 2 | Val loss: 0.2661 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4450 | Steps: 2 | Val loss: 0.3341 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 05:19:02 (running for 00:18:09.08)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00006 | RUNNING    | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.355 |  0.158 |                   99 |
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.294 |  0.149 |                   98 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.447 |  0.179 |                   45 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.614 |  0.196 |                   44 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15784268081188202
[2m[36m(func pid=37799)[0m mae:  0.11246881633996964
[2m[36m(func pid=37799)[0m rmse_per_class: [0.108, 0.235, 0.049, 0.313, 0.07, 0.175, 0.259, 0.117, 0.144, 0.109]
[2m[36m(func pid=37799)[0m 
[2m[36m(func pid=38408)[0m rmse: 0.14787264168262482
[2m[36m(func pid=38408)[0m mae:  0.09766241163015366
[2m[36m(func pid=38408)[0m rmse_per_class: [0.118, 0.2, 0.042, 0.282, 0.1, 0.16, 0.242, 0.096, 0.15, 0.089]
[2m[36m(func pid=38408)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17892350256443024
[2m[36m(func pid=49621)[0m mae:  0.13068750500679016
[2m[36m(func pid=49621)[0m rmse_per_class: [0.12, 0.266, 0.122, 0.341, 0.084, 0.189, 0.278, 0.136, 0.152, 0.099]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5885 | Steps: 2 | Val loss: 0.4492 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=37799)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3534 | Steps: 2 | Val loss: 0.2834 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=38408)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2906 | Steps: 2 | Val loss: 0.2637 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=50153)[0m rmse: 0.19673994183540344
[2m[36m(func pid=50153)[0m mae:  0.1317748874425888
[2m[36m(func pid=50153)[0m rmse_per_class: [0.184, 0.272, 0.18, 0.378, 0.056, 0.197, 0.302, 0.154, 0.149, 0.095]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4478 | Steps: 2 | Val loss: 0.3339 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 05:19:08 (running for 00:18:14.47)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 3 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00007 | RUNNING    | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.29  |  0.148 |                   99 |
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.445 |  0.179 |                   46 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.589 |  0.197 |                   45 |
| train_5a6ec_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=37799)[0m rmse: 0.15788647532463074
[2m[36m(func pid=37799)[0m mae:  0.11247573047876358
[2m[36m(func pid=37799)[0m rmse_per_class: [0.11, 0.235, 0.049, 0.313, 0.069, 0.175, 0.258, 0.117, 0.145, 0.108]
[2m[36m(func pid=38408)[0m rmse: 0.14524810016155243
[2m[36m(func pid=38408)[0m mae:  0.0959760993719101
[2m[36m(func pid=38408)[0m rmse_per_class: [0.108, 0.2, 0.04, 0.275, 0.097, 0.157, 0.246, 0.095, 0.144, 0.09]
[2m[36m(func pid=49621)[0m rmse: 0.17893685400485992
[2m[36m(func pid=49621)[0m mae:  0.13061951100826263
[2m[36m(func pid=49621)[0m rmse_per_class: [0.121, 0.266, 0.124, 0.342, 0.083, 0.189, 0.277, 0.136, 0.153, 0.099]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5786 | Steps: 2 | Val loss: 0.4411 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4490 | Steps: 2 | Val loss: 0.3338 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=50153)[0m rmse: 0.19625593721866608
[2m[36m(func pid=50153)[0m mae:  0.13164125382900238
[2m[36m(func pid=50153)[0m rmse_per_class: [0.189, 0.271, 0.173, 0.377, 0.056, 0.197, 0.3, 0.154, 0.149, 0.095]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17891642451286316
[2m[36m(func pid=49621)[0m mae:  0.13054680824279785
[2m[36m(func pid=49621)[0m rmse_per_class: [0.121, 0.266, 0.124, 0.343, 0.082, 0.189, 0.277, 0.136, 0.153, 0.099]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5913 | Steps: 2 | Val loss: 0.4367 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4493 | Steps: 2 | Val loss: 0.3343 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=50153)[0m rmse: 0.19638416171073914
[2m[36m(func pid=50153)[0m mae:  0.13184627890586853
[2m[36m(func pid=50153)[0m rmse_per_class: [0.197, 0.27, 0.17, 0.377, 0.056, 0.197, 0.297, 0.154, 0.15, 0.095]
[2m[36m(func pid=60723)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=60723)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=60723)[0m Configuration completed!
[2m[36m(func pid=60723)[0m New optimizer parameters:
[2m[36m(func pid=60723)[0m SGD (
[2m[36m(func pid=60723)[0m Parameter Group 0
[2m[36m(func pid=60723)[0m     dampening: 0
[2m[36m(func pid=60723)[0m     differentiable: False
[2m[36m(func pid=60723)[0m     foreach: None
[2m[36m(func pid=60723)[0m     lr: 0.01
[2m[36m(func pid=60723)[0m     maximize: False
[2m[36m(func pid=60723)[0m     momentum: 0.99
[2m[36m(func pid=60723)[0m     nesterov: False
[2m[36m(func pid=60723)[0m     weight_decay: 0.0001
[2m[36m(func pid=60723)[0m )
[2m[36m(func pid=60723)[0m 
== Status ==
Current time: 2024-01-07 05:19:13 (running for 00:18:20.02)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.449 |  0.179 |                   48 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.579 |  0.196 |                   46 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60725)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60725)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=60725)[0m Configuration completed!
[2m[36m(func pid=60725)[0m New optimizer parameters:
[2m[36m(func pid=60725)[0m SGD (
[2m[36m(func pid=60725)[0m Parameter Group 0
[2m[36m(func pid=60725)[0m     dampening: 0
[2m[36m(func pid=60725)[0m     differentiable: False
[2m[36m(func pid=60725)[0m     foreach: None
[2m[36m(func pid=60725)[0m     lr: 0.1
[2m[36m(func pid=60725)[0m     maximize: False
[2m[36m(func pid=60725)[0m     momentum: 0.99
[2m[36m(func pid=60725)[0m     nesterov: False
[2m[36m(func pid=60725)[0m     weight_decay: 0.0001
[2m[36m(func pid=60725)[0m )
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:19:19 (running for 00:18:25.36)
Memory usage on this node: 24.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.449 |  0.179 |                   49 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.591 |  0.196 |                   47 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.1789911836385727
[2m[36m(func pid=49621)[0m mae:  0.1305115669965744
[2m[36m(func pid=49621)[0m rmse_per_class: [0.122, 0.266, 0.126, 0.343, 0.081, 0.189, 0.276, 0.136, 0.153, 0.098]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5654 | Steps: 2 | Val loss: 0.4315 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0057 | Steps: 2 | Val loss: 0.6594 | Batch size: 32 | lr: 0.01 | Duration: 4.44s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.7936 | Steps: 2 | Val loss: 0.4320 | Batch size: 32 | lr: 0.1 | Duration: 4.58s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4478 | Steps: 2 | Val loss: 0.3343 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=50153)[0m rmse: 0.19622428715229034
[2m[36m(func pid=50153)[0m mae:  0.13200967013835907
[2m[36m(func pid=50153)[0m rmse_per_class: [0.205, 0.269, 0.165, 0.377, 0.056, 0.197, 0.293, 0.154, 0.15, 0.095]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.17975127696990967
[2m[36m(func pid=60723)[0m mae:  0.1322534829378128
[2m[36m(func pid=60723)[0m rmse_per_class: [0.115, 0.263, 0.098, 0.334, 0.098, 0.191, 0.296, 0.143, 0.141, 0.118]
[2m[36m(func pid=60723)[0m 
== Status ==
Current time: 2024-01-07 05:19:24 (running for 00:18:30.60)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.448 |  0.179 |                   50 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.565 |  0.196 |                   48 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  1.006 |  0.18  |                    1 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.1789650022983551
[2m[36m(func pid=49621)[0m mae:  0.13041314482688904
[2m[36m(func pid=49621)[0m rmse_per_class: [0.122, 0.266, 0.127, 0.344, 0.081, 0.188, 0.275, 0.136, 0.153, 0.098]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.17942066490650177
[2m[36m(func pid=60725)[0m mae:  0.13121870160102844
[2m[36m(func pid=60725)[0m rmse_per_class: [0.12, 0.264, 0.119, 0.342, 0.088, 0.191, 0.282, 0.14, 0.144, 0.105]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5613 | Steps: 2 | Val loss: 0.4246 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.7674 | Steps: 2 | Val loss: 0.5880 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4480 | Steps: 2 | Val loss: 0.3342 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5106 | Steps: 2 | Val loss: 0.3528 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=50153)[0m rmse: 0.19548723101615906
[2m[36m(func pid=50153)[0m mae:  0.13179254531860352
[2m[36m(func pid=50153)[0m rmse_per_class: [0.211, 0.268, 0.156, 0.377, 0.056, 0.197, 0.29, 0.154, 0.151, 0.096]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.17992305755615234
[2m[36m(func pid=60723)[0m mae:  0.13240526616573334
[2m[36m(func pid=60723)[0m rmse_per_class: [0.114, 0.264, 0.104, 0.335, 0.101, 0.191, 0.294, 0.141, 0.144, 0.113]
[2m[36m(func pid=60723)[0m 
== Status ==
Current time: 2024-01-07 05:19:29 (running for 00:18:35.78)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.448 |  0.179 |                   51 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.561 |  0.195 |                   49 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.767 |  0.18  |                    2 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.794 |  0.179 |                    1 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.1789437085390091
[2m[36m(func pid=49621)[0m mae:  0.1303403079509735
[2m[36m(func pid=49621)[0m rmse_per_class: [0.122, 0.266, 0.127, 0.344, 0.08, 0.188, 0.275, 0.136, 0.154, 0.098]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.1795043647289276
[2m[36m(func pid=60725)[0m mae:  0.1301119327545166
[2m[36m(func pid=60725)[0m rmse_per_class: [0.119, 0.266, 0.138, 0.346, 0.074, 0.189, 0.273, 0.14, 0.152, 0.097]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5559 | Steps: 2 | Val loss: 0.4180 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5343 | Steps: 2 | Val loss: 0.4859 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4508 | Steps: 2 | Val loss: 0.3349 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7241 | Steps: 2 | Val loss: 0.3297 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=50153)[0m rmse: 0.19468247890472412
[2m[36m(func pid=50153)[0m mae:  0.13153667747974396
[2m[36m(func pid=50153)[0m rmse_per_class: [0.215, 0.267, 0.147, 0.376, 0.056, 0.196, 0.287, 0.154, 0.152, 0.096]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.17963358759880066
[2m[36m(func pid=60723)[0m mae:  0.13208690285682678
[2m[36m(func pid=60723)[0m rmse_per_class: [0.114, 0.264, 0.108, 0.336, 0.1, 0.191, 0.291, 0.139, 0.146, 0.109]
[2m[36m(func pid=60723)[0m 
== Status ==
Current time: 2024-01-07 05:19:34 (running for 00:18:40.86)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.451 |  0.179 |                   52 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.556 |  0.195 |                   50 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.534 |  0.18  |                    3 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.511 |  0.18  |                    2 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.1789732426404953
[2m[36m(func pid=49621)[0m mae:  0.13027730584144592
[2m[36m(func pid=49621)[0m rmse_per_class: [0.122, 0.266, 0.128, 0.345, 0.079, 0.188, 0.274, 0.136, 0.154, 0.097]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.17811737954616547
[2m[36m(func pid=60725)[0m mae:  0.12626248598098755
[2m[36m(func pid=60725)[0m rmse_per_class: [0.121, 0.267, 0.139, 0.351, 0.057, 0.188, 0.267, 0.143, 0.158, 0.091]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5610 | Steps: 2 | Val loss: 0.4118 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4571 | Steps: 2 | Val loss: 0.3938 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4500 | Steps: 2 | Val loss: 0.3350 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8840 | Steps: 2 | Val loss: 0.3486 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=50153)[0m rmse: 0.19376878440380096
[2m[36m(func pid=50153)[0m mae:  0.1311972737312317
[2m[36m(func pid=50153)[0m rmse_per_class: [0.219, 0.266, 0.138, 0.376, 0.056, 0.196, 0.284, 0.154, 0.154, 0.096]
[2m[36m(func pid=50153)[0m 
== Status ==
Current time: 2024-01-07 05:19:39 (running for 00:18:45.92)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.451 |  0.179 |                   52 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.561 |  0.194 |                   51 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.457 |  0.179 |                    4 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.724 |  0.178 |                    3 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.1789977103471756
[2m[36m(func pid=60723)[0m mae:  0.13134168088436127
[2m[36m(func pid=60723)[0m rmse_per_class: [0.115, 0.265, 0.114, 0.338, 0.095, 0.19, 0.286, 0.137, 0.148, 0.104]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1789584457874298
[2m[36m(func pid=49621)[0m mae:  0.130207821726799
[2m[36m(func pid=49621)[0m rmse_per_class: [0.123, 0.266, 0.128, 0.345, 0.078, 0.188, 0.273, 0.136, 0.154, 0.097]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.18124988675117493
[2m[36m(func pid=60725)[0m mae:  0.12374816834926605
[2m[36m(func pid=60725)[0m rmse_per_class: [0.12, 0.269, 0.123, 0.357, 0.055, 0.189, 0.304, 0.149, 0.154, 0.092]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5470 | Steps: 2 | Val loss: 0.4069 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4669 | Steps: 2 | Val loss: 0.3391 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4513 | Steps: 2 | Val loss: 0.3358 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.9056 | Steps: 2 | Val loss: 0.3616 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=50153)[0m rmse: 0.19305866956710815
[2m[36m(func pid=50153)[0m mae:  0.131018728017807
[2m[36m(func pid=50153)[0m rmse_per_class: [0.223, 0.264, 0.132, 0.375, 0.056, 0.195, 0.28, 0.154, 0.155, 0.096]
[2m[36m(func pid=50153)[0m 
== Status ==
Current time: 2024-01-07 05:19:45 (running for 00:18:51.22)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.45  |  0.179 |                   53 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.547 |  0.193 |                   52 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.467 |  0.178 |                    5 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.884 |  0.181 |                    4 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.17828242480754852
[2m[36m(func pid=60723)[0m mae:  0.13027732074260712
[2m[36m(func pid=60723)[0m rmse_per_class: [0.117, 0.265, 0.121, 0.342, 0.086, 0.189, 0.278, 0.136, 0.151, 0.099]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.178948312997818
[2m[36m(func pid=49621)[0m mae:  0.13011838495731354
[2m[36m(func pid=49621)[0m rmse_per_class: [0.123, 0.266, 0.129, 0.345, 0.077, 0.188, 0.273, 0.137, 0.154, 0.097]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.1848701685667038
[2m[36m(func pid=60725)[0m mae:  0.12399154901504517
[2m[36m(func pid=60725)[0m rmse_per_class: [0.129, 0.268, 0.092, 0.364, 0.056, 0.192, 0.351, 0.154, 0.148, 0.095]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5462 | Steps: 2 | Val loss: 0.4015 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5200 | Steps: 2 | Val loss: 0.3236 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4565 | Steps: 2 | Val loss: 0.3371 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8156 | Steps: 2 | Val loss: 0.3560 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=50153)[0m rmse: 0.1920391172170639
[2m[36m(func pid=50153)[0m mae:  0.13063964247703552
[2m[36m(func pid=50153)[0m rmse_per_class: [0.225, 0.262, 0.126, 0.374, 0.056, 0.194, 0.275, 0.154, 0.157, 0.096]
[2m[36m(func pid=50153)[0m 
== Status ==
Current time: 2024-01-07 05:19:50 (running for 00:18:56.36)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.451 |  0.179 |                   54 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.546 |  0.192 |                   53 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.52  |  0.178 |                    6 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.906 |  0.185 |                    5 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.1778631955385208
[2m[36m(func pid=60723)[0m mae:  0.12907497584819794
[2m[36m(func pid=60723)[0m rmse_per_class: [0.118, 0.265, 0.129, 0.347, 0.076, 0.188, 0.27, 0.137, 0.153, 0.095]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1790146827697754
[2m[36m(func pid=49621)[0m mae:  0.13007299602031708
[2m[36m(func pid=49621)[0m rmse_per_class: [0.124, 0.266, 0.13, 0.346, 0.076, 0.188, 0.272, 0.137, 0.154, 0.096]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.1778458058834076
[2m[36m(func pid=60725)[0m mae:  0.12236630916595459
[2m[36m(func pid=60725)[0m rmse_per_class: [0.164, 0.252, 0.054, 0.359, 0.056, 0.186, 0.285, 0.155, 0.169, 0.096]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5561 | Steps: 2 | Val loss: 0.3969 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5975 | Steps: 2 | Val loss: 0.3362 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4543 | Steps: 2 | Val loss: 0.3378 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7817 | Steps: 2 | Val loss: 0.3593 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=50153)[0m rmse: 0.19094710052013397
[2m[36m(func pid=50153)[0m mae:  0.1301613599061966
[2m[36m(func pid=50153)[0m rmse_per_class: [0.224, 0.26, 0.121, 0.373, 0.056, 0.193, 0.272, 0.154, 0.16, 0.096]
[2m[36m(func pid=50153)[0m 
== Status ==
Current time: 2024-01-07 05:19:55 (running for 00:19:01.39)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.457 |  0.179 |                   55 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.556 |  0.191 |                   54 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.598 |  0.178 |                    7 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.816 |  0.178 |                    6 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.17814527451992035
[2m[36m(func pid=60723)[0m mae:  0.1278896927833557
[2m[36m(func pid=60723)[0m rmse_per_class: [0.12, 0.266, 0.136, 0.353, 0.066, 0.187, 0.267, 0.14, 0.154, 0.092]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17899641394615173
[2m[36m(func pid=49621)[0m mae:  0.12998531758785248
[2m[36m(func pid=49621)[0m rmse_per_class: [0.124, 0.266, 0.131, 0.346, 0.075, 0.188, 0.272, 0.137, 0.155, 0.096]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.1727256327867508
[2m[36m(func pid=60725)[0m mae:  0.12269525229930878
[2m[36m(func pid=60725)[0m rmse_per_class: [0.107, 0.261, 0.043, 0.332, 0.056, 0.179, 0.269, 0.155, 0.229, 0.097]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5429 | Steps: 2 | Val loss: 0.3916 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.6509 | Steps: 2 | Val loss: 0.3626 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4541 | Steps: 2 | Val loss: 0.3378 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7021 | Steps: 2 | Val loss: 0.4075 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=50153)[0m rmse: 0.1895941197872162
[2m[36m(func pid=50153)[0m mae:  0.12956397235393524
[2m[36m(func pid=50153)[0m rmse_per_class: [0.222, 0.258, 0.115, 0.372, 0.056, 0.192, 0.268, 0.154, 0.163, 0.096]
[2m[36m(func pid=50153)[0m 
== Status ==
Current time: 2024-01-07 05:20:00 (running for 00:19:06.58)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.454 |  0.179 |                   56 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.543 |  0.19  |                   55 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.651 |  0.179 |                    8 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.782 |  0.173 |                    7 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.1792139858007431
[2m[36m(func pid=60723)[0m mae:  0.12685438990592957
[2m[36m(func pid=60723)[0m rmse_per_class: [0.122, 0.268, 0.139, 0.359, 0.059, 0.186, 0.272, 0.143, 0.153, 0.091]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17892834544181824
[2m[36m(func pid=49621)[0m mae:  0.12988409399986267
[2m[36m(func pid=49621)[0m rmse_per_class: [0.124, 0.266, 0.131, 0.346, 0.075, 0.188, 0.271, 0.137, 0.155, 0.096]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.17215201258659363
[2m[36m(func pid=60725)[0m mae:  0.11996261030435562
[2m[36m(func pid=60725)[0m rmse_per_class: [0.106, 0.27, 0.046, 0.298, 0.056, 0.231, 0.317, 0.154, 0.146, 0.097]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5327 | Steps: 2 | Val loss: 0.3854 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6916 | Steps: 2 | Val loss: 0.3924 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4589 | Steps: 2 | Val loss: 0.3381 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=50153)[0m rmse: 0.18787583708763123
[2m[36m(func pid=50153)[0m mae:  0.12874522805213928
[2m[36m(func pid=50153)[0m rmse_per_class: [0.217, 0.255, 0.109, 0.371, 0.056, 0.19, 0.264, 0.154, 0.167, 0.096]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7296 | Steps: 2 | Val loss: 0.3759 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 05:20:05 (running for 00:19:11.68)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.454 |  0.179 |                   57 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.533 |  0.188 |                   56 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.692 |  0.181 |                    9 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.702 |  0.172 |                    8 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.1813269555568695
[2m[36m(func pid=60723)[0m mae:  0.126296728849411
[2m[36m(func pid=60723)[0m rmse_per_class: [0.124, 0.269, 0.139, 0.365, 0.056, 0.187, 0.287, 0.147, 0.151, 0.091]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17888642847537994
[2m[36m(func pid=49621)[0m mae:  0.12980999052524567
[2m[36m(func pid=49621)[0m rmse_per_class: [0.124, 0.266, 0.131, 0.346, 0.074, 0.188, 0.271, 0.137, 0.155, 0.096]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.17871065437793732
[2m[36m(func pid=60725)[0m mae:  0.11807310581207275
[2m[36m(func pid=60725)[0m rmse_per_class: [0.11, 0.242, 0.05, 0.318, 0.056, 0.317, 0.317, 0.148, 0.134, 0.095]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5470 | Steps: 2 | Val loss: 0.3810 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7404 | Steps: 2 | Val loss: 0.4211 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4619 | Steps: 2 | Val loss: 0.3400 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=50153)[0m rmse: 0.18648526072502136
[2m[36m(func pid=50153)[0m mae:  0.128154918551445
[2m[36m(func pid=50153)[0m rmse_per_class: [0.211, 0.253, 0.106, 0.369, 0.056, 0.189, 0.26, 0.154, 0.171, 0.096]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.6527 | Steps: 2 | Val loss: 0.3252 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 05:20:10 (running for 00:19:16.81)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.459 |  0.179 |                   58 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.547 |  0.186 |                   57 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.74  |  0.184 |                   10 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.73  |  0.179 |                    9 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.18434274196624756
[2m[36m(func pid=60723)[0m mae:  0.12637628614902496
[2m[36m(func pid=60723)[0m rmse_per_class: [0.126, 0.27, 0.136, 0.37, 0.055, 0.188, 0.31, 0.149, 0.148, 0.092]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17898312211036682
[2m[36m(func pid=49621)[0m mae:  0.1297801285982132
[2m[36m(func pid=49621)[0m rmse_per_class: [0.125, 0.266, 0.132, 0.347, 0.073, 0.188, 0.271, 0.137, 0.155, 0.095]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.16858933866024017
[2m[36m(func pid=60725)[0m mae:  0.11217441409826279
[2m[36m(func pid=60725)[0m rmse_per_class: [0.109, 0.259, 0.052, 0.283, 0.056, 0.27, 0.285, 0.129, 0.136, 0.106]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5291 | Steps: 2 | Val loss: 0.3753 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7640 | Steps: 2 | Val loss: 0.4452 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4620 | Steps: 2 | Val loss: 0.3408 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=50153)[0m rmse: 0.18472738564014435
[2m[36m(func pid=50153)[0m mae:  0.1273333877325058
[2m[36m(func pid=50153)[0m rmse_per_class: [0.203, 0.251, 0.101, 0.367, 0.056, 0.187, 0.256, 0.154, 0.176, 0.096]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6070 | Steps: 2 | Val loss: 0.4584 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 05:20:16 (running for 00:19:22.16)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.462 |  0.179 |                   59 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.529 |  0.185 |                   58 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.764 |  0.188 |                   11 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.653 |  0.169 |                   10 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.18779698014259338
[2m[36m(func pid=60723)[0m mae:  0.12694767117500305
[2m[36m(func pid=60723)[0m rmse_per_class: [0.127, 0.272, 0.133, 0.374, 0.055, 0.19, 0.338, 0.152, 0.145, 0.093]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1789616197347641
[2m[36m(func pid=49621)[0m mae:  0.12970289587974548
[2m[36m(func pid=49621)[0m rmse_per_class: [0.125, 0.266, 0.133, 0.347, 0.073, 0.188, 0.27, 0.137, 0.155, 0.095]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5282 | Steps: 2 | Val loss: 0.3708 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=60725)[0m rmse: 0.21634991466999054
[2m[36m(func pid=60725)[0m mae:  0.14598387479782104
[2m[36m(func pid=60725)[0m rmse_per_class: [0.109, 0.288, 0.05, 0.377, 0.056, 0.202, 0.305, 0.336, 0.138, 0.302]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7860 | Steps: 2 | Val loss: 0.4634 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4604 | Steps: 2 | Val loss: 0.3424 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=50153)[0m rmse: 0.1831517219543457
[2m[36m(func pid=50153)[0m mae:  0.12661351263523102
[2m[36m(func pid=50153)[0m rmse_per_class: [0.194, 0.249, 0.098, 0.366, 0.056, 0.186, 0.253, 0.154, 0.182, 0.096]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7445 | Steps: 2 | Val loss: 0.6002 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 05:20:21 (running for 00:19:27.29)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.462 |  0.179 |                   60 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.528 |  0.183 |                   59 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.786 |  0.191 |                   12 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.607 |  0.216 |                   11 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.19136975705623627
[2m[36m(func pid=60723)[0m mae:  0.12784014642238617
[2m[36m(func pid=60723)[0m rmse_per_class: [0.129, 0.273, 0.13, 0.378, 0.055, 0.194, 0.366, 0.153, 0.142, 0.094]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17902030050754547
[2m[36m(func pid=49621)[0m mae:  0.12966002523899078
[2m[36m(func pid=49621)[0m rmse_per_class: [0.125, 0.266, 0.134, 0.348, 0.072, 0.188, 0.27, 0.137, 0.155, 0.095]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5203 | Steps: 2 | Val loss: 0.3630 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=60725)[0m rmse: 0.2235267162322998
[2m[36m(func pid=60725)[0m mae:  0.1528923213481903
[2m[36m(func pid=60725)[0m rmse_per_class: [0.107, 0.292, 0.05, 0.386, 0.056, 0.224, 0.278, 0.323, 0.138, 0.381]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7551 | Steps: 2 | Val loss: 0.4730 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4663 | Steps: 2 | Val loss: 0.3431 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=50153)[0m rmse: 0.18081216514110565
[2m[36m(func pid=50153)[0m mae:  0.1254720538854599
[2m[36m(func pid=50153)[0m rmse_per_class: [0.18, 0.247, 0.091, 0.363, 0.056, 0.184, 0.25, 0.154, 0.188, 0.095]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7893 | Steps: 2 | Val loss: 0.4603 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 05:20:26 (running for 00:19:32.68)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.46  |  0.179 |                   61 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.52  |  0.181 |                   60 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.755 |  0.195 |                   13 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.745 |  0.224 |                   12 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.19466279447078705
[2m[36m(func pid=60723)[0m mae:  0.12878163158893585
[2m[36m(func pid=60723)[0m rmse_per_class: [0.13, 0.274, 0.129, 0.38, 0.056, 0.197, 0.391, 0.154, 0.14, 0.095]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1789865791797638
[2m[36m(func pid=49621)[0m mae:  0.12957394123077393
[2m[36m(func pid=49621)[0m rmse_per_class: [0.126, 0.266, 0.134, 0.348, 0.071, 0.188, 0.269, 0.137, 0.155, 0.095]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.5048 | Steps: 2 | Val loss: 0.3560 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=60725)[0m rmse: 0.21773497760295868
[2m[36m(func pid=60725)[0m mae:  0.1440659463405609
[2m[36m(func pid=60725)[0m rmse_per_class: [0.19, 0.263, 0.05, 0.386, 0.056, 0.225, 0.266, 0.219, 0.133, 0.39]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7975 | Steps: 2 | Val loss: 0.4783 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4684 | Steps: 2 | Val loss: 0.3443 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=50153)[0m rmse: 0.17863425612449646
[2m[36m(func pid=50153)[0m mae:  0.12443337589502335
[2m[36m(func pid=50153)[0m rmse_per_class: [0.167, 0.246, 0.084, 0.36, 0.056, 0.182, 0.248, 0.153, 0.195, 0.095]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.19773706793785095
[2m[36m(func pid=60723)[0m mae:  0.12980227172374725
[2m[36m(func pid=60723)[0m rmse_per_class: [0.134, 0.275, 0.133, 0.382, 0.056, 0.201, 0.407, 0.155, 0.138, 0.096]
[2m[36m(func pid=60723)[0m 
== Status ==
Current time: 2024-01-07 05:20:31 (running for 00:19:37.72)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.466 |  0.179 |                   62 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.505 |  0.179 |                   61 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.797 |  0.198 |                   14 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.789 |  0.218 |                   13 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.6607 | Steps: 2 | Val loss: 0.3941 | Batch size: 32 | lr: 0.1 | Duration: 2.91s

[2m[36m(func pid=49621)[0m rmse: 0.17902882397174835
[2m[36m(func pid=49621)[0m mae:  0.12952619791030884
[2m[36m(func pid=49621)[0m rmse_per_class: [0.126, 0.266, 0.134, 0.349, 0.07, 0.188, 0.269, 0.137, 0.155, 0.095]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5047 | Steps: 2 | Val loss: 0.3499 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=60725)[0m rmse: 0.18960435688495636
[2m[36m(func pid=60725)[0m mae:  0.12830817699432373
[2m[36m(func pid=60725)[0m rmse_per_class: [0.165, 0.251, 0.048, 0.382, 0.056, 0.213, 0.26, 0.113, 0.279, 0.128]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7303 | Steps: 2 | Val loss: 0.4750 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4679 | Steps: 2 | Val loss: 0.3444 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=50153)[0m rmse: 0.17668884992599487
[2m[36m(func pid=50153)[0m mae:  0.12350952625274658
[2m[36m(func pid=50153)[0m rmse_per_class: [0.153, 0.246, 0.079, 0.356, 0.056, 0.18, 0.247, 0.153, 0.201, 0.095]
[2m[36m(func pid=50153)[0m 
== Status ==
Current time: 2024-01-07 05:20:36 (running for 00:19:42.95)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.468 |  0.179 |                   63 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.505 |  0.177 |                   62 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.73  |  0.199 |                   15 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.661 |  0.19  |                   14 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.19908341765403748
[2m[36m(func pid=60723)[0m mae:  0.1303240954875946
[2m[36m(func pid=60723)[0m rmse_per_class: [0.144, 0.275, 0.125, 0.384, 0.056, 0.203, 0.416, 0.155, 0.138, 0.096]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.6373 | Steps: 2 | Val loss: 0.4381 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=49621)[0m rmse: 0.17893767356872559
[2m[36m(func pid=49621)[0m mae:  0.12941862642765045
[2m[36m(func pid=49621)[0m rmse_per_class: [0.126, 0.266, 0.134, 0.349, 0.07, 0.188, 0.268, 0.138, 0.156, 0.095]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5380 | Steps: 2 | Val loss: 0.3461 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=60725)[0m rmse: 0.21014845371246338
[2m[36m(func pid=60725)[0m mae:  0.14291740953922272
[2m[36m(func pid=60725)[0m rmse_per_class: [0.106, 0.417, 0.047, 0.373, 0.056, 0.203, 0.304, 0.152, 0.349, 0.094]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7114 | Steps: 2 | Val loss: 0.4691 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4696 | Steps: 2 | Val loss: 0.3457 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=50153)[0m rmse: 0.17529428005218506
[2m[36m(func pid=50153)[0m mae:  0.12284381687641144
[2m[36m(func pid=50153)[0m rmse_per_class: [0.142, 0.246, 0.075, 0.353, 0.056, 0.179, 0.246, 0.153, 0.207, 0.095]
[2m[36m(func pid=50153)[0m 
== Status ==
Current time: 2024-01-07 05:20:42 (running for 00:19:48.22)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.468 |  0.179 |                   64 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.538 |  0.175 |                   63 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.711 |  0.199 |                   16 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.637 |  0.21  |                   15 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.19900089502334595
[2m[36m(func pid=60723)[0m mae:  0.130488783121109
[2m[36m(func pid=60723)[0m rmse_per_class: [0.162, 0.273, 0.11, 0.384, 0.056, 0.204, 0.411, 0.155, 0.138, 0.097]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6501 | Steps: 2 | Val loss: 0.3725 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=49621)[0m rmse: 0.17896750569343567
[2m[36m(func pid=49621)[0m mae:  0.12937824428081512
[2m[36m(func pid=49621)[0m rmse_per_class: [0.127, 0.266, 0.135, 0.349, 0.069, 0.188, 0.268, 0.138, 0.156, 0.094]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4997 | Steps: 2 | Val loss: 0.3413 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7088 | Steps: 2 | Val loss: 0.4606 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=60725)[0m rmse: 0.17336516082286835
[2m[36m(func pid=60725)[0m mae:  0.11560387909412384
[2m[36m(func pid=60725)[0m rmse_per_class: [0.11, 0.374, 0.049, 0.283, 0.056, 0.166, 0.282, 0.155, 0.161, 0.096]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4702 | Steps: 2 | Val loss: 0.3468 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=50153)[0m rmse: 0.17378757894039154
[2m[36m(func pid=50153)[0m mae:  0.12215279042720795
[2m[36m(func pid=50153)[0m rmse_per_class: [0.129, 0.248, 0.071, 0.349, 0.056, 0.177, 0.247, 0.153, 0.212, 0.095]
[2m[36m(func pid=50153)[0m 
== Status ==
Current time: 2024-01-07 05:20:47 (running for 00:19:53.44)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.47  |  0.179 |                   65 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.5   |  0.174 |                   64 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.709 |  0.197 |                   17 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.65  |  0.173 |                   16 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.19730409979820251
[2m[36m(func pid=60723)[0m mae:  0.13004307448863983
[2m[36m(func pid=60723)[0m rmse_per_class: [0.187, 0.269, 0.089, 0.385, 0.056, 0.203, 0.392, 0.155, 0.139, 0.097]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17897775769233704
[2m[36m(func pid=49621)[0m mae:  0.12931928038597107
[2m[36m(func pid=49621)[0m rmse_per_class: [0.127, 0.266, 0.135, 0.349, 0.069, 0.188, 0.268, 0.138, 0.156, 0.094]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6059 | Steps: 2 | Val loss: 0.3722 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4886 | Steps: 2 | Val loss: 0.3360 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.7124 | Steps: 2 | Val loss: 0.4498 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=60725)[0m rmse: 0.16723480820655823
[2m[36m(func pid=60725)[0m mae:  0.10398876667022705
[2m[36m(func pid=60725)[0m rmse_per_class: [0.11, 0.222, 0.05, 0.318, 0.056, 0.28, 0.249, 0.156, 0.134, 0.097]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4786 | Steps: 2 | Val loss: 0.3479 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=50153)[0m rmse: 0.17215679585933685
[2m[36m(func pid=50153)[0m mae:  0.12133149802684784
[2m[36m(func pid=50153)[0m rmse_per_class: [0.118, 0.251, 0.065, 0.344, 0.056, 0.176, 0.249, 0.153, 0.215, 0.095]
[2m[36m(func pid=50153)[0m 
== Status ==
Current time: 2024-01-07 05:20:52 (running for 00:19:58.80)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.47  |  0.179 |                   66 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.489 |  0.172 |                   65 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.712 |  0.194 |                   18 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.606 |  0.167 |                   17 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.19379457831382751
[2m[36m(func pid=60723)[0m mae:  0.12877073884010315
[2m[36m(func pid=60723)[0m rmse_per_class: [0.207, 0.261, 0.073, 0.384, 0.056, 0.2, 0.359, 0.156, 0.144, 0.097]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1789996176958084
[2m[36m(func pid=49621)[0m mae:  0.1292661875486374
[2m[36m(func pid=49621)[0m rmse_per_class: [0.127, 0.266, 0.135, 0.35, 0.068, 0.188, 0.268, 0.138, 0.156, 0.094]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6807 | Steps: 2 | Val loss: 0.4014 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4807 | Steps: 2 | Val loss: 0.3315 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.7006 | Steps: 2 | Val loss: 0.4341 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4729 | Steps: 2 | Val loss: 0.3485 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=60725)[0m rmse: 0.18849512934684753
[2m[36m(func pid=60725)[0m mae:  0.11789979785680771
[2m[36m(func pid=60725)[0m rmse_per_class: [0.102, 0.269, 0.05, 0.294, 0.052, 0.448, 0.28, 0.156, 0.137, 0.097]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.17080779373645782
[2m[36m(func pid=50153)[0m mae:  0.12057876586914062
[2m[36m(func pid=50153)[0m rmse_per_class: [0.108, 0.254, 0.06, 0.339, 0.056, 0.175, 0.252, 0.152, 0.217, 0.095]
[2m[36m(func pid=50153)[0m 
== Status ==
Current time: 2024-01-07 05:20:57 (running for 00:20:04.12)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.479 |  0.179 |                   67 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.481 |  0.171 |                   66 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.701 |  0.188 |                   19 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.681 |  0.188 |                   18 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.1877361238002777
[2m[36m(func pid=60723)[0m mae:  0.1265917718410492
[2m[36m(func pid=60723)[0m rmse_per_class: [0.207, 0.249, 0.063, 0.383, 0.056, 0.193, 0.315, 0.156, 0.157, 0.097]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.17898084223270416
[2m[36m(func pid=49621)[0m mae:  0.12919461727142334
[2m[36m(func pid=49621)[0m rmse_per_class: [0.128, 0.266, 0.136, 0.35, 0.067, 0.188, 0.267, 0.138, 0.156, 0.094]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.7223 | Steps: 2 | Val loss: 0.4261 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4768 | Steps: 2 | Val loss: 0.3282 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4740 | Steps: 2 | Val loss: 0.3489 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6836 | Steps: 2 | Val loss: 0.4120 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=60725)[0m rmse: 0.20933552086353302
[2m[36m(func pid=60725)[0m mae:  0.1290712058544159
[2m[36m(func pid=60725)[0m rmse_per_class: [0.303, 0.279, 0.054, 0.367, 0.137, 0.329, 0.236, 0.156, 0.135, 0.097]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.1698271930217743
[2m[36m(func pid=50153)[0m mae:  0.11994318664073944
[2m[36m(func pid=50153)[0m rmse_per_class: [0.102, 0.258, 0.055, 0.333, 0.056, 0.174, 0.256, 0.152, 0.217, 0.095]
[2m[36m(func pid=50153)[0m 
== Status ==
Current time: 2024-01-07 05:21:03 (running for 00:20:09.30)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.474 |  0.179 |                   69 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.477 |  0.17  |                   67 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.701 |  0.188 |                   19 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.722 |  0.209 |                   19 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.17894776165485382
[2m[36m(func pid=49621)[0m mae:  0.12911948561668396
[2m[36m(func pid=49621)[0m rmse_per_class: [0.128, 0.266, 0.136, 0.35, 0.067, 0.188, 0.267, 0.138, 0.156, 0.094]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.17983092367649078
[2m[36m(func pid=60723)[0m mae:  0.12397028505802155
[2m[36m(func pid=60723)[0m rmse_per_class: [0.176, 0.24, 0.054, 0.381, 0.056, 0.184, 0.268, 0.156, 0.187, 0.097]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6139 | Steps: 2 | Val loss: 0.5117 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4692 | Steps: 2 | Val loss: 0.3256 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4768 | Steps: 2 | Val loss: 0.3495 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6375 | Steps: 2 | Val loss: 0.3967 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=60725)[0m rmse: 0.24294908344745636
[2m[36m(func pid=60725)[0m mae:  0.15384206175804138
[2m[36m(func pid=60725)[0m rmse_per_class: [0.5, 0.284, 0.051, 0.385, 0.382, 0.175, 0.271, 0.154, 0.133, 0.096]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.16906505823135376
[2m[36m(func pid=50153)[0m mae:  0.11939026415348053
[2m[36m(func pid=50153)[0m rmse_per_class: [0.098, 0.262, 0.052, 0.328, 0.056, 0.174, 0.26, 0.151, 0.215, 0.094]
[2m[36m(func pid=50153)[0m 
== Status ==
Current time: 2024-01-07 05:21:08 (running for 00:20:14.46)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.474 |  0.179 |                   69 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.469 |  0.169 |                   68 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.638 |  0.176 |                   21 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.614 |  0.243 |                   20 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=60723)[0m rmse: 0.1763024926185608
[2m[36m(func pid=60723)[0m mae:  0.12380687147378922
[2m[36m(func pid=60723)[0m rmse_per_class: [0.133, 0.253, 0.047, 0.377, 0.056, 0.176, 0.242, 0.156, 0.226, 0.097]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=49621)[0m rmse: 0.1789163500070572
[2m[36m(func pid=49621)[0m mae:  0.12905097007751465
[2m[36m(func pid=49621)[0m rmse_per_class: [0.128, 0.266, 0.136, 0.35, 0.066, 0.188, 0.266, 0.138, 0.156, 0.094]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5897 | Steps: 2 | Val loss: 0.5511 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4636 | Steps: 2 | Val loss: 0.3235 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4783 | Steps: 2 | Val loss: 0.3499 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6231 | Steps: 2 | Val loss: 0.3903 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=60725)[0m rmse: 0.2440597116947174
[2m[36m(func pid=60725)[0m mae:  0.15899798274040222
[2m[36m(func pid=60725)[0m rmse_per_class: [0.249, 0.257, 0.067, 0.385, 0.53, 0.216, 0.295, 0.142, 0.206, 0.092]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.1684015691280365
[2m[36m(func pid=50153)[0m mae:  0.11881983280181885
[2m[36m(func pid=50153)[0m rmse_per_class: [0.096, 0.266, 0.049, 0.322, 0.056, 0.174, 0.265, 0.151, 0.212, 0.094]
[2m[36m(func pid=50153)[0m 
== Status ==
Current time: 2024-01-07 05:21:13 (running for 00:20:19.77)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.477 |  0.179 |                   70 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.464 |  0.168 |                   69 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.623 |  0.179 |                   22 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.59  |  0.244 |                   21 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.17884404957294464
[2m[36m(func pid=49621)[0m mae:  0.12893758714199066
[2m[36m(func pid=49621)[0m rmse_per_class: [0.128, 0.266, 0.136, 0.35, 0.066, 0.188, 0.266, 0.138, 0.156, 0.094]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.17866918444633484
[2m[36m(func pid=60723)[0m mae:  0.12574103474617004
[2m[36m(func pid=60723)[0m rmse_per_class: [0.101, 0.288, 0.042, 0.37, 0.056, 0.171, 0.256, 0.155, 0.25, 0.097]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4659 | Steps: 2 | Val loss: 0.3208 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5688 | Steps: 2 | Val loss: 0.5239 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5975 | Steps: 2 | Val loss: 0.3871 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4889 | Steps: 2 | Val loss: 0.3515 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=50153)[0m rmse: 0.16750876605510712
[2m[36m(func pid=50153)[0m mae:  0.11804614961147308
[2m[36m(func pid=50153)[0m rmse_per_class: [0.096, 0.269, 0.046, 0.316, 0.056, 0.174, 0.269, 0.15, 0.205, 0.094]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.22316376864910126
[2m[36m(func pid=60725)[0m mae:  0.14796140789985657
[2m[36m(func pid=60725)[0m rmse_per_class: [0.101, 0.219, 0.052, 0.384, 0.384, 0.227, 0.3, 0.103, 0.373, 0.089]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:21:19 (running for 00:20:25.16)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.478 |  0.179 |                   71 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.466 |  0.168 |                   70 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.598 |  0.182 |                   23 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.569 |  0.223 |                   22 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.17892715334892273
[2m[36m(func pid=49621)[0m mae:  0.12891481816768646
[2m[36m(func pid=49621)[0m rmse_per_class: [0.129, 0.266, 0.136, 0.351, 0.065, 0.188, 0.266, 0.138, 0.156, 0.094]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.1820305436849594
[2m[36m(func pid=60723)[0m mae:  0.12732471525669098
[2m[36m(func pid=60723)[0m rmse_per_class: [0.096, 0.318, 0.042, 0.357, 0.056, 0.176, 0.28, 0.155, 0.242, 0.097]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4611 | Steps: 2 | Val loss: 0.3191 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5547 | Steps: 2 | Val loss: 0.4721 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4823 | Steps: 2 | Val loss: 0.3530 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5941 | Steps: 2 | Val loss: 0.3785 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=50153)[0m rmse: 0.1667664349079132
[2m[36m(func pid=50153)[0m mae:  0.11736470460891724
[2m[36m(func pid=50153)[0m rmse_per_class: [0.096, 0.27, 0.044, 0.311, 0.056, 0.175, 0.274, 0.149, 0.198, 0.094]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.2005351036787033
[2m[36m(func pid=60725)[0m mae:  0.13326725363731384
[2m[36m(func pid=60725)[0m rmse_per_class: [0.107, 0.285, 0.043, 0.37, 0.174, 0.228, 0.28, 0.145, 0.143, 0.231]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:21:24 (running for 00:20:30.39)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.482 |  0.179 |                   73 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.461 |  0.167 |                   71 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.598 |  0.182 |                   23 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.555 |  0.201 |                   23 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.17899051308631897
[2m[36m(func pid=49621)[0m mae:  0.1288781315088272
[2m[36m(func pid=49621)[0m rmse_per_class: [0.13, 0.266, 0.137, 0.351, 0.065, 0.188, 0.266, 0.139, 0.156, 0.093]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.18111738562583923
[2m[36m(func pid=60723)[0m mae:  0.1260720044374466
[2m[36m(func pid=60723)[0m rmse_per_class: [0.102, 0.321, 0.044, 0.333, 0.056, 0.193, 0.299, 0.155, 0.211, 0.096]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4720 | Steps: 2 | Val loss: 0.3175 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5737 | Steps: 2 | Val loss: 0.3913 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4818 | Steps: 2 | Val loss: 0.3539 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5700 | Steps: 2 | Val loss: 0.3646 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=50153)[0m rmse: 0.16598980128765106
[2m[36m(func pid=50153)[0m mae:  0.11662246286869049
[2m[36m(func pid=50153)[0m rmse_per_class: [0.098, 0.271, 0.043, 0.306, 0.056, 0.176, 0.278, 0.148, 0.191, 0.093]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.17375695705413818
[2m[36m(func pid=60725)[0m mae:  0.11104510724544525
[2m[36m(func pid=60725)[0m rmse_per_class: [0.103, 0.263, 0.044, 0.289, 0.073, 0.221, 0.256, 0.221, 0.135, 0.133]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:21:29 (running for 00:20:35.74)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.15724999457597733
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00008 | RUNNING    | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.482 |  0.179 |                   74 |
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.472 |  0.166 |                   72 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.594 |  0.181 |                   24 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.574 |  0.174 |                   24 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.17902256548404694
[2m[36m(func pid=49621)[0m mae:  0.12884008884429932
[2m[36m(func pid=49621)[0m rmse_per_class: [0.13, 0.266, 0.137, 0.351, 0.064, 0.188, 0.266, 0.139, 0.156, 0.093]
[2m[36m(func pid=49621)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.17516639828681946
[2m[36m(func pid=60723)[0m mae:  0.12163424491882324
[2m[36m(func pid=60723)[0m rmse_per_class: [0.106, 0.293, 0.046, 0.303, 0.056, 0.218, 0.31, 0.155, 0.168, 0.096]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4750 | Steps: 2 | Val loss: 0.3161 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5119 | Steps: 2 | Val loss: 0.3543 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=49621)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4839 | Steps: 2 | Val loss: 0.3543 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5806 | Steps: 2 | Val loss: 0.3595 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=50153)[0m rmse: 0.16523422300815582
[2m[36m(func pid=50153)[0m mae:  0.1159132719039917
[2m[36m(func pid=50153)[0m rmse_per_class: [0.099, 0.27, 0.043, 0.302, 0.056, 0.177, 0.282, 0.147, 0.183, 0.093]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.16322970390319824
[2m[36m(func pid=60725)[0m mae:  0.10038896650075912
[2m[36m(func pid=60725)[0m rmse_per_class: [0.111, 0.208, 0.064, 0.322, 0.051, 0.187, 0.275, 0.189, 0.138, 0.087]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:21:34 (running for 00:20:40.97)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 3 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.475 |  0.165 |                   73 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.57  |  0.175 |                   25 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.512 |  0.163 |                   25 |
| train_5a6ec_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING)


[2m[36m(func pid=49621)[0m rmse: 0.17898441851139069
[2m[36m(func pid=49621)[0m mae:  0.12875884771347046
[2m[36m(func pid=49621)[0m rmse_per_class: [0.13, 0.266, 0.137, 0.351, 0.064, 0.188, 0.265, 0.139, 0.156, 0.093]
[2m[36m(func pid=60723)[0m rmse: 0.17095474898815155
[2m[36m(func pid=60723)[0m mae:  0.11708319187164307
[2m[36m(func pid=60723)[0m rmse_per_class: [0.108, 0.25, 0.048, 0.303, 0.056, 0.239, 0.316, 0.154, 0.139, 0.095]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4618 | Steps: 2 | Val loss: 0.3147 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5283 | Steps: 2 | Val loss: 0.3750 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5711 | Steps: 2 | Val loss: 0.3615 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=50153)[0m rmse: 0.16440799832344055
[2m[36m(func pid=50153)[0m mae:  0.11516300588846207
[2m[36m(func pid=50153)[0m rmse_per_class: [0.1, 0.268, 0.043, 0.299, 0.056, 0.178, 0.285, 0.146, 0.176, 0.092]
[2m[36m(func pid=50153)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.17831328511238098
[2m[36m(func pid=60725)[0m mae:  0.11259831488132477
[2m[36m(func pid=60725)[0m rmse_per_class: [0.312, 0.254, 0.05, 0.288, 0.055, 0.174, 0.305, 0.111, 0.138, 0.094]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.17519491910934448
[2m[36m(func pid=60723)[0m mae:  0.11571724712848663
[2m[36m(func pid=60723)[0m rmse_per_class: [0.109, 0.241, 0.048, 0.351, 0.056, 0.25, 0.319, 0.151, 0.133, 0.094]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=50153)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4646 | Steps: 2 | Val loss: 0.3129 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5241 | Steps: 2 | Val loss: 0.4092 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6055 | Steps: 2 | Val loss: 0.3560 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 05:21:40 (running for 00:20:46.22)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00009 | RUNNING    | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.462 |  0.164 |                   74 |
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.571 |  0.175 |                   27 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.528 |  0.178 |                   26 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=67138)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67138)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=67138)[0m Configuration completed!
[2m[36m(func pid=67138)[0m New optimizer parameters:
[2m[36m(func pid=67138)[0m SGD (
[2m[36m(func pid=67138)[0m Parameter Group 0
[2m[36m(func pid=67138)[0m     dampening: 0
[2m[36m(func pid=67138)[0m     differentiable: False
[2m[36m(func pid=67138)[0m     foreach: None
[2m[36m(func pid=67138)[0m     lr: 0.0001
[2m[36m(func pid=67138)[0m     maximize: False
[2m[36m(func pid=67138)[0m     momentum: 0.9
[2m[36m(func pid=67138)[0m     nesterov: False
[2m[36m(func pid=67138)[0m     weight_decay: 0.0001
[2m[36m(func pid=67138)[0m )
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=50153)[0m rmse: 0.1635318100452423
[2m[36m(func pid=50153)[0m mae:  0.11433162540197372
[2m[36m(func pid=50153)[0m rmse_per_class: [0.101, 0.265, 0.043, 0.297, 0.056, 0.18, 0.288, 0.145, 0.168, 0.092]
[2m[36m(func pid=60725)[0m rmse: 0.20814073085784912
[2m[36m(func pid=60725)[0m mae:  0.13126133382320404
[2m[36m(func pid=60725)[0m rmse_per_class: [0.359, 0.262, 0.056, 0.313, 0.056, 0.376, 0.31, 0.119, 0.135, 0.096]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.1790294349193573
[2m[36m(func pid=60723)[0m mae:  0.1155569925904274
[2m[36m(func pid=60723)[0m rmse_per_class: [0.11, 0.255, 0.048, 0.382, 0.056, 0.251, 0.317, 0.147, 0.133, 0.092]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5388 | Steps: 2 | Val loss: 0.4295 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0472 | Steps: 2 | Val loss: 0.7193 | Batch size: 32 | lr: 0.0001 | Duration: 4.40s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5879 | Steps: 2 | Val loss: 0.3396 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=67138)[0m rmse: 0.17989811301231384
[2m[36m(func pid=67138)[0m mae:  0.13242384791374207
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.263, 0.097, 0.333, 0.1, 0.191, 0.298, 0.143, 0.141, 0.119]
[2m[36m(func pid=60725)[0m rmse: 0.200490802526474
[2m[36m(func pid=60725)[0m mae:  0.12904876470565796
[2m[36m(func pid=60725)[0m rmse_per_class: [0.164, 0.227, 0.058, 0.35, 0.056, 0.357, 0.285, 0.147, 0.265, 0.096]
[2m[36m(func pid=60723)[0m rmse: 0.17735476791858673
[2m[36m(func pid=60723)[0m mae:  0.11402510106563568
[2m[36m(func pid=60723)[0m rmse_per_class: [0.109, 0.268, 0.049, 0.381, 0.056, 0.237, 0.309, 0.137, 0.134, 0.094]
== Status ==
Current time: 2024-01-07 05:21:45 (running for 00:20:51.29)
Memory usage on this node: 21.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.605 |  0.179 |                   28 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.524 |  0.208 |                   27 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


== Status ==
Current time: 2024-01-07 05:21:51 (running for 00:20:58.13)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.588 |  0.177 |                   29 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.524 |  0.208 |                   27 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=67747)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=67747)[0m Configuration completed!
[2m[36m(func pid=67747)[0m New optimizer parameters:
[2m[36m(func pid=67747)[0m SGD (
[2m[36m(func pid=67747)[0m Parameter Group 0
[2m[36m(func pid=67747)[0m     dampening: 0
[2m[36m(func pid=67747)[0m     differentiable: False
[2m[36m(func pid=67747)[0m     foreach: None
[2m[36m(func pid=67747)[0m     lr: 0.001
[2m[36m(func pid=67747)[0m     maximize: False
[2m[36m(func pid=67747)[0m     momentum: 0.9
[2m[36m(func pid=67747)[0m     nesterov: False
[2m[36m(func pid=67747)[0m     weight_decay: 0.0001
[2m[36m(func pid=67747)[0m )
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5509 | Steps: 2 | Val loss: 0.3298 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5669 | Steps: 2 | Val loss: 0.4808 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0406 | Steps: 2 | Val loss: 0.7193 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0423 | Steps: 2 | Val loss: 0.7133 | Batch size: 32 | lr: 0.001 | Duration: 4.49s
== Status ==
Current time: 2024-01-07 05:21:57 (running for 00:21:03.17)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.588 |  0.177 |                   29 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.539 |  0.2   |                   28 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  1.047 |  0.18  |                    1 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.17135420441627502
[2m[36m(func pid=60723)[0m mae:  0.11195573955774307
[2m[36m(func pid=60723)[0m rmse_per_class: [0.107, 0.276, 0.051, 0.334, 0.056, 0.212, 0.294, 0.124, 0.134, 0.125]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18038801848888397
[2m[36m(func pid=67138)[0m mae:  0.13285721838474274
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.20409110188484192
[2m[36m(func pid=60725)[0m mae:  0.13300231099128723
[2m[36m(func pid=60725)[0m rmse_per_class: [0.102, 0.217, 0.05, 0.374, 0.056, 0.158, 0.25, 0.154, 0.583, 0.096]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17989863455295563
[2m[36m(func pid=67747)[0m mae:  0.13242092728614807
[2m[36m(func pid=67747)[0m rmse_per_class: [0.114, 0.263, 0.097, 0.333, 0.1, 0.191, 0.298, 0.143, 0.141, 0.119]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5257 | Steps: 2 | Val loss: 0.3416 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0361 | Steps: 2 | Val loss: 0.7234 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.6310 | Steps: 2 | Val loss: 0.4580 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0107 | Steps: 2 | Val loss: 0.7052 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 05:22:02 (running for 00:21:08.64)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.526 |  0.173 |                   31 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.567 |  0.204 |                   29 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  1.041 |  0.18  |                    2 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  1.042 |  0.18  |                    1 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.17293907701969147
[2m[36m(func pid=60723)[0m mae:  0.11583168804645538
[2m[36m(func pid=60723)[0m rmse_per_class: [0.101, 0.281, 0.058, 0.279, 0.056, 0.182, 0.283, 0.143, 0.134, 0.211]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18065352737903595
[2m[36m(func pid=67138)[0m mae:  0.13309679925441742
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.105, 0.191, 0.3, 0.142, 0.143, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.1890382319688797
[2m[36m(func pid=60725)[0m mae:  0.11642654985189438
[2m[36m(func pid=60725)[0m rmse_per_class: [0.1, 0.29, 0.054, 0.377, 0.056, 0.192, 0.274, 0.155, 0.297, 0.095]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.18036961555480957
[2m[36m(func pid=67747)[0m mae:  0.1328366994857788
[2m[36m(func pid=67747)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5124 | Steps: 2 | Val loss: 0.3748 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.0279 | Steps: 2 | Val loss: 0.7278 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5998 | Steps: 2 | Val loss: 0.4512 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.9618 | Steps: 2 | Val loss: 0.6936 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 05:22:07 (running for 00:21:13.82)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.512 |  0.194 |                   32 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.631 |  0.189 |                   30 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  1.036 |  0.181 |                    3 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  1.011 |  0.18  |                    2 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.19410213828086853
[2m[36m(func pid=60723)[0m mae:  0.13029244542121887
[2m[36m(func pid=60723)[0m rmse_per_class: [0.098, 0.287, 0.062, 0.317, 0.056, 0.18, 0.29, 0.205, 0.135, 0.311]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18079327046871185
[2m[36m(func pid=67138)[0m mae:  0.133224755525589
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.331, 0.106, 0.191, 0.302, 0.142, 0.143, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.19097109138965607
[2m[36m(func pid=60725)[0m mae:  0.1142684817314148
[2m[36m(func pid=60725)[0m rmse_per_class: [0.1, 0.42, 0.048, 0.368, 0.056, 0.207, 0.331, 0.155, 0.132, 0.092]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.18057841062545776
[2m[36m(func pid=67747)[0m mae:  0.1330181062221527
[2m[36m(func pid=67747)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.104, 0.191, 0.299, 0.142, 0.143, 0.115]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.5269 | Steps: 2 | Val loss: 0.4201 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.0208 | Steps: 2 | Val loss: 0.7308 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.6149 | Steps: 2 | Val loss: 0.4408 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8982 | Steps: 2 | Val loss: 0.6745 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=60723)[0m rmse: 0.21064725518226624
[2m[36m(func pid=60723)[0m mae:  0.14189784228801727
[2m[36m(func pid=60723)[0m rmse_per_class: [0.1, 0.292, 0.061, 0.36, 0.056, 0.196, 0.287, 0.258, 0.136, 0.36]
[2m[36m(func pid=60723)[0m 
== Status ==
Current time: 2024-01-07 05:22:13 (running for 00:21:19.52)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.527 |  0.211 |                   33 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.6   |  0.191 |                   31 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  1.021 |  0.181 |                    5 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.962 |  0.181 |                    3 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m rmse: 0.18088248372077942
[2m[36m(func pid=67138)[0m mae:  0.13330893218517303
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.107, 0.191, 0.303, 0.142, 0.143, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.18817868828773499
[2m[36m(func pid=60725)[0m mae:  0.1104956865310669
[2m[36m(func pid=60725)[0m rmse_per_class: [0.219, 0.373, 0.041, 0.322, 0.056, 0.196, 0.293, 0.155, 0.137, 0.089]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.18063515424728394
[2m[36m(func pid=67747)[0m mae:  0.133073627948761
[2m[36m(func pid=67747)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.332, 0.105, 0.191, 0.3, 0.142, 0.143, 0.114]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5655 | Steps: 2 | Val loss: 0.4628 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.0081 | Steps: 2 | Val loss: 0.7322 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.6385 | Steps: 2 | Val loss: 0.4088 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8293 | Steps: 2 | Val loss: 0.6486 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=60723)[0m rmse: 0.2159091979265213
[2m[36m(func pid=60723)[0m mae:  0.14608708024024963
[2m[36m(func pid=60723)[0m rmse_per_class: [0.108, 0.294, 0.052, 0.377, 0.056, 0.209, 0.278, 0.292, 0.137, 0.357]
[2m[36m(func pid=60723)[0m 
== Status ==
Current time: 2024-01-07 05:22:18 (running for 00:21:24.69)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.566 |  0.216 |                   34 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.615 |  0.188 |                   32 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  1.008 |  0.181 |                    6 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.898 |  0.181 |                    4 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m rmse: 0.18094229698181152
[2m[36m(func pid=67138)[0m mae:  0.1333630084991455
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.107, 0.191, 0.304, 0.142, 0.142, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.16982759535312653
[2m[36m(func pid=60725)[0m mae:  0.10119609534740448
[2m[36m(func pid=60725)[0m rmse_per_class: [0.209, 0.221, 0.049, 0.319, 0.056, 0.172, 0.273, 0.153, 0.138, 0.109]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.18064963817596436
[2m[36m(func pid=67747)[0m mae:  0.13308124244213104
[2m[36m(func pid=67747)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.331, 0.106, 0.191, 0.301, 0.141, 0.143, 0.114]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.5831 | Steps: 2 | Val loss: 0.4883 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.9947 | Steps: 2 | Val loss: 0.7315 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5502 | Steps: 2 | Val loss: 0.3852 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.7676 | Steps: 2 | Val loss: 0.6176 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=60723)[0m rmse: 0.2163781225681305
[2m[36m(func pid=60723)[0m mae:  0.14569918811321259
[2m[36m(func pid=60723)[0m rmse_per_class: [0.13, 0.294, 0.047, 0.382, 0.056, 0.216, 0.269, 0.299, 0.137, 0.333]
[2m[36m(func pid=60723)[0m 
== Status ==
Current time: 2024-01-07 05:22:23 (running for 00:21:29.81)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.583 |  0.216 |                   35 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.639 |  0.17  |                   33 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.995 |  0.181 |                    7 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.829 |  0.181 |                    5 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m rmse: 0.18098758161067963
[2m[36m(func pid=67138)[0m mae:  0.13339798152446747
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.33, 0.108, 0.191, 0.304, 0.142, 0.142, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.17516842484474182
[2m[36m(func pid=60725)[0m mae:  0.10699282586574554
[2m[36m(func pid=60725)[0m rmse_per_class: [0.105, 0.215, 0.064, 0.344, 0.056, 0.168, 0.296, 0.143, 0.138, 0.223]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1805858612060547
[2m[36m(func pid=67747)[0m mae:  0.13301631808280945
[2m[36m(func pid=67747)[0m rmse_per_class: [0.113, 0.264, 0.103, 0.331, 0.106, 0.191, 0.301, 0.141, 0.143, 0.113]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6048 | Steps: 2 | Val loss: 0.4941 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.9852 | Steps: 2 | Val loss: 0.7294 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.5292 | Steps: 2 | Val loss: 0.3955 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7065 | Steps: 2 | Val loss: 0.5842 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=60723)[0m rmse: 0.21484580636024475
[2m[36m(func pid=60723)[0m mae:  0.1434142142534256
[2m[36m(func pid=60723)[0m rmse_per_class: [0.156, 0.293, 0.047, 0.385, 0.056, 0.22, 0.262, 0.296, 0.136, 0.298]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18101146817207336
[2m[36m(func pid=67138)[0m mae:  0.13341495394706726
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.33, 0.108, 0.191, 0.305, 0.143, 0.142, 0.116]
== Status ==
Current time: 2024-01-07 05:22:28 (running for 00:21:35.02)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.605 |  0.215 |                   36 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.55  |  0.175 |                   34 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.985 |  0.181 |                    8 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.768 |  0.181 |                    6 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.18589825928211212
[2m[36m(func pid=60725)[0m mae:  0.11971926689147949
[2m[36m(func pid=60725)[0m rmse_per_class: [0.104, 0.245, 0.061, 0.266, 0.056, 0.242, 0.294, 0.121, 0.133, 0.337]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.18049809336662292
[2m[36m(func pid=67747)[0m mae:  0.13292542099952698
[2m[36m(func pid=67747)[0m rmse_per_class: [0.114, 0.264, 0.103, 0.331, 0.106, 0.191, 0.3, 0.141, 0.143, 0.113]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5891 | Steps: 2 | Val loss: 0.4803 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.9746 | Steps: 2 | Val loss: 0.7264 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.5244 | Steps: 2 | Val loss: 0.3898 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.6468 | Steps: 2 | Val loss: 0.5503 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=60723)[0m rmse: 0.21198201179504395
[2m[36m(func pid=60723)[0m mae:  0.14027747511863708
[2m[36m(func pid=60723)[0m rmse_per_class: [0.186, 0.29, 0.049, 0.386, 0.056, 0.221, 0.257, 0.283, 0.135, 0.258]
[2m[36m(func pid=60723)[0m 
== Status ==
Current time: 2024-01-07 05:22:34 (running for 00:21:40.21)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.589 |  0.212 |                   37 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.529 |  0.186 |                   35 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.975 |  0.181 |                    9 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.707 |  0.18  |                    7 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m rmse: 0.18101757764816284
[2m[36m(func pid=67138)[0m mae:  0.1334095299243927
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.108, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.18846958875656128
[2m[36m(func pid=60725)[0m mae:  0.1221935898065567
[2m[36m(func pid=60725)[0m rmse_per_class: [0.106, 0.241, 0.073, 0.332, 0.056, 0.17, 0.253, 0.098, 0.28, 0.275]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1803784817457199
[2m[36m(func pid=67747)[0m mae:  0.1328052282333374
[2m[36m(func pid=67747)[0m rmse_per_class: [0.114, 0.264, 0.104, 0.331, 0.106, 0.191, 0.3, 0.14, 0.143, 0.112]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6023 | Steps: 2 | Val loss: 0.4594 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.9625 | Steps: 2 | Val loss: 0.7216 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4807 | Steps: 2 | Val loss: 0.4428 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6105 | Steps: 2 | Val loss: 0.5181 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=60723)[0m rmse: 0.2073320597410202
[2m[36m(func pid=60723)[0m mae:  0.1363956332206726
[2m[36m(func pid=60723)[0m rmse_per_class: [0.219, 0.284, 0.052, 0.386, 0.056, 0.221, 0.251, 0.257, 0.133, 0.214]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.1810397207736969
[2m[36m(func pid=67138)[0m mae:  0.13342860341072083
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.191, 0.306, 0.143, 0.142, 0.116]
[2m[36m(func pid=67138)[0m 
== Status ==
Current time: 2024-01-07 05:22:39 (running for 00:21:45.49)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.602 |  0.207 |                   38 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.524 |  0.188 |                   36 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.962 |  0.181 |                   10 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.647 |  0.18  |                    8 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60725)[0m rmse: 0.19349029660224915
[2m[36m(func pid=60725)[0m mae:  0.12686042487621307
[2m[36m(func pid=60725)[0m rmse_per_class: [0.101, 0.226, 0.05, 0.373, 0.055, 0.191, 0.25, 0.122, 0.444, 0.123]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1802418977022171
[2m[36m(func pid=67747)[0m mae:  0.13267412781715393
[2m[36m(func pid=67747)[0m rmse_per_class: [0.114, 0.264, 0.105, 0.331, 0.105, 0.19, 0.299, 0.14, 0.143, 0.111]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.5509 | Steps: 2 | Val loss: 0.4378 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.9475 | Steps: 2 | Val loss: 0.7166 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.5152 | Steps: 2 | Val loss: 0.4504 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5823 | Steps: 2 | Val loss: 0.4893 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=60723)[0m rmse: 0.20014452934265137
[2m[36m(func pid=60723)[0m mae:  0.13232092559337616
[2m[36m(func pid=60723)[0m rmse_per_class: [0.245, 0.269, 0.055, 0.386, 0.056, 0.219, 0.245, 0.215, 0.139, 0.173]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18105639517307281
[2m[36m(func pid=67138)[0m mae:  0.13343775272369385
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.191, 0.306, 0.144, 0.142, 0.116]
== Status ==
Current time: 2024-01-07 05:22:44 (running for 00:21:50.68)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.551 |  0.2   |                   39 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.481 |  0.193 |                   37 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.947 |  0.181 |                   11 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.611 |  0.18  |                    9 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.19108840823173523
[2m[36m(func pid=60725)[0m mae:  0.12018629163503647
[2m[36m(func pid=60725)[0m rmse_per_class: [0.155, 0.241, 0.052, 0.379, 0.052, 0.202, 0.248, 0.252, 0.238, 0.094]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.18004490435123444
[2m[36m(func pid=67747)[0m mae:  0.13249234855175018
[2m[36m(func pid=67747)[0m rmse_per_class: [0.114, 0.264, 0.106, 0.331, 0.105, 0.19, 0.298, 0.14, 0.144, 0.11]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.5182 | Steps: 2 | Val loss: 0.4200 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.9386 | Steps: 2 | Val loss: 0.7108 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=60723)[0m rmse: 0.19049975275993347
[2m[36m(func pid=60723)[0m mae:  0.12805300951004028
[2m[36m(func pid=60723)[0m rmse_per_class: [0.232, 0.245, 0.056, 0.385, 0.056, 0.215, 0.237, 0.166, 0.178, 0.134]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.5621 | Steps: 2 | Val loss: 0.4612 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5503 | Steps: 2 | Val loss: 0.4635 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 05:22:49 (running for 00:21:56.00)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.518 |  0.19  |                   40 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.515 |  0.191 |                   38 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.939 |  0.181 |                   12 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.582 |  0.18  |                   10 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m rmse: 0.18107636272907257
[2m[36m(func pid=67138)[0m mae:  0.13345280289649963
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.263, 0.098, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1798524558544159
[2m[36m(func pid=67747)[0m mae:  0.1323186308145523
[2m[36m(func pid=67747)[0m rmse_per_class: [0.114, 0.264, 0.106, 0.331, 0.104, 0.189, 0.298, 0.139, 0.144, 0.109]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.20662209391593933
[2m[36m(func pid=60725)[0m mae:  0.12132082879543304
[2m[36m(func pid=60725)[0m rmse_per_class: [0.261, 0.339, 0.049, 0.377, 0.051, 0.194, 0.264, 0.305, 0.135, 0.091]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4832 | Steps: 2 | Val loss: 0.4117 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.9271 | Steps: 2 | Val loss: 0.7045 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5289 | Steps: 2 | Val loss: 0.4414 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=60723)[0m rmse: 0.18414315581321716
[2m[36m(func pid=60723)[0m mae:  0.12586353719234467
[2m[36m(func pid=60723)[0m rmse_per_class: [0.173, 0.219, 0.059, 0.385, 0.056, 0.211, 0.231, 0.129, 0.273, 0.105]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.5821 | Steps: 2 | Val loss: 0.4291 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 05:22:55 (running for 00:22:01.21)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.483 |  0.184 |                   41 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.562 |  0.207 |                   39 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.927 |  0.181 |                   13 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.55  |  0.18  |                   11 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m rmse: 0.18107369542121887
[2m[36m(func pid=67138)[0m mae:  0.13345028460025787
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.263, 0.098, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17963893711566925
[2m[36m(func pid=67747)[0m mae:  0.1321304738521576
[2m[36m(func pid=67747)[0m rmse_per_class: [0.114, 0.264, 0.107, 0.331, 0.103, 0.189, 0.297, 0.139, 0.144, 0.109]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.2011854201555252
[2m[36m(func pid=60725)[0m mae:  0.1159435287117958
[2m[36m(func pid=60725)[0m rmse_per_class: [0.203, 0.411, 0.049, 0.352, 0.082, 0.173, 0.268, 0.248, 0.133, 0.091]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4740 | Steps: 2 | Val loss: 0.4106 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.9138 | Steps: 2 | Val loss: 0.6980 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5112 | Steps: 2 | Val loss: 0.4225 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=60723)[0m rmse: 0.18631820380687714
[2m[36m(func pid=60723)[0m mae:  0.128079354763031
[2m[36m(func pid=60723)[0m rmse_per_class: [0.104, 0.211, 0.062, 0.385, 0.056, 0.207, 0.238, 0.123, 0.384, 0.092]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5066 | Steps: 2 | Val loss: 0.3761 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 05:23:00 (running for 00:22:06.32)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.474 |  0.186 |                   42 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.582 |  0.201 |                   40 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.914 |  0.181 |                   14 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.529 |  0.18  |                   12 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m rmse: 0.18107745051383972
[2m[36m(func pid=67138)[0m mae:  0.13345648348331451
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.263, 0.098, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1794455498456955
[2m[36m(func pid=67747)[0m mae:  0.13196825981140137
[2m[36m(func pid=67747)[0m rmse_per_class: [0.115, 0.264, 0.107, 0.331, 0.102, 0.189, 0.296, 0.139, 0.145, 0.108]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4999 | Steps: 2 | Val loss: 0.4171 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=60725)[0m rmse: 0.1666208803653717
[2m[36m(func pid=60725)[0m mae:  0.09637178480625153
[2m[36m(func pid=60725)[0m rmse_per_class: [0.111, 0.306, 0.046, 0.291, 0.141, 0.172, 0.246, 0.129, 0.133, 0.092]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.9019 | Steps: 2 | Val loss: 0.6915 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4970 | Steps: 2 | Val loss: 0.4067 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=60723)[0m rmse: 0.19475220143795013
[2m[36m(func pid=60723)[0m mae:  0.1329195648431778
[2m[36m(func pid=60723)[0m rmse_per_class: [0.095, 0.231, 0.057, 0.384, 0.056, 0.204, 0.255, 0.133, 0.441, 0.091]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4688 | Steps: 2 | Val loss: 0.3845 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=67138)[0m rmse: 0.1811019331216812
[2m[36m(func pid=67138)[0m mae:  0.1334781050682068
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.263, 0.097, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=67138)[0m 
== Status ==
Current time: 2024-01-07 05:23:05 (running for 00:22:11.53)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.5   |  0.195 |                   43 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.507 |  0.167 |                   41 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.902 |  0.181 |                   15 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.511 |  0.179 |                   13 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67747)[0m rmse: 0.1792532503604889
[2m[36m(func pid=67747)[0m mae:  0.13179656863212585
[2m[36m(func pid=67747)[0m rmse_per_class: [0.115, 0.264, 0.108, 0.332, 0.1, 0.189, 0.295, 0.138, 0.145, 0.107]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4990 | Steps: 2 | Val loss: 0.4187 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=60725)[0m rmse: 0.16840854287147522
[2m[36m(func pid=60725)[0m mae:  0.09753663837909698
[2m[36m(func pid=60725)[0m rmse_per_class: [0.104, 0.207, 0.056, 0.359, 0.175, 0.192, 0.242, 0.122, 0.134, 0.094]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8893 | Steps: 2 | Val loss: 0.6846 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=60723)[0m rmse: 0.19906345009803772
[2m[36m(func pid=60723)[0m mae:  0.13554932177066803
[2m[36m(func pid=60723)[0m rmse_per_class: [0.101, 0.282, 0.047, 0.384, 0.056, 0.199, 0.269, 0.141, 0.419, 0.093]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4870 | Steps: 2 | Val loss: 0.3934 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.5069 | Steps: 2 | Val loss: 0.4161 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=67138)[0m rmse: 0.1810804307460785
[2m[36m(func pid=67138)[0m mae:  0.133462056517601
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.097, 0.328, 0.109, 0.19, 0.306, 0.145, 0.142, 0.117]
== Status ==
Current time: 2024-01-07 05:23:10 (running for 00:22:16.73)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.499 |  0.199 |                   44 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.469 |  0.168 |                   42 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.889 |  0.181 |                   16 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.497 |  0.179 |                   14 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17901402711868286
[2m[36m(func pid=67747)[0m mae:  0.1315986067056656
[2m[36m(func pid=67747)[0m rmse_per_class: [0.115, 0.264, 0.108, 0.332, 0.099, 0.188, 0.293, 0.138, 0.145, 0.107]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5091 | Steps: 2 | Val loss: 0.4138 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=60725)[0m rmse: 0.1703343540430069
[2m[36m(func pid=60725)[0m mae:  0.10125704109668732
[2m[36m(func pid=60725)[0m rmse_per_class: [0.103, 0.227, 0.062, 0.297, 0.198, 0.215, 0.233, 0.142, 0.132, 0.094]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8810 | Steps: 2 | Val loss: 0.6778 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=60723)[0m rmse: 0.1995960921049118
[2m[36m(func pid=60723)[0m mae:  0.13570228219032288
[2m[36m(func pid=60723)[0m rmse_per_class: [0.104, 0.345, 0.043, 0.382, 0.056, 0.191, 0.276, 0.146, 0.359, 0.094]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4763 | Steps: 2 | Val loss: 0.3823 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5370 | Steps: 2 | Val loss: 0.4548 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 05:23:16 (running for 00:22:22.14)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.509 |  0.2   |                   45 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.507 |  0.17  |                   43 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.881 |  0.181 |                   17 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.487 |  0.179 |                   15 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m rmse: 0.1810792237520218
[2m[36m(func pid=67138)[0m mae:  0.1334569752216339
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.097, 0.329, 0.109, 0.19, 0.306, 0.145, 0.142, 0.117]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17877301573753357
[2m[36m(func pid=67747)[0m mae:  0.13140101730823517
[2m[36m(func pid=67747)[0m rmse_per_class: [0.116, 0.264, 0.108, 0.332, 0.097, 0.188, 0.292, 0.138, 0.146, 0.106]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5109 | Steps: 2 | Val loss: 0.3991 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=60725)[0m rmse: 0.18993940949440002
[2m[36m(func pid=60725)[0m mae:  0.11699966341257095
[2m[36m(func pid=60725)[0m rmse_per_class: [0.097, 0.239, 0.065, 0.297, 0.239, 0.208, 0.243, 0.148, 0.271, 0.092]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8622 | Steps: 2 | Val loss: 0.6705 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=60723)[0m rmse: 0.19643482565879822
[2m[36m(func pid=60723)[0m mae:  0.1328614354133606
[2m[36m(func pid=60723)[0m rmse_per_class: [0.104, 0.399, 0.043, 0.377, 0.055, 0.178, 0.276, 0.149, 0.289, 0.095]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4677 | Steps: 2 | Val loss: 0.3733 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6235 | Steps: 2 | Val loss: 0.5123 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 05:23:21 (running for 00:22:27.38)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.511 |  0.196 |                   46 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.537 |  0.19  |                   44 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.862 |  0.181 |                   18 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.476 |  0.179 |                   16 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m rmse: 0.18105970323085785
[2m[36m(func pid=67138)[0m mae:  0.13344740867614746
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.108, 0.19, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5180 | Steps: 2 | Val loss: 0.3751 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=67747)[0m rmse: 0.17855919897556305
[2m[36m(func pid=67747)[0m mae:  0.13122281432151794
[2m[36m(func pid=67747)[0m rmse_per_class: [0.116, 0.264, 0.108, 0.333, 0.096, 0.188, 0.291, 0.138, 0.146, 0.106]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.21271541714668274
[2m[36m(func pid=60725)[0m mae:  0.13047710061073303
[2m[36m(func pid=60725)[0m rmse_per_class: [0.233, 0.229, 0.059, 0.352, 0.286, 0.173, 0.266, 0.15, 0.292, 0.087]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8540 | Steps: 2 | Val loss: 0.6630 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=60723)[0m rmse: 0.18950824439525604
[2m[36m(func pid=60723)[0m mae:  0.12703990936279297
[2m[36m(func pid=60723)[0m rmse_per_class: [0.104, 0.424, 0.045, 0.368, 0.054, 0.164, 0.269, 0.15, 0.222, 0.095]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4626 | Steps: 2 | Val loss: 0.3658 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5748 | Steps: 2 | Val loss: 0.5174 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 05:23:26 (running for 00:22:32.61)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.518 |  0.19  |                   47 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.623 |  0.213 |                   45 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.854 |  0.181 |                   19 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.468 |  0.179 |                   17 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m rmse: 0.18106874823570251
[2m[36m(func pid=67138)[0m mae:  0.13345232605934143
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.328, 0.108, 0.19, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5023 | Steps: 2 | Val loss: 0.3469 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=67747)[0m rmse: 0.17833483219146729
[2m[36m(func pid=67747)[0m mae:  0.13104267418384552
[2m[36m(func pid=67747)[0m rmse_per_class: [0.116, 0.264, 0.108, 0.333, 0.094, 0.188, 0.29, 0.137, 0.146, 0.106]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.21453864872455597
[2m[36m(func pid=60725)[0m mae:  0.12765362858772278
[2m[36m(func pid=60725)[0m rmse_per_class: [0.389, 0.218, 0.055, 0.369, 0.28, 0.168, 0.26, 0.146, 0.157, 0.103]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.1792173832654953
[2m[36m(func pid=60723)[0m mae:  0.1189349889755249
[2m[36m(func pid=60723)[0m rmse_per_class: [0.101, 0.399, 0.046, 0.349, 0.053, 0.164, 0.26, 0.151, 0.173, 0.095]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8461 | Steps: 2 | Val loss: 0.6559 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4632 | Steps: 2 | Val loss: 0.3599 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5159 | Steps: 2 | Val loss: 0.4637 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 05:23:31 (running for 00:22:37.95)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.502 |  0.179 |                   48 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.575 |  0.215 |                   46 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.846 |  0.181 |                   20 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.463 |  0.178 |                   18 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m rmse: 0.1810579001903534
[2m[36m(func pid=67138)[0m mae:  0.13344018161296844
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.328, 0.108, 0.19, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4747 | Steps: 2 | Val loss: 0.3226 | Batch size: 32 | lr: 0.01 | Duration: 2.60s
[2m[36m(func pid=67747)[0m rmse: 0.17814943194389343
[2m[36m(func pid=67747)[0m mae:  0.13089126348495483
[2m[36m(func pid=67747)[0m rmse_per_class: [0.117, 0.264, 0.108, 0.333, 0.093, 0.188, 0.289, 0.137, 0.146, 0.106]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.20463700592517853
[2m[36m(func pid=60725)[0m mae:  0.11893795430660248
[2m[36m(func pid=60725)[0m rmse_per_class: [0.248, 0.244, 0.043, 0.361, 0.203, 0.174, 0.254, 0.14, 0.134, 0.245]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.16581982374191284
[2m[36m(func pid=60723)[0m mae:  0.10939028114080429
[2m[36m(func pid=60723)[0m rmse_per_class: [0.095, 0.319, 0.049, 0.313, 0.052, 0.186, 0.252, 0.152, 0.144, 0.096]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8350 | Steps: 2 | Val loss: 0.6490 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4540 | Steps: 2 | Val loss: 0.3549 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5138 | Steps: 2 | Val loss: 0.4035 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4391 | Steps: 2 | Val loss: 0.3081 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 05:23:37 (running for 00:22:43.30)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.475 |  0.166 |                   49 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.516 |  0.205 |                   47 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.835 |  0.181 |                   21 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.463 |  0.178 |                   19 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m rmse: 0.1810561865568161
[2m[36m(func pid=67138)[0m mae:  0.1334371566772461
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.328, 0.107, 0.191, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17799998819828033
[2m[36m(func pid=67747)[0m mae:  0.13076597452163696
[2m[36m(func pid=67747)[0m rmse_per_class: [0.117, 0.264, 0.108, 0.334, 0.091, 0.188, 0.288, 0.137, 0.147, 0.106]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.18253877758979797
[2m[36m(func pid=60725)[0m mae:  0.10580314695835114
[2m[36m(func pid=60725)[0m rmse_per_class: [0.115, 0.302, 0.045, 0.335, 0.113, 0.181, 0.279, 0.132, 0.135, 0.189]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.15428426861763
[2m[36m(func pid=60723)[0m mae:  0.10166122019290924
[2m[36m(func pid=60723)[0m rmse_per_class: [0.092, 0.227, 0.052, 0.276, 0.053, 0.214, 0.248, 0.153, 0.133, 0.096]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8223 | Steps: 2 | Val loss: 0.6420 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4506 | Steps: 2 | Val loss: 0.3506 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4590 | Steps: 2 | Val loss: 0.3085 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4866 | Steps: 2 | Val loss: 0.3598 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=67138)[0m rmse: 0.18105700612068176
[2m[36m(func pid=67138)[0m mae:  0.13343878090381622
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.328, 0.107, 0.191, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=67138)[0m 
== Status ==
Current time: 2024-01-07 05:23:42 (running for 00:22:48.38)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.439 |  0.154 |                   50 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.514 |  0.183 |                   48 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.822 |  0.181 |                   22 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.454 |  0.178 |                   20 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67747)[0m rmse: 0.177840918302536
[2m[36m(func pid=67747)[0m mae:  0.13062332570552826
[2m[36m(func pid=67747)[0m rmse_per_class: [0.117, 0.264, 0.109, 0.334, 0.09, 0.188, 0.288, 0.137, 0.147, 0.105]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.15706871449947357
[2m[36m(func pid=60723)[0m mae:  0.101484015583992
[2m[36m(func pid=60723)[0m rmse_per_class: [0.096, 0.225, 0.05, 0.289, 0.056, 0.221, 0.25, 0.154, 0.133, 0.096]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.16009607911109924
[2m[36m(func pid=60725)[0m mae:  0.09438662976026535
[2m[36m(func pid=60725)[0m rmse_per_class: [0.103, 0.257, 0.047, 0.287, 0.064, 0.174, 0.32, 0.12, 0.135, 0.095]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8150 | Steps: 2 | Val loss: 0.6357 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4583 | Steps: 2 | Val loss: 0.3475 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4540 | Steps: 2 | Val loss: 0.3230 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=67138)[0m rmse: 0.18105801939964294
[2m[36m(func pid=67138)[0m mae:  0.13343748450279236
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.107, 0.191, 0.305, 0.145, 0.142, 0.118]
== Status ==
Current time: 2024-01-07 05:23:47 (running for 00:22:53.57)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.459 |  0.157 |                   51 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.487 |  0.16  |                   49 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.815 |  0.181 |                   23 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.451 |  0.178 |                   21 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4667 | Steps: 2 | Val loss: 0.3350 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=67747)[0m rmse: 0.17768338322639465
[2m[36m(func pid=67747)[0m mae:  0.13049092888832092
[2m[36m(func pid=67747)[0m rmse_per_class: [0.118, 0.264, 0.109, 0.334, 0.088, 0.188, 0.287, 0.137, 0.147, 0.105]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.16277234256267548
[2m[36m(func pid=60723)[0m mae:  0.10459613800048828
[2m[36m(func pid=60723)[0m rmse_per_class: [0.106, 0.25, 0.048, 0.308, 0.066, 0.213, 0.252, 0.154, 0.135, 0.096]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.14556221663951874
[2m[36m(func pid=60725)[0m mae:  0.08637881278991699
[2m[36m(func pid=60725)[0m rmse_per_class: [0.1, 0.212, 0.052, 0.315, 0.051, 0.154, 0.238, 0.112, 0.131, 0.092]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.8080 | Steps: 2 | Val loss: 0.6290 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4411 | Steps: 2 | Val loss: 0.3445 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=67138)[0m rmse: 0.18104828894138336
[2m[36m(func pid=67138)[0m mae:  0.13342657685279846
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.107, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=67138)[0m 
== Status ==
Current time: 2024-01-07 05:23:52 (running for 00:22:58.77)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.454 |  0.163 |                   52 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.467 |  0.146 |                   50 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.808 |  0.181 |                   24 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.458 |  0.178 |                   22 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5064 | Steps: 2 | Val loss: 0.3411 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5123 | Steps: 2 | Val loss: 0.3531 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=67747)[0m rmse: 0.17754404246807098
[2m[36m(func pid=67747)[0m mae:  0.13036268949508667
[2m[36m(func pid=67747)[0m rmse_per_class: [0.118, 0.264, 0.109, 0.335, 0.087, 0.188, 0.286, 0.136, 0.147, 0.105]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.1670217514038086
[2m[36m(func pid=60723)[0m mae:  0.1075989231467247
[2m[36m(func pid=60723)[0m rmse_per_class: [0.132, 0.267, 0.048, 0.303, 0.083, 0.201, 0.25, 0.154, 0.136, 0.096]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7958 | Steps: 2 | Val loss: 0.6226 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=60725)[0m rmse: 0.15993031859397888
[2m[36m(func pid=60725)[0m mae:  0.0962827205657959
[2m[36m(func pid=60725)[0m rmse_per_class: [0.094, 0.202, 0.054, 0.301, 0.052, 0.189, 0.292, 0.131, 0.193, 0.092]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4425 | Steps: 2 | Val loss: 0.3422 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 05:23:57 (running for 00:23:03.97)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.506 |  0.167 |                   53 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.512 |  0.16  |                   51 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.796 |  0.181 |                   25 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.441 |  0.178 |                   23 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67138)[0m rmse: 0.18104830384254456
[2m[36m(func pid=67138)[0m mae:  0.13341987133026123
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.107, 0.191, 0.305, 0.145, 0.142, 0.118]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5030 | Steps: 2 | Val loss: 0.3513 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=67747)[0m rmse: 0.17742757499217987
[2m[36m(func pid=67747)[0m mae:  0.13027238845825195
[2m[36m(func pid=67747)[0m rmse_per_class: [0.119, 0.264, 0.109, 0.335, 0.086, 0.188, 0.286, 0.136, 0.147, 0.105]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4662 | Steps: 2 | Val loss: 0.4075 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=60723)[0m rmse: 0.1706094592809677
[2m[36m(func pid=60723)[0m mae:  0.11003123223781586
[2m[36m(func pid=60723)[0m rmse_per_class: [0.169, 0.276, 0.047, 0.287, 0.108, 0.189, 0.245, 0.153, 0.136, 0.096]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7891 | Steps: 2 | Val loss: 0.6161 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=60725)[0m rmse: 0.1760108470916748
[2m[36m(func pid=60725)[0m mae:  0.10857043415307999
[2m[36m(func pid=60725)[0m rmse_per_class: [0.142, 0.207, 0.052, 0.273, 0.054, 0.241, 0.321, 0.144, 0.236, 0.092]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4360 | Steps: 2 | Val loss: 0.3400 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 05:24:03 (running for 00:23:09.31)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.503 |  0.171 |                   54 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.466 |  0.176 |                   52 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.789 |  0.181 |                   26 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.442 |  0.177 |                   24 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4870 | Steps: 2 | Val loss: 0.3524 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=67138)[0m rmse: 0.18102742731571198
[2m[36m(func pid=67138)[0m mae:  0.1334034651517868
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.106, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1772865504026413
[2m[36m(func pid=67747)[0m mae:  0.13016971945762634
[2m[36m(func pid=67747)[0m rmse_per_class: [0.119, 0.264, 0.108, 0.335, 0.085, 0.188, 0.285, 0.136, 0.147, 0.105]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4963 | Steps: 2 | Val loss: 0.4480 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=60723)[0m rmse: 0.17541424930095673
[2m[36m(func pid=60723)[0m mae:  0.11287091672420502
[2m[36m(func pid=60723)[0m rmse_per_class: [0.212, 0.28, 0.045, 0.273, 0.142, 0.179, 0.239, 0.153, 0.136, 0.095]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7786 | Steps: 2 | Val loss: 0.6099 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4416 | Steps: 2 | Val loss: 0.3385 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=60725)[0m rmse: 0.19208750128746033
[2m[36m(func pid=60725)[0m mae:  0.11754336208105087
[2m[36m(func pid=60725)[0m rmse_per_class: [0.306, 0.212, 0.05, 0.322, 0.055, 0.233, 0.311, 0.188, 0.151, 0.093]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:24:08 (running for 00:23:14.47)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.487 |  0.175 |                   55 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.496 |  0.192 |                   53 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.779 |  0.181 |                   27 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.436 |  0.177 |                   25 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4863 | Steps: 2 | Val loss: 0.3509 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=67138)[0m rmse: 0.18100331723690033
[2m[36m(func pid=67138)[0m mae:  0.13338547945022583
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.106, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17718324065208435
[2m[36m(func pid=67747)[0m mae:  0.1300855576992035
[2m[36m(func pid=67747)[0m rmse_per_class: [0.12, 0.263, 0.109, 0.335, 0.084, 0.188, 0.285, 0.136, 0.148, 0.105]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5419 | Steps: 2 | Val loss: 0.4485 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=60723)[0m rmse: 0.18285077810287476
[2m[36m(func pid=60723)[0m mae:  0.11673089116811752
[2m[36m(func pid=60723)[0m rmse_per_class: [0.253, 0.28, 0.044, 0.278, 0.187, 0.172, 0.234, 0.151, 0.135, 0.095]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.7682 | Steps: 2 | Val loss: 0.6033 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4453 | Steps: 2 | Val loss: 0.3374 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=60725)[0m rmse: 0.19663235545158386
[2m[36m(func pid=60725)[0m mae:  0.11686694622039795
[2m[36m(func pid=60725)[0m rmse_per_class: [0.397, 0.211, 0.06, 0.356, 0.055, 0.172, 0.273, 0.217, 0.13, 0.094]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4819 | Steps: 2 | Val loss: 0.3505 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=67138)[0m rmse: 0.18100088834762573
[2m[36m(func pid=67138)[0m mae:  0.13337402045726776
[2m[36m(func pid=67138)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.106, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=67138)[0m 
== Status ==
Current time: 2024-01-07 05:24:13 (running for 00:23:19.92)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.486 |  0.183 |                   56 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.542 |  0.197 |                   54 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.768 |  0.181 |                   28 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.445 |  0.177 |                   27 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67747)[0m rmse: 0.1770724058151245
[2m[36m(func pid=67747)[0m mae:  0.1299915760755539
[2m[36m(func pid=67747)[0m rmse_per_class: [0.12, 0.263, 0.109, 0.335, 0.082, 0.188, 0.285, 0.136, 0.148, 0.104]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5028 | Steps: 2 | Val loss: 0.4179 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=60723)[0m rmse: 0.19135025143623352
[2m[36m(func pid=60723)[0m mae:  0.12094125896692276
[2m[36m(func pid=60723)[0m rmse_per_class: [0.275, 0.278, 0.045, 0.301, 0.239, 0.168, 0.232, 0.148, 0.134, 0.093]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7614 | Steps: 2 | Val loss: 0.5979 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4345 | Steps: 2 | Val loss: 0.3361 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=60725)[0m rmse: 0.18913887441158295
[2m[36m(func pid=60725)[0m mae:  0.10906960070133209
[2m[36m(func pid=60725)[0m rmse_per_class: [0.295, 0.207, 0.081, 0.365, 0.055, 0.165, 0.321, 0.174, 0.133, 0.095]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4368 | Steps: 2 | Val loss: 0.3536 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=67138)[0m rmse: 0.1810092031955719
[2m[36m(func pid=67138)[0m mae:  0.1333772838115692
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.106, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=67138)[0m 
== Status ==
Current time: 2024-01-07 05:24:19 (running for 00:23:25.17)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.482 |  0.191 |                   57 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.503 |  0.189 |                   55 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.761 |  0.181 |                   29 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.434 |  0.177 |                   28 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67747)[0m rmse: 0.1770070195198059
[2m[36m(func pid=67747)[0m mae:  0.12993872165679932
[2m[36m(func pid=67747)[0m rmse_per_class: [0.121, 0.263, 0.108, 0.335, 0.082, 0.189, 0.284, 0.136, 0.148, 0.104]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5013 | Steps: 2 | Val loss: 0.4003 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=60723)[0m rmse: 0.19787505269050598
[2m[36m(func pid=60723)[0m mae:  0.1240442767739296
[2m[36m(func pid=60723)[0m rmse_per_class: [0.269, 0.273, 0.055, 0.324, 0.29, 0.169, 0.233, 0.142, 0.132, 0.09]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.7543 | Steps: 2 | Val loss: 0.5924 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4397 | Steps: 2 | Val loss: 0.3352 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=60725)[0m rmse: 0.18209561705589294
[2m[36m(func pid=60725)[0m mae:  0.10415638983249664
[2m[36m(func pid=60725)[0m rmse_per_class: [0.176, 0.222, 0.097, 0.357, 0.055, 0.181, 0.372, 0.13, 0.132, 0.099]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4389 | Steps: 2 | Val loss: 0.3601 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=67138)[0m rmse: 0.1810128390789032
[2m[36m(func pid=67138)[0m mae:  0.13337889313697815
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.106, 0.191, 0.304, 0.145, 0.142, 0.119]
[2m[36m(func pid=67138)[0m 
== Status ==
Current time: 2024-01-07 05:24:24 (running for 00:23:30.26)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.437 |  0.198 |                   58 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.501 |  0.182 |                   56 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.754 |  0.181 |                   30 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.44  |  0.177 |                   29 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67747)[0m rmse: 0.17694923281669617
[2m[36m(func pid=67747)[0m mae:  0.12987127900123596
[2m[36m(func pid=67747)[0m rmse_per_class: [0.121, 0.263, 0.109, 0.335, 0.08, 0.189, 0.284, 0.136, 0.148, 0.104]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4818 | Steps: 2 | Val loss: 0.3811 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=60723)[0m rmse: 0.2007935792207718
[2m[36m(func pid=60723)[0m mae:  0.12585604190826416
[2m[36m(func pid=60723)[0m rmse_per_class: [0.239, 0.265, 0.075, 0.344, 0.323, 0.174, 0.237, 0.132, 0.132, 0.087]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.7506 | Steps: 2 | Val loss: 0.5865 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4427 | Steps: 2 | Val loss: 0.3346 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=60725)[0m rmse: 0.16965849697589874
[2m[36m(func pid=60725)[0m mae:  0.09673207998275757
[2m[36m(func pid=60725)[0m rmse_per_class: [0.121, 0.282, 0.101, 0.316, 0.054, 0.172, 0.259, 0.118, 0.14, 0.133]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4338 | Steps: 2 | Val loss: 0.3688 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=67138)[0m rmse: 0.1809668242931366
[2m[36m(func pid=67138)[0m mae:  0.13334140181541443
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.304, 0.145, 0.142, 0.119]
[2m[36m(func pid=67138)[0m 
== Status ==
Current time: 2024-01-07 05:24:29 (running for 00:23:35.38)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.439 |  0.201 |                   59 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.482 |  0.17  |                   57 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.751 |  0.181 |                   31 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.443 |  0.177 |                   30 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=67747)[0m rmse: 0.17688383162021637
[2m[36m(func pid=67747)[0m mae:  0.12980756163597107
[2m[36m(func pid=67747)[0m rmse_per_class: [0.122, 0.263, 0.109, 0.336, 0.079, 0.189, 0.284, 0.136, 0.148, 0.104]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4775 | Steps: 2 | Val loss: 0.3762 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=60723)[0m rmse: 0.20027923583984375
[2m[36m(func pid=60723)[0m mae:  0.12744106352329254
[2m[36m(func pid=60723)[0m rmse_per_class: [0.187, 0.253, 0.097, 0.357, 0.333, 0.183, 0.242, 0.118, 0.143, 0.089]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7410 | Steps: 2 | Val loss: 0.5811 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4370 | Steps: 2 | Val loss: 0.3338 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=60725)[0m rmse: 0.1704031527042389
[2m[36m(func pid=60725)[0m mae:  0.10070345550775528
[2m[36m(func pid=60725)[0m rmse_per_class: [0.097, 0.262, 0.073, 0.287, 0.053, 0.167, 0.247, 0.122, 0.207, 0.19]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4229 | Steps: 2 | Val loss: 0.3756 | Batch size: 32 | lr: 0.01 | Duration: 2.60s
[2m[36m(func pid=67138)[0m rmse: 0.18094755709171295
[2m[36m(func pid=67138)[0m mae:  0.13332635164260864
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.304, 0.145, 0.142, 0.118]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17684273421764374
[2m[36m(func pid=67747)[0m mae:  0.12973998486995697
[2m[36m(func pid=67747)[0m rmse_per_class: [0.122, 0.263, 0.11, 0.336, 0.078, 0.189, 0.283, 0.136, 0.148, 0.104]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4521 | Steps: 2 | Val loss: 0.3888 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 05:24:35 (running for 00:23:41.97)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.423 |  0.199 |                   61 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.478 |  0.17  |                   58 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.741 |  0.181 |                   32 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.437 |  0.177 |                   31 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.1989966481924057
[2m[36m(func pid=60723)[0m mae:  0.12940341234207153
[2m[36m(func pid=60723)[0m rmse_per_class: [0.133, 0.24, 0.108, 0.368, 0.302, 0.193, 0.251, 0.104, 0.185, 0.107]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7335 | Steps: 2 | Val loss: 0.5760 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4327 | Steps: 2 | Val loss: 0.3330 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=60725)[0m rmse: 0.17029765248298645
[2m[36m(func pid=60725)[0m mae:  0.1054828017950058
[2m[36m(func pid=60725)[0m rmse_per_class: [0.095, 0.209, 0.056, 0.289, 0.052, 0.162, 0.285, 0.136, 0.233, 0.185]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4223 | Steps: 2 | Val loss: 0.3852 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=67138)[0m rmse: 0.18093135952949524
[2m[36m(func pid=67138)[0m mae:  0.13331478834152222
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1767866611480713
[2m[36m(func pid=67747)[0m mae:  0.12966778874397278
[2m[36m(func pid=67747)[0m rmse_per_class: [0.123, 0.263, 0.11, 0.336, 0.077, 0.189, 0.283, 0.136, 0.148, 0.104]
[2m[36m(func pid=67747)[0m 
== Status ==
Current time: 2024-01-07 05:24:40 (running for 00:23:46.98)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.422 |  0.201 |                   62 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.452 |  0.17  |                   59 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.734 |  0.181 |                   33 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.433 |  0.177 |                   32 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.20141330361366272
[2m[36m(func pid=60723)[0m mae:  0.13284069299697876
[2m[36m(func pid=60723)[0m rmse_per_class: [0.104, 0.231, 0.103, 0.375, 0.247, 0.202, 0.268, 0.101, 0.237, 0.147]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4551 | Steps: 2 | Val loss: 0.4098 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.7262 | Steps: 2 | Val loss: 0.5710 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4374 | Steps: 2 | Val loss: 0.3326 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4086 | Steps: 2 | Val loss: 0.3985 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=60725)[0m rmse: 0.1653362661600113
[2m[36m(func pid=60725)[0m mae:  0.1048610657453537
[2m[36m(func pid=60725)[0m rmse_per_class: [0.112, 0.201, 0.051, 0.277, 0.051, 0.178, 0.281, 0.145, 0.193, 0.164]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18090251088142395
[2m[36m(func pid=67138)[0m mae:  0.13329468667507172
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17671999335289001
[2m[36m(func pid=67747)[0m mae:  0.12959513068199158
[2m[36m(func pid=67747)[0m rmse_per_class: [0.123, 0.262, 0.11, 0.336, 0.077, 0.189, 0.283, 0.136, 0.148, 0.104]
[2m[36m(func pid=67747)[0m 
== Status ==
Current time: 2024-01-07 05:24:46 (running for 00:23:52.31)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.409 |  0.203 |                   63 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.455 |  0.165 |                   60 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.726 |  0.181 |                   34 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.437 |  0.177 |                   33 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.20321345329284668
[2m[36m(func pid=60723)[0m mae:  0.13620853424072266
[2m[36m(func pid=60723)[0m rmse_per_class: [0.099, 0.223, 0.086, 0.38, 0.187, 0.209, 0.287, 0.11, 0.261, 0.191]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4818 | Steps: 2 | Val loss: 0.4099 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7225 | Steps: 2 | Val loss: 0.5660 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4333 | Steps: 2 | Val loss: 0.3318 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=60725)[0m rmse: 0.16647927463054657
[2m[36m(func pid=60725)[0m mae:  0.1026158556342125
[2m[36m(func pid=60725)[0m rmse_per_class: [0.193, 0.207, 0.049, 0.274, 0.059, 0.171, 0.25, 0.148, 0.149, 0.164]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18087288737297058
[2m[36m(func pid=67138)[0m mae:  0.1332722157239914
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4365 | Steps: 2 | Val loss: 0.4162 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=67747)[0m rmse: 0.1765940636396408
[2m[36m(func pid=67747)[0m mae:  0.1294967383146286
[2m[36m(func pid=67747)[0m rmse_per_class: [0.124, 0.262, 0.11, 0.336, 0.076, 0.189, 0.283, 0.136, 0.148, 0.103]
[2m[36m(func pid=67747)[0m 
== Status ==
Current time: 2024-01-07 05:24:51 (running for 00:23:57.82)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.437 |  0.202 |                   64 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.482 |  0.166 |                   61 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.723 |  0.181 |                   35 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.433 |  0.177 |                   34 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.20225508511066437
[2m[36m(func pid=60723)[0m mae:  0.13822606205940247
[2m[36m(func pid=60723)[0m rmse_per_class: [0.1, 0.22, 0.065, 0.382, 0.138, 0.213, 0.301, 0.124, 0.253, 0.225]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.7090 | Steps: 2 | Val loss: 0.5611 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4511 | Steps: 2 | Val loss: 0.4076 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4303 | Steps: 2 | Val loss: 0.3310 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=67138)[0m rmse: 0.18085119128227234
[2m[36m(func pid=67138)[0m mae:  0.13324525952339172
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.18113520741462708
[2m[36m(func pid=60725)[0m mae:  0.10553822666406631
[2m[36m(func pid=60725)[0m rmse_per_class: [0.307, 0.207, 0.049, 0.292, 0.096, 0.162, 0.245, 0.144, 0.138, 0.172]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4703 | Steps: 2 | Val loss: 0.4311 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=67747)[0m rmse: 0.17648407816886902
[2m[36m(func pid=67747)[0m mae:  0.12940451502799988
[2m[36m(func pid=67747)[0m rmse_per_class: [0.123, 0.262, 0.109, 0.336, 0.076, 0.189, 0.282, 0.136, 0.148, 0.103]
[2m[36m(func pid=67747)[0m 
== Status ==
Current time: 2024-01-07 05:24:56 (running for 00:24:02.99)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.47  |  0.2   |                   65 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.451 |  0.181 |                   62 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.709 |  0.181 |                   36 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.43  |  0.176 |                   35 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.19962987303733826
[2m[36m(func pid=60723)[0m mae:  0.138280987739563
[2m[36m(func pid=60723)[0m rmse_per_class: [0.101, 0.224, 0.052, 0.383, 0.102, 0.216, 0.307, 0.144, 0.231, 0.238]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7075 | Steps: 2 | Val loss: 0.5566 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4156 | Steps: 2 | Val loss: 0.4239 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4336 | Steps: 2 | Val loss: 0.3305 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=67138)[0m rmse: 0.18083055317401886
[2m[36m(func pid=67138)[0m mae:  0.1332310140132904
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4843 | Steps: 2 | Val loss: 0.4355 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=60725)[0m rmse: 0.19009429216384888
[2m[36m(func pid=60725)[0m mae:  0.10661306232213974
[2m[36m(func pid=60725)[0m rmse_per_class: [0.323, 0.205, 0.045, 0.306, 0.172, 0.167, 0.246, 0.133, 0.14, 0.164]
[2m[36m(func pid=60725)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1763852834701538
[2m[36m(func pid=67747)[0m mae:  0.12930895388126373
[2m[36m(func pid=67747)[0m rmse_per_class: [0.124, 0.262, 0.109, 0.336, 0.075, 0.189, 0.282, 0.136, 0.148, 0.103]
[2m[36m(func pid=67747)[0m 
== Status ==
Current time: 2024-01-07 05:25:01 (running for 00:24:08.01)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.484 |  0.196 |                   66 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.416 |  0.19  |                   63 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.707 |  0.181 |                   37 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.434 |  0.176 |                   36 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.19630976021289825
[2m[36m(func pid=60723)[0m mae:  0.13673797249794006
[2m[36m(func pid=60723)[0m rmse_per_class: [0.1, 0.234, 0.048, 0.383, 0.077, 0.216, 0.308, 0.161, 0.208, 0.227]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.7012 | Steps: 2 | Val loss: 0.5526 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4348 | Steps: 2 | Val loss: 0.4504 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4283 | Steps: 2 | Val loss: 0.3298 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=67138)[0m rmse: 0.18081523478031158
[2m[36m(func pid=67138)[0m mae:  0.13321754336357117
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.105, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4756 | Steps: 2 | Val loss: 0.4227 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=67747)[0m rmse: 0.17629137635231018
[2m[36m(func pid=67747)[0m mae:  0.12919048964977264
[2m[36m(func pid=67747)[0m rmse_per_class: [0.124, 0.262, 0.11, 0.336, 0.075, 0.188, 0.282, 0.136, 0.148, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.18938323855400085
[2m[36m(func pid=60725)[0m mae:  0.10469269752502441
[2m[36m(func pid=60725)[0m rmse_per_class: [0.239, 0.212, 0.075, 0.312, 0.243, 0.17, 0.241, 0.123, 0.148, 0.132]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:25:06 (running for 00:24:13.12)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.476 |  0.192 |                   67 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.435 |  0.189 |                   64 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.701 |  0.181 |                   38 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.428 |  0.176 |                   37 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.19231975078582764
[2m[36m(func pid=60723)[0m mae:  0.1335548609495163
[2m[36m(func pid=60723)[0m rmse_per_class: [0.098, 0.25, 0.048, 0.382, 0.063, 0.215, 0.303, 0.182, 0.182, 0.2]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6949 | Steps: 2 | Val loss: 0.5480 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4301 | Steps: 2 | Val loss: 0.3293 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4595 | Steps: 2 | Val loss: 0.4475 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=67138)[0m rmse: 0.1807738095521927
[2m[36m(func pid=67138)[0m mae:  0.13318702578544617
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4679 | Steps: 2 | Val loss: 0.3963 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=67747)[0m rmse: 0.17619577050209045
[2m[36m(func pid=67747)[0m mae:  0.12908175587654114
[2m[36m(func pid=67747)[0m rmse_per_class: [0.125, 0.262, 0.11, 0.335, 0.074, 0.188, 0.282, 0.136, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.18528780341148376
[2m[36m(func pid=60725)[0m mae:  0.10367502272129059
[2m[36m(func pid=60725)[0m rmse_per_class: [0.145, 0.228, 0.106, 0.318, 0.266, 0.171, 0.246, 0.119, 0.161, 0.093]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:25:12 (running for 00:24:18.47)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.468 |  0.187 |                   68 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.459 |  0.185 |                   65 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.695 |  0.181 |                   39 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.43  |  0.176 |                   38 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.187360942363739
[2m[36m(func pid=60723)[0m mae:  0.12871791422367096
[2m[36m(func pid=60723)[0m rmse_per_class: [0.097, 0.266, 0.049, 0.38, 0.057, 0.211, 0.293, 0.201, 0.155, 0.165]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6904 | Steps: 2 | Val loss: 0.5440 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4285 | Steps: 2 | Val loss: 0.3285 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4860 | Steps: 2 | Val loss: 0.4132 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=67138)[0m rmse: 0.18076522648334503
[2m[36m(func pid=67138)[0m mae:  0.1331813931465149
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.144, 0.143, 0.118]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4384 | Steps: 2 | Val loss: 0.3665 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=67747)[0m rmse: 0.17602530121803284
[2m[36m(func pid=67747)[0m mae:  0.1289537250995636
[2m[36m(func pid=67747)[0m rmse_per_class: [0.125, 0.262, 0.109, 0.335, 0.074, 0.188, 0.281, 0.136, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.17840494215488434
[2m[36m(func pid=60725)[0m mae:  0.10224709659814835
[2m[36m(func pid=60725)[0m rmse_per_class: [0.103, 0.232, 0.104, 0.322, 0.227, 0.169, 0.255, 0.116, 0.17, 0.086]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:25:17 (running for 00:24:23.65)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.438 |  0.182 |                   69 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.486 |  0.178 |                   66 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.69  |  0.181 |                   40 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.428 |  0.176 |                   39 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.181777223944664
[2m[36m(func pid=60723)[0m mae:  0.12277040630578995
[2m[36m(func pid=60723)[0m rmse_per_class: [0.116, 0.262, 0.051, 0.375, 0.053, 0.204, 0.279, 0.21, 0.139, 0.128]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6818 | Steps: 2 | Val loss: 0.5403 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4278 | Steps: 2 | Val loss: 0.3280 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4628 | Steps: 2 | Val loss: 0.3900 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=67138)[0m rmse: 0.18075647950172424
[2m[36m(func pid=67138)[0m mae:  0.13317301869392395
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.144, 0.143, 0.117]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4014 | Steps: 2 | Val loss: 0.3378 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=67747)[0m rmse: 0.1759127825498581
[2m[36m(func pid=67747)[0m mae:  0.128858283162117
[2m[36m(func pid=67747)[0m rmse_per_class: [0.125, 0.262, 0.108, 0.335, 0.073, 0.188, 0.281, 0.136, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.16700956225395203
[2m[36m(func pid=60725)[0m mae:  0.09980569034814835
[2m[36m(func pid=60725)[0m rmse_per_class: [0.097, 0.214, 0.07, 0.312, 0.179, 0.158, 0.24, 0.113, 0.199, 0.087]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:25:22 (running for 00:24:28.72)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.401 |  0.177 |                   70 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.463 |  0.167 |                   67 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.682 |  0.181 |                   41 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.428 |  0.176 |                   40 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.1768587976694107
[2m[36m(func pid=60723)[0m mae:  0.11652135848999023
[2m[36m(func pid=60723)[0m rmse_per_class: [0.177, 0.236, 0.05, 0.365, 0.052, 0.194, 0.262, 0.196, 0.133, 0.104]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6790 | Steps: 2 | Val loss: 0.5368 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4305 | Steps: 2 | Val loss: 0.3276 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4402 | Steps: 2 | Val loss: 0.3966 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=67138)[0m rmse: 0.1807352602481842
[2m[36m(func pid=67138)[0m mae:  0.1331540197134018
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.144, 0.143, 0.117]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3709 | Steps: 2 | Val loss: 0.3152 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=67747)[0m rmse: 0.17583128809928894
[2m[36m(func pid=67747)[0m mae:  0.12878002226352692
[2m[36m(func pid=67747)[0m rmse_per_class: [0.125, 0.262, 0.108, 0.335, 0.073, 0.188, 0.281, 0.136, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.164747953414917
[2m[36m(func pid=60725)[0m mae:  0.10051977634429932
[2m[36m(func pid=60725)[0m rmse_per_class: [0.117, 0.204, 0.042, 0.286, 0.149, 0.171, 0.263, 0.127, 0.2, 0.088]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:25:27 (running for 00:24:33.89)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.371 |  0.172 |                   71 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.44  |  0.165 |                   68 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.679 |  0.181 |                   42 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.431 |  0.176 |                   41 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.17196404933929443
[2m[36m(func pid=60723)[0m mae:  0.11118495464324951
[2m[36m(func pid=60723)[0m rmse_per_class: [0.251, 0.207, 0.046, 0.349, 0.051, 0.182, 0.246, 0.165, 0.132, 0.091]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6745 | Steps: 2 | Val loss: 0.5331 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4295 | Steps: 2 | Val loss: 0.3273 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4162 | Steps: 2 | Val loss: 0.4097 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=67138)[0m rmse: 0.18070480227470398
[2m[36m(func pid=67138)[0m mae:  0.13313284516334534
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.144, 0.143, 0.117]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3865 | Steps: 2 | Val loss: 0.3011 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=67747)[0m rmse: 0.17578712105751038
[2m[36m(func pid=67747)[0m mae:  0.12872561812400818
[2m[36m(func pid=67747)[0m rmse_per_class: [0.125, 0.262, 0.109, 0.335, 0.072, 0.188, 0.281, 0.136, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.16984517872333527
[2m[36m(func pid=60725)[0m mae:  0.10148666054010391
[2m[36m(func pid=60725)[0m rmse_per_class: [0.163, 0.196, 0.048, 0.273, 0.131, 0.179, 0.304, 0.163, 0.153, 0.088]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:25:33 (running for 00:24:39.14)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.387 |  0.167 |                   72 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.416 |  0.17  |                   69 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.675 |  0.181 |                   43 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.43  |  0.176 |                   42 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.1669997274875641
[2m[36m(func pid=60723)[0m mae:  0.10668454319238663
[2m[36m(func pid=60723)[0m rmse_per_class: [0.296, 0.2, 0.043, 0.325, 0.051, 0.169, 0.235, 0.131, 0.132, 0.086]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.6694 | Steps: 2 | Val loss: 0.5295 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4289 | Steps: 2 | Val loss: 0.3270 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4607 | Steps: 2 | Val loss: 0.3891 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=67138)[0m rmse: 0.1806780993938446
[2m[36m(func pid=67138)[0m mae:  0.1331092268228531
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.104, 0.191, 0.303, 0.144, 0.143, 0.117]
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3804 | Steps: 2 | Val loss: 0.2899 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17575986683368683
[2m[36m(func pid=67747)[0m mae:  0.12867411971092224
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.262, 0.109, 0.335, 0.072, 0.188, 0.281, 0.136, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.1761372685432434
[2m[36m(func pid=60725)[0m mae:  0.10101896524429321
[2m[36m(func pid=60725)[0m rmse_per_class: [0.193, 0.193, 0.049, 0.287, 0.12, 0.166, 0.31, 0.221, 0.132, 0.09]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:25:38 (running for 00:24:44.55)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.38  |  0.161 |                   73 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.461 |  0.176 |                   70 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.669 |  0.181 |                   44 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.429 |  0.176 |                   43 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.16078916192054749
[2m[36m(func pid=60723)[0m mae:  0.1019405722618103
[2m[36m(func pid=60723)[0m rmse_per_class: [0.29, 0.211, 0.044, 0.295, 0.052, 0.16, 0.229, 0.107, 0.133, 0.087]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6560 | Steps: 2 | Val loss: 0.5249 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4297 | Steps: 2 | Val loss: 0.3269 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4332 | Steps: 2 | Val loss: 0.3624 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=67138)[0m rmse: 0.18065878748893738
[2m[36m(func pid=67138)[0m mae:  0.13308776915073395
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.104, 0.191, 0.303, 0.144, 0.143, 0.117]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3796 | Steps: 2 | Val loss: 0.2843 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=67747)[0m rmse: 0.1757967472076416
[2m[36m(func pid=67747)[0m mae:  0.12865880131721497
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.262, 0.11, 0.335, 0.071, 0.188, 0.281, 0.136, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.17910011112689972
[2m[36m(func pid=60725)[0m mae:  0.09887880831956863
[2m[36m(func pid=60725)[0m rmse_per_class: [0.189, 0.198, 0.049, 0.306, 0.105, 0.163, 0.282, 0.252, 0.132, 0.115]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:25:43 (running for 00:24:49.63)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.38  |  0.156 |                   74 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.433 |  0.179 |                   71 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.656 |  0.181 |                   45 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.43  |  0.176 |                   44 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.15584765374660492
[2m[36m(func pid=60723)[0m mae:  0.09827382862567902
[2m[36m(func pid=60723)[0m rmse_per_class: [0.247, 0.225, 0.045, 0.275, 0.053, 0.159, 0.23, 0.102, 0.134, 0.088]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.6648 | Steps: 2 | Val loss: 0.5211 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4251 | Steps: 2 | Val loss: 0.3265 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4116 | Steps: 2 | Val loss: 0.3708 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=67138)[0m rmse: 0.18061573803424835
[2m[36m(func pid=67138)[0m mae:  0.1330539733171463
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.104, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3766 | Steps: 2 | Val loss: 0.2856 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=67747)[0m rmse: 0.17573142051696777
[2m[36m(func pid=67747)[0m mae:  0.12859408557415009
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.262, 0.11, 0.335, 0.071, 0.188, 0.28, 0.136, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.16745802760124207
[2m[36m(func pid=60725)[0m mae:  0.09398313611745834
[2m[36m(func pid=60725)[0m rmse_per_class: [0.132, 0.203, 0.051, 0.297, 0.082, 0.169, 0.27, 0.172, 0.135, 0.163]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:25:48 (running for 00:24:54.72)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15649999678134918
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.377 |  0.154 |                   75 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.412 |  0.167 |                   72 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.665 |  0.181 |                   46 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.425 |  0.176 |                   45 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.15386711061000824
[2m[36m(func pid=60723)[0m mae:  0.09648410975933075
[2m[36m(func pid=60723)[0m rmse_per_class: [0.184, 0.239, 0.048, 0.28, 0.054, 0.166, 0.234, 0.109, 0.134, 0.09]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.6481 | Steps: 2 | Val loss: 0.5173 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4254 | Steps: 2 | Val loss: 0.3262 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4048 | Steps: 2 | Val loss: 0.3882 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=67138)[0m rmse: 0.18059536814689636
[2m[36m(func pid=67138)[0m mae:  0.1330423802137375
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.4023 | Steps: 2 | Val loss: 0.2925 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=67747)[0m rmse: 0.17570559680461884
[2m[36m(func pid=67747)[0m mae:  0.12855277955532074
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.262, 0.11, 0.335, 0.071, 0.188, 0.28, 0.136, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60725)[0m rmse: 0.1757778823375702
[2m[36m(func pid=60725)[0m mae:  0.10380600392818451
[2m[36m(func pid=60725)[0m rmse_per_class: [0.1, 0.205, 0.059, 0.324, 0.061, 0.179, 0.395, 0.107, 0.136, 0.192]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:25:53 (running for 00:24:59.94)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15649999678134918
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.402 |  0.155 |                   76 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.405 |  0.176 |                   73 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.648 |  0.181 |                   47 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.425 |  0.176 |                   46 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.154792919754982
[2m[36m(func pid=60723)[0m mae:  0.09673509746789932
[2m[36m(func pid=60723)[0m rmse_per_class: [0.129, 0.247, 0.05, 0.302, 0.054, 0.179, 0.241, 0.119, 0.134, 0.092]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.6493 | Steps: 2 | Val loss: 0.5140 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4216 | Steps: 2 | Val loss: 0.3256 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4277 | Steps: 2 | Val loss: 0.3977 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=67138)[0m rmse: 0.1805698275566101
[2m[36m(func pid=67138)[0m mae:  0.13302257657051086
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17563040554523468
[2m[36m(func pid=67747)[0m mae:  0.12849506735801697
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.262, 0.11, 0.334, 0.071, 0.188, 0.28, 0.135, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.4155 | Steps: 2 | Val loss: 0.2999 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=60725)[0m rmse: 0.17925891280174255
[2m[36m(func pid=60725)[0m mae:  0.10718075186014175
[2m[36m(func pid=60725)[0m rmse_per_class: [0.099, 0.206, 0.071, 0.34, 0.053, 0.167, 0.374, 0.117, 0.138, 0.226]
[2m[36m(func pid=60725)[0m 
== Status ==
Current time: 2024-01-07 05:25:59 (running for 00:25:05.24)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15649999678134918
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.415 |  0.158 |                   77 |
| train_5a6ec_00011 | RUNNING    | 192.168.7.53:60725 | 0.1    |       0.99 |         0.0001 |  0.428 |  0.179 |                   74 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.649 |  0.181 |                   48 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.422 |  0.176 |                   47 |
| train_5a6ec_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.15764519572257996
[2m[36m(func pid=60723)[0m mae:  0.09823501855134964
[2m[36m(func pid=60723)[0m rmse_per_class: [0.1, 0.25, 0.051, 0.322, 0.055, 0.195, 0.249, 0.127, 0.134, 0.092]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.6502 | Steps: 2 | Val loss: 0.5108 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4224 | Steps: 2 | Val loss: 0.3252 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=60725)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4253 | Steps: 2 | Val loss: 0.3833 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=67138)[0m rmse: 0.18053773045539856
[2m[36m(func pid=67138)[0m mae:  0.13299889862537384
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1755818873643875
[2m[36m(func pid=67747)[0m mae:  0.12843407690525055
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.262, 0.11, 0.334, 0.07, 0.188, 0.28, 0.135, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.4373 | Steps: 2 | Val loss: 0.3015 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=60725)[0m rmse: 0.1696539670228958
[2m[36m(func pid=60725)[0m mae:  0.09966803342103958
[2m[36m(func pid=60725)[0m rmse_per_class: [0.105, 0.218, 0.082, 0.325, 0.051, 0.175, 0.245, 0.131, 0.176, 0.188]
[2m[36m(func pid=60723)[0m rmse: 0.15958945453166962
[2m[36m(func pid=60723)[0m mae:  0.09929095953702927
[2m[36m(func pid=60723)[0m rmse_per_class: [0.093, 0.249, 0.052, 0.325, 0.055, 0.21, 0.255, 0.132, 0.133, 0.093]
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4234 | Steps: 2 | Val loss: 0.3251 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.6376 | Steps: 2 | Val loss: 0.5082 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=67138)[0m rmse: 0.18052420020103455
[2m[36m(func pid=67138)[0m mae:  0.13298946619033813
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=67747)[0m rmse: 0.17559567093849182
[2m[36m(func pid=67747)[0m mae:  0.12840259075164795
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.261, 0.111, 0.334, 0.07, 0.188, 0.28, 0.135, 0.147, 0.103]
== Status ==
Current time: 2024-01-07 05:26:04 (running for 00:25:10.60)
Memory usage on this node: 21.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.415 |  0.158 |                   77 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.65  |  0.181 |                   49 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.422 |  0.176 |                   48 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


== Status ==
Current time: 2024-01-07 05:26:12 (running for 00:25:18.90)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.415 |  0.158 |                   77 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.65  |  0.181 |                   49 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.423 |  0.176 |                   49 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=78481)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=78481)[0m Configuration completed!
[2m[36m(func pid=78481)[0m New optimizer parameters:
[2m[36m(func pid=78481)[0m SGD (
[2m[36m(func pid=78481)[0m Parameter Group 0
[2m[36m(func pid=78481)[0m     dampening: 0
[2m[36m(func pid=78481)[0m     differentiable: False
[2m[36m(func pid=78481)[0m     foreach: None
[2m[36m(func pid=78481)[0m     lr: 0.01
[2m[36m(func pid=78481)[0m     maximize: False
[2m[36m(func pid=78481)[0m     momentum: 0.9
[2m[36m(func pid=78481)[0m     nesterov: False
[2m[36m(func pid=78481)[0m     weight_decay: 0.0001
[2m[36m(func pid=78481)[0m )
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.4456 | Steps: 2 | Val loss: 0.2977 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6380 | Steps: 2 | Val loss: 0.5040 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4240 | Steps: 2 | Val loss: 0.3250 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0054 | Steps: 2 | Val loss: 0.6619 | Batch size: 32 | lr: 0.01 | Duration: 4.67s
== Status ==
Current time: 2024-01-07 05:26:17 (running for 00:25:23.91)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.437 |  0.16  |                   78 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.638 |  0.181 |                   50 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.423 |  0.176 |                   49 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.15904468297958374
[2m[36m(func pid=60723)[0m mae:  0.09915383905172348
[2m[36m(func pid=60723)[0m rmse_per_class: [0.091, 0.241, 0.053, 0.309, 0.055, 0.226, 0.258, 0.135, 0.131, 0.092]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18043938279151917
[2m[36m(func pid=67138)[0m mae:  0.1329217106103897
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17559954524040222
[2m[36m(func pid=67747)[0m mae:  0.1283712089061737
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.261, 0.112, 0.334, 0.069, 0.188, 0.28, 0.135, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1797684282064438
[2m[36m(func pid=78481)[0m mae:  0.13227775692939758
[2m[36m(func pid=78481)[0m rmse_per_class: [0.115, 0.263, 0.098, 0.334, 0.098, 0.191, 0.297, 0.143, 0.141, 0.118]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.4342 | Steps: 2 | Val loss: 0.2950 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6322 | Steps: 2 | Val loss: 0.5021 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4247 | Steps: 2 | Val loss: 0.3249 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.7739 | Steps: 2 | Val loss: 0.5978 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 05:26:23 (running for 00:25:29.32)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.434 |  0.156 |                   80 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.638 |  0.18  |                   51 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.176 |                   50 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  1.005 |  0.18  |                    1 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.15579615533351898
[2m[36m(func pid=60723)[0m mae:  0.0983993336558342
[2m[36m(func pid=60723)[0m rmse_per_class: [0.091, 0.227, 0.052, 0.281, 0.055, 0.235, 0.258, 0.136, 0.132, 0.092]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18045063316822052
[2m[36m(func pid=67138)[0m mae:  0.13292500376701355
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.33, 0.103, 0.191, 0.302, 0.144, 0.143, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17565199732780457
[2m[36m(func pid=67747)[0m mae:  0.12836185097694397
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.261, 0.113, 0.334, 0.069, 0.188, 0.28, 0.135, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.18005672097206116
[2m[36m(func pid=78481)[0m mae:  0.1325320303440094
[2m[36m(func pid=78481)[0m rmse_per_class: [0.114, 0.264, 0.104, 0.334, 0.101, 0.191, 0.295, 0.141, 0.144, 0.113]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.4439 | Steps: 2 | Val loss: 0.2977 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4209 | Steps: 2 | Val loss: 0.3245 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.6345 | Steps: 2 | Val loss: 0.4994 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5643 | Steps: 2 | Val loss: 0.5100 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 05:26:28 (running for 00:25:34.49)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.444 |  0.154 |                   81 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.632 |  0.18  |                   52 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.425 |  0.176 |                   51 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.774 |  0.18  |                    2 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.15383803844451904
[2m[36m(func pid=60723)[0m mae:  0.09874822199344635
[2m[36m(func pid=60723)[0m rmse_per_class: [0.092, 0.207, 0.056, 0.263, 0.055, 0.232, 0.257, 0.137, 0.15, 0.09]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18042044341564178
[2m[36m(func pid=67138)[0m mae:  0.13290202617645264
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.33, 0.103, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1756073236465454
[2m[36m(func pid=67747)[0m mae:  0.128319650888443
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.261, 0.113, 0.334, 0.069, 0.188, 0.28, 0.135, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.17977365851402283
[2m[36m(func pid=78481)[0m mae:  0.13225719332695007
[2m[36m(func pid=78481)[0m rmse_per_class: [0.114, 0.264, 0.107, 0.335, 0.1, 0.191, 0.293, 0.139, 0.145, 0.11]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3896 | Steps: 2 | Val loss: 0.3040 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.6260 | Steps: 2 | Val loss: 0.4969 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4235 | Steps: 2 | Val loss: 0.3244 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4598 | Steps: 2 | Val loss: 0.4269 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=67138)[0m rmse: 0.1803998500108719
[2m[36m(func pid=67138)[0m mae:  0.13288749754428864
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.33, 0.103, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=67138)[0m 
== Status ==
Current time: 2024-01-07 05:26:33 (running for 00:25:39.95)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.444 |  0.154 |                   81 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.626 |  0.18  |                   54 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.421 |  0.176 |                   52 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.564 |  0.18  |                    3 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.15730121731758118
[2m[36m(func pid=60723)[0m mae:  0.10184520483016968
[2m[36m(func pid=60723)[0m rmse_per_class: [0.094, 0.197, 0.055, 0.279, 0.054, 0.206, 0.255, 0.136, 0.207, 0.089]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1756056249141693
[2m[36m(func pid=67747)[0m mae:  0.12828481197357178
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.261, 0.114, 0.334, 0.069, 0.188, 0.28, 0.135, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.17906136810779572
[2m[36m(func pid=78481)[0m mae:  0.13158079981803894
[2m[36m(func pid=78481)[0m rmse_per_class: [0.115, 0.264, 0.11, 0.335, 0.097, 0.19, 0.289, 0.137, 0.146, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.6221 | Steps: 2 | Val loss: 0.4947 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4192 | Steps: 2 | Val loss: 0.3240 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.4079 | Steps: 2 | Val loss: 0.3134 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4479 | Steps: 2 | Val loss: 0.3703 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 05:26:38 (running for 00:25:45.03)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.39  |  0.157 |                   82 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.626 |  0.18  |                   54 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.419 |  0.176 |                   54 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.46  |  0.179 |                    4 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=67747)[0m rmse: 0.17564089596271515
[2m[36m(func pid=67747)[0m mae:  0.12823966145515442
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.261, 0.115, 0.334, 0.068, 0.188, 0.28, 0.135, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18038229644298553
[2m[36m(func pid=67138)[0m mae:  0.13287734985351562
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.33, 0.103, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=60723)[0m rmse: 0.164083793759346
[2m[36m(func pid=60723)[0m mae:  0.10665174573659897
[2m[36m(func pid=60723)[0m rmse_per_class: [0.094, 0.204, 0.052, 0.312, 0.054, 0.175, 0.255, 0.134, 0.272, 0.088]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1780133694410324
[2m[36m(func pid=78481)[0m mae:  0.130591481924057
[2m[36m(func pid=78481)[0m rmse_per_class: [0.116, 0.264, 0.112, 0.336, 0.091, 0.19, 0.285, 0.136, 0.147, 0.103]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3822 | Steps: 2 | Val loss: 0.3258 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4205 | Steps: 2 | Val loss: 0.3237 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.6227 | Steps: 2 | Val loss: 0.4916 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4562 | Steps: 2 | Val loss: 0.3378 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 05:26:44 (running for 00:25:50.37)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.408 |  0.164 |                   83 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.622 |  0.18  |                   55 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.421 |  0.176 |                   55 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.448 |  0.178 |                    5 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.17056865990161896
[2m[36m(func pid=60723)[0m mae:  0.1115700975060463
[2m[36m(func pid=60723)[0m rmse_per_class: [0.096, 0.222, 0.047, 0.338, 0.054, 0.158, 0.257, 0.133, 0.313, 0.086]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18034598231315613
[2m[36m(func pid=67138)[0m mae:  0.13284988701343536
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.103, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.175633504986763
[2m[36m(func pid=67747)[0m mae:  0.12818549573421478
[2m[36m(func pid=67747)[0m rmse_per_class: [0.127, 0.261, 0.115, 0.334, 0.068, 0.188, 0.28, 0.135, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.17682303488254547
[2m[36m(func pid=78481)[0m mae:  0.1294439733028412
[2m[36m(func pid=78481)[0m rmse_per_class: [0.118, 0.264, 0.113, 0.337, 0.084, 0.189, 0.28, 0.135, 0.148, 0.1]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3874 | Steps: 2 | Val loss: 0.3399 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6124 | Steps: 2 | Val loss: 0.4887 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4213 | Steps: 2 | Val loss: 0.3236 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4767 | Steps: 2 | Val loss: 0.3223 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 05:26:49 (running for 00:25:55.61)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.387 |  0.175 |                   85 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.623 |  0.18  |                   56 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.421 |  0.176 |                   55 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.456 |  0.177 |                    6 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.17546603083610535
[2m[36m(func pid=60723)[0m mae:  0.11541189253330231
[2m[36m(func pid=60723)[0m rmse_per_class: [0.1, 0.244, 0.044, 0.355, 0.054, 0.158, 0.258, 0.132, 0.323, 0.086]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.1803353875875473
[2m[36m(func pid=67138)[0m mae:  0.13283637166023254
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17557743191719055
[2m[36m(func pid=67747)[0m mae:  0.12813270092010498
[2m[36m(func pid=67747)[0m rmse_per_class: [0.127, 0.261, 0.115, 0.334, 0.067, 0.188, 0.279, 0.135, 0.147, 0.102]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.17567799985408783
[2m[36m(func pid=78481)[0m mae:  0.1282927691936493
[2m[36m(func pid=78481)[0m rmse_per_class: [0.12, 0.263, 0.113, 0.337, 0.078, 0.188, 0.276, 0.135, 0.149, 0.098]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3860 | Steps: 2 | Val loss: 0.3506 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.6136 | Steps: 2 | Val loss: 0.4864 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4186 | Steps: 2 | Val loss: 0.3232 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4836 | Steps: 2 | Val loss: 0.3163 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 05:26:54 (running for 00:26:00.87)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.386 |  0.178 |                   86 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.612 |  0.18  |                   57 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.421 |  0.176 |                   56 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.477 |  0.176 |                    7 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.17768129706382751
[2m[36m(func pid=60723)[0m mae:  0.11730978637933731
[2m[36m(func pid=60723)[0m rmse_per_class: [0.113, 0.262, 0.041, 0.365, 0.054, 0.166, 0.256, 0.129, 0.304, 0.087]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.1803145557641983
[2m[36m(func pid=67138)[0m mae:  0.1328222006559372
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17545001208782196
[2m[36m(func pid=67747)[0m mae:  0.1280495971441269
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.26, 0.114, 0.334, 0.067, 0.188, 0.279, 0.135, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.17456766963005066
[2m[36m(func pid=78481)[0m mae:  0.12714706361293793
[2m[36m(func pid=78481)[0m rmse_per_class: [0.122, 0.262, 0.112, 0.337, 0.071, 0.187, 0.273, 0.136, 0.149, 0.096]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3923 | Steps: 2 | Val loss: 0.3555 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6065 | Steps: 2 | Val loss: 0.4843 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4151 | Steps: 2 | Val loss: 0.3227 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.5006 | Steps: 2 | Val loss: 0.3144 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 05:26:59 (running for 00:26:06.05)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.392 |  0.177 |                   87 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.614 |  0.18  |                   58 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.419 |  0.175 |                   57 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.484 |  0.175 |                    8 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.1774550825357437
[2m[36m(func pid=60723)[0m mae:  0.11715737730264664
[2m[36m(func pid=60723)[0m rmse_per_class: [0.136, 0.27, 0.04, 0.37, 0.054, 0.175, 0.25, 0.124, 0.264, 0.092]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18032436072826385
[2m[36m(func pid=67138)[0m mae:  0.13282373547554016
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17525848746299744
[2m[36m(func pid=67747)[0m mae:  0.12793506681919098
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.26, 0.113, 0.334, 0.067, 0.188, 0.279, 0.135, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.17364738881587982
[2m[36m(func pid=78481)[0m mae:  0.12615345418453217
[2m[36m(func pid=78481)[0m rmse_per_class: [0.124, 0.261, 0.11, 0.338, 0.066, 0.186, 0.27, 0.136, 0.15, 0.095]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3916 | Steps: 2 | Val loss: 0.3559 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6071 | Steps: 2 | Val loss: 0.4825 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4187 | Steps: 2 | Val loss: 0.3225 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4863 | Steps: 2 | Val loss: 0.3133 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 05:27:05 (running for 00:26:11.17)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.392 |  0.176 |                   88 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.606 |  0.18  |                   59 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.415 |  0.175 |                   58 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.501 |  0.174 |                    9 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.17615535855293274
[2m[36m(func pid=60723)[0m mae:  0.11592856794595718
[2m[36m(func pid=60723)[0m rmse_per_class: [0.161, 0.274, 0.041, 0.372, 0.053, 0.182, 0.243, 0.119, 0.213, 0.105]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18030226230621338
[2m[36m(func pid=67138)[0m mae:  0.13280606269836426
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1752125471830368
[2m[36m(func pid=67747)[0m mae:  0.12788119912147522
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.26, 0.113, 0.334, 0.067, 0.188, 0.279, 0.135, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.17270994186401367
[2m[36m(func pid=78481)[0m mae:  0.1252320110797882
[2m[36m(func pid=78481)[0m rmse_per_class: [0.127, 0.26, 0.106, 0.337, 0.062, 0.185, 0.269, 0.137, 0.15, 0.094]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.4000 | Steps: 2 | Val loss: 0.3514 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6057 | Steps: 2 | Val loss: 0.4797 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4159 | Steps: 2 | Val loss: 0.3223 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4766 | Steps: 2 | Val loss: 0.3126 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 05:27:10 (running for 00:26:16.25)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.4   |  0.175 |                   89 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.607 |  0.18  |                   60 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.419 |  0.175 |                   59 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.486 |  0.173 |                   10 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.17484574019908905
[2m[36m(func pid=60723)[0m mae:  0.11426322162151337
[2m[36m(func pid=60723)[0m rmse_per_class: [0.177, 0.27, 0.042, 0.371, 0.053, 0.187, 0.235, 0.113, 0.169, 0.131]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18026188015937805
[2m[36m(func pid=67138)[0m mae:  0.1327785849571228
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.3, 0.143, 0.143, 0.116]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17517141997814178
[2m[36m(func pid=67747)[0m mae:  0.12784989178180695
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.26, 0.113, 0.334, 0.067, 0.188, 0.279, 0.135, 0.147, 0.103]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.17232537269592285
[2m[36m(func pid=78481)[0m mae:  0.12474384158849716
[2m[36m(func pid=78481)[0m rmse_per_class: [0.13, 0.258, 0.104, 0.337, 0.059, 0.185, 0.267, 0.137, 0.152, 0.093]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3935 | Steps: 2 | Val loss: 0.3428 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5983 | Steps: 2 | Val loss: 0.4774 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4173 | Steps: 2 | Val loss: 0.3220 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4701 | Steps: 2 | Val loss: 0.3114 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 05:27:15 (running for 00:26:21.34)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.393 |  0.173 |                   90 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.606 |  0.18  |                   61 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.416 |  0.175 |                   60 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.477 |  0.172 |                   11 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.1731378436088562
[2m[36m(func pid=60723)[0m mae:  0.11230885982513428
[2m[36m(func pid=60723)[0m rmse_per_class: [0.175, 0.257, 0.042, 0.369, 0.052, 0.191, 0.227, 0.105, 0.144, 0.167]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.1802264302968979
[2m[36m(func pid=67138)[0m mae:  0.13274911046028137
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.3, 0.143, 0.143, 0.115]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17514297366142273
[2m[36m(func pid=67747)[0m mae:  0.1277991235256195
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.26, 0.113, 0.334, 0.067, 0.188, 0.279, 0.135, 0.147, 0.102]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.17203760147094727
[2m[36m(func pid=78481)[0m mae:  0.12431538105010986
[2m[36m(func pid=78481)[0m rmse_per_class: [0.133, 0.257, 0.103, 0.337, 0.057, 0.184, 0.266, 0.138, 0.153, 0.093]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3814 | Steps: 2 | Val loss: 0.3328 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5982 | Steps: 2 | Val loss: 0.4748 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4190 | Steps: 2 | Val loss: 0.3219 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4490 | Steps: 2 | Val loss: 0.3099 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 05:27:20 (running for 00:26:26.46)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.381 |  0.171 |                   91 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.598 |  0.18  |                   62 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.417 |  0.175 |                   61 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.47  |  0.172 |                   12 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.1712677776813507
[2m[36m(func pid=60723)[0m mae:  0.11089422553777695
[2m[36m(func pid=60723)[0m rmse_per_class: [0.159, 0.237, 0.043, 0.365, 0.051, 0.193, 0.221, 0.099, 0.135, 0.21]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.1801980435848236
[2m[36m(func pid=67138)[0m mae:  0.13272739946842194
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.3, 0.143, 0.143, 0.115]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17515382170677185
[2m[36m(func pid=67747)[0m mae:  0.127778097987175
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.26, 0.113, 0.334, 0.067, 0.188, 0.279, 0.135, 0.147, 0.102]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.171808123588562
[2m[36m(func pid=78481)[0m mae:  0.12401175498962402
[2m[36m(func pid=78481)[0m rmse_per_class: [0.135, 0.256, 0.101, 0.336, 0.056, 0.184, 0.266, 0.138, 0.154, 0.092]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3738 | Steps: 2 | Val loss: 0.3221 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5920 | Steps: 2 | Val loss: 0.4718 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4164 | Steps: 2 | Val loss: 0.3217 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4336 | Steps: 2 | Val loss: 0.3087 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 05:27:25 (running for 00:26:31.63)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.374 |  0.169 |                   92 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.598 |  0.18  |                   63 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.419 |  0.175 |                   62 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.449 |  0.172 |                   13 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.1686144769191742
[2m[36m(func pid=60723)[0m mae:  0.10945777595043182
[2m[36m(func pid=60723)[0m rmse_per_class: [0.135, 0.219, 0.043, 0.357, 0.051, 0.192, 0.218, 0.095, 0.132, 0.244]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18015947937965393
[2m[36m(func pid=67138)[0m mae:  0.1326991617679596
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.3, 0.143, 0.143, 0.115]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1751466989517212
[2m[36m(func pid=67747)[0m mae:  0.1277443915605545
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.26, 0.114, 0.334, 0.066, 0.188, 0.279, 0.135, 0.147, 0.102]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.17168861627578735
[2m[36m(func pid=78481)[0m mae:  0.12380842864513397
[2m[36m(func pid=78481)[0m rmse_per_class: [0.137, 0.255, 0.101, 0.336, 0.055, 0.183, 0.265, 0.138, 0.154, 0.092]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3850 | Steps: 2 | Val loss: 0.3111 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5929 | Steps: 2 | Val loss: 0.4691 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4164 | Steps: 2 | Val loss: 0.3215 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4235 | Steps: 2 | Val loss: 0.3081 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 05:27:30 (running for 00:26:36.73)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.385 |  0.165 |                   93 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.592 |  0.18  |                   64 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.416 |  0.175 |                   63 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.434 |  0.172 |                   14 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.16543200612068176
[2m[36m(func pid=60723)[0m mae:  0.1073627918958664
[2m[36m(func pid=60723)[0m rmse_per_class: [0.112, 0.209, 0.046, 0.343, 0.05, 0.189, 0.219, 0.096, 0.132, 0.257]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18010880053043365
[2m[36m(func pid=67138)[0m mae:  0.13265904784202576
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.101, 0.191, 0.3, 0.143, 0.144, 0.115]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17511926591396332
[2m[36m(func pid=67747)[0m mae:  0.12769736349582672
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.26, 0.114, 0.334, 0.066, 0.188, 0.279, 0.135, 0.147, 0.102]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1714927852153778
[2m[36m(func pid=78481)[0m mae:  0.12359817326068878
[2m[36m(func pid=78481)[0m rmse_per_class: [0.137, 0.254, 0.1, 0.335, 0.055, 0.183, 0.266, 0.137, 0.155, 0.093]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3757 | Steps: 2 | Val loss: 0.2986 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.5903 | Steps: 2 | Val loss: 0.4671 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4177 | Steps: 2 | Val loss: 0.3213 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4136 | Steps: 2 | Val loss: 0.3082 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 05:27:35 (running for 00:26:41.90)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.376 |  0.161 |                   94 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.593 |  0.18  |                   65 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.416 |  0.175 |                   64 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.424 |  0.171 |                   15 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.1608933061361313
[2m[36m(func pid=60723)[0m mae:  0.10369787365198135
[2m[36m(func pid=60723)[0m rmse_per_class: [0.102, 0.204, 0.052, 0.32, 0.051, 0.183, 0.223, 0.104, 0.132, 0.238]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.1800871640443802
[2m[36m(func pid=67138)[0m mae:  0.13264799118041992
[2m[36m(func pid=67138)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.101, 0.19, 0.3, 0.142, 0.144, 0.115]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1751728504896164
[2m[36m(func pid=67747)[0m mae:  0.12767121195793152
[2m[36m(func pid=67747)[0m rmse_per_class: [0.127, 0.26, 0.115, 0.334, 0.066, 0.188, 0.279, 0.135, 0.147, 0.102]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.17112722992897034
[2m[36m(func pid=78481)[0m mae:  0.1233285665512085
[2m[36m(func pid=78481)[0m rmse_per_class: [0.136, 0.253, 0.099, 0.334, 0.055, 0.183, 0.266, 0.137, 0.156, 0.093]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3603 | Steps: 2 | Val loss: 0.2865 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.5889 | Steps: 2 | Val loss: 0.4648 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4164 | Steps: 2 | Val loss: 0.3212 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4148 | Steps: 2 | Val loss: 0.3080 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 05:27:40 (running for 00:26:46.96)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.36  |  0.155 |                   95 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.59  |  0.18  |                   66 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.418 |  0.175 |                   65 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.414 |  0.171 |                   16 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.1549157351255417
[2m[36m(func pid=60723)[0m mae:  0.09874332696199417
[2m[36m(func pid=60723)[0m rmse_per_class: [0.096, 0.204, 0.057, 0.287, 0.055, 0.175, 0.228, 0.119, 0.132, 0.196]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=67138)[0m rmse: 0.18003924190998077
[2m[36m(func pid=67138)[0m mae:  0.13261154294013977
[2m[36m(func pid=67138)[0m rmse_per_class: [0.115, 0.264, 0.099, 0.33, 0.101, 0.19, 0.3, 0.142, 0.144, 0.115]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17505964636802673
[2m[36m(func pid=67747)[0m mae:  0.12760929763317108
[2m[36m(func pid=67747)[0m rmse_per_class: [0.127, 0.26, 0.114, 0.334, 0.066, 0.188, 0.278, 0.135, 0.147, 0.102]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.17027422785758972
[2m[36m(func pid=78481)[0m mae:  0.12274297326803207
[2m[36m(func pid=78481)[0m rmse_per_class: [0.131, 0.253, 0.097, 0.332, 0.055, 0.182, 0.267, 0.136, 0.156, 0.094]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3666 | Steps: 2 | Val loss: 0.2793 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.5813 | Steps: 2 | Val loss: 0.4624 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4141 | Steps: 2 | Val loss: 0.3210 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 05:27:45 (running for 00:26:52.07)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.367 |  0.151 |                   96 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.589 |  0.18  |                   67 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.416 |  0.175 |                   66 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.415 |  0.17  |                   17 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.15060311555862427
[2m[36m(func pid=60723)[0m mae:  0.09481515735387802
[2m[36m(func pid=60723)[0m rmse_per_class: [0.093, 0.209, 0.06, 0.262, 0.062, 0.166, 0.235, 0.139, 0.131, 0.147]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4073 | Steps: 2 | Val loss: 0.3065 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=67138)[0m rmse: 0.18000194430351257
[2m[36m(func pid=67138)[0m mae:  0.1325768083333969
[2m[36m(func pid=67138)[0m rmse_per_class: [0.115, 0.264, 0.099, 0.33, 0.101, 0.19, 0.3, 0.142, 0.144, 0.115]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17503178119659424
[2m[36m(func pid=67747)[0m mae:  0.1275678277015686
[2m[36m(func pid=67747)[0m rmse_per_class: [0.127, 0.26, 0.114, 0.334, 0.066, 0.188, 0.278, 0.135, 0.147, 0.102]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1687888354063034
[2m[36m(func pid=78481)[0m mae:  0.1218232661485672
[2m[36m(func pid=78481)[0m rmse_per_class: [0.125, 0.253, 0.091, 0.329, 0.054, 0.181, 0.269, 0.135, 0.157, 0.095]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3476 | Steps: 2 | Val loss: 0.2772 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.5749 | Steps: 2 | Val loss: 0.4599 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4145 | Steps: 2 | Val loss: 0.3207 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 05:27:51 (running for 00:26:57.14)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.348 |  0.15  |                   97 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.581 |  0.18  |                   68 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.414 |  0.175 |                   67 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.407 |  0.169 |                   18 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.15025457739830017
[2m[36m(func pid=60723)[0m mae:  0.09367375075817108
[2m[36m(func pid=60723)[0m rmse_per_class: [0.092, 0.218, 0.06, 0.258, 0.071, 0.161, 0.247, 0.153, 0.131, 0.11]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4018 | Steps: 2 | Val loss: 0.3049 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=67138)[0m rmse: 0.1799827516078949
[2m[36m(func pid=67138)[0m mae:  0.13255645334720612
[2m[36m(func pid=67138)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.101, 0.19, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17492055892944336
[2m[36m(func pid=67747)[0m mae:  0.12749269604682922
[2m[36m(func pid=67747)[0m rmse_per_class: [0.127, 0.26, 0.113, 0.334, 0.066, 0.187, 0.278, 0.135, 0.147, 0.102]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.16713514924049377
[2m[36m(func pid=78481)[0m mae:  0.12089549005031586
[2m[36m(func pid=78481)[0m rmse_per_class: [0.118, 0.252, 0.082, 0.326, 0.054, 0.18, 0.27, 0.134, 0.158, 0.097]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3546 | Steps: 2 | Val loss: 0.2798 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.5799 | Steps: 2 | Val loss: 0.4581 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4144 | Steps: 2 | Val loss: 0.3207 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 05:27:56 (running for 00:27:02.35)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.355 |  0.153 |                   98 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.575 |  0.18  |                   69 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.415 |  0.175 |                   68 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.402 |  0.167 |                   19 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.152703657746315
[2m[36m(func pid=60723)[0m mae:  0.09463973343372345
[2m[36m(func pid=60723)[0m rmse_per_class: [0.092, 0.227, 0.056, 0.272, 0.081, 0.16, 0.263, 0.155, 0.131, 0.091]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3999 | Steps: 2 | Val loss: 0.3033 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=67138)[0m rmse: 0.17994779348373413
[2m[36m(func pid=67138)[0m mae:  0.13253390789031982
[2m[36m(func pid=67138)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.3, 0.142, 0.144, 0.115]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17496554553508759
[2m[36m(func pid=67747)[0m mae:  0.12749308347702026
[2m[36m(func pid=67747)[0m rmse_per_class: [0.127, 0.26, 0.113, 0.334, 0.065, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.16582831740379333
[2m[36m(func pid=78481)[0m mae:  0.12016920000314713
[2m[36m(func pid=78481)[0m rmse_per_class: [0.112, 0.252, 0.074, 0.324, 0.054, 0.18, 0.272, 0.133, 0.158, 0.1]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3858 | Steps: 2 | Val loss: 0.2851 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.5804 | Steps: 2 | Val loss: 0.4562 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4139 | Steps: 2 | Val loss: 0.3206 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 05:28:01 (running for 00:27:07.39)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00010 | RUNNING    | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.386 |  0.155 |                   99 |
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.58  |  0.18  |                   70 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.414 |  0.175 |                   69 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.4   |  0.166 |                   20 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.15503932535648346
[2m[36m(func pid=60723)[0m mae:  0.09643571078777313
[2m[36m(func pid=60723)[0m rmse_per_class: [0.092, 0.232, 0.049, 0.281, 0.093, 0.164, 0.277, 0.147, 0.131, 0.085]
[2m[36m(func pid=60723)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3947 | Steps: 2 | Val loss: 0.3014 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=67138)[0m rmse: 0.17990939319133759
[2m[36m(func pid=67138)[0m mae:  0.1325034499168396
[2m[36m(func pid=67138)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.17499086260795593
[2m[36m(func pid=67747)[0m mae:  0.12747828662395477
[2m[36m(func pid=67747)[0m rmse_per_class: [0.127, 0.259, 0.114, 0.334, 0.065, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.16462159156799316
[2m[36m(func pid=78481)[0m mae:  0.1194813996553421
[2m[36m(func pid=78481)[0m rmse_per_class: [0.106, 0.252, 0.067, 0.321, 0.054, 0.18, 0.273, 0.133, 0.157, 0.103]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=60723)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3566 | Steps: 2 | Val loss: 0.2906 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.5735 | Steps: 2 | Val loss: 0.4549 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4107 | Steps: 2 | Val loss: 0.3199 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 05:28:06 (running for 00:27:12.53)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 3 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.58  |  0.18  |                   71 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.414 |  0.175 |                   70 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.395 |  0.165 |                   21 |
| train_5a6ec_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 TERMINATED)


[2m[36m(func pid=60723)[0m rmse: 0.15660445392131805
[2m[36m(func pid=60723)[0m mae:  0.09821034222841263
[2m[36m(func pid=60723)[0m rmse_per_class: [0.093, 0.234, 0.046, 0.279, 0.105, 0.17, 0.287, 0.137, 0.131, 0.084]
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3914 | Steps: 2 | Val loss: 0.3001 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=67138)[0m rmse: 0.1799185574054718
[2m[36m(func pid=67138)[0m mae:  0.13250987231731415
[2m[36m(func pid=67138)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.1747080385684967
[2m[36m(func pid=67747)[0m mae:  0.12731805443763733
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.26, 0.112, 0.334, 0.065, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.16408511996269226
[2m[36m(func pid=78481)[0m mae:  0.11912675201892853
[2m[36m(func pid=78481)[0m rmse_per_class: [0.103, 0.253, 0.062, 0.319, 0.055, 0.179, 0.274, 0.133, 0.156, 0.107]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.5721 | Steps: 2 | Val loss: 0.4533 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4147 | Steps: 2 | Val loss: 0.3197 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3902 | Steps: 2 | Val loss: 0.2993 | Batch size: 32 | lr: 0.01 | Duration: 2.62s
[2m[36m(func pid=67138)[0m rmse: 0.17989754676818848
[2m[36m(func pid=67138)[0m mae:  0.13249368965625763
[2m[36m(func pid=67138)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.299, 0.142, 0.144, 0.114]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=67747)[0m rmse: 0.174728125333786
[2m[36m(func pid=67747)[0m mae:  0.12728819251060486
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.259, 0.113, 0.334, 0.065, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=78481)[0m rmse: 0.16400368511676788
[2m[36m(func pid=78481)[0m mae:  0.11898183822631836
[2m[36m(func pid=78481)[0m rmse_per_class: [0.101, 0.253, 0.061, 0.317, 0.055, 0.179, 0.274, 0.134, 0.154, 0.11]
== Status ==
Current time: 2024-01-07 05:28:12 (running for 00:27:18.90)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.572 |  0.18  |                   73 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.411 |  0.175 |                   71 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.391 |  0.164 |                   22 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=83767)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=83767)[0m Configuration completed!
[2m[36m(func pid=83767)[0m New optimizer parameters:
[2m[36m(func pid=83767)[0m SGD (
[2m[36m(func pid=83767)[0m Parameter Group 0
[2m[36m(func pid=83767)[0m     dampening: 0
[2m[36m(func pid=83767)[0m     differentiable: False
[2m[36m(func pid=83767)[0m     foreach: None
[2m[36m(func pid=83767)[0m     lr: 0.1
[2m[36m(func pid=83767)[0m     maximize: False
[2m[36m(func pid=83767)[0m     momentum: 0.9
[2m[36m(func pid=83767)[0m     nesterov: False
[2m[36m(func pid=83767)[0m     weight_decay: 0.0001
[2m[36m(func pid=83767)[0m )
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5746 | Steps: 2 | Val loss: 0.4520 | Batch size: 32 | lr: 0.0001 | Duration: 2.63s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3890 | Steps: 2 | Val loss: 0.2988 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4140 | Steps: 2 | Val loss: 0.3197 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 05:28:18 (running for 00:27:24.15)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00012 | RUNNING    | 192.168.7.53:67138 | 0.0001 |       0.9  |         0.0001 |  0.575 |  0.18  |                   74 |
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.415 |  0.175 |                   72 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.39  |  0.164 |                   23 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_5a6ec_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67138)[0m rmse: 0.17988832294940948
[2m[36m(func pid=67138)[0m mae:  0.13248780369758606
[2m[36m(func pid=67138)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.299, 0.142, 0.144, 0.114]
[2m[36m(func pid=67138)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.7998 | Steps: 2 | Val loss: 0.4368 | Batch size: 32 | lr: 0.1 | Duration: 4.36s
[2m[36m(func pid=67747)[0m rmse: 0.17469026148319244
[2m[36m(func pid=67747)[0m mae:  0.12724827229976654
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.259, 0.113, 0.334, 0.065, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.16413187980651855
[2m[36m(func pid=78481)[0m mae:  0.11897899210453033
[2m[36m(func pid=78481)[0m rmse_per_class: [0.1, 0.254, 0.06, 0.316, 0.055, 0.179, 0.275, 0.136, 0.152, 0.114]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=67138)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.5700 | Steps: 2 | Val loss: 0.4506 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=83767)[0m rmse: 0.17922934889793396
[2m[36m(func pid=83767)[0m mae:  0.1311706155538559
[2m[36m(func pid=83767)[0m rmse_per_class: [0.121, 0.264, 0.117, 0.342, 0.088, 0.191, 0.283, 0.139, 0.143, 0.105]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4125 | Steps: 2 | Val loss: 0.3193 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3908 | Steps: 2 | Val loss: 0.2985 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=67138)[0m rmse: 0.17987245321273804
[2m[36m(func pid=67138)[0m mae:  0.13247661292552948
[2m[36m(func pid=67138)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.299, 0.142, 0.144, 0.114]
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.4917 | Steps: 2 | Val loss: 0.3614 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=67747)[0m rmse: 0.17469771206378937
[2m[36m(func pid=67747)[0m mae:  0.12717695534229279
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.259, 0.114, 0.334, 0.065, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=78481)[0m rmse: 0.1645645797252655
[2m[36m(func pid=78481)[0m mae:  0.11904464662075043
[2m[36m(func pid=78481)[0m rmse_per_class: [0.101, 0.255, 0.062, 0.315, 0.055, 0.179, 0.275, 0.137, 0.149, 0.117]
[2m[36m(func pid=83767)[0m rmse: 0.17834283411502838
[2m[36m(func pid=83767)[0m mae:  0.1297144591808319
[2m[36m(func pid=83767)[0m rmse_per_class: [0.121, 0.264, 0.127, 0.343, 0.078, 0.189, 0.276, 0.139, 0.148, 0.098]
== Status ==
Current time: 2024-01-07 05:28:23 (running for 00:27:29.52)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.414 |  0.175 |                   73 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.389 |  0.164 |                   24 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.8   |  0.179 |                    1 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 05:28:30 (running for 00:27:36.88)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   74 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.389 |  0.164 |                   24 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.8   |  0.179 |                    1 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67747)[0m 
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=84699)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=84699)[0m Configuration completed!
[2m[36m(func pid=84699)[0m New optimizer parameters:
[2m[36m(func pid=84699)[0m SGD (
[2m[36m(func pid=84699)[0m Parameter Group 0
[2m[36m(func pid=84699)[0m     dampening: 0
[2m[36m(func pid=84699)[0m     differentiable: False
[2m[36m(func pid=84699)[0m     foreach: None
[2m[36m(func pid=84699)[0m     lr: 0.0001
[2m[36m(func pid=84699)[0m     maximize: False
[2m[36m(func pid=84699)[0m     momentum: 0.99
[2m[36m(func pid=84699)[0m     nesterov: False
[2m[36m(func pid=84699)[0m     weight_decay: 1e-05
[2m[36m(func pid=84699)[0m )
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=67747)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4133 | Steps: 2 | Val loss: 0.3192 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3856 | Steps: 2 | Val loss: 0.2988 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.6387 | Steps: 2 | Val loss: 0.3259 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0440 | Steps: 2 | Val loss: 0.7188 | Batch size: 32 | lr: 0.0001 | Duration: 4.80s
== Status ==
Current time: 2024-01-07 05:28:35 (running for 00:27:41.90)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00013 | RUNNING    | 192.168.7.53:67747 | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   74 |
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.391 |  0.165 |                   25 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.492 |  0.178 |                    2 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=67747)[0m rmse: 0.17464569211006165
[2m[36m(func pid=67747)[0m mae:  0.12713828682899475
[2m[36m(func pid=67747)[0m rmse_per_class: [0.126, 0.259, 0.113, 0.334, 0.065, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=78481)[0m rmse: 0.1650640219449997
[2m[36m(func pid=78481)[0m mae:  0.11934026330709457
[2m[36m(func pid=78481)[0m rmse_per_class: [0.101, 0.255, 0.062, 0.316, 0.056, 0.18, 0.275, 0.138, 0.149, 0.119]
[2m[36m(func pid=83767)[0m rmse: 0.177056223154068
[2m[36m(func pid=83767)[0m mae:  0.1273016631603241
[2m[36m(func pid=83767)[0m rmse_per_class: [0.132, 0.264, 0.13, 0.347, 0.062, 0.188, 0.265, 0.14, 0.151, 0.092]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17990900576114655
[2m[36m(func pid=84699)[0m mae:  0.13243499398231506
[2m[36m(func pid=84699)[0m rmse_per_class: [0.114, 0.263, 0.097, 0.333, 0.1, 0.191, 0.298, 0.143, 0.141, 0.119]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3839 | Steps: 2 | Val loss: 0.2990 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6747 | Steps: 2 | Val loss: 0.3236 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0422 | Steps: 2 | Val loss: 0.7186 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=78481)[0m rmse: 0.16558532416820526
[2m[36m(func pid=78481)[0m mae:  0.11963536590337753
[2m[36m(func pid=78481)[0m rmse_per_class: [0.103, 0.255, 0.064, 0.317, 0.057, 0.18, 0.274, 0.138, 0.148, 0.12]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.17760154604911804
[2m[36m(func pid=83767)[0m mae:  0.12570184469223022
[2m[36m(func pid=83767)[0m rmse_per_class: [0.159, 0.262, 0.113, 0.352, 0.055, 0.188, 0.26, 0.143, 0.153, 0.091]
[2m[36m(func pid=84699)[0m rmse: 0.18039071559906006
[2m[36m(func pid=84699)[0m mae:  0.13285908102989197
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
== Status ==
Current time: 2024-01-07 05:28:41 (running for 00:27:47.72)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.384 |  0.166 |                   27 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.639 |  0.177 |                    3 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  1.044 |  0.18  |                    1 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=85321)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=85321)[0m Configuration completed!
[2m[36m(func pid=85321)[0m New optimizer parameters:
[2m[36m(func pid=85321)[0m SGD (
[2m[36m(func pid=85321)[0m Parameter Group 0
[2m[36m(func pid=85321)[0m     dampening: 0
[2m[36m(func pid=85321)[0m     differentiable: False
[2m[36m(func pid=85321)[0m     foreach: None
[2m[36m(func pid=85321)[0m     lr: 0.001
[2m[36m(func pid=85321)[0m     maximize: False
[2m[36m(func pid=85321)[0m     momentum: 0.99
[2m[36m(func pid=85321)[0m     nesterov: False
[2m[36m(func pid=85321)[0m     weight_decay: 1e-05
[2m[36m(func pid=85321)[0m )
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3829 | Steps: 2 | Val loss: 0.2990 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 05:28:46 (running for 00:27:53.02)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.383 |  0.166 |                   28 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.675 |  0.178 |                    4 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  1.042 |  0.18  |                    2 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16596511006355286
[2m[36m(func pid=78481)[0m mae:  0.11986907571554184
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.255, 0.065, 0.318, 0.057, 0.18, 0.274, 0.137, 0.148, 0.119]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.6124 | Steps: 2 | Val loss: 0.3292 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0366 | Steps: 2 | Val loss: 0.7226 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0414 | Steps: 2 | Val loss: 0.7132 | Batch size: 32 | lr: 0.001 | Duration: 4.50s
[2m[36m(func pid=83767)[0m rmse: 0.17695695161819458
[2m[36m(func pid=83767)[0m mae:  0.12484346330165863
[2m[36m(func pid=83767)[0m rmse_per_class: [0.172, 0.256, 0.097, 0.352, 0.055, 0.185, 0.256, 0.146, 0.158, 0.092]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3806 | Steps: 2 | Val loss: 0.2995 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=84699)[0m rmse: 0.1806538850069046
[2m[36m(func pid=84699)[0m mae:  0.1330937147140503
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.105, 0.191, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.17988517880439758
[2m[36m(func pid=85321)[0m mae:  0.1324104517698288
[2m[36m(func pid=85321)[0m rmse_per_class: [0.114, 0.263, 0.097, 0.333, 0.1, 0.191, 0.298, 0.143, 0.141, 0.119]
[2m[36m(func pid=85321)[0m 
== Status ==
Current time: 2024-01-07 05:28:51 (running for 00:27:58.07)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.381 |  0.167 |                   29 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.612 |  0.177 |                    5 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  1.037 |  0.181 |                    3 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  1.041 |  0.18  |                    1 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16657228767871857
[2m[36m(func pid=78481)[0m mae:  0.12025170028209686
[2m[36m(func pid=78481)[0m rmse_per_class: [0.108, 0.254, 0.068, 0.321, 0.058, 0.18, 0.273, 0.136, 0.149, 0.118]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5568 | Steps: 2 | Val loss: 0.3209 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.0269 | Steps: 2 | Val loss: 0.7263 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0058 | Steps: 2 | Val loss: 0.7035 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=83767)[0m rmse: 0.16689343750476837
[2m[36m(func pid=83767)[0m mae:  0.11906244605779648
[2m[36m(func pid=83767)[0m rmse_per_class: [0.109, 0.252, 0.059, 0.333, 0.056, 0.181, 0.26, 0.145, 0.182, 0.092]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3808 | Steps: 2 | Val loss: 0.2997 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=84699)[0m rmse: 0.18077422678470612
[2m[36m(func pid=84699)[0m mae:  0.1332094371318817
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.331, 0.106, 0.192, 0.302, 0.142, 0.143, 0.115]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.18035100400447845
[2m[36m(func pid=85321)[0m mae:  0.13281357288360596
[2m[36m(func pid=85321)[0m rmse_per_class: [0.114, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1669614017009735
[2m[36m(func pid=78481)[0m mae:  0.12044662237167358
[2m[36m(func pid=78481)[0m rmse_per_class: [0.111, 0.254, 0.069, 0.322, 0.059, 0.181, 0.273, 0.135, 0.149, 0.118]
== Status ==
Current time: 2024-01-07 05:28:57 (running for 00:28:03.40)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.381 |  0.167 |                   30 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.557 |  0.167 |                    6 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  1.027 |  0.181 |                    4 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  1.006 |  0.18  |                    2 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5012 | Steps: 2 | Val loss: 0.3321 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.0146 | Steps: 2 | Val loss: 0.7279 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.9516 | Steps: 2 | Val loss: 0.6880 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=83767)[0m rmse: 0.16429349780082703
[2m[36m(func pid=83767)[0m mae:  0.1160152405500412
[2m[36m(func pid=83767)[0m rmse_per_class: [0.101, 0.256, 0.045, 0.303, 0.056, 0.194, 0.286, 0.139, 0.172, 0.091]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3880 | Steps: 2 | Val loss: 0.2992 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=85321)[0m rmse: 0.18056371808052063
[2m[36m(func pid=85321)[0m mae:  0.13300302624702454
[2m[36m(func pid=85321)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.104, 0.191, 0.299, 0.142, 0.143, 0.115]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.1808524876832962
[2m[36m(func pid=84699)[0m mae:  0.13328547775745392
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.107, 0.192, 0.303, 0.142, 0.143, 0.115]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4677 | Steps: 2 | Val loss: 0.3185 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 05:29:02 (running for 00:28:08.75)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.388 |  0.167 |                   31 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.501 |  0.164 |                    7 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  1.015 |  0.181 |                    5 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.952 |  0.181 |                    3 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16679123044013977
[2m[36m(func pid=78481)[0m mae:  0.12024316936731339
[2m[36m(func pid=78481)[0m rmse_per_class: [0.113, 0.253, 0.068, 0.323, 0.06, 0.181, 0.271, 0.134, 0.149, 0.116]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8731 | Steps: 2 | Val loss: 0.6611 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.0003 | Steps: 2 | Val loss: 0.7269 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=83767)[0m rmse: 0.16137950122356415
[2m[36m(func pid=83767)[0m mae:  0.11341826617717743
[2m[36m(func pid=83767)[0m rmse_per_class: [0.101, 0.247, 0.047, 0.294, 0.056, 0.206, 0.297, 0.127, 0.137, 0.103]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3777 | Steps: 2 | Val loss: 0.2978 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=85321)[0m rmse: 0.1806008517742157
[2m[36m(func pid=85321)[0m mae:  0.13303890824317932
[2m[36m(func pid=85321)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.332, 0.105, 0.191, 0.3, 0.142, 0.143, 0.114]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.1809236854314804
[2m[36m(func pid=84699)[0m mae:  0.13334426283836365
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.107, 0.191, 0.303, 0.142, 0.142, 0.116]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4343 | Steps: 2 | Val loss: 0.3305 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
== Status ==
Current time: 2024-01-07 05:29:07 (running for 00:28:14.03)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.378 |  0.166 |                   32 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.468 |  0.161 |                    8 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  1     |  0.181 |                    6 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.873 |  0.181 |                    4 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16586366295814514
[2m[36m(func pid=78481)[0m mae:  0.11961108446121216
[2m[36m(func pid=78481)[0m rmse_per_class: [0.112, 0.252, 0.065, 0.322, 0.061, 0.18, 0.27, 0.133, 0.149, 0.114]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.7874 | Steps: 2 | Val loss: 0.6229 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=83767)[0m rmse: 0.17623521387577057
[2m[36m(func pid=83767)[0m mae:  0.12307620048522949
[2m[36m(func pid=83767)[0m rmse_per_class: [0.101, 0.256, 0.046, 0.321, 0.055, 0.181, 0.293, 0.179, 0.133, 0.198]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.9784 | Steps: 2 | Val loss: 0.7232 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3779 | Steps: 2 | Val loss: 0.2970 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=85321)[0m rmse: 0.18052738904953003
[2m[36m(func pid=85321)[0m mae:  0.13297346234321594
[2m[36m(func pid=85321)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.331, 0.106, 0.191, 0.3, 0.141, 0.143, 0.113]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.18095935881137848
[2m[36m(func pid=84699)[0m mae:  0.1333637535572052
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.33, 0.108, 0.191, 0.304, 0.143, 0.142, 0.116]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4283 | Steps: 2 | Val loss: 0.3508 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=78481)[0m rmse: 0.16528120636940002
[2m[36m(func pid=78481)[0m mae:  0.1191985160112381
[2m[36m(func pid=78481)[0m rmse_per_class: [0.111, 0.252, 0.063, 0.323, 0.061, 0.18, 0.268, 0.132, 0.149, 0.113]
[2m[36m(func pid=78481)[0m 
== Status ==
Current time: 2024-01-07 05:29:13 (running for 00:28:19.30)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.378 |  0.165 |                   33 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.434 |  0.176 |                    9 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.978 |  0.181 |                    7 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.787 |  0.181 |                    5 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.6994 | Steps: 2 | Val loss: 0.5766 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=83767)[0m rmse: 0.18788333237171173
[2m[36m(func pid=83767)[0m mae:  0.1304689645767212
[2m[36m(func pid=83767)[0m rmse_per_class: [0.103, 0.264, 0.045, 0.355, 0.054, 0.185, 0.277, 0.215, 0.133, 0.247]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.9638 | Steps: 2 | Val loss: 0.7165 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3758 | Steps: 2 | Val loss: 0.2960 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=85321)[0m rmse: 0.18040016293525696
[2m[36m(func pid=85321)[0m mae:  0.13284529745578766
[2m[36m(func pid=85321)[0m rmse_per_class: [0.113, 0.264, 0.103, 0.331, 0.106, 0.191, 0.3, 0.14, 0.143, 0.112]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4433 | Steps: 2 | Val loss: 0.3148 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=84699)[0m rmse: 0.180982768535614
[2m[36m(func pid=84699)[0m mae:  0.13338056206703186
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.33, 0.108, 0.191, 0.304, 0.143, 0.142, 0.116]
[2m[36m(func pid=84699)[0m 
== Status ==
Current time: 2024-01-07 05:29:18 (running for 00:28:24.52)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.376 |  0.165 |                   34 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.428 |  0.188 |                   10 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.964 |  0.181 |                    8 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.699 |  0.18  |                    6 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.1645953506231308
[2m[36m(func pid=78481)[0m mae:  0.11872555315494537
[2m[36m(func pid=78481)[0m rmse_per_class: [0.11, 0.251, 0.061, 0.322, 0.062, 0.18, 0.267, 0.132, 0.15, 0.111]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.6238 | Steps: 2 | Val loss: 0.5264 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=83767)[0m rmse: 0.17428170144557953
[2m[36m(func pid=83767)[0m mae:  0.12271936237812042
[2m[36m(func pid=83767)[0m rmse_per_class: [0.096, 0.248, 0.048, 0.353, 0.055, 0.186, 0.265, 0.164, 0.138, 0.19]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.9425 | Steps: 2 | Val loss: 0.7076 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=85321)[0m rmse: 0.18019573390483856
[2m[36m(func pid=85321)[0m mae:  0.1326386034488678
[2m[36m(func pid=85321)[0m rmse_per_class: [0.114, 0.264, 0.105, 0.331, 0.105, 0.191, 0.298, 0.139, 0.144, 0.111]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3751 | Steps: 2 | Val loss: 0.2949 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4110 | Steps: 2 | Val loss: 0.2936 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=84699)[0m rmse: 0.18101565539836884
[2m[36m(func pid=84699)[0m mae:  0.13340316712856293
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.329, 0.108, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=84699)[0m 
== Status ==
Current time: 2024-01-07 05:29:23 (running for 00:28:29.87)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.375 |  0.164 |                   35 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.443 |  0.174 |                   11 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.942 |  0.181 |                    9 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.624 |  0.18  |                    7 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16387125849723816
[2m[36m(func pid=78481)[0m mae:  0.11825422197580338
[2m[36m(func pid=78481)[0m rmse_per_class: [0.109, 0.251, 0.058, 0.321, 0.063, 0.18, 0.266, 0.131, 0.15, 0.11]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.5622 | Steps: 2 | Val loss: 0.4775 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=83767)[0m rmse: 0.1625761091709137
[2m[36m(func pid=83767)[0m mae:  0.116036556661129
[2m[36m(func pid=83767)[0m rmse_per_class: [0.098, 0.238, 0.045, 0.334, 0.07, 0.179, 0.261, 0.12, 0.17, 0.11]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.1799686700105667
[2m[36m(func pid=85321)[0m mae:  0.13239268958568573
[2m[36m(func pid=85321)[0m rmse_per_class: [0.114, 0.264, 0.107, 0.332, 0.104, 0.19, 0.297, 0.139, 0.144, 0.109]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.9188 | Steps: 2 | Val loss: 0.6963 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3756 | Steps: 2 | Val loss: 0.2943 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3825 | Steps: 2 | Val loss: 0.2898 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.5151 | Steps: 2 | Val loss: 0.4331 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 05:29:28 (running for 00:28:35.13)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.375 |  0.164 |                   35 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.411 |  0.163 |                   12 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.919 |  0.181 |                   10 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.562 |  0.18  |                    8 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.1635255515575409
[2m[36m(func pid=78481)[0m mae:  0.11794547736644745
[2m[36m(func pid=78481)[0m rmse_per_class: [0.107, 0.251, 0.058, 0.321, 0.064, 0.179, 0.265, 0.131, 0.15, 0.109]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.18101730942726135
[2m[36m(func pid=84699)[0m mae:  0.1334008425474167
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.329, 0.108, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1621987670660019
[2m[36m(func pid=83767)[0m mae:  0.11342760175466537
[2m[36m(func pid=83767)[0m rmse_per_class: [0.096, 0.254, 0.047, 0.304, 0.106, 0.173, 0.277, 0.123, 0.15, 0.09]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.17972199618816376
[2m[36m(func pid=85321)[0m mae:  0.13210779428482056
[2m[36m(func pid=85321)[0m rmse_per_class: [0.115, 0.264, 0.109, 0.333, 0.103, 0.189, 0.294, 0.138, 0.145, 0.107]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3748 | Steps: 2 | Val loss: 0.2934 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8960 | Steps: 2 | Val loss: 0.6837 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3982 | Steps: 2 | Val loss: 0.2946 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4785 | Steps: 2 | Val loss: 0.3955 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=78481)[0m rmse: 0.16317886114120483
[2m[36m(func pid=78481)[0m mae:  0.11756168305873871
[2m[36m(func pid=78481)[0m rmse_per_class: [0.106, 0.251, 0.059, 0.32, 0.064, 0.179, 0.265, 0.13, 0.151, 0.108]
[2m[36m(func pid=78481)[0m 
== Status ==
Current time: 2024-01-07 05:29:34 (running for 00:28:40.50)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.375 |  0.163 |                   37 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.382 |  0.162 |                   13 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.919 |  0.181 |                   10 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.515 |  0.18  |                    9 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=84699)[0m rmse: 0.18101584911346436
[2m[36m(func pid=84699)[0m mae:  0.1333918273448944
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1652757078409195
[2m[36m(func pid=83767)[0m mae:  0.11398646980524063
[2m[36m(func pid=83767)[0m rmse_per_class: [0.102, 0.236, 0.048, 0.293, 0.162, 0.178, 0.283, 0.125, 0.136, 0.09]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.1794600933790207
[2m[36m(func pid=85321)[0m mae:  0.1317906379699707
[2m[36m(func pid=85321)[0m rmse_per_class: [0.115, 0.264, 0.112, 0.334, 0.101, 0.189, 0.292, 0.137, 0.146, 0.105]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3737 | Steps: 2 | Val loss: 0.2922 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8706 | Steps: 2 | Val loss: 0.6695 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3905 | Steps: 2 | Val loss: 0.3031 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4610 | Steps: 2 | Val loss: 0.3665 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=78481)[0m rmse: 0.16253992915153503
[2m[36m(func pid=78481)[0m mae:  0.11711321771144867
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.25, 0.057, 0.317, 0.065, 0.179, 0.265, 0.13, 0.15, 0.107]
== Status ==
Current time: 2024-01-07 05:29:39 (running for 00:28:45.82)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.374 |  0.163 |                   38 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.398 |  0.165 |                   14 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.896 |  0.181 |                   11 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.478 |  0.179 |                   10 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.18100512027740479
[2m[36m(func pid=84699)[0m mae:  0.13337838649749756
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.17185571789741516
[2m[36m(func pid=83767)[0m mae:  0.11729468405246735
[2m[36m(func pid=83767)[0m rmse_per_class: [0.11, 0.244, 0.047, 0.316, 0.197, 0.188, 0.274, 0.119, 0.134, 0.09]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.17917266488075256
[2m[36m(func pid=85321)[0m mae:  0.13140462338924408
[2m[36m(func pid=85321)[0m rmse_per_class: [0.116, 0.265, 0.115, 0.335, 0.098, 0.188, 0.288, 0.136, 0.148, 0.102]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3751 | Steps: 2 | Val loss: 0.2913 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3755 | Steps: 2 | Val loss: 0.2983 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8448 | Steps: 2 | Val loss: 0.6542 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4511 | Steps: 2 | Val loss: 0.3461 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 05:29:45 (running for 00:28:51.15)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.375 |  0.162 |                   39 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.391 |  0.172 |                   15 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.871 |  0.181 |                   12 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.461 |  0.179 |                   11 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16215506196022034
[2m[36m(func pid=78481)[0m mae:  0.11668837070465088
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.25, 0.058, 0.315, 0.065, 0.179, 0.265, 0.129, 0.15, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.166649729013443
[2m[36m(func pid=83767)[0m mae:  0.1153971403837204
[2m[36m(func pid=83767)[0m rmse_per_class: [0.095, 0.239, 0.05, 0.332, 0.161, 0.178, 0.264, 0.113, 0.136, 0.099]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.18096502125263214
[2m[36m(func pid=84699)[0m mae:  0.13334189355373383
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.108, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.17895224690437317
[2m[36m(func pid=85321)[0m mae:  0.13104543089866638
[2m[36m(func pid=85321)[0m rmse_per_class: [0.117, 0.265, 0.119, 0.337, 0.094, 0.188, 0.285, 0.136, 0.149, 0.1]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3746 | Steps: 2 | Val loss: 0.2905 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3654 | Steps: 2 | Val loss: 0.2947 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8159 | Steps: 2 | Val loss: 0.6374 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4482 | Steps: 2 | Val loss: 0.3334 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 05:29:50 (running for 00:28:56.45)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.375 |  0.162 |                   40 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.376 |  0.167 |                   16 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.845 |  0.181 |                   13 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.451 |  0.179 |                   12 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16166792809963226
[2m[36m(func pid=78481)[0m mae:  0.11629843711853027
[2m[36m(func pid=78481)[0m rmse_per_class: [0.104, 0.25, 0.058, 0.314, 0.065, 0.178, 0.265, 0.129, 0.149, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.16291341185569763
[2m[36m(func pid=83767)[0m mae:  0.11463487148284912
[2m[36m(func pid=83767)[0m rmse_per_class: [0.095, 0.231, 0.047, 0.333, 0.097, 0.172, 0.264, 0.119, 0.141, 0.13]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.1809195876121521
[2m[36m(func pid=84699)[0m mae:  0.13330116868019104
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.108, 0.19, 0.305, 0.144, 0.142, 0.116]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.17876765131950378
[2m[36m(func pid=85321)[0m mae:  0.1306561380624771
[2m[36m(func pid=85321)[0m rmse_per_class: [0.117, 0.265, 0.122, 0.339, 0.091, 0.187, 0.281, 0.136, 0.151, 0.099]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3730 | Steps: 2 | Val loss: 0.2902 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3544 | Steps: 2 | Val loss: 0.2904 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4519 | Steps: 2 | Val loss: 0.3272 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7900 | Steps: 2 | Val loss: 0.6197 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 05:29:55 (running for 00:29:01.63)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.373 |  0.162 |                   41 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.365 |  0.163 |                   17 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.816 |  0.181 |                   14 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.448 |  0.179 |                   13 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.161512553691864
[2m[36m(func pid=78481)[0m mae:  0.11613551527261734
[2m[36m(func pid=78481)[0m rmse_per_class: [0.104, 0.249, 0.058, 0.314, 0.065, 0.178, 0.265, 0.129, 0.148, 0.105]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.16032792627811432
[2m[36m(func pid=83767)[0m mae:  0.11398148536682129
[2m[36m(func pid=83767)[0m rmse_per_class: [0.098, 0.228, 0.045, 0.311, 0.068, 0.176, 0.276, 0.116, 0.142, 0.142]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.17864671349525452
[2m[36m(func pid=85321)[0m mae:  0.13028475642204285
[2m[36m(func pid=85321)[0m rmse_per_class: [0.118, 0.266, 0.125, 0.341, 0.087, 0.187, 0.278, 0.136, 0.152, 0.097]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.18086370825767517
[2m[36m(func pid=84699)[0m mae:  0.1332552134990692
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.108, 0.19, 0.305, 0.144, 0.142, 0.116]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3784 | Steps: 2 | Val loss: 0.2902 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3562 | Steps: 2 | Val loss: 0.2809 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4589 | Steps: 2 | Val loss: 0.3263 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7648 | Steps: 2 | Val loss: 0.6020 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 05:30:00 (running for 00:29:06.76)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.378 |  0.162 |                   42 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.354 |  0.16  |                   18 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.79  |  0.181 |                   15 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.452 |  0.179 |                   14 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16160646080970764
[2m[36m(func pid=78481)[0m mae:  0.11611044406890869
[2m[36m(func pid=78481)[0m rmse_per_class: [0.104, 0.249, 0.06, 0.314, 0.065, 0.178, 0.265, 0.129, 0.147, 0.105]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15570510923862457
[2m[36m(func pid=83767)[0m mae:  0.10995370149612427
[2m[36m(func pid=83767)[0m rmse_per_class: [0.121, 0.224, 0.046, 0.292, 0.06, 0.175, 0.275, 0.109, 0.14, 0.115]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.17860250174999237
[2m[36m(func pid=85321)[0m mae:  0.12992605566978455
[2m[36m(func pid=85321)[0m rmse_per_class: [0.119, 0.266, 0.129, 0.343, 0.082, 0.186, 0.275, 0.137, 0.154, 0.095]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.1807834357023239
[2m[36m(func pid=84699)[0m mae:  0.1331830769777298
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.107, 0.19, 0.304, 0.144, 0.142, 0.115]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3712 | Steps: 2 | Val loss: 0.2903 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3634 | Steps: 2 | Val loss: 0.2726 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4696 | Steps: 2 | Val loss: 0.3298 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7385 | Steps: 2 | Val loss: 0.5842 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=78481)[0m rmse: 0.16157646477222443
[2m[36m(func pid=78481)[0m mae:  0.11611628532409668
[2m[36m(func pid=78481)[0m rmse_per_class: [0.104, 0.248, 0.058, 0.315, 0.065, 0.178, 0.265, 0.129, 0.146, 0.106]
[2m[36m(func pid=78481)[0m 
== Status ==
Current time: 2024-01-07 05:30:05 (running for 00:29:12.04)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.371 |  0.162 |                   43 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.356 |  0.156 |                   19 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.765 |  0.181 |                   16 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.459 |  0.179 |                   15 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83767)[0m rmse: 0.15072965621948242
[2m[36m(func pid=83767)[0m mae:  0.10570363700389862
[2m[36m(func pid=83767)[0m rmse_per_class: [0.106, 0.222, 0.046, 0.29, 0.06, 0.168, 0.26, 0.107, 0.144, 0.104]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.1786341369152069
[2m[36m(func pid=85321)[0m mae:  0.12957227230072021
[2m[36m(func pid=85321)[0m rmse_per_class: [0.12, 0.266, 0.132, 0.346, 0.078, 0.186, 0.272, 0.137, 0.155, 0.094]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.18071997165679932
[2m[36m(func pid=84699)[0m mae:  0.1331348866224289
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.107, 0.19, 0.304, 0.143, 0.143, 0.115]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3711 | Steps: 2 | Val loss: 0.2905 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3534 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4864 | Steps: 2 | Val loss: 0.3365 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 05:30:11 (running for 00:29:17.34)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.371 |  0.162 |                   44 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.363 |  0.151 |                   20 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.738 |  0.181 |                   17 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.47  |  0.179 |                   16 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16159307956695557
[2m[36m(func pid=78481)[0m mae:  0.11616871505975723
[2m[36m(func pid=78481)[0m rmse_per_class: [0.104, 0.248, 0.057, 0.316, 0.066, 0.178, 0.265, 0.129, 0.146, 0.107]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14993073046207428
[2m[36m(func pid=83767)[0m mae:  0.10404278337955475
[2m[36m(func pid=83767)[0m rmse_per_class: [0.099, 0.22, 0.048, 0.311, 0.061, 0.169, 0.241, 0.108, 0.146, 0.097]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.7152 | Steps: 2 | Val loss: 0.5657 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=85321)[0m rmse: 0.17876288294792175
[2m[36m(func pid=85321)[0m mae:  0.12922799587249756
[2m[36m(func pid=85321)[0m rmse_per_class: [0.121, 0.267, 0.135, 0.348, 0.074, 0.186, 0.269, 0.138, 0.156, 0.093]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.18063919246196747
[2m[36m(func pid=84699)[0m mae:  0.13307200372219086
[2m[36m(func pid=84699)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.107, 0.19, 0.303, 0.143, 0.143, 0.115]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3433 | Steps: 2 | Val loss: 0.2767 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3759 | Steps: 2 | Val loss: 0.2908 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.5003 | Steps: 2 | Val loss: 0.3452 | Batch size: 32 | lr: 0.001 | Duration: 2.62s
== Status ==
Current time: 2024-01-07 05:30:16 (running for 00:29:22.82)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.376 |  0.162 |                   45 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.353 |  0.15  |                   21 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.715 |  0.181 |                   18 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.486 |  0.179 |                   17 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16183796525001526
[2m[36m(func pid=78481)[0m mae:  0.11626683175563812
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.248, 0.057, 0.317, 0.066, 0.177, 0.265, 0.128, 0.145, 0.108]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15228572487831116
[2m[36m(func pid=83767)[0m mae:  0.1055925115942955
[2m[36m(func pid=83767)[0m rmse_per_class: [0.113, 0.218, 0.049, 0.323, 0.062, 0.169, 0.241, 0.108, 0.146, 0.095]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6932 | Steps: 2 | Val loss: 0.5478 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=85321)[0m rmse: 0.1789712905883789
[2m[36m(func pid=85321)[0m mae:  0.12892089784145355
[2m[36m(func pid=85321)[0m rmse_per_class: [0.122, 0.268, 0.137, 0.35, 0.07, 0.186, 0.268, 0.139, 0.157, 0.093]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3735 | Steps: 2 | Val loss: 0.2909 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3531 | Steps: 2 | Val loss: 0.2773 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=84699)[0m rmse: 0.18056565523147583
[2m[36m(func pid=84699)[0m mae:  0.1330150067806244
[2m[36m(func pid=84699)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.329, 0.106, 0.19, 0.303, 0.143, 0.143, 0.115]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.5189 | Steps: 2 | Val loss: 0.3563 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=78481)[0m rmse: 0.16175881028175354
[2m[36m(func pid=78481)[0m mae:  0.116313636302948
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.248, 0.054, 0.318, 0.067, 0.177, 0.265, 0.128, 0.144, 0.111]
== Status ==
Current time: 2024-01-07 05:30:21 (running for 00:29:28.05)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.373 |  0.162 |                   46 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.343 |  0.152 |                   22 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.693 |  0.181 |                   19 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.5   |  0.179 |                   18 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1535383015871048
[2m[36m(func pid=83767)[0m mae:  0.10711611807346344
[2m[36m(func pid=83767)[0m rmse_per_class: [0.119, 0.218, 0.045, 0.315, 0.067, 0.167, 0.254, 0.106, 0.146, 0.098]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.1794205605983734
[2m[36m(func pid=85321)[0m mae:  0.12872575223445892
[2m[36m(func pid=85321)[0m rmse_per_class: [0.123, 0.268, 0.141, 0.352, 0.067, 0.186, 0.268, 0.14, 0.158, 0.092]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6695 | Steps: 2 | Val loss: 0.5302 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3708 | Steps: 2 | Val loss: 0.2914 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3356 | Steps: 2 | Val loss: 0.2722 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5316 | Steps: 2 | Val loss: 0.3683 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=84699)[0m rmse: 0.18049418926239014
[2m[36m(func pid=84699)[0m mae:  0.13295438885688782
[2m[36m(func pid=84699)[0m rmse_per_class: [0.114, 0.264, 0.1, 0.329, 0.106, 0.19, 0.302, 0.143, 0.143, 0.115]
[2m[36m(func pid=84699)[0m 
== Status ==
Current time: 2024-01-07 05:30:27 (running for 00:29:33.46)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.371 |  0.162 |                   47 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.353 |  0.154 |                   23 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.669 |  0.18  |                   20 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.519 |  0.179 |                   19 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.162118062376976
[2m[36m(func pid=78481)[0m mae:  0.11654255539178848
[2m[36m(func pid=78481)[0m rmse_per_class: [0.106, 0.248, 0.054, 0.319, 0.067, 0.177, 0.265, 0.128, 0.145, 0.112]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15093272924423218
[2m[36m(func pid=83767)[0m mae:  0.10517867654561996
[2m[36m(func pid=83767)[0m rmse_per_class: [0.106, 0.219, 0.043, 0.289, 0.077, 0.168, 0.261, 0.104, 0.142, 0.101]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.17991971969604492
[2m[36m(func pid=85321)[0m mae:  0.12853321433067322
[2m[36m(func pid=85321)[0m rmse_per_class: [0.124, 0.268, 0.143, 0.355, 0.064, 0.186, 0.268, 0.141, 0.158, 0.092]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6476 | Steps: 2 | Val loss: 0.5133 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3701 | Steps: 2 | Val loss: 0.2919 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3354 | Steps: 2 | Val loss: 0.2727 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5516 | Steps: 2 | Val loss: 0.3809 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=84699)[0m rmse: 0.18041859567165375
[2m[36m(func pid=84699)[0m mae:  0.1328868865966797
[2m[36m(func pid=84699)[0m rmse_per_class: [0.114, 0.264, 0.1, 0.33, 0.105, 0.19, 0.301, 0.143, 0.143, 0.114]
[2m[36m(func pid=84699)[0m 
== Status ==
Current time: 2024-01-07 05:30:32 (running for 00:29:38.59)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.37  |  0.162 |                   48 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.336 |  0.151 |                   24 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.648 |  0.18  |                   21 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.532 |  0.18  |                   20 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16243578493595123
[2m[36m(func pid=78481)[0m mae:  0.11676355451345444
[2m[36m(func pid=78481)[0m rmse_per_class: [0.106, 0.248, 0.054, 0.32, 0.067, 0.177, 0.264, 0.128, 0.145, 0.114]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15183070302009583
[2m[36m(func pid=83767)[0m mae:  0.10510573536157608
[2m[36m(func pid=83767)[0m rmse_per_class: [0.1, 0.22, 0.045, 0.284, 0.09, 0.169, 0.263, 0.107, 0.141, 0.099]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.18054789304733276
[2m[36m(func pid=85321)[0m mae:  0.12841899693012238
[2m[36m(func pid=85321)[0m rmse_per_class: [0.125, 0.269, 0.146, 0.357, 0.062, 0.186, 0.27, 0.142, 0.158, 0.091]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6203 | Steps: 2 | Val loss: 0.4970 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3705 | Steps: 2 | Val loss: 0.2923 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3394 | Steps: 2 | Val loss: 0.2741 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5606 | Steps: 2 | Val loss: 0.3925 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=84699)[0m rmse: 0.18036498129367828
[2m[36m(func pid=84699)[0m mae:  0.13283835351467133
[2m[36m(func pid=84699)[0m rmse_per_class: [0.114, 0.265, 0.1, 0.33, 0.105, 0.19, 0.301, 0.142, 0.144, 0.114]
[2m[36m(func pid=84699)[0m 
== Status ==
Current time: 2024-01-07 05:30:37 (running for 00:29:43.72)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.37  |  0.163 |                   49 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.335 |  0.152 |                   25 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.62  |  0.18  |                   22 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.552 |  0.181 |                   21 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16279500722885132
[2m[36m(func pid=78481)[0m mae:  0.1169595718383789
[2m[36m(func pid=78481)[0m rmse_per_class: [0.106, 0.248, 0.055, 0.321, 0.067, 0.177, 0.264, 0.128, 0.146, 0.115]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15318255126476288
[2m[36m(func pid=83767)[0m mae:  0.10570955276489258
[2m[36m(func pid=83767)[0m rmse_per_class: [0.102, 0.219, 0.043, 0.295, 0.1, 0.169, 0.256, 0.107, 0.146, 0.095]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.18108679354190826
[2m[36m(func pid=85321)[0m mae:  0.12827792763710022
[2m[36m(func pid=85321)[0m rmse_per_class: [0.126, 0.269, 0.147, 0.359, 0.06, 0.187, 0.271, 0.143, 0.159, 0.091]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.6087 | Steps: 2 | Val loss: 0.4813 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3679 | Steps: 2 | Val loss: 0.2923 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3314 | Steps: 2 | Val loss: 0.2759 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5762 | Steps: 2 | Val loss: 0.4027 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=84699)[0m rmse: 0.18027529120445251
[2m[36m(func pid=84699)[0m mae:  0.13276022672653198
[2m[36m(func pid=84699)[0m rmse_per_class: [0.114, 0.265, 0.101, 0.33, 0.104, 0.19, 0.3, 0.142, 0.144, 0.113]
[2m[36m(func pid=84699)[0m 
== Status ==
Current time: 2024-01-07 05:30:42 (running for 00:29:48.72)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.368 |  0.163 |                   50 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.339 |  0.153 |                   26 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.609 |  0.18  |                   23 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.561 |  0.181 |                   22 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.1627921164035797
[2m[36m(func pid=78481)[0m mae:  0.11696481704711914
[2m[36m(func pid=78481)[0m rmse_per_class: [0.106, 0.249, 0.055, 0.321, 0.066, 0.177, 0.264, 0.128, 0.147, 0.116]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.1815253347158432
[2m[36m(func pid=85321)[0m mae:  0.1280975490808487
[2m[36m(func pid=85321)[0m rmse_per_class: [0.127, 0.27, 0.147, 0.36, 0.058, 0.187, 0.273, 0.144, 0.159, 0.091]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15465989708900452
[2m[36m(func pid=83767)[0m mae:  0.10652683675289154
[2m[36m(func pid=83767)[0m rmse_per_class: [0.117, 0.217, 0.041, 0.304, 0.1, 0.168, 0.255, 0.102, 0.151, 0.093]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5883 | Steps: 2 | Val loss: 0.4665 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3675 | Steps: 2 | Val loss: 0.2920 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5850 | Steps: 2 | Val loss: 0.4133 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3318 | Steps: 2 | Val loss: 0.2769 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=84699)[0m rmse: 0.1801670342683792
[2m[36m(func pid=84699)[0m mae:  0.13267335295677185
[2m[36m(func pid=84699)[0m rmse_per_class: [0.114, 0.265, 0.102, 0.331, 0.103, 0.19, 0.299, 0.141, 0.144, 0.113]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1625802218914032
[2m[36m(func pid=78481)[0m mae:  0.11686152219772339
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.248, 0.054, 0.32, 0.067, 0.177, 0.263, 0.128, 0.147, 0.115]
[2m[36m(func pid=78481)[0m 
== Status ==
Current time: 2024-01-07 05:30:47 (running for 00:29:53.96)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.367 |  0.163 |                   51 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.331 |  0.155 |                   27 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.588 |  0.18  |                   24 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.576 |  0.182 |                   23 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.18222400546073914
[2m[36m(func pid=85321)[0m mae:  0.12807807326316833
[2m[36m(func pid=85321)[0m rmse_per_class: [0.127, 0.27, 0.148, 0.362, 0.057, 0.188, 0.276, 0.145, 0.159, 0.091]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15457388758659363
[2m[36m(func pid=83767)[0m mae:  0.10645608603954315
[2m[36m(func pid=83767)[0m rmse_per_class: [0.112, 0.216, 0.043, 0.31, 0.097, 0.167, 0.252, 0.102, 0.15, 0.096]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5745 | Steps: 2 | Val loss: 0.4526 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3732 | Steps: 2 | Val loss: 0.2917 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3262 | Steps: 2 | Val loss: 0.2699 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.6010 | Steps: 2 | Val loss: 0.4232 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 05:30:52 (running for 00:29:59.08)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.367 |  0.163 |                   51 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.332 |  0.155 |                   28 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.575 |  0.18  |                   25 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.585 |  0.182 |                   24 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=84699)[0m rmse: 0.18005375564098358
[2m[36m(func pid=84699)[0m mae:  0.13257227838039398
[2m[36m(func pid=84699)[0m rmse_per_class: [0.115, 0.265, 0.102, 0.331, 0.102, 0.19, 0.298, 0.141, 0.145, 0.112]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.16252659261226654
[2m[36m(func pid=78481)[0m mae:  0.11676665395498276
[2m[36m(func pid=78481)[0m rmse_per_class: [0.106, 0.248, 0.054, 0.32, 0.066, 0.177, 0.264, 0.128, 0.147, 0.115]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.1829078495502472
[2m[36m(func pid=85321)[0m mae:  0.12805494666099548
[2m[36m(func pid=85321)[0m rmse_per_class: [0.128, 0.271, 0.149, 0.363, 0.056, 0.188, 0.279, 0.146, 0.159, 0.091]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14984846115112305
[2m[36m(func pid=83767)[0m mae:  0.10250456631183624
[2m[36m(func pid=83767)[0m rmse_per_class: [0.097, 0.216, 0.048, 0.289, 0.09, 0.167, 0.245, 0.104, 0.142, 0.1]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5594 | Steps: 2 | Val loss: 0.4393 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3705 | Steps: 2 | Val loss: 0.2909 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.6338 | Steps: 2 | Val loss: 0.4360 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3388 | Steps: 2 | Val loss: 0.2695 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 05:30:58 (running for 00:30:04.59)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.37  |  0.162 |                   53 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.326 |  0.15  |                   29 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.575 |  0.18  |                   25 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.601 |  0.183 |                   25 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16223150491714478
[2m[36m(func pid=78481)[0m mae:  0.11640562117099762
[2m[36m(func pid=78481)[0m rmse_per_class: [0.107, 0.247, 0.055, 0.318, 0.067, 0.177, 0.264, 0.127, 0.146, 0.115]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17992174625396729
[2m[36m(func pid=84699)[0m mae:  0.13245490193367004
[2m[36m(func pid=84699)[0m rmse_per_class: [0.115, 0.265, 0.103, 0.331, 0.101, 0.19, 0.297, 0.14, 0.145, 0.112]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.18389654159545898
[2m[36m(func pid=85321)[0m mae:  0.12818177044391632
[2m[36m(func pid=85321)[0m rmse_per_class: [0.129, 0.271, 0.15, 0.365, 0.055, 0.189, 0.283, 0.147, 0.158, 0.092]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15013852715492249
[2m[36m(func pid=83767)[0m mae:  0.10267988592386246
[2m[36m(func pid=83767)[0m rmse_per_class: [0.105, 0.215, 0.048, 0.285, 0.083, 0.166, 0.248, 0.104, 0.139, 0.107]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3664 | Steps: 2 | Val loss: 0.2899 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.6165 | Steps: 2 | Val loss: 0.4431 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5430 | Steps: 2 | Val loss: 0.4274 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3291 | Steps: 2 | Val loss: 0.2700 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 05:31:03 (running for 00:30:09.85)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.366 |  0.162 |                   54 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.339 |  0.15  |                   30 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.559 |  0.18  |                   26 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.634 |  0.184 |                   26 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.1616152822971344
[2m[36m(func pid=78481)[0m mae:  0.11602935940027237
[2m[36m(func pid=78481)[0m rmse_per_class: [0.106, 0.247, 0.053, 0.315, 0.067, 0.177, 0.264, 0.127, 0.146, 0.114]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.18448969721794128
[2m[36m(func pid=85321)[0m mae:  0.12816688418388367
[2m[36m(func pid=85321)[0m rmse_per_class: [0.13, 0.271, 0.15, 0.366, 0.055, 0.189, 0.287, 0.147, 0.158, 0.092]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17985466122627258
[2m[36m(func pid=84699)[0m mae:  0.13238278031349182
[2m[36m(func pid=84699)[0m rmse_per_class: [0.115, 0.265, 0.104, 0.332, 0.101, 0.19, 0.296, 0.14, 0.145, 0.111]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15010017156600952
[2m[36m(func pid=83767)[0m mae:  0.10347585380077362
[2m[36m(func pid=83767)[0m rmse_per_class: [0.112, 0.215, 0.041, 0.284, 0.077, 0.166, 0.256, 0.101, 0.142, 0.108]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3657 | Steps: 2 | Val loss: 0.2893 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6260 | Steps: 2 | Val loss: 0.4507 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5315 | Steps: 2 | Val loss: 0.4163 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3295 | Steps: 2 | Val loss: 0.2723 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 05:31:08 (running for 00:30:15.12)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.366 |  0.161 |                   55 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.329 |  0.15  |                   31 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.543 |  0.18  |                   27 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.617 |  0.184 |                   27 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16123278439044952
[2m[36m(func pid=78481)[0m mae:  0.11579576879739761
[2m[36m(func pid=78481)[0m rmse_per_class: [0.106, 0.247, 0.052, 0.314, 0.067, 0.177, 0.265, 0.126, 0.146, 0.113]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.18524885177612305
[2m[36m(func pid=85321)[0m mae:  0.1282356232404709
[2m[36m(func pid=85321)[0m rmse_per_class: [0.13, 0.272, 0.151, 0.367, 0.055, 0.19, 0.29, 0.148, 0.158, 0.092]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15108229219913483
[2m[36m(func pid=83767)[0m mae:  0.10430505126714706
[2m[36m(func pid=83767)[0m rmse_per_class: [0.106, 0.215, 0.041, 0.299, 0.081, 0.166, 0.256, 0.099, 0.144, 0.104]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.179755300283432
[2m[36m(func pid=84699)[0m mae:  0.1322806030511856
[2m[36m(func pid=84699)[0m rmse_per_class: [0.115, 0.265, 0.105, 0.332, 0.1, 0.19, 0.295, 0.139, 0.146, 0.11]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3667 | Steps: 2 | Val loss: 0.2887 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.6357 | Steps: 2 | Val loss: 0.4573 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3234 | Steps: 2 | Val loss: 0.2792 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5181 | Steps: 2 | Val loss: 0.4061 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 05:31:14 (running for 00:30:20.33)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.367 |  0.161 |                   56 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.33  |  0.151 |                   32 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.532 |  0.18  |                   28 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.626 |  0.185 |                   28 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16082355380058289
[2m[36m(func pid=78481)[0m mae:  0.11547104269266129
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.247, 0.051, 0.313, 0.067, 0.177, 0.264, 0.126, 0.146, 0.112]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.18600872159004211
[2m[36m(func pid=85321)[0m mae:  0.12834583222866058
[2m[36m(func pid=85321)[0m rmse_per_class: [0.131, 0.272, 0.152, 0.368, 0.054, 0.191, 0.294, 0.149, 0.158, 0.092]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15469637513160706
[2m[36m(func pid=83767)[0m mae:  0.10677896440029144
[2m[36m(func pid=83767)[0m rmse_per_class: [0.103, 0.213, 0.046, 0.32, 0.086, 0.166, 0.254, 0.1, 0.161, 0.099]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.179681658744812
[2m[36m(func pid=84699)[0m mae:  0.13219627737998962
[2m[36m(func pid=84699)[0m rmse_per_class: [0.115, 0.265, 0.106, 0.333, 0.099, 0.19, 0.294, 0.139, 0.146, 0.109]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3643 | Steps: 2 | Val loss: 0.2887 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.6570 | Steps: 2 | Val loss: 0.4644 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3240 | Steps: 2 | Val loss: 0.2794 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5096 | Steps: 2 | Val loss: 0.3966 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 05:31:19 (running for 00:30:25.59)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.364 |  0.161 |                   57 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.323 |  0.155 |                   33 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.518 |  0.18  |                   29 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.636 |  0.186 |                   29 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16081438958644867
[2m[36m(func pid=78481)[0m mae:  0.11542171239852905
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.246, 0.052, 0.314, 0.067, 0.177, 0.263, 0.126, 0.146, 0.111]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.1868690550327301
[2m[36m(func pid=85321)[0m mae:  0.12850479781627655
[2m[36m(func pid=85321)[0m rmse_per_class: [0.132, 0.272, 0.152, 0.369, 0.054, 0.192, 0.298, 0.149, 0.157, 0.092]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1561133861541748
[2m[36m(func pid=83767)[0m mae:  0.10692526400089264
[2m[36m(func pid=83767)[0m rmse_per_class: [0.113, 0.214, 0.05, 0.314, 0.092, 0.166, 0.251, 0.099, 0.168, 0.094]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17957215011119843
[2m[36m(func pid=84699)[0m mae:  0.13207831978797913
[2m[36m(func pid=84699)[0m rmse_per_class: [0.116, 0.265, 0.107, 0.333, 0.098, 0.19, 0.293, 0.139, 0.146, 0.109]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3638 | Steps: 2 | Val loss: 0.2886 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.6434 | Steps: 2 | Val loss: 0.4681 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3225 | Steps: 2 | Val loss: 0.2699 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5013 | Steps: 2 | Val loss: 0.3878 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=78481)[0m rmse: 0.16083315014839172
[2m[36m(func pid=78481)[0m mae:  0.11534176021814346
[2m[36m(func pid=78481)[0m rmse_per_class: [0.104, 0.246, 0.053, 0.315, 0.068, 0.177, 0.262, 0.126, 0.147, 0.111]
== Status ==
Current time: 2024-01-07 05:31:24 (running for 00:30:30.86)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.364 |  0.161 |                   58 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.324 |  0.156 |                   34 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.51  |  0.18  |                   30 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.657 |  0.187 |                   30 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.18750643730163574
[2m[36m(func pid=85321)[0m mae:  0.12857361137866974
[2m[36m(func pid=85321)[0m rmse_per_class: [0.133, 0.272, 0.152, 0.37, 0.054, 0.192, 0.302, 0.15, 0.157, 0.093]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15107443928718567
[2m[36m(func pid=83767)[0m mae:  0.1023811548948288
[2m[36m(func pid=83767)[0m rmse_per_class: [0.123, 0.215, 0.044, 0.29, 0.092, 0.167, 0.243, 0.099, 0.145, 0.093]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.1794600784778595
[2m[36m(func pid=84699)[0m mae:  0.1319587677717209
[2m[36m(func pid=84699)[0m rmse_per_class: [0.116, 0.265, 0.108, 0.334, 0.097, 0.19, 0.292, 0.138, 0.147, 0.108]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3655 | Steps: 2 | Val loss: 0.2887 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.6472 | Steps: 2 | Val loss: 0.4704 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3328 | Steps: 2 | Val loss: 0.2640 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=78481)[0m rmse: 0.16089023649692535
[2m[36m(func pid=78481)[0m mae:  0.11529804766178131
[2m[36m(func pid=78481)[0m rmse_per_class: [0.103, 0.246, 0.055, 0.316, 0.068, 0.176, 0.262, 0.126, 0.147, 0.111]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4764 | Steps: 2 | Val loss: 0.3810 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 05:31:29 (running for 00:30:35.98)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.365 |  0.161 |                   59 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.322 |  0.151 |                   35 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.501 |  0.179 |                   31 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.643 |  0.188 |                   31 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.1880854368209839
[2m[36m(func pid=85321)[0m mae:  0.1286509782075882
[2m[36m(func pid=85321)[0m rmse_per_class: [0.134, 0.272, 0.152, 0.37, 0.054, 0.193, 0.305, 0.15, 0.157, 0.093]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14676162600517273
[2m[36m(func pid=83767)[0m mae:  0.0991712361574173
[2m[36m(func pid=83767)[0m rmse_per_class: [0.117, 0.213, 0.041, 0.275, 0.083, 0.167, 0.24, 0.098, 0.137, 0.095]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17946235835552216
[2m[36m(func pid=84699)[0m mae:  0.13190582394599915
[2m[36m(func pid=84699)[0m rmse_per_class: [0.116, 0.265, 0.109, 0.334, 0.097, 0.19, 0.291, 0.138, 0.147, 0.107]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3690 | Steps: 2 | Val loss: 0.2889 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.6478 | Steps: 2 | Val loss: 0.4709 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3193 | Steps: 2 | Val loss: 0.2673 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 05:31:35 (running for 00:30:41.15)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.369 |  0.161 |                   60 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.333 |  0.147 |                   36 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.476 |  0.179 |                   32 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.647 |  0.188 |                   32 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16096247732639313
[2m[36m(func pid=78481)[0m mae:  0.11530305445194244
[2m[36m(func pid=78481)[0m rmse_per_class: [0.104, 0.246, 0.055, 0.317, 0.068, 0.176, 0.261, 0.125, 0.148, 0.11]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.1885421723127365
[2m[36m(func pid=85321)[0m mae:  0.12865863740444183
[2m[36m(func pid=85321)[0m rmse_per_class: [0.135, 0.272, 0.15, 0.371, 0.055, 0.194, 0.309, 0.151, 0.156, 0.093]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4834 | Steps: 2 | Val loss: 0.3740 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=83767)[0m rmse: 0.148319810628891
[2m[36m(func pid=83767)[0m mae:  0.10166367143392563
[2m[36m(func pid=83767)[0m rmse_per_class: [0.114, 0.21, 0.04, 0.28, 0.076, 0.167, 0.252, 0.098, 0.148, 0.097]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3635 | Steps: 2 | Val loss: 0.2892 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.6512 | Steps: 2 | Val loss: 0.4721 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=84699)[0m rmse: 0.17939606308937073
[2m[36m(func pid=84699)[0m mae:  0.13180814683437347
[2m[36m(func pid=84699)[0m rmse_per_class: [0.116, 0.265, 0.11, 0.335, 0.096, 0.19, 0.29, 0.137, 0.147, 0.106]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3240 | Steps: 2 | Val loss: 0.2771 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=78481)[0m rmse: 0.161010280251503
[2m[36m(func pid=78481)[0m mae:  0.11538638919591904
[2m[36m(func pid=78481)[0m rmse_per_class: [0.103, 0.245, 0.054, 0.319, 0.068, 0.176, 0.26, 0.125, 0.15, 0.109]
[2m[36m(func pid=78481)[0m 
== Status ==
Current time: 2024-01-07 05:31:40 (running for 00:30:46.49)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.364 |  0.161 |                   61 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.319 |  0.148 |                   37 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.483 |  0.179 |                   33 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.648 |  0.189 |                   33 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.1891794502735138
[2m[36m(func pid=85321)[0m mae:  0.12876774370670319
[2m[36m(func pid=85321)[0m rmse_per_class: [0.137, 0.273, 0.15, 0.371, 0.055, 0.194, 0.312, 0.151, 0.155, 0.093]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15361960232257843
[2m[36m(func pid=83767)[0m mae:  0.1064872294664383
[2m[36m(func pid=83767)[0m rmse_per_class: [0.107, 0.211, 0.041, 0.305, 0.073, 0.165, 0.263, 0.097, 0.174, 0.1]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4765 | Steps: 2 | Val loss: 0.3675 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3627 | Steps: 2 | Val loss: 0.2895 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.6481 | Steps: 2 | Val loss: 0.4726 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3273 | Steps: 2 | Val loss: 0.2739 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=84699)[0m rmse: 0.17931023240089417
[2m[36m(func pid=84699)[0m mae:  0.13169313967227936
[2m[36m(func pid=84699)[0m rmse_per_class: [0.117, 0.265, 0.111, 0.335, 0.095, 0.189, 0.289, 0.137, 0.148, 0.106]
[2m[36m(func pid=84699)[0m 
== Status ==
Current time: 2024-01-07 05:31:45 (running for 00:30:51.64)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.363 |  0.161 |                   62 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.324 |  0.154 |                   38 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.477 |  0.179 |                   34 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.651 |  0.189 |                   34 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.1610826551914215
[2m[36m(func pid=78481)[0m mae:  0.11547832190990448
[2m[36m(func pid=78481)[0m rmse_per_class: [0.104, 0.245, 0.053, 0.32, 0.068, 0.176, 0.26, 0.125, 0.151, 0.109]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.18983015418052673
[2m[36m(func pid=85321)[0m mae:  0.1289326250553131
[2m[36m(func pid=85321)[0m rmse_per_class: [0.138, 0.273, 0.151, 0.372, 0.055, 0.195, 0.314, 0.151, 0.155, 0.094]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15163654088974
[2m[36m(func pid=83767)[0m mae:  0.10371287912130356
[2m[36m(func pid=83767)[0m rmse_per_class: [0.11, 0.211, 0.05, 0.306, 0.076, 0.164, 0.254, 0.097, 0.15, 0.099]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4652 | Steps: 2 | Val loss: 0.3615 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3630 | Steps: 2 | Val loss: 0.2898 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6405 | Steps: 2 | Val loss: 0.4721 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3224 | Steps: 2 | Val loss: 0.2648 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=84699)[0m rmse: 0.17922940850257874
[2m[36m(func pid=84699)[0m mae:  0.13157251477241516
[2m[36m(func pid=84699)[0m rmse_per_class: [0.117, 0.265, 0.112, 0.336, 0.094, 0.189, 0.288, 0.137, 0.148, 0.105]
[2m[36m(func pid=84699)[0m 
== Status ==
Current time: 2024-01-07 05:31:50 (running for 00:30:56.87)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.363 |  0.161 |                   63 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.327 |  0.152 |                   39 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.465 |  0.179 |                   35 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.648 |  0.19  |                   35 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.16119061410427094
[2m[36m(func pid=78481)[0m mae:  0.11556506156921387
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.245, 0.052, 0.321, 0.068, 0.176, 0.259, 0.125, 0.151, 0.109]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.190437451004982
[2m[36m(func pid=85321)[0m mae:  0.12908615171909332
[2m[36m(func pid=85321)[0m rmse_per_class: [0.14, 0.273, 0.152, 0.373, 0.055, 0.196, 0.316, 0.152, 0.155, 0.094]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14720000326633453
[2m[36m(func pid=83767)[0m mae:  0.09881134331226349
[2m[36m(func pid=83767)[0m rmse_per_class: [0.12, 0.211, 0.047, 0.282, 0.08, 0.163, 0.242, 0.097, 0.136, 0.093]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4686 | Steps: 2 | Val loss: 0.3568 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3640 | Steps: 2 | Val loss: 0.2899 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6584 | Steps: 2 | Val loss: 0.4718 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3168 | Steps: 2 | Val loss: 0.2638 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=84699)[0m rmse: 0.17918017506599426
[2m[36m(func pid=84699)[0m mae:  0.1314697563648224
[2m[36m(func pid=84699)[0m rmse_per_class: [0.117, 0.265, 0.114, 0.336, 0.093, 0.189, 0.287, 0.137, 0.149, 0.104]
[2m[36m(func pid=84699)[0m 
== Status ==
Current time: 2024-01-07 05:31:55 (running for 00:31:02.06)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.363 |  0.161 |                   63 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.322 |  0.147 |                   40 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.469 |  0.179 |                   36 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.658 |  0.191 |                   37 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.1911098062992096
[2m[36m(func pid=85321)[0m mae:  0.12926682829856873
[2m[36m(func pid=85321)[0m rmse_per_class: [0.142, 0.273, 0.153, 0.373, 0.055, 0.196, 0.318, 0.152, 0.154, 0.094]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1612718254327774
[2m[36m(func pid=78481)[0m mae:  0.11562956869602203
[2m[36m(func pid=78481)[0m rmse_per_class: [0.106, 0.245, 0.052, 0.321, 0.068, 0.176, 0.26, 0.125, 0.151, 0.109]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14736486971378326
[2m[36m(func pid=83767)[0m mae:  0.09856466948986053
[2m[36m(func pid=83767)[0m rmse_per_class: [0.129, 0.215, 0.041, 0.274, 0.085, 0.164, 0.241, 0.098, 0.136, 0.093]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4615 | Steps: 2 | Val loss: 0.3527 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3663 | Steps: 2 | Val loss: 0.2898 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6995 | Steps: 2 | Val loss: 0.4732 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3274 | Steps: 2 | Val loss: 0.2675 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
== Status ==
Current time: 2024-01-07 05:32:01 (running for 00:31:07.15)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.364 |  0.161 |                   64 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.317 |  0.147 |                   41 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.462 |  0.179 |                   37 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.658 |  0.191 |                   37 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=84699)[0m rmse: 0.17911818623542786
[2m[36m(func pid=84699)[0m mae:  0.13136796653270721
[2m[36m(func pid=84699)[0m rmse_per_class: [0.118, 0.265, 0.115, 0.337, 0.092, 0.189, 0.286, 0.136, 0.149, 0.103]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.19207681715488434
[2m[36m(func pid=85321)[0m mae:  0.12955737113952637
[2m[36m(func pid=85321)[0m rmse_per_class: [0.145, 0.273, 0.157, 0.374, 0.055, 0.197, 0.319, 0.152, 0.154, 0.094]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.161163792014122
[2m[36m(func pid=78481)[0m mae:  0.11551077663898468
[2m[36m(func pid=78481)[0m rmse_per_class: [0.107, 0.244, 0.051, 0.321, 0.069, 0.176, 0.259, 0.124, 0.15, 0.11]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14984336495399475
[2m[36m(func pid=83767)[0m mae:  0.10148817300796509
[2m[36m(func pid=83767)[0m rmse_per_class: [0.12, 0.213, 0.041, 0.28, 0.09, 0.165, 0.249, 0.097, 0.146, 0.097]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6412 | Steps: 2 | Val loss: 0.4681 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3650 | Steps: 2 | Val loss: 0.2895 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4585 | Steps: 2 | Val loss: 0.3489 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3178 | Steps: 2 | Val loss: 0.2739 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 05:32:06 (running for 00:31:12.50)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.366 |  0.161 |                   65 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.327 |  0.15  |                   42 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.462 |  0.179 |                   37 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.7   |  0.192 |                   38 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.1610986441373825
[2m[36m(func pid=78481)[0m mae:  0.1154068261384964
[2m[36m(func pid=78481)[0m rmse_per_class: [0.108, 0.244, 0.052, 0.321, 0.069, 0.176, 0.26, 0.124, 0.148, 0.11]
[2m[36m(func pid=85321)[0m rmse: 0.19225113093852997
[2m[36m(func pid=85321)[0m mae:  0.129531130194664
[2m[36m(func pid=85321)[0m rmse_per_class: [0.147, 0.273, 0.155, 0.374, 0.055, 0.197, 0.32, 0.153, 0.154, 0.094]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1530674546957016
[2m[36m(func pid=83767)[0m mae:  0.10476116091012955
[2m[36m(func pid=83767)[0m rmse_per_class: [0.106, 0.21, 0.047, 0.295, 0.091, 0.164, 0.259, 0.096, 0.163, 0.1]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.1790633499622345
[2m[36m(func pid=84699)[0m mae:  0.13126814365386963
[2m[36m(func pid=84699)[0m rmse_per_class: [0.118, 0.265, 0.116, 0.337, 0.091, 0.189, 0.285, 0.136, 0.149, 0.103]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6288 | Steps: 2 | Val loss: 0.4647 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3681 | Steps: 2 | Val loss: 0.2891 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3160 | Steps: 2 | Val loss: 0.2746 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4550 | Steps: 2 | Val loss: 0.3459 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 05:32:11 (running for 00:31:17.63)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.365 |  0.161 |                   66 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.318 |  0.153 |                   43 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.459 |  0.179 |                   38 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.629 |  0.193 |                   40 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.19274193048477173
[2m[36m(func pid=85321)[0m mae:  0.12969167530536652
[2m[36m(func pid=85321)[0m rmse_per_class: [0.15, 0.272, 0.156, 0.374, 0.056, 0.198, 0.321, 0.153, 0.153, 0.095]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.16092006862163544
[2m[36m(func pid=78481)[0m mae:  0.11519169807434082
[2m[36m(func pid=78481)[0m rmse_per_class: [0.108, 0.243, 0.053, 0.32, 0.068, 0.176, 0.26, 0.124, 0.146, 0.111]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1527015119791031
[2m[36m(func pid=83767)[0m mae:  0.10418234020471573
[2m[36m(func pid=83767)[0m rmse_per_class: [0.105, 0.212, 0.051, 0.303, 0.089, 0.164, 0.256, 0.096, 0.152, 0.099]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17902988195419312
[2m[36m(func pid=84699)[0m mae:  0.1311810165643692
[2m[36m(func pid=84699)[0m rmse_per_class: [0.118, 0.265, 0.117, 0.338, 0.09, 0.189, 0.284, 0.136, 0.15, 0.102]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6240 | Steps: 2 | Val loss: 0.4601 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3658 | Steps: 2 | Val loss: 0.2882 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3165 | Steps: 2 | Val loss: 0.2710 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4530 | Steps: 2 | Val loss: 0.3432 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 05:32:16 (running for 00:31:22.64)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.368 |  0.161 |                   67 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.316 |  0.153 |                   44 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.455 |  0.179 |                   39 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.624 |  0.193 |                   41 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.19308538734912872
[2m[36m(func pid=85321)[0m mae:  0.12982895970344543
[2m[36m(func pid=85321)[0m rmse_per_class: [0.153, 0.272, 0.156, 0.374, 0.056, 0.199, 0.32, 0.153, 0.154, 0.095]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1508026421070099
[2m[36m(func pid=83767)[0m mae:  0.10227743536233902
[2m[36m(func pid=83767)[0m rmse_per_class: [0.112, 0.209, 0.046, 0.301, 0.09, 0.163, 0.247, 0.096, 0.146, 0.098]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.16040530800819397
[2m[36m(func pid=78481)[0m mae:  0.11485709249973297
[2m[36m(func pid=78481)[0m rmse_per_class: [0.107, 0.242, 0.052, 0.318, 0.068, 0.176, 0.261, 0.123, 0.144, 0.112]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.1789887696504593
[2m[36m(func pid=84699)[0m mae:  0.13109207153320312
[2m[36m(func pid=84699)[0m rmse_per_class: [0.119, 0.265, 0.118, 0.338, 0.089, 0.189, 0.283, 0.136, 0.15, 0.102]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6147 | Steps: 2 | Val loss: 0.4556 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3584 | Steps: 2 | Val loss: 0.2875 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3335 | Steps: 2 | Val loss: 0.2705 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4489 | Steps: 2 | Val loss: 0.3408 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 05:32:21 (running for 00:31:27.89)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.366 |  0.16  |                   68 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.317 |  0.151 |                   45 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.453 |  0.179 |                   40 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.615 |  0.194 |                   42 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.19351842999458313
[2m[36m(func pid=85321)[0m mae:  0.1300361454486847
[2m[36m(func pid=85321)[0m rmse_per_class: [0.157, 0.272, 0.157, 0.375, 0.056, 0.199, 0.319, 0.153, 0.153, 0.095]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1600288450717926
[2m[36m(func pid=78481)[0m mae:  0.1146577000617981
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.242, 0.051, 0.317, 0.068, 0.176, 0.262, 0.123, 0.144, 0.113]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1517002284526825
[2m[36m(func pid=83767)[0m mae:  0.1025005355477333
[2m[36m(func pid=83767)[0m rmse_per_class: [0.131, 0.21, 0.041, 0.295, 0.089, 0.163, 0.246, 0.096, 0.151, 0.095]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17896036803722382
[2m[36m(func pid=84699)[0m mae:  0.13100749254226685
[2m[36m(func pid=84699)[0m rmse_per_class: [0.119, 0.266, 0.119, 0.339, 0.089, 0.189, 0.282, 0.136, 0.151, 0.101]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6058 | Steps: 2 | Val loss: 0.4503 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3628 | Steps: 2 | Val loss: 0.2865 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3113 | Steps: 2 | Val loss: 0.2680 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4492 | Steps: 2 | Val loss: 0.3387 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 05:32:26 (running for 00:31:32.97)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.358 |  0.16  |                   69 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.334 |  0.152 |                   46 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.449 |  0.179 |                   41 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.606 |  0.194 |                   43 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.19382166862487793
[2m[36m(func pid=85321)[0m mae:  0.13018551468849182
[2m[36m(func pid=85321)[0m rmse_per_class: [0.161, 0.271, 0.158, 0.375, 0.056, 0.199, 0.317, 0.153, 0.154, 0.095]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1595408022403717
[2m[36m(func pid=78481)[0m mae:  0.11426752805709839
[2m[36m(func pid=78481)[0m rmse_per_class: [0.103, 0.241, 0.052, 0.314, 0.067, 0.176, 0.262, 0.123, 0.144, 0.113]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14988966286182404
[2m[36m(func pid=83767)[0m mae:  0.10117604583501816
[2m[36m(func pid=83767)[0m rmse_per_class: [0.124, 0.212, 0.045, 0.281, 0.087, 0.162, 0.25, 0.096, 0.149, 0.093]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.1789083629846573
[2m[36m(func pid=84699)[0m mae:  0.13089707493782043
[2m[36m(func pid=84699)[0m rmse_per_class: [0.119, 0.266, 0.12, 0.34, 0.087, 0.189, 0.281, 0.136, 0.151, 0.101]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5965 | Steps: 2 | Val loss: 0.4446 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3656 | Steps: 2 | Val loss: 0.2856 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3083 | Steps: 2 | Val loss: 0.2662 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 05:32:32 (running for 00:31:38.19)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.363 |  0.16  |                   70 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.311 |  0.15  |                   47 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.449 |  0.179 |                   42 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.597 |  0.194 |                   44 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4481 | Steps: 2 | Val loss: 0.3372 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=85321)[0m rmse: 0.19404521584510803
[2m[36m(func pid=85321)[0m mae:  0.13034266233444214
[2m[36m(func pid=85321)[0m rmse_per_class: [0.165, 0.271, 0.158, 0.375, 0.056, 0.2, 0.314, 0.154, 0.154, 0.095]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15901213884353638
[2m[36m(func pid=78481)[0m mae:  0.11387643963098526
[2m[36m(func pid=78481)[0m rmse_per_class: [0.101, 0.241, 0.052, 0.312, 0.067, 0.176, 0.262, 0.123, 0.144, 0.113]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1476878970861435
[2m[36m(func pid=83767)[0m mae:  0.09967629611492157
[2m[36m(func pid=83767)[0m rmse_per_class: [0.109, 0.209, 0.05, 0.275, 0.084, 0.161, 0.254, 0.095, 0.147, 0.092]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17889031767845154
[2m[36m(func pid=84699)[0m mae:  0.13081321120262146
[2m[36m(func pid=84699)[0m rmse_per_class: [0.119, 0.266, 0.121, 0.34, 0.086, 0.189, 0.28, 0.136, 0.151, 0.1]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6024 | Steps: 2 | Val loss: 0.4409 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3620 | Steps: 2 | Val loss: 0.2853 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3124 | Steps: 2 | Val loss: 0.2681 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 05:32:37 (running for 00:31:43.20)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.366 |  0.159 |                   71 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.308 |  0.148 |                   48 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.448 |  0.179 |                   43 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.602 |  0.195 |                   45 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.1945747435092926
[2m[36m(func pid=85321)[0m mae:  0.13068681955337524
[2m[36m(func pid=85321)[0m rmse_per_class: [0.171, 0.27, 0.16, 0.375, 0.056, 0.2, 0.311, 0.154, 0.154, 0.095]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15889200568199158
[2m[36m(func pid=78481)[0m mae:  0.11376818269491196
[2m[36m(func pid=78481)[0m rmse_per_class: [0.101, 0.241, 0.052, 0.312, 0.067, 0.176, 0.262, 0.122, 0.144, 0.113]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4466 | Steps: 2 | Val loss: 0.3362 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=83767)[0m rmse: 0.14849473536014557
[2m[36m(func pid=83767)[0m mae:  0.10066473484039307
[2m[36m(func pid=83767)[0m rmse_per_class: [0.112, 0.207, 0.046, 0.284, 0.081, 0.161, 0.255, 0.095, 0.152, 0.093]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5921 | Steps: 2 | Val loss: 0.4326 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=84699)[0m rmse: 0.17887428402900696
[2m[36m(func pid=84699)[0m mae:  0.1307213455438614
[2m[36m(func pid=84699)[0m rmse_per_class: [0.12, 0.265, 0.122, 0.341, 0.086, 0.189, 0.279, 0.136, 0.151, 0.1]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3648 | Steps: 2 | Val loss: 0.2856 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3132 | Steps: 2 | Val loss: 0.2681 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=85321)[0m rmse: 0.19409485161304474
[2m[36m(func pid=85321)[0m mae:  0.1304929554462433
[2m[36m(func pid=85321)[0m rmse_per_class: [0.175, 0.269, 0.154, 0.375, 0.056, 0.2, 0.308, 0.154, 0.155, 0.095]
[2m[36m(func pid=85321)[0m 
== Status ==
Current time: 2024-01-07 05:32:42 (running for 00:31:48.36)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.362 |  0.159 |                   72 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.312 |  0.148 |                   49 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.447 |  0.179 |                   44 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.592 |  0.194 |                   46 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.15917645394802094
[2m[36m(func pid=78481)[0m mae:  0.11387433111667633
[2m[36m(func pid=78481)[0m rmse_per_class: [0.102, 0.241, 0.054, 0.312, 0.066, 0.176, 0.262, 0.122, 0.145, 0.112]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1487136334180832
[2m[36m(func pid=83767)[0m mae:  0.10059299319982529
[2m[36m(func pid=83767)[0m rmse_per_class: [0.123, 0.205, 0.041, 0.294, 0.078, 0.163, 0.244, 0.096, 0.151, 0.092]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4445 | Steps: 2 | Val loss: 0.3350 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5697 | Steps: 2 | Val loss: 0.4263 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3608 | Steps: 2 | Val loss: 0.2858 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3147 | Steps: 2 | Val loss: 0.2663 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=84699)[0m rmse: 0.1788436621427536
[2m[36m(func pid=84699)[0m mae:  0.1306348592042923
[2m[36m(func pid=84699)[0m rmse_per_class: [0.12, 0.266, 0.123, 0.341, 0.085, 0.189, 0.279, 0.136, 0.152, 0.099]
[2m[36m(func pid=84699)[0m 
== Status ==
Current time: 2024-01-07 05:32:47 (running for 00:31:53.40)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.365 |  0.159 |                   73 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.313 |  0.149 |                   50 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.445 |  0.179 |                   45 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.57  |  0.194 |                   47 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.19394956529140472
[2m[36m(func pid=85321)[0m mae:  0.1305670142173767
[2m[36m(func pid=85321)[0m rmse_per_class: [0.181, 0.268, 0.152, 0.375, 0.056, 0.2, 0.304, 0.154, 0.155, 0.095]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1593482941389084
[2m[36m(func pid=78481)[0m mae:  0.11392414569854736
[2m[36m(func pid=78481)[0m rmse_per_class: [0.103, 0.242, 0.055, 0.312, 0.066, 0.176, 0.261, 0.122, 0.146, 0.111]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1477433741092682
[2m[36m(func pid=83767)[0m mae:  0.09937784075737
[2m[36m(func pid=83767)[0m rmse_per_class: [0.126, 0.206, 0.041, 0.295, 0.078, 0.164, 0.234, 0.096, 0.145, 0.094]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4471 | Steps: 2 | Val loss: 0.3344 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5669 | Steps: 2 | Val loss: 0.4213 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3599 | Steps: 2 | Val loss: 0.2861 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3203 | Steps: 2 | Val loss: 0.2667 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=84699)[0m rmse: 0.17882905900478363
[2m[36m(func pid=84699)[0m mae:  0.13054858148097992
[2m[36m(func pid=84699)[0m rmse_per_class: [0.12, 0.266, 0.124, 0.342, 0.084, 0.189, 0.278, 0.136, 0.152, 0.099]
[2m[36m(func pid=84699)[0m 
== Status ==
Current time: 2024-01-07 05:32:52 (running for 00:31:58.55)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.361 |  0.159 |                   74 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.315 |  0.148 |                   51 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.447 |  0.179 |                   46 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.567 |  0.194 |                   48 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.1939365267753601
[2m[36m(func pid=85321)[0m mae:  0.13077011704444885
[2m[36m(func pid=85321)[0m rmse_per_class: [0.187, 0.268, 0.15, 0.375, 0.056, 0.2, 0.299, 0.154, 0.156, 0.096]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15951935946941376
[2m[36m(func pid=78481)[0m mae:  0.11400099843740463
[2m[36m(func pid=78481)[0m rmse_per_class: [0.103, 0.242, 0.054, 0.314, 0.066, 0.175, 0.26, 0.122, 0.146, 0.111]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14803117513656616
[2m[36m(func pid=83767)[0m mae:  0.09990085661411285
[2m[36m(func pid=83767)[0m rmse_per_class: [0.109, 0.206, 0.044, 0.293, 0.087, 0.162, 0.241, 0.095, 0.147, 0.095]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4488 | Steps: 2 | Val loss: 0.3341 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5591 | Steps: 2 | Val loss: 0.4159 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3623 | Steps: 2 | Val loss: 0.2865 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3077 | Steps: 2 | Val loss: 0.2699 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=85321)[0m rmse: 0.1936352401971817
[2m[36m(func pid=85321)[0m mae:  0.1308300495147705
[2m[36m(func pid=85321)[0m rmse_per_class: [0.194, 0.266, 0.146, 0.374, 0.056, 0.199, 0.294, 0.154, 0.157, 0.096]
[2m[36m(func pid=85321)[0m 
== Status ==
Current time: 2024-01-07 05:32:57 (running for 00:32:03.64)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.36  |  0.16  |                   75 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.32  |  0.148 |                   52 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.449 |  0.179 |                   47 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.559 |  0.194 |                   49 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=84699)[0m rmse: 0.17883257567882538
[2m[36m(func pid=84699)[0m mae:  0.13047704100608826
[2m[36m(func pid=84699)[0m rmse_per_class: [0.121, 0.266, 0.125, 0.342, 0.083, 0.189, 0.277, 0.136, 0.152, 0.098]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.159643292427063
[2m[36m(func pid=78481)[0m mae:  0.11408048868179321
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.242, 0.053, 0.315, 0.067, 0.175, 0.259, 0.122, 0.147, 0.111]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15054678916931152
[2m[36m(func pid=83767)[0m mae:  0.10110874474048615
[2m[36m(func pid=83767)[0m rmse_per_class: [0.102, 0.207, 0.06, 0.292, 0.095, 0.161, 0.252, 0.095, 0.146, 0.096]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5545 | Steps: 2 | Val loss: 0.4105 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4451 | Steps: 2 | Val loss: 0.3341 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3654 | Steps: 2 | Val loss: 0.2870 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3194 | Steps: 2 | Val loss: 0.2733 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 05:33:02 (running for 00:32:08.69)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.362 |  0.16  |                   76 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.308 |  0.151 |                   53 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.449 |  0.179 |                   47 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.555 |  0.193 |                   50 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.1931595504283905
[2m[36m(func pid=85321)[0m mae:  0.1308182179927826
[2m[36m(func pid=85321)[0m rmse_per_class: [0.2, 0.265, 0.141, 0.374, 0.056, 0.199, 0.289, 0.154, 0.158, 0.096]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.1788407266139984
[2m[36m(func pid=84699)[0m mae:  0.13040073215961456
[2m[36m(func pid=84699)[0m rmse_per_class: [0.121, 0.265, 0.126, 0.343, 0.082, 0.188, 0.276, 0.136, 0.153, 0.098]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15994985401630402
[2m[36m(func pid=78481)[0m mae:  0.11425286531448364
[2m[36m(func pid=78481)[0m rmse_per_class: [0.107, 0.242, 0.053, 0.317, 0.067, 0.175, 0.259, 0.122, 0.148, 0.11]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1526639461517334
[2m[36m(func pid=83767)[0m mae:  0.10259386152029037
[2m[36m(func pid=83767)[0m rmse_per_class: [0.116, 0.207, 0.048, 0.3, 0.101, 0.161, 0.256, 0.098, 0.148, 0.093]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5469 | Steps: 2 | Val loss: 0.4049 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4492 | Steps: 2 | Val loss: 0.3340 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3577 | Steps: 2 | Val loss: 0.2871 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3188 | Steps: 2 | Val loss: 0.2735 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 05:33:07 (running for 00:32:13.92)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.365 |  0.16  |                   77 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.319 |  0.153 |                   54 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.445 |  0.179 |                   48 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.547 |  0.192 |                   51 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.19248303771018982
[2m[36m(func pid=85321)[0m mae:  0.13071367144584656
[2m[36m(func pid=85321)[0m rmse_per_class: [0.205, 0.263, 0.135, 0.373, 0.056, 0.198, 0.284, 0.154, 0.16, 0.096]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17883461713790894
[2m[36m(func pid=84699)[0m mae:  0.1303209364414215
[2m[36m(func pid=84699)[0m rmse_per_class: [0.121, 0.266, 0.127, 0.343, 0.081, 0.188, 0.276, 0.136, 0.153, 0.098]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15990598499774933
[2m[36m(func pid=78481)[0m mae:  0.11426585912704468
[2m[36m(func pid=78481)[0m rmse_per_class: [0.109, 0.241, 0.051, 0.318, 0.067, 0.175, 0.259, 0.122, 0.148, 0.109]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15284375846385956
[2m[36m(func pid=83767)[0m mae:  0.10246136039495468
[2m[36m(func pid=83767)[0m rmse_per_class: [0.131, 0.208, 0.04, 0.302, 0.098, 0.161, 0.256, 0.097, 0.144, 0.092]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6045 | Steps: 2 | Val loss: 0.4029 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3598 | Steps: 2 | Val loss: 0.2868 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4488 | Steps: 2 | Val loss: 0.3341 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3240 | Steps: 2 | Val loss: 0.2692 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 05:33:13 (running for 00:32:19.30)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.358 |  0.16  |                   78 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.319 |  0.153 |                   55 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.449 |  0.179 |                   49 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.605 |  0.192 |                   52 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.19238856434822083
[2m[36m(func pid=85321)[0m mae:  0.13090582191944122
[2m[36m(func pid=85321)[0m rmse_per_class: [0.212, 0.262, 0.134, 0.373, 0.056, 0.198, 0.278, 0.154, 0.161, 0.096]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15972352027893066
[2m[36m(func pid=78481)[0m mae:  0.11414697021245956
[2m[36m(func pid=78481)[0m rmse_per_class: [0.111, 0.241, 0.05, 0.318, 0.067, 0.175, 0.259, 0.121, 0.147, 0.108]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14984139800071716
[2m[36m(func pid=83767)[0m mae:  0.10065386444330215
[2m[36m(func pid=83767)[0m rmse_per_class: [0.117, 0.209, 0.04, 0.29, 0.098, 0.162, 0.25, 0.095, 0.142, 0.095]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17883770167827606
[2m[36m(func pid=84699)[0m mae:  0.1302574872970581
[2m[36m(func pid=84699)[0m rmse_per_class: [0.122, 0.266, 0.128, 0.343, 0.08, 0.188, 0.275, 0.136, 0.153, 0.097]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5526 | Steps: 2 | Val loss: 0.3984 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3588 | Steps: 2 | Val loss: 0.2863 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3099 | Steps: 2 | Val loss: 0.2661 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4486 | Steps: 2 | Val loss: 0.3344 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 05:33:18 (running for 00:32:24.60)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.36  |  0.16  |                   79 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.324 |  0.15  |                   56 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.449 |  0.179 |                   50 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.553 |  0.192 |                   53 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.19161133468151093
[2m[36m(func pid=85321)[0m mae:  0.13071371614933014
[2m[36m(func pid=85321)[0m rmse_per_class: [0.215, 0.26, 0.129, 0.372, 0.056, 0.197, 0.273, 0.154, 0.164, 0.096]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15935111045837402
[2m[36m(func pid=78481)[0m mae:  0.11385677754878998
[2m[36m(func pid=78481)[0m rmse_per_class: [0.111, 0.24, 0.048, 0.317, 0.067, 0.175, 0.259, 0.121, 0.147, 0.108]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14809423685073853
[2m[36m(func pid=83767)[0m mae:  0.09929554164409637
[2m[36m(func pid=83767)[0m rmse_per_class: [0.113, 0.206, 0.046, 0.281, 0.093, 0.164, 0.243, 0.096, 0.143, 0.097]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17883244156837463
[2m[36m(func pid=84699)[0m mae:  0.1301824152469635
[2m[36m(func pid=84699)[0m rmse_per_class: [0.122, 0.266, 0.128, 0.344, 0.079, 0.188, 0.275, 0.136, 0.154, 0.097]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5358 | Steps: 2 | Val loss: 0.3915 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3554 | Steps: 2 | Val loss: 0.2858 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3099 | Steps: 2 | Val loss: 0.2682 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4491 | Steps: 2 | Val loss: 0.3348 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 05:33:23 (running for 00:32:29.86)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.359 |  0.159 |                   80 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.31  |  0.148 |                   57 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.449 |  0.179 |                   51 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.536 |  0.19  |                   54 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.1900964379310608
[2m[36m(func pid=85321)[0m mae:  0.13006742298603058
[2m[36m(func pid=85321)[0m rmse_per_class: [0.214, 0.258, 0.121, 0.371, 0.056, 0.196, 0.269, 0.154, 0.167, 0.096]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15900269150733948
[2m[36m(func pid=78481)[0m mae:  0.11363724619150162
[2m[36m(func pid=78481)[0m rmse_per_class: [0.111, 0.24, 0.047, 0.316, 0.068, 0.174, 0.26, 0.121, 0.146, 0.107]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15002131462097168
[2m[36m(func pid=83767)[0m mae:  0.10066667944192886
[2m[36m(func pid=83767)[0m rmse_per_class: [0.113, 0.207, 0.051, 0.286, 0.094, 0.162, 0.243, 0.096, 0.151, 0.098]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17883466184139252
[2m[36m(func pid=84699)[0m mae:  0.13011272251605988
[2m[36m(func pid=84699)[0m rmse_per_class: [0.122, 0.266, 0.129, 0.344, 0.078, 0.188, 0.274, 0.136, 0.154, 0.097]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5392 | Steps: 2 | Val loss: 0.3880 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3572 | Steps: 2 | Val loss: 0.2855 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3032 | Steps: 2 | Val loss: 0.2700 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 05:33:28 (running for 00:32:34.99)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.355 |  0.159 |                   81 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.31  |  0.15  |                   58 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.449 |  0.179 |                   52 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.539 |  0.189 |                   55 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.18916010856628418
[2m[36m(func pid=85321)[0m mae:  0.12975187599658966
[2m[36m(func pid=85321)[0m rmse_per_class: [0.214, 0.256, 0.118, 0.37, 0.056, 0.194, 0.264, 0.154, 0.17, 0.096]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4520 | Steps: 2 | Val loss: 0.3352 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=83767)[0m rmse: 0.15033388137817383
[2m[36m(func pid=83767)[0m mae:  0.1012263149023056
[2m[36m(func pid=83767)[0m rmse_per_class: [0.111, 0.208, 0.047, 0.293, 0.09, 0.16, 0.245, 0.095, 0.156, 0.097]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15876856446266174
[2m[36m(func pid=78481)[0m mae:  0.11348160356283188
[2m[36m(func pid=78481)[0m rmse_per_class: [0.11, 0.239, 0.047, 0.317, 0.068, 0.174, 0.26, 0.12, 0.145, 0.107]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.1787864863872528
[2m[36m(func pid=84699)[0m mae:  0.13000142574310303
[2m[36m(func pid=84699)[0m rmse_per_class: [0.122, 0.266, 0.13, 0.345, 0.077, 0.188, 0.273, 0.137, 0.154, 0.096]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5419 | Steps: 2 | Val loss: 0.3831 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3581 | Steps: 2 | Val loss: 0.2853 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3149 | Steps: 2 | Val loss: 0.2728 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 05:33:33 (running for 00:32:40.06)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.357 |  0.159 |                   82 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.303 |  0.15  |                   59 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.452 |  0.179 |                   53 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.542 |  0.188 |                   56 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.18779084086418152
[2m[36m(func pid=85321)[0m mae:  0.12921088933944702
[2m[36m(func pid=85321)[0m rmse_per_class: [0.21, 0.254, 0.113, 0.369, 0.056, 0.193, 0.26, 0.154, 0.174, 0.096]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4523 | Steps: 2 | Val loss: 0.3353 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=78481)[0m rmse: 0.15859942138195038
[2m[36m(func pid=78481)[0m mae:  0.11331348121166229
[2m[36m(func pid=78481)[0m rmse_per_class: [0.108, 0.239, 0.049, 0.316, 0.068, 0.174, 0.259, 0.12, 0.146, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15214140713214874
[2m[36m(func pid=83767)[0m mae:  0.10269520431756973
[2m[36m(func pid=83767)[0m rmse_per_class: [0.129, 0.206, 0.041, 0.301, 0.083, 0.16, 0.253, 0.095, 0.157, 0.095]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5293 | Steps: 2 | Val loss: 0.3767 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=84699)[0m rmse: 0.17876563966274261
[2m[36m(func pid=84699)[0m mae:  0.1299244463443756
[2m[36m(func pid=84699)[0m rmse_per_class: [0.123, 0.266, 0.13, 0.345, 0.077, 0.188, 0.273, 0.137, 0.154, 0.096]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3597 | Steps: 2 | Val loss: 0.2847 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3044 | Steps: 2 | Val loss: 0.2694 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 05:33:39 (running for 00:32:45.20)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.358 |  0.159 |                   83 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.315 |  0.152 |                   60 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.452 |  0.179 |                   54 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.529 |  0.186 |                   57 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.18593797087669373
[2m[36m(func pid=85321)[0m mae:  0.12835952639579773
[2m[36m(func pid=85321)[0m rmse_per_class: [0.203, 0.251, 0.107, 0.367, 0.056, 0.192, 0.256, 0.154, 0.178, 0.096]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15821078419685364
[2m[36m(func pid=78481)[0m mae:  0.11297978460788727
[2m[36m(func pid=78481)[0m rmse_per_class: [0.106, 0.239, 0.049, 0.316, 0.069, 0.174, 0.259, 0.12, 0.145, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14975526928901672
[2m[36m(func pid=83767)[0m mae:  0.10132026672363281
[2m[36m(func pid=83767)[0m rmse_per_class: [0.128, 0.204, 0.04, 0.292, 0.076, 0.159, 0.255, 0.095, 0.149, 0.098]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4533 | Steps: 2 | Val loss: 0.3357 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5257 | Steps: 2 | Val loss: 0.3723 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=84699)[0m rmse: 0.1787576973438263
[2m[36m(func pid=84699)[0m mae:  0.12986262142658234
[2m[36m(func pid=84699)[0m rmse_per_class: [0.123, 0.266, 0.131, 0.345, 0.076, 0.188, 0.272, 0.137, 0.154, 0.096]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3645 | Steps: 2 | Val loss: 0.2842 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3074 | Steps: 2 | Val loss: 0.2645 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 05:33:44 (running for 00:32:50.38)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.36  |  0.158 |                   84 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.304 |  0.15  |                   61 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.453 |  0.179 |                   55 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.526 |  0.184 |                   58 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.1844177544116974
[2m[36m(func pid=85321)[0m mae:  0.1277054101228714
[2m[36m(func pid=85321)[0m rmse_per_class: [0.196, 0.249, 0.103, 0.365, 0.056, 0.19, 0.252, 0.154, 0.183, 0.096]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15793243050575256
[2m[36m(func pid=78481)[0m mae:  0.11268681287765503
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.239, 0.049, 0.315, 0.069, 0.174, 0.257, 0.12, 0.145, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14600571990013123
[2m[36m(func pid=83767)[0m mae:  0.09858979284763336
[2m[36m(func pid=83767)[0m rmse_per_class: [0.11, 0.203, 0.045, 0.281, 0.074, 0.161, 0.248, 0.095, 0.145, 0.097]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4566 | Steps: 2 | Val loss: 0.3371 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5601 | Steps: 2 | Val loss: 0.3688 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3583 | Steps: 2 | Val loss: 0.2841 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3220 | Steps: 2 | Val loss: 0.2645 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=84699)[0m rmse: 0.1788388192653656
[2m[36m(func pid=84699)[0m mae:  0.1298254430294037
[2m[36m(func pid=84699)[0m rmse_per_class: [0.124, 0.266, 0.132, 0.346, 0.075, 0.188, 0.272, 0.137, 0.154, 0.096]
[2m[36m(func pid=84699)[0m 
== Status ==
Current time: 2024-01-07 05:33:49 (running for 00:32:55.40)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.365 |  0.158 |                   85 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.307 |  0.146 |                   62 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.457 |  0.179 |                   56 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.56  |  0.183 |                   59 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.1829693466424942
[2m[36m(func pid=85321)[0m mae:  0.12698739767074585
[2m[36m(func pid=85321)[0m rmse_per_class: [0.188, 0.247, 0.1, 0.363, 0.056, 0.188, 0.25, 0.154, 0.188, 0.096]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1580306738615036
[2m[36m(func pid=78481)[0m mae:  0.11266463994979858
[2m[36m(func pid=78481)[0m rmse_per_class: [0.104, 0.239, 0.051, 0.314, 0.069, 0.175, 0.257, 0.12, 0.145, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14667579531669617
[2m[36m(func pid=83767)[0m mae:  0.09799448400735855
[2m[36m(func pid=83767)[0m rmse_per_class: [0.107, 0.204, 0.06, 0.28, 0.076, 0.162, 0.243, 0.095, 0.14, 0.1]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4579 | Steps: 2 | Val loss: 0.3376 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5125 | Steps: 2 | Val loss: 0.3607 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2996 | Steps: 2 | Val loss: 0.2682 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3578 | Steps: 2 | Val loss: 0.2838 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 05:33:54 (running for 00:33:00.41)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.358 |  0.158 |                   86 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.322 |  0.147 |                   63 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.458 |  0.179 |                   57 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.56  |  0.183 |                   59 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=84699)[0m rmse: 0.17881982028484344
[2m[36m(func pid=84699)[0m mae:  0.12975512444972992
[2m[36m(func pid=84699)[0m rmse_per_class: [0.124, 0.266, 0.132, 0.346, 0.074, 0.188, 0.271, 0.137, 0.155, 0.095]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.18056076765060425
[2m[36m(func pid=85321)[0m mae:  0.125794917345047
[2m[36m(func pid=85321)[0m rmse_per_class: [0.175, 0.246, 0.092, 0.359, 0.056, 0.186, 0.248, 0.154, 0.194, 0.096]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14902715384960175
[2m[36m(func pid=83767)[0m mae:  0.09971850365400314
[2m[36m(func pid=83767)[0m rmse_per_class: [0.127, 0.205, 0.043, 0.291, 0.083, 0.161, 0.249, 0.095, 0.139, 0.098]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15800265967845917
[2m[36m(func pid=78481)[0m mae:  0.11256919801235199
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.239, 0.052, 0.313, 0.069, 0.175, 0.257, 0.119, 0.146, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4622 | Steps: 2 | Val loss: 0.3393 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.5037 | Steps: 2 | Val loss: 0.3531 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3049 | Steps: 2 | Val loss: 0.2712 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3552 | Steps: 2 | Val loss: 0.2835 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 05:33:59 (running for 00:33:05.90)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.358 |  0.158 |                   87 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.3   |  0.149 |                   64 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.462 |  0.179 |                   58 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.513 |  0.181 |                   60 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=84699)[0m rmse: 0.1788884699344635
[2m[36m(func pid=84699)[0m mae:  0.1297151893377304
[2m[36m(func pid=84699)[0m rmse_per_class: [0.124, 0.266, 0.133, 0.347, 0.073, 0.188, 0.271, 0.137, 0.155, 0.095]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.1782257854938507
[2m[36m(func pid=85321)[0m mae:  0.12462358176708221
[2m[36m(func pid=85321)[0m rmse_per_class: [0.161, 0.246, 0.085, 0.355, 0.056, 0.184, 0.246, 0.154, 0.199, 0.095]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15183904767036438
[2m[36m(func pid=83767)[0m mae:  0.10108097642660141
[2m[36m(func pid=83767)[0m rmse_per_class: [0.146, 0.205, 0.039, 0.294, 0.092, 0.159, 0.252, 0.095, 0.142, 0.094]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15793590247631073
[2m[36m(func pid=78481)[0m mae:  0.11259162425994873
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.239, 0.052, 0.312, 0.069, 0.175, 0.258, 0.119, 0.146, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4592 | Steps: 2 | Val loss: 0.3394 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5071 | Steps: 2 | Val loss: 0.3485 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3077 | Steps: 2 | Val loss: 0.2680 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3567 | Steps: 2 | Val loss: 0.2832 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=85321)[0m rmse: 0.17655979096889496
[2m[36m(func pid=85321)[0m mae:  0.12378358840942383
[2m[36m(func pid=85321)[0m rmse_per_class: [0.149, 0.246, 0.081, 0.352, 0.056, 0.182, 0.246, 0.153, 0.204, 0.095]
[2m[36m(func pid=85321)[0m 
== Status ==
Current time: 2024-01-07 05:34:05 (running for 00:33:11.31)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.355 |  0.158 |                   88 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.305 |  0.152 |                   65 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.462 |  0.179 |                   58 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.507 |  0.177 |                   62 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=84699)[0m rmse: 0.17880545556545258
[2m[36m(func pid=84699)[0m mae:  0.1296018660068512
[2m[36m(func pid=84699)[0m rmse_per_class: [0.124, 0.266, 0.133, 0.347, 0.073, 0.188, 0.27, 0.137, 0.155, 0.095]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14992742240428925
[2m[36m(func pid=83767)[0m mae:  0.09967932105064392
[2m[36m(func pid=83767)[0m rmse_per_class: [0.132, 0.207, 0.04, 0.286, 0.099, 0.159, 0.249, 0.095, 0.142, 0.09]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15780843794345856
[2m[36m(func pid=78481)[0m mae:  0.11248920857906342
[2m[36m(func pid=78481)[0m rmse_per_class: [0.104, 0.239, 0.051, 0.31, 0.069, 0.175, 0.258, 0.119, 0.147, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5070 | Steps: 2 | Val loss: 0.3441 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4602 | Steps: 2 | Val loss: 0.3406 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2996 | Steps: 2 | Val loss: 0.2647 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3535 | Steps: 2 | Val loss: 0.2831 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 05:34:10 (running for 00:33:16.48)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.357 |  0.158 |                   89 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.308 |  0.15  |                   66 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.459 |  0.179 |                   59 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.507 |  0.175 |                   63 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.17497172951698303
[2m[36m(func pid=85321)[0m mae:  0.12297964096069336
[2m[36m(func pid=85321)[0m rmse_per_class: [0.138, 0.248, 0.077, 0.348, 0.056, 0.18, 0.247, 0.153, 0.208, 0.095]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17881013453006744
[2m[36m(func pid=84699)[0m mae:  0.12952573597431183
[2m[36m(func pid=84699)[0m rmse_per_class: [0.125, 0.266, 0.134, 0.347, 0.072, 0.188, 0.27, 0.137, 0.155, 0.095]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14748351275920868
[2m[36m(func pid=83767)[0m mae:  0.09811016172170639
[2m[36m(func pid=83767)[0m rmse_per_class: [0.11, 0.209, 0.046, 0.281, 0.099, 0.16, 0.241, 0.095, 0.143, 0.09]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15782339870929718
[2m[36m(func pid=78481)[0m mae:  0.11251555383205414
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.238, 0.051, 0.31, 0.07, 0.175, 0.258, 0.119, 0.147, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5096 | Steps: 2 | Val loss: 0.3403 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4617 | Steps: 2 | Val loss: 0.3415 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2997 | Steps: 2 | Val loss: 0.2646 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3562 | Steps: 2 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 05:34:15 (running for 00:33:21.61)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.354 |  0.158 |                   90 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.3   |  0.147 |                   67 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.46  |  0.179 |                   60 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.51  |  0.174 |                   64 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=85321)[0m rmse: 0.17357027530670166
[2m[36m(func pid=85321)[0m mae:  0.12226352840662003
[2m[36m(func pid=85321)[0m rmse_per_class: [0.127, 0.25, 0.072, 0.344, 0.056, 0.178, 0.249, 0.153, 0.21, 0.095]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17882269620895386
[2m[36m(func pid=84699)[0m mae:  0.12946254014968872
[2m[36m(func pid=84699)[0m rmse_per_class: [0.125, 0.266, 0.134, 0.347, 0.071, 0.188, 0.27, 0.137, 0.155, 0.095]
[2m[36m(func pid=83767)[0m rmse: 0.14711883664131165
[2m[36m(func pid=83767)[0m mae:  0.09828948974609375
[2m[36m(func pid=83767)[0m rmse_per_class: [0.103, 0.207, 0.048, 0.284, 0.096, 0.161, 0.239, 0.095, 0.148, 0.091]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15800191462039948
[2m[36m(func pid=78481)[0m mae:  0.112642303109169
[2m[36m(func pid=78481)[0m rmse_per_class: [0.106, 0.238, 0.052, 0.309, 0.069, 0.175, 0.259, 0.118, 0.147, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4987 | Steps: 2 | Val loss: 0.3365 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3014 | Steps: 2 | Val loss: 0.2685 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4646 | Steps: 2 | Val loss: 0.3421 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3572 | Steps: 2 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.63s
[2m[36m(func pid=85321)[0m rmse: 0.17223480343818665
[2m[36m(func pid=85321)[0m mae:  0.12157154083251953
[2m[36m(func pid=85321)[0m rmse_per_class: [0.117, 0.253, 0.067, 0.339, 0.056, 0.177, 0.252, 0.153, 0.213, 0.095]
[2m[36m(func pid=85321)[0m 
== Status ==
Current time: 2024-01-07 05:34:21 (running for 00:33:28.03)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.356 |  0.158 |                   91 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.301 |  0.149 |                   69 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.462 |  0.179 |                   61 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.499 |  0.172 |                   65 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83767)[0m rmse: 0.14923134446144104
[2m[36m(func pid=83767)[0m mae:  0.09996699541807175
[2m[36m(func pid=83767)[0m rmse_per_class: [0.117, 0.205, 0.046, 0.291, 0.087, 0.161, 0.247, 0.096, 0.146, 0.095]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15803048014640808
[2m[36m(func pid=78481)[0m mae:  0.11263960599899292
[2m[36m(func pid=78481)[0m rmse_per_class: [0.108, 0.238, 0.051, 0.309, 0.069, 0.175, 0.259, 0.118, 0.147, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17880074679851532
[2m[36m(func pid=84699)[0m mae:  0.12938664853572845
[2m[36m(func pid=84699)[0m rmse_per_class: [0.125, 0.266, 0.135, 0.348, 0.071, 0.188, 0.269, 0.137, 0.155, 0.094]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.5020 | Steps: 2 | Val loss: 0.3334 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3032 | Steps: 2 | Val loss: 0.2695 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3555 | Steps: 2 | Val loss: 0.2835 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4647 | Steps: 2 | Val loss: 0.3425 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=85321)[0m rmse: 0.17113232612609863
[2m[36m(func pid=85321)[0m mae:  0.12092987447977066
[2m[36m(func pid=85321)[0m rmse_per_class: [0.109, 0.257, 0.063, 0.334, 0.056, 0.176, 0.256, 0.152, 0.213, 0.095]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14963063597679138
[2m[36m(func pid=83767)[0m mae:  0.1004536896944046
[2m[36m(func pid=83767)[0m rmse_per_class: [0.123, 0.205, 0.041, 0.287, 0.082, 0.161, 0.254, 0.096, 0.142, 0.106]
== Status ==
Current time: 2024-01-07 05:34:26 (running for 00:33:33.09)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.357 |  0.158 |                   92 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.303 |  0.15  |                   70 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.465 |  0.179 |                   62 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.502 |  0.171 |                   66 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1581091284751892
[2m[36m(func pid=78481)[0m mae:  0.112727090716362
[2m[36m(func pid=78481)[0m rmse_per_class: [0.108, 0.238, 0.05, 0.31, 0.07, 0.174, 0.259, 0.118, 0.147, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17874543368816376
[2m[36m(func pid=84699)[0m mae:  0.1293002963066101
[2m[36m(func pid=84699)[0m rmse_per_class: [0.126, 0.266, 0.135, 0.348, 0.07, 0.188, 0.269, 0.138, 0.155, 0.094]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4747 | Steps: 2 | Val loss: 0.3293 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3060 | Steps: 2 | Val loss: 0.2662 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3544 | Steps: 2 | Val loss: 0.2837 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=85321)[0m rmse: 0.16970717906951904
[2m[36m(func pid=85321)[0m mae:  0.12000006437301636
[2m[36m(func pid=85321)[0m rmse_per_class: [0.102, 0.261, 0.057, 0.328, 0.056, 0.175, 0.26, 0.152, 0.212, 0.095]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4711 | Steps: 2 | Val loss: 0.3434 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 05:34:32 (running for 00:33:38.15)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.356 |  0.158 |                   93 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.306 |  0.147 |                   71 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.465 |  0.179 |                   63 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.475 |  0.17  |                   67 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83767)[0m rmse: 0.14744366705417633
[2m[36m(func pid=83767)[0m mae:  0.09896419197320938
[2m[36m(func pid=83767)[0m rmse_per_class: [0.124, 0.204, 0.039, 0.278, 0.075, 0.16, 0.253, 0.094, 0.143, 0.104]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15811488032341003
[2m[36m(func pid=78481)[0m mae:  0.11264391243457794
[2m[36m(func pid=78481)[0m rmse_per_class: [0.108, 0.238, 0.05, 0.312, 0.07, 0.174, 0.258, 0.118, 0.147, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4750 | Steps: 2 | Val loss: 0.3262 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=84699)[0m rmse: 0.17873701453208923
[2m[36m(func pid=84699)[0m mae:  0.12922802567481995
[2m[36m(func pid=84699)[0m rmse_per_class: [0.126, 0.266, 0.135, 0.348, 0.07, 0.188, 0.268, 0.138, 0.155, 0.094]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2994 | Steps: 2 | Val loss: 0.2643 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3586 | Steps: 2 | Val loss: 0.2838 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=85321)[0m rmse: 0.16859331727027893
[2m[36m(func pid=85321)[0m mae:  0.11915095895528793
[2m[36m(func pid=85321)[0m rmse_per_class: [0.098, 0.264, 0.053, 0.322, 0.056, 0.174, 0.264, 0.152, 0.209, 0.094]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4689 | Steps: 2 | Val loss: 0.3445 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 05:34:37 (running for 00:33:43.51)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.354 |  0.158 |                   94 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.299 |  0.146 |                   72 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.471 |  0.179 |                   64 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.475 |  0.169 |                   68 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83767)[0m rmse: 0.14553013443946838
[2m[36m(func pid=83767)[0m mae:  0.09776268899440765
[2m[36m(func pid=83767)[0m rmse_per_class: [0.114, 0.203, 0.04, 0.28, 0.074, 0.159, 0.247, 0.094, 0.146, 0.098]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1581258773803711
[2m[36m(func pid=78481)[0m mae:  0.11259306967258453
[2m[36m(func pid=78481)[0m rmse_per_class: [0.108, 0.238, 0.05, 0.312, 0.07, 0.174, 0.258, 0.118, 0.147, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4706 | Steps: 2 | Val loss: 0.3234 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=84699)[0m rmse: 0.1787482500076294
[2m[36m(func pid=84699)[0m mae:  0.12916164100170135
[2m[36m(func pid=84699)[0m rmse_per_class: [0.126, 0.266, 0.135, 0.348, 0.069, 0.188, 0.268, 0.138, 0.155, 0.094]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2998 | Steps: 2 | Val loss: 0.2634 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3585 | Steps: 2 | Val loss: 0.2838 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=85321)[0m rmse: 0.16753454506397247
[2m[36m(func pid=85321)[0m mae:  0.1183217391371727
[2m[36m(func pid=85321)[0m rmse_per_class: [0.096, 0.266, 0.049, 0.316, 0.056, 0.174, 0.268, 0.151, 0.204, 0.094]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4705 | Steps: 2 | Val loss: 0.3453 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 05:34:42 (running for 00:33:48.78)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.359 |  0.158 |                   95 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.3   |  0.145 |                   73 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.469 |  0.179 |                   65 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.471 |  0.168 |                   69 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83767)[0m rmse: 0.1451682597398758
[2m[36m(func pid=83767)[0m mae:  0.09721322357654572
[2m[36m(func pid=83767)[0m rmse_per_class: [0.11, 0.202, 0.044, 0.283, 0.077, 0.158, 0.239, 0.096, 0.15, 0.091]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1580856591463089
[2m[36m(func pid=78481)[0m mae:  0.1126251071691513
[2m[36m(func pid=78481)[0m rmse_per_class: [0.109, 0.238, 0.049, 0.312, 0.069, 0.174, 0.258, 0.118, 0.148, 0.105]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4749 | Steps: 2 | Val loss: 0.3209 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=84699)[0m rmse: 0.17873552441596985
[2m[36m(func pid=84699)[0m mae:  0.1290830820798874
[2m[36m(func pid=84699)[0m rmse_per_class: [0.126, 0.266, 0.136, 0.348, 0.068, 0.188, 0.268, 0.138, 0.155, 0.094]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3074 | Steps: 2 | Val loss: 0.2629 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3548 | Steps: 2 | Val loss: 0.2836 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=85321)[0m rmse: 0.1665518581867218
[2m[36m(func pid=85321)[0m mae:  0.11746779829263687
[2m[36m(func pid=85321)[0m rmse_per_class: [0.096, 0.268, 0.046, 0.311, 0.056, 0.174, 0.273, 0.15, 0.198, 0.094]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4723 | Steps: 2 | Val loss: 0.3457 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 05:34:47 (running for 00:33:53.83)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15949999541044235
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.358 |  0.158 |                   96 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.307 |  0.145 |                   74 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.471 |  0.179 |                   66 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.475 |  0.167 |                   70 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83767)[0m rmse: 0.14517836272716522
[2m[36m(func pid=83767)[0m mae:  0.09655334055423737
[2m[36m(func pid=83767)[0m rmse_per_class: [0.109, 0.202, 0.05, 0.283, 0.081, 0.159, 0.236, 0.095, 0.144, 0.092]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15782517194747925
[2m[36m(func pid=78481)[0m mae:  0.1125379353761673
[2m[36m(func pid=78481)[0m rmse_per_class: [0.107, 0.237, 0.048, 0.312, 0.069, 0.174, 0.258, 0.118, 0.149, 0.105]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4624 | Steps: 2 | Val loss: 0.3190 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=84699)[0m rmse: 0.17870831489562988
[2m[36m(func pid=84699)[0m mae:  0.12900735437870026
[2m[36m(func pid=84699)[0m rmse_per_class: [0.127, 0.266, 0.136, 0.349, 0.068, 0.188, 0.267, 0.138, 0.155, 0.094]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3018 | Steps: 2 | Val loss: 0.2623 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3557 | Steps: 2 | Val loss: 0.2832 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=85321)[0m rmse: 0.16565078496932983
[2m[36m(func pid=85321)[0m mae:  0.11668938398361206
[2m[36m(func pid=85321)[0m rmse_per_class: [0.096, 0.268, 0.044, 0.306, 0.056, 0.175, 0.277, 0.15, 0.191, 0.093]
[2m[36m(func pid=85321)[0m 
== Status ==
Current time: 2024-01-07 05:34:53 (running for 00:33:59.26)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.355 |  0.158 |                   97 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.302 |  0.145 |                   75 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.472 |  0.179 |                   67 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.462 |  0.166 |                   71 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83767)[0m rmse: 0.14497092366218567
[2m[36m(func pid=83767)[0m mae:  0.09655716270208359
[2m[36m(func pid=83767)[0m rmse_per_class: [0.116, 0.204, 0.043, 0.276, 0.082, 0.158, 0.241, 0.094, 0.143, 0.092]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.1574908196926117
[2m[36m(func pid=78481)[0m mae:  0.11238384246826172
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.236, 0.047, 0.313, 0.07, 0.174, 0.258, 0.118, 0.149, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4741 | Steps: 2 | Val loss: 0.3464 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4794 | Steps: 2 | Val loss: 0.3161 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2976 | Steps: 2 | Val loss: 0.2666 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=84699)[0m rmse: 0.17870287597179413
[2m[36m(func pid=84699)[0m mae:  0.12894991040229797
[2m[36m(func pid=84699)[0m rmse_per_class: [0.127, 0.266, 0.136, 0.349, 0.067, 0.188, 0.267, 0.138, 0.155, 0.094]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3527 | Steps: 2 | Val loss: 0.2834 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=85321)[0m rmse: 0.16455113887786865
[2m[36m(func pid=85321)[0m mae:  0.11569112539291382
[2m[36m(func pid=85321)[0m rmse_per_class: [0.097, 0.266, 0.043, 0.301, 0.056, 0.177, 0.28, 0.149, 0.183, 0.093]
[2m[36m(func pid=85321)[0m 
== Status ==
Current time: 2024-01-07 05:34:58 (running for 00:34:04.44)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00014 | RUNNING    | 192.168.7.53:78481 | 0.01   |       0.9  |         0.0001 |  0.356 |  0.157 |                   98 |
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.298 |  0.148 |                   76 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.474 |  0.179 |                   68 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.479 |  0.165 |                   72 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83767)[0m rmse: 0.14810708165168762
[2m[36m(func pid=83767)[0m mae:  0.09919042885303497
[2m[36m(func pid=83767)[0m rmse_per_class: [0.119, 0.203, 0.042, 0.282, 0.084, 0.159, 0.249, 0.095, 0.157, 0.092]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=78481)[0m rmse: 0.15753969550132751
[2m[36m(func pid=78481)[0m mae:  0.11243599653244019
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.236, 0.047, 0.314, 0.07, 0.174, 0.258, 0.118, 0.149, 0.106]
[2m[36m(func pid=78481)[0m 
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4840 | Steps: 2 | Val loss: 0.3481 | Batch size: 32 | lr: 0.0001 | Duration: 3.23s
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4634 | Steps: 2 | Val loss: 0.3143 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=78481)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3521 | Steps: 2 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3053 | Steps: 2 | Val loss: 0.2699 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=84699)[0m rmse: 0.17879340052604675
[2m[36m(func pid=84699)[0m mae:  0.12891781330108643
[2m[36m(func pid=84699)[0m rmse_per_class: [0.127, 0.266, 0.137, 0.349, 0.067, 0.188, 0.267, 0.138, 0.155, 0.093]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.1637379378080368
[2m[36m(func pid=85321)[0m mae:  0.1149001270532608
[2m[36m(func pid=85321)[0m rmse_per_class: [0.098, 0.264, 0.042, 0.298, 0.056, 0.179, 0.283, 0.148, 0.176, 0.093]
[2m[36m(func pid=85321)[0m 
== Status ==
Current time: 2024-01-07 05:35:03 (running for 00:34:09.85)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 3 RUNNING, 15 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767 | 0.1    |       0.9  |         0.0001 |  0.298 |  0.148 |                   76 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699 | 0.0001 |       0.99 |         1e-05  |  0.484 |  0.179 |                   69 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321 | 0.001  |       0.99 |         1e-05  |  0.463 |  0.164 |                   73 |
| train_5a6ec_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462 | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835 | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258 | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692 | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174 | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277 | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799 | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408 | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153 | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723 | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=78481)[0m rmse: 0.15751007199287415
[2m[36m(func pid=78481)[0m mae:  0.1123524159193039
[2m[36m(func pid=78481)[0m rmse_per_class: [0.105, 0.235, 0.048, 0.315, 0.07, 0.174, 0.257, 0.117, 0.149, 0.105]
[2m[36m(func pid=83767)[0m rmse: 0.15017835795879364
[2m[36m(func pid=83767)[0m mae:  0.10075891017913818
[2m[36m(func pid=83767)[0m rmse_per_class: [0.125, 0.202, 0.041, 0.288, 0.086, 0.16, 0.255, 0.094, 0.158, 0.093]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4753 | Steps: 2 | Val loss: 0.3126 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4773 | Steps: 2 | Val loss: 0.3493 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3029 | Steps: 2 | Val loss: 0.2662 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=85321)[0m rmse: 0.1629948616027832
[2m[36m(func pid=85321)[0m mae:  0.11413156986236572
[2m[36m(func pid=85321)[0m rmse_per_class: [0.099, 0.262, 0.042, 0.295, 0.056, 0.182, 0.287, 0.146, 0.169, 0.092]
[2m[36m(func pid=85321)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17881742119789124
[2m[36m(func pid=84699)[0m mae:  0.128858745098114
[2m[36m(func pid=84699)[0m rmse_per_class: [0.128, 0.266, 0.138, 0.349, 0.066, 0.188, 0.266, 0.138, 0.155, 0.093]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14817315340042114
[2m[36m(func pid=83767)[0m mae:  0.09798772633075714
[2m[36m(func pid=83767)[0m rmse_per_class: [0.12, 0.203, 0.05, 0.28, 0.095, 0.159, 0.25, 0.094, 0.139, 0.092]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=85321)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4578 | Steps: 2 | Val loss: 0.3106 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4768 | Steps: 2 | Val loss: 0.3495 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2954 | Steps: 2 | Val loss: 0.2617 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 05:35:08 (running for 00:34:15.03)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.303 |  0.148 |                   78 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.477 |  0.179 |                   70 |
| train_5a6ec_00017 | RUNNING    | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.475 |  0.163 |                   74 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101124)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=101124)[0m Configuration completed!
[2m[36m(func pid=101124)[0m New optimizer parameters:
[2m[36m(func pid=101124)[0m SGD (
[2m[36m(func pid=101124)[0m Parameter Group 0
[2m[36m(func pid=101124)[0m     dampening: 0
[2m[36m(func pid=101124)[0m     differentiable: False
[2m[36m(func pid=101124)[0m     foreach: None
[2m[36m(func pid=101124)[0m     lr: 0.01
[2m[36m(func pid=101124)[0m     maximize: False
[2m[36m(func pid=101124)[0m     momentum: 0.99
[2m[36m(func pid=101124)[0m     nesterov: False
[2m[36m(func pid=101124)[0m     weight_decay: 1e-05
[2m[36m(func pid=101124)[0m )
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=85321)[0m rmse: 0.16232435405254364
[2m[36m(func pid=85321)[0m mae:  0.11335070431232452
[2m[36m(func pid=85321)[0m rmse_per_class: [0.1, 0.258, 0.043, 0.294, 0.056, 0.184, 0.289, 0.145, 0.162, 0.092]
[2m[36m(func pid=84699)[0m rmse: 0.17873945832252502
[2m[36m(func pid=84699)[0m mae:  0.12876072525978088
[2m[36m(func pid=84699)[0m rmse_per_class: [0.128, 0.266, 0.138, 0.349, 0.066, 0.188, 0.266, 0.138, 0.155, 0.093]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1450822651386261
[2m[36m(func pid=83767)[0m mae:  0.09538937360048294
[2m[36m(func pid=83767)[0m rmse_per_class: [0.113, 0.203, 0.046, 0.271, 0.099, 0.158, 0.243, 0.094, 0.134, 0.09]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0072 | Steps: 2 | Val loss: 0.6600 | Batch size: 32 | lr: 0.01 | Duration: 4.52s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4902 | Steps: 2 | Val loss: 0.3514 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3041 | Steps: 2 | Val loss: 0.2621 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=101124)[0m rmse: 0.17977818846702576
[2m[36m(func pid=101124)[0m mae:  0.13228096067905426
[2m[36m(func pid=101124)[0m rmse_per_class: [0.115, 0.263, 0.098, 0.334, 0.098, 0.191, 0.297, 0.143, 0.141, 0.118]
[2m[36m(func pid=84699)[0m rmse: 0.17883595824241638
[2m[36m(func pid=84699)[0m mae:  0.12874218821525574
[2m[36m(func pid=84699)[0m rmse_per_class: [0.129, 0.266, 0.139, 0.35, 0.065, 0.188, 0.266, 0.138, 0.155, 0.093]
[2m[36m(func pid=83767)[0m rmse: 0.14534440636634827
[2m[36m(func pid=83767)[0m mae:  0.09604210406541824
[2m[36m(func pid=83767)[0m rmse_per_class: [0.116, 0.204, 0.039, 0.274, 0.1, 0.158, 0.241, 0.094, 0.138, 0.088]
== Status ==
Current time: 2024-01-07 05:35:14 (running for 00:34:20.34)
Memory usage on this node: 21.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.295 |  0.145 |                   79 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.477 |  0.179 |                   71 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101740)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=101740)[0m Configuration completed!
[2m[36m(func pid=101740)[0m New optimizer parameters:
[2m[36m(func pid=101740)[0m SGD (
[2m[36m(func pid=101740)[0m Parameter Group 0
[2m[36m(func pid=101740)[0m     dampening: 0
[2m[36m(func pid=101740)[0m     differentiable: False
[2m[36m(func pid=101740)[0m     foreach: None
[2m[36m(func pid=101740)[0m     lr: 0.1
[2m[36m(func pid=101740)[0m     maximize: False
[2m[36m(func pid=101740)[0m     momentum: 0.99
[2m[36m(func pid=101740)[0m     nesterov: False
[2m[36m(func pid=101740)[0m     weight_decay: 1e-05
[2m[36m(func pid=101740)[0m )
[2m[36m(func pid=101740)[0m 
== Status ==
Current time: 2024-01-07 05:35:20 (running for 00:34:27.12)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.295 |  0.145 |                   79 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.477 |  0.179 |                   71 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  1.007 |  0.18  |                    1 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.7615 | Steps: 2 | Val loss: 0.5878 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3067 | Steps: 2 | Val loss: 0.2683 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4813 | Steps: 2 | Val loss: 0.3515 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.7902 | Steps: 2 | Val loss: 0.4304 | Batch size: 32 | lr: 0.1 | Duration: 4.69s
== Status ==
Current time: 2024-01-07 05:35:26 (running for 00:34:32.14)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.304 |  0.145 |                   80 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.49  |  0.179 |                   72 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.762 |  0.18  |                    2 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_5a6ec_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.17997416853904724
[2m[36m(func pid=101124)[0m mae:  0.13244253396987915
[2m[36m(func pid=101124)[0m rmse_per_class: [0.114, 0.264, 0.104, 0.334, 0.101, 0.191, 0.294, 0.141, 0.144, 0.113]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14976517856121063
[2m[36m(func pid=83767)[0m mae:  0.09955785423517227
[2m[36m(func pid=83767)[0m rmse_per_class: [0.122, 0.204, 0.04, 0.293, 0.101, 0.158, 0.244, 0.094, 0.153, 0.089]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17874769866466522
[2m[36m(func pid=84699)[0m mae:  0.12861989438533783
[2m[36m(func pid=84699)[0m rmse_per_class: [0.129, 0.266, 0.139, 0.35, 0.065, 0.188, 0.265, 0.139, 0.155, 0.093]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.17919769883155823
[2m[36m(func pid=101740)[0m mae:  0.1310512125492096
[2m[36m(func pid=101740)[0m rmse_per_class: [0.121, 0.264, 0.117, 0.342, 0.088, 0.191, 0.282, 0.139, 0.144, 0.104]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5327 | Steps: 2 | Val loss: 0.4847 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3046 | Steps: 2 | Val loss: 0.2746 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4808 | Steps: 2 | Val loss: 0.3527 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5048 | Steps: 2 | Val loss: 0.3517 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 05:35:31 (running for 00:34:37.19)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.307 |  0.15  |                   81 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.481 |  0.179 |                   73 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.533 |  0.18  |                    3 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.79  |  0.179 |                    1 |
| train_5a6ec_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.17965076863765717
[2m[36m(func pid=101124)[0m mae:  0.13207708299160004
[2m[36m(func pid=101124)[0m rmse_per_class: [0.114, 0.264, 0.108, 0.336, 0.099, 0.191, 0.291, 0.139, 0.146, 0.109]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15286462008953094
[2m[36m(func pid=83767)[0m mae:  0.10200802981853485
[2m[36m(func pid=83767)[0m rmse_per_class: [0.123, 0.203, 0.047, 0.309, 0.095, 0.159, 0.246, 0.095, 0.159, 0.092]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17881162464618683
[2m[36m(func pid=84699)[0m mae:  0.12859024107456207
[2m[36m(func pid=84699)[0m rmse_per_class: [0.129, 0.266, 0.139, 0.35, 0.064, 0.188, 0.265, 0.139, 0.156, 0.093]
[2m[36m(func pid=84699)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.17879191040992737
[2m[36m(func pid=101740)[0m mae:  0.12975069880485535
[2m[36m(func pid=101740)[0m rmse_per_class: [0.123, 0.265, 0.126, 0.345, 0.077, 0.189, 0.275, 0.139, 0.152, 0.096]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4518 | Steps: 2 | Val loss: 0.3919 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2994 | Steps: 2 | Val loss: 0.2738 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=84699)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4823 | Steps: 2 | Val loss: 0.3535 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7418 | Steps: 2 | Val loss: 0.3285 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 05:35:36 (running for 00:34:42.28)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.305 |  0.153 |                   82 |
| train_5a6ec_00016 | RUNNING    | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.481 |  0.179 |                   74 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.452 |  0.179 |                    4 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.505 |  0.179 |                    2 |
| train_5a6ec_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.17901156842708588
[2m[36m(func pid=101124)[0m mae:  0.13130822777748108
[2m[36m(func pid=101124)[0m rmse_per_class: [0.115, 0.265, 0.115, 0.338, 0.094, 0.19, 0.285, 0.137, 0.148, 0.104]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1526113897562027
[2m[36m(func pid=83767)[0m mae:  0.10100798308849335
[2m[36m(func pid=83767)[0m rmse_per_class: [0.12, 0.202, 0.069, 0.303, 0.09, 0.158, 0.248, 0.094, 0.149, 0.094]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=84699)[0m rmse: 0.17884045839309692
[2m[36m(func pid=84699)[0m mae:  0.12854523956775665
[2m[36m(func pid=84699)[0m rmse_per_class: [0.13, 0.266, 0.14, 0.35, 0.064, 0.188, 0.265, 0.139, 0.155, 0.093]
[2m[36m(func pid=101740)[0m rmse: 0.17674502730369568
[2m[36m(func pid=101740)[0m mae:  0.12574896216392517
[2m[36m(func pid=101740)[0m rmse_per_class: [0.132, 0.266, 0.105, 0.35, 0.058, 0.189, 0.272, 0.143, 0.161, 0.091]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4662 | Steps: 2 | Val loss: 0.3377 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3021 | Steps: 2 | Val loss: 0.2650 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.9082 | Steps: 2 | Val loss: 0.3503 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=101124)[0m rmse: 0.17832790315151215
[2m[36m(func pid=101124)[0m mae:  0.13023026287555695
[2m[36m(func pid=101124)[0m rmse_per_class: [0.117, 0.266, 0.123, 0.342, 0.085, 0.189, 0.277, 0.136, 0.15, 0.099]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14659900963306427
[2m[36m(func pid=83767)[0m mae:  0.09695993363857269
[2m[36m(func pid=83767)[0m rmse_per_class: [0.112, 0.202, 0.054, 0.284, 0.087, 0.159, 0.242, 0.094, 0.14, 0.093]
[2m[36m(func pid=101740)[0m rmse: 0.18097718060016632
[2m[36m(func pid=101740)[0m mae:  0.12468264251947403
[2m[36m(func pid=101740)[0m rmse_per_class: [0.139, 0.268, 0.08, 0.36, 0.055, 0.192, 0.312, 0.149, 0.162, 0.093]
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5211 | Steps: 2 | Val loss: 0.3235 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 05:35:41 (running for 00:34:47.58)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.299 |  0.153 |                   83 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.466 |  0.178 |                    5 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.742 |  0.177 |                    3 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=102978)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=102978)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=102978)[0m Configuration completed!
[2m[36m(func pid=102978)[0m New optimizer parameters:
[2m[36m(func pid=102978)[0m SGD (
[2m[36m(func pid=102978)[0m Parameter Group 0
[2m[36m(func pid=102978)[0m     dampening: 0
[2m[36m(func pid=102978)[0m     differentiable: False
[2m[36m(func pid=102978)[0m     foreach: None
[2m[36m(func pid=102978)[0m     lr: 0.0001
[2m[36m(func pid=102978)[0m     maximize: False
[2m[36m(func pid=102978)[0m     momentum: 0.9
[2m[36m(func pid=102978)[0m     nesterov: False
[2m[36m(func pid=102978)[0m     weight_decay: 1e-05
[2m[36m(func pid=102978)[0m )
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:35:46 (running for 00:34:52.79)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.302 |  0.147 |                   84 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.521 |  0.178 |                    6 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.908 |  0.181 |                    4 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.17793846130371094
[2m[36m(func pid=101124)[0m mae:  0.1289965957403183
[2m[36m(func pid=101124)[0m rmse_per_class: [0.12, 0.266, 0.131, 0.347, 0.074, 0.188, 0.27, 0.137, 0.152, 0.094]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3033 | Steps: 2 | Val loss: 0.2602 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.9181 | Steps: 2 | Val loss: 0.3662 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.6076 | Steps: 2 | Val loss: 0.3369 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0420 | Steps: 2 | Val loss: 0.7190 | Batch size: 32 | lr: 0.0001 | Duration: 4.21s
[2m[36m(func pid=101740)[0m rmse: 0.18545062839984894
[2m[36m(func pid=101740)[0m mae:  0.12565621733665466
[2m[36m(func pid=101740)[0m rmse_per_class: [0.147, 0.267, 0.057, 0.369, 0.056, 0.196, 0.355, 0.154, 0.158, 0.096]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14314308762550354
[2m[36m(func pid=83767)[0m mae:  0.09498152881860733
[2m[36m(func pid=83767)[0m rmse_per_class: [0.114, 0.202, 0.039, 0.271, 0.083, 0.157, 0.241, 0.094, 0.14, 0.09]
[2m[36m(func pid=83767)[0m 
== Status ==
Current time: 2024-01-07 05:35:51 (running for 00:34:58.03)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.303 |  0.143 |                   85 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.608 |  0.178 |                    7 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.918 |  0.185 |                    5 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.17837801575660706
[2m[36m(func pid=101124)[0m mae:  0.12784413993358612
[2m[36m(func pid=101124)[0m rmse_per_class: [0.123, 0.268, 0.137, 0.353, 0.064, 0.187, 0.267, 0.14, 0.154, 0.092]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.17990097403526306
[2m[36m(func pid=102978)[0m mae:  0.1324288547039032
[2m[36m(func pid=102978)[0m rmse_per_class: [0.115, 0.263, 0.097, 0.333, 0.1, 0.191, 0.298, 0.143, 0.141, 0.119]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3083 | Steps: 2 | Val loss: 0.2627 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.7958 | Steps: 2 | Val loss: 0.3619 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.6490 | Steps: 2 | Val loss: 0.3635 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0404 | Steps: 2 | Val loss: 0.7189 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=101740)[0m rmse: 0.17947404086589813
[2m[36m(func pid=101740)[0m mae:  0.12442232668399811
[2m[36m(func pid=101740)[0m rmse_per_class: [0.152, 0.252, 0.051, 0.366, 0.056, 0.188, 0.282, 0.155, 0.197, 0.097]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14577016234397888
[2m[36m(func pid=83767)[0m mae:  0.09674929082393646
[2m[36m(func pid=83767)[0m rmse_per_class: [0.117, 0.203, 0.041, 0.273, 0.087, 0.157, 0.246, 0.094, 0.15, 0.088]
[2m[36m(func pid=83767)[0m 
== Status ==
Current time: 2024-01-07 05:35:57 (running for 00:35:03.32)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.308 |  0.146 |                   86 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.649 |  0.18  |                    8 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.796 |  0.179 |                    6 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  1.042 |  0.18  |                    1 |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.17960648238658905
[2m[36m(func pid=101124)[0m mae:  0.1269581913948059
[2m[36m(func pid=101124)[0m rmse_per_class: [0.125, 0.269, 0.138, 0.359, 0.058, 0.187, 0.272, 0.143, 0.154, 0.091]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.1803891658782959
[2m[36m(func pid=102978)[0m mae:  0.13285936415195465
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3010 | Steps: 2 | Val loss: 0.2686 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7644 | Steps: 2 | Val loss: 0.3545 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7187 | Steps: 2 | Val loss: 0.3954 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0348 | Steps: 2 | Val loss: 0.7232 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=83767)[0m rmse: 0.14942021667957306
[2m[36m(func pid=83767)[0m mae:  0.10011310875415802
[2m[36m(func pid=83767)[0m rmse_per_class: [0.111, 0.204, 0.04, 0.286, 0.09, 0.158, 0.253, 0.094, 0.171, 0.089]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.17333845794200897
[2m[36m(func pid=101740)[0m mae:  0.12285151332616806
[2m[36m(func pid=101740)[0m rmse_per_class: [0.099, 0.278, 0.046, 0.337, 0.056, 0.182, 0.271, 0.155, 0.214, 0.097]
[2m[36m(func pid=101740)[0m 
== Status ==
Current time: 2024-01-07 05:36:02 (running for 00:35:08.50)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.301 |  0.149 |                   87 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.719 |  0.182 |                    9 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.764 |  0.173 |                    7 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  1.04  |  0.18  |                    2 |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.1820482313632965
[2m[36m(func pid=101124)[0m mae:  0.12664279341697693
[2m[36m(func pid=101124)[0m rmse_per_class: [0.127, 0.27, 0.136, 0.365, 0.055, 0.188, 0.289, 0.147, 0.152, 0.091]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18065622448921204
[2m[36m(func pid=102978)[0m mae:  0.13309545814990997
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.105, 0.191, 0.3, 0.142, 0.143, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2955 | Steps: 2 | Val loss: 0.2709 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7464 | Steps: 2 | Val loss: 0.4102 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7422 | Steps: 2 | Val loss: 0.4245 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.0278 | Steps: 2 | Val loss: 0.7275 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=83767)[0m rmse: 0.15067169070243835
[2m[36m(func pid=83767)[0m mae:  0.10003803670406342
[2m[36m(func pid=83767)[0m rmse_per_class: [0.117, 0.202, 0.053, 0.296, 0.095, 0.158, 0.249, 0.093, 0.154, 0.09]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.17421095073223114
[2m[36m(func pid=101740)[0m mae:  0.12004120647907257
[2m[36m(func pid=101740)[0m rmse_per_class: [0.107, 0.295, 0.043, 0.295, 0.056, 0.242, 0.32, 0.154, 0.133, 0.097]
[2m[36m(func pid=101740)[0m 
== Status ==
Current time: 2024-01-07 05:36:07 (running for 00:35:13.85)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.296 |  0.151 |                   88 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.742 |  0.185 |                   10 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.746 |  0.174 |                    8 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  1.035 |  0.181 |                    3 |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.18502210080623627
[2m[36m(func pid=101124)[0m mae:  0.12690792977809906
[2m[36m(func pid=101124)[0m rmse_per_class: [0.13, 0.271, 0.131, 0.37, 0.055, 0.19, 0.312, 0.149, 0.15, 0.092]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18079452216625214
[2m[36m(func pid=102978)[0m mae:  0.1332288533449173
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.106, 0.192, 0.302, 0.142, 0.143, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7535 | Steps: 2 | Val loss: 0.3677 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3018 | Steps: 2 | Val loss: 0.2720 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7761 | Steps: 2 | Val loss: 0.4479 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.0220 | Steps: 2 | Val loss: 0.7314 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=101740)[0m rmse: 0.17929251492023468
[2m[36m(func pid=101740)[0m mae:  0.1185435801744461
[2m[36m(func pid=101740)[0m rmse_per_class: [0.11, 0.241, 0.048, 0.31, 0.056, 0.333, 0.319, 0.147, 0.135, 0.094]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15137681365013123
[2m[36m(func pid=83767)[0m mae:  0.09898440539836884
[2m[36m(func pid=83767)[0m rmse_per_class: [0.13, 0.204, 0.061, 0.299, 0.097, 0.157, 0.245, 0.094, 0.135, 0.092]
[2m[36m(func pid=83767)[0m 
== Status ==
Current time: 2024-01-07 05:36:12 (running for 00:35:18.95)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.302 |  0.151 |                   89 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.776 |  0.188 |                   11 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.754 |  0.179 |                    9 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  1.028 |  0.181 |                    4 |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.18822221457958221
[2m[36m(func pid=101124)[0m mae:  0.12749654054641724
[2m[36m(func pid=101124)[0m rmse_per_class: [0.131, 0.273, 0.123, 0.375, 0.055, 0.192, 0.341, 0.151, 0.148, 0.093]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18090084195137024
[2m[36m(func pid=102978)[0m mae:  0.13333123922348022
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.107, 0.192, 0.303, 0.142, 0.143, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.6670 | Steps: 2 | Val loss: 0.3277 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3085 | Steps: 2 | Val loss: 0.2656 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8066 | Steps: 2 | Val loss: 0.4655 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.0103 | Steps: 2 | Val loss: 0.7322 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=101740)[0m rmse: 0.17177855968475342
[2m[36m(func pid=101740)[0m mae:  0.11463282257318497
[2m[36m(func pid=101740)[0m rmse_per_class: [0.107, 0.269, 0.052, 0.289, 0.056, 0.266, 0.287, 0.139, 0.134, 0.117]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14713814854621887
[2m[36m(func pid=83767)[0m mae:  0.09658633917570114
[2m[36m(func pid=83767)[0m rmse_per_class: [0.11, 0.203, 0.053, 0.282, 0.097, 0.158, 0.243, 0.094, 0.133, 0.098]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.191496342420578
[2m[36m(func pid=101124)[0m mae:  0.1283404529094696
[2m[36m(func pid=101124)[0m rmse_per_class: [0.133, 0.274, 0.116, 0.378, 0.056, 0.196, 0.37, 0.153, 0.144, 0.094]
== Status ==
Current time: 2024-01-07 05:36:17 (running for 00:35:24.00)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.309 |  0.147 |                   90 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.807 |  0.191 |                   12 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.667 |  0.172 |                   10 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  1.022 |  0.181 |                    5 |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18095631897449493
[2m[36m(func pid=102978)[0m mae:  0.13337989151477814
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.107, 0.191, 0.304, 0.142, 0.142, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6374 | Steps: 2 | Val loss: 0.4698 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2970 | Steps: 2 | Val loss: 0.2630 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7706 | Steps: 2 | Val loss: 0.4742 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.9989 | Steps: 2 | Val loss: 0.7321 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=101740)[0m rmse: 0.21739153563976288
[2m[36m(func pid=101740)[0m mae:  0.14704608917236328
[2m[36m(func pid=101740)[0m rmse_per_class: [0.107, 0.295, 0.049, 0.377, 0.056, 0.204, 0.301, 0.32, 0.136, 0.329]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14518582820892334
[2m[36m(func pid=83767)[0m mae:  0.09681840240955353
[2m[36m(func pid=83767)[0m rmse_per_class: [0.097, 0.203, 0.04, 0.269, 0.099, 0.159, 0.246, 0.094, 0.147, 0.096]
[2m[36m(func pid=83767)[0m 
== Status ==
Current time: 2024-01-07 05:36:23 (running for 00:35:29.25)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.297 |  0.145 |                   91 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.771 |  0.194 |                   13 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.637 |  0.217 |                   11 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  1.01  |  0.181 |                    6 |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.19427062571048737
[2m[36m(func pid=101124)[0m mae:  0.12924082577228546
[2m[36m(func pid=101124)[0m rmse_per_class: [0.137, 0.275, 0.108, 0.38, 0.056, 0.2, 0.394, 0.154, 0.142, 0.095]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18100804090499878
[2m[36m(func pid=102978)[0m mae:  0.13341519236564636
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.33, 0.108, 0.191, 0.304, 0.143, 0.142, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7601 | Steps: 2 | Val loss: 0.6018 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2990 | Steps: 2 | Val loss: 0.2668 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8179 | Steps: 2 | Val loss: 0.4793 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.9858 | Steps: 2 | Val loss: 0.7301 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=101740)[0m rmse: 0.22083806991577148
[2m[36m(func pid=101740)[0m mae:  0.151007741689682
[2m[36m(func pid=101740)[0m rmse_per_class: [0.109, 0.298, 0.049, 0.386, 0.056, 0.225, 0.27, 0.301, 0.134, 0.38]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14838023483753204
[2m[36m(func pid=83767)[0m mae:  0.09924204647541046
[2m[36m(func pid=83767)[0m rmse_per_class: [0.109, 0.204, 0.04, 0.278, 0.097, 0.158, 0.249, 0.094, 0.16, 0.094]
[2m[36m(func pid=83767)[0m 
== Status ==
Current time: 2024-01-07 05:36:28 (running for 00:35:34.36)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.299 |  0.148 |                   92 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.818 |  0.197 |                   14 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.76  |  0.221 |                   12 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.999 |  0.181 |                    7 |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.19701483845710754
[2m[36m(func pid=101124)[0m mae:  0.1302626132965088
[2m[36m(func pid=101124)[0m rmse_per_class: [0.145, 0.276, 0.105, 0.382, 0.056, 0.203, 0.41, 0.155, 0.141, 0.096]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.1810881644487381
[2m[36m(func pid=102978)[0m mae:  0.13347552716732025
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.33, 0.108, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7766 | Steps: 2 | Val loss: 0.4632 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2927 | Steps: 2 | Val loss: 0.2711 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7302 | Steps: 2 | Val loss: 0.4733 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.9758 | Steps: 2 | Val loss: 0.7268 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=101740)[0m rmse: 0.21520209312438965
[2m[36m(func pid=101740)[0m mae:  0.1469310224056244
[2m[36m(func pid=101740)[0m rmse_per_class: [0.193, 0.29, 0.05, 0.385, 0.056, 0.225, 0.269, 0.186, 0.184, 0.313]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.15120625495910645
[2m[36m(func pid=83767)[0m mae:  0.0999956727027893
[2m[36m(func pid=83767)[0m rmse_per_class: [0.138, 0.201, 0.046, 0.295, 0.095, 0.157, 0.249, 0.094, 0.147, 0.091]
[2m[36m(func pid=83767)[0m 
== Status ==
Current time: 2024-01-07 05:36:33 (running for 00:35:39.71)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.293 |  0.151 |                   93 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.73  |  0.198 |                   15 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.777 |  0.215 |                   13 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.986 |  0.181 |                    8 |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.19812323153018951
[2m[36m(func pid=101124)[0m mae:  0.130731463432312
[2m[36m(func pid=101124)[0m rmse_per_class: [0.157, 0.276, 0.097, 0.383, 0.056, 0.206, 0.415, 0.155, 0.14, 0.096]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18112273514270782
[2m[36m(func pid=102978)[0m mae:  0.133500874042511
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.6424 | Steps: 2 | Val loss: 0.4065 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2930 | Steps: 2 | Val loss: 0.2709 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7114 | Steps: 2 | Val loss: 0.4657 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.9637 | Steps: 2 | Val loss: 0.7224 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=101740)[0m rmse: 0.20742836594581604
[2m[36m(func pid=101740)[0m mae:  0.14334116876125336
[2m[36m(func pid=101740)[0m rmse_per_class: [0.102, 0.231, 0.047, 0.377, 0.056, 0.214, 0.272, 0.12, 0.559, 0.095]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.1500045359134674
[2m[36m(func pid=83767)[0m mae:  0.09884209930896759
[2m[36m(func pid=83767)[0m rmse_per_class: [0.134, 0.202, 0.047, 0.299, 0.09, 0.157, 0.247, 0.093, 0.14, 0.091]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.19839563965797424
[2m[36m(func pid=101124)[0m mae:  0.13101211190223694
[2m[36m(func pid=101124)[0m rmse_per_class: [0.177, 0.274, 0.087, 0.384, 0.056, 0.208, 0.407, 0.155, 0.14, 0.097]
== Status ==
Current time: 2024-01-07 05:36:38 (running for 00:35:45.06)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.293 |  0.15  |                   94 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.711 |  0.198 |                   16 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.642 |  0.207 |                   14 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.976 |  0.181 |                    9 |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18114303052425385
[2m[36m(func pid=102978)[0m mae:  0.13351471722126007
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.191, 0.306, 0.143, 0.142, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.6024 | Steps: 2 | Val loss: 0.3621 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2939 | Steps: 2 | Val loss: 0.2680 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6957 | Steps: 2 | Val loss: 0.4564 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.9502 | Steps: 2 | Val loss: 0.7174 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=101740)[0m rmse: 0.192304328083992
[2m[36m(func pid=101740)[0m mae:  0.12614384293556213
[2m[36m(func pid=101740)[0m rmse_per_class: [0.105, 0.403, 0.066, 0.345, 0.056, 0.185, 0.269, 0.147, 0.251, 0.095]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=83767)[0m rmse: 0.14684270322322845
[2m[36m(func pid=83767)[0m mae:  0.09761027991771698
[2m[36m(func pid=83767)[0m rmse_per_class: [0.109, 0.204, 0.043, 0.295, 0.087, 0.157, 0.242, 0.093, 0.147, 0.093]
[2m[36m(func pid=83767)[0m 
== Status ==
Current time: 2024-01-07 05:36:43 (running for 00:35:50.11)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.294 |  0.147 |                   95 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.696 |  0.197 |                   17 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.602 |  0.192 |                   15 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.964 |  0.181 |                   10 |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.1970939189195633
[2m[36m(func pid=101124)[0m mae:  0.13058891892433167
[2m[36m(func pid=101124)[0m rmse_per_class: [0.197, 0.269, 0.076, 0.384, 0.056, 0.207, 0.387, 0.155, 0.142, 0.097]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.1811736524105072
[2m[36m(func pid=102978)[0m mae:  0.13353237509727478
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.191, 0.306, 0.143, 0.142, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2906 | Steps: 2 | Val loss: 0.2644 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6014 | Steps: 2 | Val loss: 0.4020 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6946 | Steps: 2 | Val loss: 0.4443 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.9358 | Steps: 2 | Val loss: 0.7113 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=83767)[0m rmse: 0.14504487812519073
[2m[36m(func pid=83767)[0m mae:  0.0966053158044815
[2m[36m(func pid=83767)[0m rmse_per_class: [0.104, 0.2, 0.042, 0.286, 0.085, 0.156, 0.239, 0.094, 0.151, 0.093]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.17620201408863068
[2m[36m(func pid=101740)[0m mae:  0.1161268800497055
[2m[36m(func pid=101740)[0m rmse_per_class: [0.11, 0.425, 0.045, 0.283, 0.056, 0.168, 0.29, 0.155, 0.134, 0.097]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.19362443685531616
[2m[36m(func pid=101124)[0m mae:  0.12922164797782898
[2m[36m(func pid=101124)[0m rmse_per_class: [0.208, 0.26, 0.069, 0.384, 0.056, 0.204, 0.355, 0.155, 0.148, 0.097]
[2m[36m(func pid=101124)[0m 
== Status ==
Current time: 2024-01-07 05:36:49 (running for 00:35:55.57)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.291 |  0.145 |                   96 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.695 |  0.194 |                   18 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.601 |  0.176 |                   16 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.936 |  0.181 |                   12 |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.18115359544754028
[2m[36m(func pid=102978)[0m mae:  0.1335163116455078
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.109, 0.191, 0.306, 0.144, 0.142, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3147 | Steps: 2 | Val loss: 0.2657 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6928 | Steps: 2 | Val loss: 0.3907 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6849 | Steps: 2 | Val loss: 0.4271 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.9257 | Steps: 2 | Val loss: 0.7050 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=83767)[0m rmse: 0.14726951718330383
[2m[36m(func pid=83767)[0m mae:  0.098051056265831
[2m[36m(func pid=83767)[0m rmse_per_class: [0.127, 0.201, 0.04, 0.284, 0.08, 0.157, 0.246, 0.094, 0.152, 0.093]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.1626901924610138
[2m[36m(func pid=101740)[0m mae:  0.10488196462392807
[2m[36m(func pid=101740)[0m rmse_per_class: [0.11, 0.226, 0.047, 0.308, 0.053, 0.228, 0.267, 0.156, 0.136, 0.097]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.18741437792778015
[2m[36m(func pid=101124)[0m mae:  0.12677492201328278
[2m[36m(func pid=101124)[0m rmse_per_class: [0.194, 0.248, 0.064, 0.382, 0.056, 0.198, 0.315, 0.156, 0.164, 0.097]
[2m[36m(func pid=101124)[0m 
== Status ==
Current time: 2024-01-07 05:36:54 (running for 00:36:00.69)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.315 |  0.147 |                   97 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.685 |  0.187 |                   19 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.693 |  0.163 |                   17 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.926 |  0.181 |                   13 |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.18113945424556732
[2m[36m(func pid=102978)[0m mae:  0.13350190222263336
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.263, 0.098, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3036 | Steps: 2 | Val loss: 0.2674 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6332 | Steps: 2 | Val loss: 0.3756 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6834 | Steps: 2 | Val loss: 0.4086 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.9151 | Steps: 2 | Val loss: 0.6986 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=83767)[0m rmse: 0.14759156107902527
[2m[36m(func pid=83767)[0m mae:  0.09817907959222794
[2m[36m(func pid=83767)[0m rmse_per_class: [0.134, 0.2, 0.043, 0.278, 0.08, 0.157, 0.255, 0.094, 0.144, 0.092]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.1779460310935974
[2m[36m(func pid=101740)[0m mae:  0.1110486164689064
[2m[36m(func pid=101740)[0m rmse_per_class: [0.1, 0.256, 0.053, 0.272, 0.096, 0.337, 0.277, 0.156, 0.134, 0.097]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.18046219646930695
[2m[36m(func pid=101124)[0m mae:  0.12460726499557495
[2m[36m(func pid=101124)[0m rmse_per_class: [0.165, 0.241, 0.061, 0.379, 0.056, 0.19, 0.266, 0.155, 0.194, 0.097]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18112564086914062
[2m[36m(func pid=102978)[0m mae:  0.13349348306655884
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.263, 0.098, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
== Status ==
Current time: 2024-01-07 05:36:59 (running for 00:36:05.82)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.304 |  0.148 |                   98 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.683 |  0.18  |                   20 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.633 |  0.178 |                   18 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.915 |  0.181 |                   14 |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2904 | Steps: 2 | Val loss: 0.2631 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.5803 | Steps: 2 | Val loss: 0.4535 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6429 | Steps: 2 | Val loss: 0.3935 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.9028 | Steps: 2 | Val loss: 0.6919 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=83767)[0m rmse: 0.14509929716587067
[2m[36m(func pid=83767)[0m mae:  0.0964370146393776
[2m[36m(func pid=83767)[0m rmse_per_class: [0.108, 0.2, 0.044, 0.275, 0.084, 0.157, 0.248, 0.096, 0.148, 0.091]
[2m[36m(func pid=83767)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.21432814002037048
[2m[36m(func pid=101740)[0m mae:  0.13668407499790192
[2m[36m(func pid=101740)[0m rmse_per_class: [0.315, 0.292, 0.05, 0.365, 0.293, 0.195, 0.248, 0.155, 0.132, 0.097]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.17707094550132751
[2m[36m(func pid=101124)[0m mae:  0.12428387254476547
[2m[36m(func pid=101124)[0m rmse_per_class: [0.121, 0.257, 0.053, 0.375, 0.056, 0.181, 0.242, 0.155, 0.234, 0.097]
[2m[36m(func pid=101124)[0m 
== Status ==
Current time: 2024-01-07 05:37:04 (running for 00:36:11.06)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00015 | RUNNING    | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.29  |  0.145 |                   99 |
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.643 |  0.177 |                   21 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.58  |  0.214 |                   19 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.903 |  0.181 |                   15 |
| train_5a6ec_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.1811198592185974
[2m[36m(func pid=102978)[0m mae:  0.13348707556724548
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.097, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=83767)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2885 | Steps: 2 | Val loss: 0.2633 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6459 | Steps: 2 | Val loss: 0.6023 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6250 | Steps: 2 | Val loss: 0.3856 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8911 | Steps: 2 | Val loss: 0.6851 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=83767)[0m rmse: 0.1445605754852295
[2m[36m(func pid=83767)[0m mae:  0.09566160291433334
[2m[36m(func pid=83767)[0m rmse_per_class: [0.1, 0.2, 0.043, 0.279, 0.093, 0.158, 0.241, 0.097, 0.146, 0.089]
[2m[36m(func pid=101740)[0m rmse: 0.24495668709278107
[2m[36m(func pid=101740)[0m mae:  0.16362591087818146
[2m[36m(func pid=101740)[0m rmse_per_class: [0.407, 0.299, 0.049, 0.385, 0.39, 0.194, 0.295, 0.153, 0.18, 0.097]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.17931070923805237
[2m[36m(func pid=101124)[0m mae:  0.12588287889957428
[2m[36m(func pid=101124)[0m rmse_per_class: [0.097, 0.298, 0.045, 0.367, 0.056, 0.173, 0.254, 0.155, 0.252, 0.097]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18111959099769592
[2m[36m(func pid=102978)[0m mae:  0.13349021971225739
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.097, 0.329, 0.109, 0.19, 0.306, 0.145, 0.142, 0.117]
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.7044 | Steps: 2 | Val loss: 0.5770 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5941 | Steps: 2 | Val loss: 0.3809 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=101740)[0m rmse: 0.25054389238357544
[2m[36m(func pid=101740)[0m mae:  0.16604475677013397
[2m[36m(func pid=101740)[0m rmse_per_class: [0.165, 0.298, 0.052, 0.385, 0.466, 0.218, 0.302, 0.129, 0.396, 0.095]
[2m[36m(func pid=101124)[0m rmse: 0.18162788450717926
[2m[36m(func pid=101124)[0m mae:  0.126832976937294
[2m[36m(func pid=101124)[0m rmse_per_class: [0.098, 0.327, 0.042, 0.352, 0.056, 0.172, 0.277, 0.155, 0.24, 0.097]
== Status ==
Current time: 2024-01-07 05:37:09 (running for 00:36:16.07)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.625 |  0.179 |                   22 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.646 |  0.245 |                   20 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.903 |  0.181 |                   15 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 05:37:17 (running for 00:36:23.28)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.625 |  0.179 |                   22 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.704 |  0.251 |                   21 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.903 |  0.181 |                   15 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=107141)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=107141)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=107141)[0m Configuration completed!
[2m[36m(func pid=107141)[0m New optimizer parameters:
[2m[36m(func pid=107141)[0m SGD (
[2m[36m(func pid=107141)[0m Parameter Group 0
[2m[36m(func pid=107141)[0m     dampening: 0
[2m[36m(func pid=107141)[0m     differentiable: False
[2m[36m(func pid=107141)[0m     foreach: None
[2m[36m(func pid=107141)[0m     lr: 0.001
[2m[36m(func pid=107141)[0m     maximize: False
[2m[36m(func pid=107141)[0m     momentum: 0.9
[2m[36m(func pid=107141)[0m     nesterov: False
[2m[36m(func pid=107141)[0m     weight_decay: 1e-05
[2m[36m(func pid=107141)[0m )
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5750 | Steps: 2 | Val loss: 0.3693 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6158 | Steps: 2 | Val loss: 0.4729 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8736 | Steps: 2 | Val loss: 0.6777 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0401 | Steps: 2 | Val loss: 0.7135 | Batch size: 32 | lr: 0.001 | Duration: 4.63s
== Status ==
Current time: 2024-01-07 05:37:22 (running for 00:36:28.29)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.594 |  0.182 |                   23 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.704 |  0.251 |                   21 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.891 |  0.181 |                   16 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.17905065417289734
[2m[36m(func pid=101124)[0m mae:  0.12471510469913483
[2m[36m(func pid=101124)[0m rmse_per_class: [0.104, 0.325, 0.042, 0.327, 0.056, 0.185, 0.296, 0.155, 0.205, 0.096]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.231843501329422
[2m[36m(func pid=101740)[0m mae:  0.14530205726623535
[2m[36m(func pid=101740)[0m rmse_per_class: [0.101, 0.282, 0.056, 0.372, 0.409, 0.221, 0.268, 0.15, 0.373, 0.085]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18110047280788422
[2m[36m(func pid=102978)[0m mae:  0.1334754377603531
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.097, 0.328, 0.109, 0.19, 0.306, 0.145, 0.142, 0.117]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1799079328775406
[2m[36m(func pid=107141)[0m mae:  0.13242696225643158
[2m[36m(func pid=107141)[0m rmse_per_class: [0.115, 0.263, 0.097, 0.333, 0.1, 0.191, 0.298, 0.143, 0.141, 0.119]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5392 | Steps: 2 | Val loss: 0.4088 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5721 | Steps: 2 | Val loss: 0.3558 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8676 | Steps: 2 | Val loss: 0.6708 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0139 | Steps: 2 | Val loss: 0.7053 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 05:37:27 (running for 00:36:33.73)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.575 |  0.179 |                   24 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.616 |  0.232 |                   22 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.868 |  0.181 |                   18 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  1.04  |  0.18  |                    1 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.18108811974525452
[2m[36m(func pid=102978)[0m mae:  0.133467435836792
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.097, 0.328, 0.108, 0.19, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.1728459894657135
[2m[36m(func pid=101124)[0m mae:  0.12015315145254135
[2m[36m(func pid=101124)[0m rmse_per_class: [0.107, 0.29, 0.043, 0.298, 0.056, 0.213, 0.308, 0.154, 0.163, 0.096]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.21061453223228455
[2m[36m(func pid=101740)[0m mae:  0.13492415845394135
[2m[36m(func pid=101740)[0m rmse_per_class: [0.107, 0.233, 0.065, 0.345, 0.124, 0.223, 0.264, 0.269, 0.132, 0.345]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.18035989999771118
[2m[36m(func pid=107141)[0m mae:  0.13282708823680878
[2m[36m(func pid=107141)[0m rmse_per_class: [0.113, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8581 | Steps: 2 | Val loss: 0.6638 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5525 | Steps: 2 | Val loss: 0.3856 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5737 | Steps: 2 | Val loss: 0.3536 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.9609 | Steps: 2 | Val loss: 0.6937 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 05:37:32 (running for 00:36:38.86)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.572 |  0.173 |                   25 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.539 |  0.211 |                   23 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.858 |  0.181 |                   19 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  1.014 |  0.18  |                    2 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.18107421696186066
[2m[36m(func pid=102978)[0m mae:  0.13345704972743988
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.328, 0.108, 0.19, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.1703047901391983
[2m[36m(func pid=101124)[0m mae:  0.11623096466064453
[2m[36m(func pid=101124)[0m rmse_per_class: [0.109, 0.246, 0.046, 0.304, 0.056, 0.24, 0.316, 0.152, 0.138, 0.095]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.1779041290283203
[2m[36m(func pid=101740)[0m mae:  0.11514946073293686
[2m[36m(func pid=101740)[0m rmse_per_class: [0.109, 0.211, 0.048, 0.283, 0.051, 0.218, 0.255, 0.209, 0.136, 0.261]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1805984526872635
[2m[36m(func pid=107141)[0m mae:  0.13303866982460022
[2m[36m(func pid=107141)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.104, 0.191, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8452 | Steps: 2 | Val loss: 0.6567 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5751 | Steps: 2 | Val loss: 0.3579 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5321 | Steps: 2 | Val loss: 0.4006 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8964 | Steps: 2 | Val loss: 0.6744 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 05:37:38 (running for 00:36:44.14)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.574 |  0.17  |                   26 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.552 |  0.178 |                   24 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.845 |  0.181 |                   20 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.961 |  0.181 |                    3 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.17572295665740967
[2m[36m(func pid=101124)[0m mae:  0.11566315591335297
[2m[36m(func pid=101124)[0m rmse_per_class: [0.109, 0.243, 0.047, 0.349, 0.056, 0.259, 0.319, 0.149, 0.133, 0.093]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18107099831104279
[2m[36m(func pid=102978)[0m mae:  0.13344897329807281
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.328, 0.108, 0.191, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.15731000900268555
[2m[36m(func pid=101740)[0m mae:  0.09889723360538483
[2m[36m(func pid=101740)[0m rmse_per_class: [0.099, 0.285, 0.044, 0.28, 0.055, 0.185, 0.267, 0.14, 0.134, 0.085]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1806705892086029
[2m[36m(func pid=107141)[0m mae:  0.13311074674129486
[2m[36m(func pid=107141)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.332, 0.105, 0.191, 0.301, 0.142, 0.143, 0.115]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6001 | Steps: 2 | Val loss: 0.3527 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5244 | Steps: 2 | Val loss: 0.3748 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8356 | Steps: 2 | Val loss: 0.6500 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8238 | Steps: 2 | Val loss: 0.6481 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 05:37:43 (running for 00:36:49.22)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.6   |  0.179 |                   28 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.532 |  0.157 |                   25 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.845 |  0.181 |                   20 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.896 |  0.181 |                    4 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.17889484763145447
[2m[36m(func pid=101124)[0m mae:  0.11552529036998749
[2m[36m(func pid=101124)[0m rmse_per_class: [0.11, 0.258, 0.048, 0.368, 0.056, 0.265, 0.317, 0.142, 0.133, 0.091]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.1584198772907257
[2m[36m(func pid=101740)[0m mae:  0.10259542614221573
[2m[36m(func pid=101740)[0m rmse_per_class: [0.164, 0.235, 0.043, 0.278, 0.056, 0.185, 0.296, 0.103, 0.133, 0.091]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18110734224319458
[2m[36m(func pid=102978)[0m mae:  0.13346794247627258
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.328, 0.108, 0.191, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.18066713213920593
[2m[36m(func pid=107141)[0m mae:  0.13311097025871277
[2m[36m(func pid=107141)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.331, 0.106, 0.191, 0.301, 0.141, 0.143, 0.114]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5667 | Steps: 2 | Val loss: 0.3394 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4986 | Steps: 2 | Val loss: 0.4029 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8247 | Steps: 2 | Val loss: 0.6433 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.7644 | Steps: 2 | Val loss: 0.6164 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=101124)[0m rmse: 0.1765945553779602
[2m[36m(func pid=101124)[0m mae:  0.11421746015548706
[2m[36m(func pid=101124)[0m rmse_per_class: [0.109, 0.27, 0.048, 0.355, 0.056, 0.257, 0.311, 0.129, 0.134, 0.096]
[2m[36m(func pid=101124)[0m 
== Status ==
Current time: 2024-01-07 05:37:48 (running for 00:36:54.42)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.567 |  0.177 |                   29 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.524 |  0.158 |                   26 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.836 |  0.181 |                   21 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.824 |  0.181 |                    5 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.1955307275056839
[2m[36m(func pid=101740)[0m mae:  0.1266367882490158
[2m[36m(func pid=101740)[0m rmse_per_class: [0.246, 0.209, 0.048, 0.341, 0.056, 0.324, 0.306, 0.135, 0.195, 0.095]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18111209571361542
[2m[36m(func pid=102978)[0m mae:  0.13346898555755615
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.108, 0.191, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.18059760332107544
[2m[36m(func pid=107141)[0m mae:  0.13305021822452545
[2m[36m(func pid=107141)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.331, 0.106, 0.191, 0.301, 0.141, 0.143, 0.113]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5575 | Steps: 2 | Val loss: 0.3368 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5018 | Steps: 2 | Val loss: 0.4264 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8131 | Steps: 2 | Val loss: 0.6365 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7009 | Steps: 2 | Val loss: 0.5826 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 05:37:53 (running for 00:36:59.72)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.567 |  0.177 |                   29 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.502 |  0.197 |                   28 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.825 |  0.181 |                   22 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.764 |  0.181 |                    6 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.17243145406246185
[2m[36m(func pid=101124)[0m mae:  0.1137101799249649
[2m[36m(func pid=101124)[0m rmse_per_class: [0.106, 0.278, 0.049, 0.312, 0.056, 0.227, 0.3, 0.127, 0.134, 0.134]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.19677546620368958
[2m[36m(func pid=101740)[0m mae:  0.12538465857505798
[2m[36m(func pid=101740)[0m rmse_per_class: [0.139, 0.234, 0.057, 0.35, 0.056, 0.367, 0.261, 0.147, 0.263, 0.094]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18110418319702148
[2m[36m(func pid=102978)[0m mae:  0.13345873355865479
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.107, 0.191, 0.305, 0.145, 0.142, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.18053871393203735
[2m[36m(func pid=107141)[0m mae:  0.13298536837100983
[2m[36m(func pid=107141)[0m rmse_per_class: [0.114, 0.264, 0.103, 0.331, 0.106, 0.191, 0.301, 0.14, 0.143, 0.113]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5837 | Steps: 2 | Val loss: 0.4855 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5325 | Steps: 2 | Val loss: 0.3553 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.8068 | Steps: 2 | Val loss: 0.6298 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.6493 | Steps: 2 | Val loss: 0.5482 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=101740)[0m rmse: 0.17922791838645935
[2m[36m(func pid=101740)[0m mae:  0.11200275272130966
[2m[36m(func pid=101740)[0m rmse_per_class: [0.096, 0.249, 0.053, 0.34, 0.056, 0.198, 0.333, 0.151, 0.224, 0.093]
[2m[36m(func pid=101740)[0m 
== Status ==
Current time: 2024-01-07 05:37:58 (running for 00:37:04.89)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.558 |  0.172 |                   30 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.584 |  0.179 |                   29 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.813 |  0.181 |                   23 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.701 |  0.181 |                    7 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.17856314778327942
[2m[36m(func pid=101124)[0m mae:  0.12060527503490448
[2m[36m(func pid=101124)[0m rmse_per_class: [0.1, 0.284, 0.05, 0.285, 0.056, 0.186, 0.293, 0.176, 0.134, 0.221]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18108974397182465
[2m[36m(func pid=102978)[0m mae:  0.13344548642635345
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.107, 0.191, 0.305, 0.145, 0.142, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1804007887840271
[2m[36m(func pid=107141)[0m mae:  0.1328541487455368
[2m[36m(func pid=107141)[0m rmse_per_class: [0.114, 0.264, 0.104, 0.331, 0.106, 0.191, 0.3, 0.14, 0.143, 0.112]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.6906 | Steps: 2 | Val loss: 0.5021 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5269 | Steps: 2 | Val loss: 0.3943 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7947 | Steps: 2 | Val loss: 0.6230 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6090 | Steps: 2 | Val loss: 0.5162 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 05:38:04 (running for 00:37:10.25)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.532 |  0.179 |                   31 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.691 |  0.191 |                   30 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.807 |  0.181 |                   24 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.649 |  0.18  |                    8 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.190516397356987
[2m[36m(func pid=101740)[0m mae:  0.12110620737075806
[2m[36m(func pid=101740)[0m rmse_per_class: [0.106, 0.262, 0.05, 0.333, 0.056, 0.177, 0.519, 0.153, 0.156, 0.094]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18107277154922485
[2m[36m(func pid=102978)[0m mae:  0.1334289014339447
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.107, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.1993485987186432
[2m[36m(func pid=101124)[0m mae:  0.13486862182617188
[2m[36m(func pid=101124)[0m rmse_per_class: [0.103, 0.289, 0.049, 0.331, 0.056, 0.178, 0.296, 0.246, 0.135, 0.31]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.18027475476264954
[2m[36m(func pid=107141)[0m mae:  0.13273274898529053
[2m[36m(func pid=107141)[0m rmse_per_class: [0.114, 0.264, 0.105, 0.331, 0.105, 0.19, 0.3, 0.14, 0.144, 0.111]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.6864 | Steps: 2 | Val loss: 0.4586 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7859 | Steps: 2 | Val loss: 0.6162 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.5576 | Steps: 2 | Val loss: 0.4406 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5760 | Steps: 2 | Val loss: 0.4872 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 05:38:09 (running for 00:37:15.34)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.527 |  0.199 |                   32 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.691 |  0.191 |                   30 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.786 |  0.181 |                   26 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.609 |  0.18  |                    9 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.1810556799173355
[2m[36m(func pid=102978)[0m mae:  0.1334165632724762
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.106, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.1810556799173355
[2m[36m(func pid=101740)[0m mae:  0.1130429282784462
[2m[36m(func pid=101740)[0m rmse_per_class: [0.1, 0.265, 0.054, 0.296, 0.056, 0.194, 0.43, 0.15, 0.169, 0.098]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.21335120499134064
[2m[36m(func pid=101124)[0m mae:  0.14444500207901
[2m[36m(func pid=101124)[0m rmse_per_class: [0.112, 0.292, 0.046, 0.365, 0.056, 0.193, 0.29, 0.289, 0.136, 0.353]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.180104598402977
[2m[36m(func pid=107141)[0m mae:  0.13257429003715515
[2m[36m(func pid=107141)[0m rmse_per_class: [0.114, 0.264, 0.105, 0.331, 0.105, 0.19, 0.299, 0.139, 0.144, 0.11]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7782 | Steps: 2 | Val loss: 0.6096 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.6095 | Steps: 2 | Val loss: 0.3845 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5763 | Steps: 2 | Val loss: 0.4751 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5494 | Steps: 2 | Val loss: 0.4619 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 05:38:14 (running for 00:37:20.34)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.558 |  0.213 |                   33 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.686 |  0.181 |                   31 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.778 |  0.181 |                   27 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.576 |  0.18  |                   10 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.18103118240833282
[2m[36m(func pid=102978)[0m mae:  0.13339205086231232
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.106, 0.191, 0.305, 0.146, 0.142, 0.119]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.17295587062835693
[2m[36m(func pid=101740)[0m mae:  0.1056080088019371
[2m[36m(func pid=101740)[0m rmse_per_class: [0.117, 0.263, 0.051, 0.336, 0.056, 0.193, 0.244, 0.14, 0.222, 0.107]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.2179098129272461
[2m[36m(func pid=101124)[0m mae:  0.14701959490776062
[2m[36m(func pid=101124)[0m rmse_per_class: [0.129, 0.293, 0.044, 0.378, 0.056, 0.206, 0.279, 0.303, 0.136, 0.354]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1799020767211914
[2m[36m(func pid=107141)[0m mae:  0.13239264488220215
[2m[36m(func pid=107141)[0m rmse_per_class: [0.115, 0.264, 0.106, 0.331, 0.104, 0.189, 0.298, 0.139, 0.145, 0.109]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.7715 | Steps: 2 | Val loss: 0.6038 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4979 | Steps: 2 | Val loss: 0.3986 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.5846 | Steps: 2 | Val loss: 0.4882 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5292 | Steps: 2 | Val loss: 0.4400 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 05:38:19 (running for 00:37:25.56)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.576 |  0.218 |                   34 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.609 |  0.173 |                   32 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.772 |  0.181 |                   28 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.549 |  0.18  |                   11 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.18101558089256287
[2m[36m(func pid=102978)[0m mae:  0.13337919116020203
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.106, 0.191, 0.305, 0.146, 0.142, 0.119]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.17633052170276642
[2m[36m(func pid=101740)[0m mae:  0.11340431869029999
[2m[36m(func pid=101740)[0m rmse_per_class: [0.163, 0.24, 0.042, 0.312, 0.056, 0.186, 0.287, 0.121, 0.227, 0.129]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.2173137664794922
[2m[36m(func pid=101124)[0m mae:  0.145648792386055
[2m[36m(func pid=101124)[0m rmse_per_class: [0.149, 0.293, 0.043, 0.382, 0.056, 0.214, 0.268, 0.298, 0.136, 0.333]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17967815697193146
[2m[36m(func pid=107141)[0m mae:  0.13220295310020447
[2m[36m(func pid=107141)[0m rmse_per_class: [0.115, 0.264, 0.107, 0.331, 0.103, 0.189, 0.297, 0.139, 0.145, 0.108]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5048 | Steps: 2 | Val loss: 0.3767 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7611 | Steps: 2 | Val loss: 0.5978 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6086 | Steps: 2 | Val loss: 0.4810 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5023 | Steps: 2 | Val loss: 0.4203 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 05:38:24 (running for 00:37:30.72)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.585 |  0.217 |                   35 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.505 |  0.167 |                   34 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.772 |  0.181 |                   28 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.529 |  0.18  |                   12 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.16703593730926514
[2m[36m(func pid=101740)[0m mae:  0.10722576081752777
[2m[36m(func pid=101740)[0m rmse_per_class: [0.161, 0.208, 0.05, 0.262, 0.056, 0.161, 0.303, 0.102, 0.164, 0.204]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18100032210350037
[2m[36m(func pid=102978)[0m mae:  0.13336218893527985
[2m[36m(func pid=102978)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.106, 0.191, 0.305, 0.146, 0.142, 0.119]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.214007169008255
[2m[36m(func pid=101124)[0m mae:  0.14232003688812256
[2m[36m(func pid=101124)[0m rmse_per_class: [0.172, 0.29, 0.042, 0.384, 0.056, 0.218, 0.26, 0.281, 0.135, 0.299]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17942896485328674
[2m[36m(func pid=107141)[0m mae:  0.13199511170387268
[2m[36m(func pid=107141)[0m rmse_per_class: [0.115, 0.264, 0.107, 0.332, 0.101, 0.189, 0.296, 0.139, 0.146, 0.107]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4944 | Steps: 2 | Val loss: 0.4435 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.7548 | Steps: 2 | Val loss: 0.5920 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5920 | Steps: 2 | Val loss: 0.4584 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 05:38:29 (running for 00:37:35.80)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.609 |  0.214 |                   36 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.494 |  0.176 |                   35 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.761 |  0.181 |                   29 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.502 |  0.179 |                   13 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.1761297583580017
[2m[36m(func pid=101740)[0m mae:  0.11323720216751099
[2m[36m(func pid=101740)[0m rmse_per_class: [0.094, 0.295, 0.052, 0.334, 0.054, 0.182, 0.291, 0.106, 0.13, 0.224]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4957 | Steps: 2 | Val loss: 0.4048 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=102978)[0m rmse: 0.18097655475139618
[2m[36m(func pid=102978)[0m mae:  0.1333424299955368
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.20910927653312683
[2m[36m(func pid=101124)[0m mae:  0.13789372146129608
[2m[36m(func pid=101124)[0m rmse_per_class: [0.203, 0.285, 0.043, 0.385, 0.056, 0.22, 0.254, 0.252, 0.134, 0.258]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17920145392417908
[2m[36m(func pid=107141)[0m mae:  0.13180971145629883
[2m[36m(func pid=107141)[0m rmse_per_class: [0.116, 0.264, 0.107, 0.332, 0.1, 0.188, 0.295, 0.138, 0.146, 0.107]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.5767 | Steps: 2 | Val loss: 0.4551 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.7470 | Steps: 2 | Val loss: 0.5859 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.5607 | Steps: 2 | Val loss: 0.4325 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 05:38:34 (running for 00:37:40.94)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.592 |  0.209 |                   37 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.577 |  0.186 |                   36 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.755 |  0.181 |                   30 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.496 |  0.179 |                   14 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.18639910221099854
[2m[36m(func pid=101740)[0m mae:  0.11354448646306992
[2m[36m(func pid=101740)[0m rmse_per_class: [0.109, 0.384, 0.069, 0.371, 0.051, 0.161, 0.249, 0.157, 0.133, 0.18]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4887 | Steps: 2 | Val loss: 0.3921 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=102978)[0m rmse: 0.18095891177654266
[2m[36m(func pid=102978)[0m mae:  0.1333296149969101
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.304, 0.146, 0.142, 0.119]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.20185963809490204
[2m[36m(func pid=101124)[0m mae:  0.13275811076164246
[2m[36m(func pid=101124)[0m rmse_per_class: [0.23, 0.273, 0.046, 0.385, 0.056, 0.22, 0.249, 0.215, 0.133, 0.212]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1789560317993164
[2m[36m(func pid=107141)[0m mae:  0.13161256909370422
[2m[36m(func pid=107141)[0m rmse_per_class: [0.116, 0.264, 0.107, 0.332, 0.098, 0.188, 0.294, 0.138, 0.146, 0.106]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5526 | Steps: 2 | Val loss: 0.4119 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7360 | Steps: 2 | Val loss: 0.5811 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.5335 | Steps: 2 | Val loss: 0.4115 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 05:38:39 (running for 00:37:46.10)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.561 |  0.202 |                   38 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.553 |  0.189 |                   37 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.747 |  0.181 |                   31 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.489 |  0.179 |                   15 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.189289391040802
[2m[36m(func pid=101740)[0m mae:  0.11101405322551727
[2m[36m(func pid=101740)[0m rmse_per_class: [0.108, 0.309, 0.092, 0.373, 0.086, 0.181, 0.253, 0.237, 0.131, 0.123]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18094401061534882
[2m[36m(func pid=102978)[0m mae:  0.133318692445755
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4767 | Steps: 2 | Val loss: 0.3816 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=101124)[0m rmse: 0.19193843007087708
[2m[36m(func pid=101124)[0m mae:  0.12729109823703766
[2m[36m(func pid=101124)[0m rmse_per_class: [0.241, 0.251, 0.051, 0.384, 0.056, 0.218, 0.243, 0.171, 0.141, 0.164]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17871889472007751
[2m[36m(func pid=107141)[0m mae:  0.1314188539981842
[2m[36m(func pid=107141)[0m rmse_per_class: [0.116, 0.264, 0.107, 0.333, 0.097, 0.188, 0.292, 0.138, 0.146, 0.106]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4938 | Steps: 2 | Val loss: 0.4244 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7352 | Steps: 2 | Val loss: 0.5757 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4994 | Steps: 2 | Val loss: 0.3947 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 05:38:45 (running for 00:37:51.25)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.533 |  0.192 |                   39 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.494 |  0.197 |                   38 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.736 |  0.181 |                   32 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.477 |  0.179 |                   16 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.19725483655929565
[2m[36m(func pid=101740)[0m mae:  0.11685657501220703
[2m[36m(func pid=101740)[0m rmse_per_class: [0.098, 0.222, 0.076, 0.345, 0.237, 0.194, 0.268, 0.219, 0.225, 0.087]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18091073632240295
[2m[36m(func pid=102978)[0m mae:  0.1332922875881195
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4620 | Steps: 2 | Val loss: 0.3722 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=101124)[0m rmse: 0.18092486262321472
[2m[36m(func pid=101124)[0m mae:  0.12225507199764252
[2m[36m(func pid=101124)[0m rmse_per_class: [0.21, 0.222, 0.055, 0.382, 0.056, 0.215, 0.233, 0.135, 0.177, 0.123]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1784854531288147
[2m[36m(func pid=107141)[0m mae:  0.13122588396072388
[2m[36m(func pid=107141)[0m rmse_per_class: [0.117, 0.264, 0.107, 0.333, 0.095, 0.188, 0.291, 0.138, 0.147, 0.105]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.7259 | Steps: 2 | Val loss: 0.5713 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.5015 | Steps: 2 | Val loss: 0.4493 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5063 | Steps: 2 | Val loss: 0.3905 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=102978)[0m rmse: 0.18090908229351044
[2m[36m(func pid=102978)[0m mae:  0.13328731060028076
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:38:50 (running for 00:37:56.30)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.499 |  0.181 |                   40 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.494 |  0.197 |                   38 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.726 |  0.181 |                   34 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.462 |  0.178 |                   17 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.2165720909833908
[2m[36m(func pid=101740)[0m mae:  0.1311507225036621
[2m[36m(func pid=101740)[0m rmse_per_class: [0.106, 0.269, 0.054, 0.317, 0.356, 0.201, 0.258, 0.129, 0.39, 0.086]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4694 | Steps: 2 | Val loss: 0.3654 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=101124)[0m rmse: 0.17739912867546082
[2m[36m(func pid=101124)[0m mae:  0.12112104892730713
[2m[36m(func pid=101124)[0m rmse_per_class: [0.145, 0.214, 0.067, 0.381, 0.056, 0.212, 0.226, 0.122, 0.254, 0.097]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7218 | Steps: 2 | Val loss: 0.5662 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4872 | Steps: 2 | Val loss: 0.4775 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=107141)[0m rmse: 0.1782776117324829
[2m[36m(func pid=107141)[0m mae:  0.13105817139148712
[2m[36m(func pid=107141)[0m rmse_per_class: [0.117, 0.264, 0.107, 0.334, 0.093, 0.188, 0.29, 0.137, 0.147, 0.105]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4737 | Steps: 2 | Val loss: 0.3914 | Batch size: 32 | lr: 0.01 | Duration: 2.63s
[2m[36m(func pid=102978)[0m rmse: 0.1808779537677765
[2m[36m(func pid=102978)[0m mae:  0.13326293230056763
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
== Status ==
Current time: 2024-01-07 05:38:55 (running for 00:38:01.39)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.506 |  0.177 |                   41 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.501 |  0.217 |                   39 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.722 |  0.181 |                   35 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.469 |  0.178 |                   18 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.21493804454803467
[2m[36m(func pid=101740)[0m mae:  0.13047067821025848
[2m[36m(func pid=101740)[0m rmse_per_class: [0.231, 0.289, 0.051, 0.283, 0.343, 0.194, 0.272, 0.108, 0.288, 0.091]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4596 | Steps: 2 | Val loss: 0.3594 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=101124)[0m rmse: 0.18436694145202637
[2m[36m(func pid=101124)[0m mae:  0.12510038912296295
[2m[36m(func pid=101124)[0m rmse_per_class: [0.096, 0.245, 0.072, 0.38, 0.056, 0.21, 0.235, 0.128, 0.33, 0.091]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.7151 | Steps: 2 | Val loss: 0.5614 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=107141)[0m rmse: 0.1781061589717865
[2m[36m(func pid=107141)[0m mae:  0.13091334700584412
[2m[36m(func pid=107141)[0m rmse_per_class: [0.118, 0.264, 0.107, 0.334, 0.092, 0.188, 0.289, 0.137, 0.147, 0.105]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5494 | Steps: 2 | Val loss: 0.5013 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4754 | Steps: 2 | Val loss: 0.3982 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 05:39:00 (running for 00:38:06.67)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.474 |  0.184 |                   42 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.549 |  0.196 |                   41 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.722 |  0.181 |                   35 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.46  |  0.178 |                   19 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.19559147953987122
[2m[36m(func pid=101740)[0m mae:  0.11902709305286407
[2m[36m(func pid=101740)[0m rmse_per_class: [0.245, 0.29, 0.049, 0.276, 0.28, 0.168, 0.261, 0.127, 0.167, 0.093]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18084485828876495
[2m[36m(func pid=102978)[0m mae:  0.13323654234409332
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4550 | Steps: 2 | Val loss: 0.3543 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=101124)[0m rmse: 0.19438427686691284
[2m[36m(func pid=101124)[0m mae:  0.13132712244987488
[2m[36m(func pid=101124)[0m rmse_per_class: [0.097, 0.301, 0.062, 0.379, 0.056, 0.208, 0.256, 0.138, 0.356, 0.092]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5812 | Steps: 2 | Val loss: 0.4246 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7066 | Steps: 2 | Val loss: 0.5565 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=107141)[0m rmse: 0.17792442440986633
[2m[36m(func pid=107141)[0m mae:  0.1307561695575714
[2m[36m(func pid=107141)[0m rmse_per_class: [0.118, 0.264, 0.107, 0.334, 0.09, 0.188, 0.289, 0.137, 0.147, 0.105]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5116 | Steps: 2 | Val loss: 0.4026 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 05:39:05 (running for 00:38:11.73)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.475 |  0.194 |                   43 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.549 |  0.196 |                   41 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.707 |  0.181 |                   37 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.455 |  0.178 |                   20 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.17933699488639832
[2m[36m(func pid=101740)[0m mae:  0.10503886640071869
[2m[36m(func pid=101740)[0m rmse_per_class: [0.168, 0.278, 0.05, 0.29, 0.221, 0.184, 0.239, 0.14, 0.13, 0.092]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18082915246486664
[2m[36m(func pid=102978)[0m mae:  0.1332213431596756
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4499 | Steps: 2 | Val loss: 0.3503 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=101124)[0m rmse: 0.19922421872615814
[2m[36m(func pid=101124)[0m mae:  0.1349014937877655
[2m[36m(func pid=101124)[0m rmse_per_class: [0.102, 0.352, 0.052, 0.376, 0.056, 0.204, 0.271, 0.144, 0.341, 0.094]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.5244 | Steps: 2 | Val loss: 0.3847 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.7010 | Steps: 2 | Val loss: 0.5522 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=107141)[0m rmse: 0.17776918411254883
[2m[36m(func pid=107141)[0m mae:  0.13062059879302979
[2m[36m(func pid=107141)[0m rmse_per_class: [0.118, 0.264, 0.107, 0.335, 0.089, 0.188, 0.288, 0.137, 0.148, 0.104]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5409 | Steps: 2 | Val loss: 0.3912 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 05:39:10 (running for 00:38:16.82)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.512 |  0.199 |                   44 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.581 |  0.179 |                   42 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.701 |  0.181 |                   38 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.45  |  0.178 |                   21 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.18080170452594757
[2m[36m(func pid=102978)[0m mae:  0.13320015370845795
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.17488594353199005
[2m[36m(func pid=101740)[0m mae:  0.10431106388568878
[2m[36m(func pid=101740)[0m rmse_per_class: [0.095, 0.233, 0.054, 0.277, 0.163, 0.294, 0.266, 0.146, 0.13, 0.09]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.19812336564064026
[2m[36m(func pid=101124)[0m mae:  0.1340152621269226
[2m[36m(func pid=101124)[0m rmse_per_class: [0.104, 0.383, 0.048, 0.368, 0.056, 0.195, 0.277, 0.147, 0.309, 0.095]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4442 | Steps: 2 | Val loss: 0.3469 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6920 | Steps: 2 | Val loss: 0.5475 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5012 | Steps: 2 | Val loss: 0.3963 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=107141)[0m rmse: 0.17762885987758636
[2m[36m(func pid=107141)[0m mae:  0.13050046563148499
[2m[36m(func pid=107141)[0m rmse_per_class: [0.119, 0.264, 0.107, 0.335, 0.088, 0.188, 0.287, 0.137, 0.148, 0.104]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5252 | Steps: 2 | Val loss: 0.3697 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 05:39:15 (running for 00:38:21.90)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.541 |  0.198 |                   45 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.524 |  0.175 |                   43 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.692 |  0.181 |                   39 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.444 |  0.178 |                   22 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.18076984584331512
[2m[36m(func pid=102978)[0m mae:  0.13317424058914185
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.1823020875453949
[2m[36m(func pid=101740)[0m mae:  0.10865770280361176
[2m[36m(func pid=101740)[0m rmse_per_class: [0.101, 0.269, 0.063, 0.337, 0.105, 0.302, 0.25, 0.145, 0.163, 0.088]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.19190499186515808
[2m[36m(func pid=101124)[0m mae:  0.12918075919151306
[2m[36m(func pid=101124)[0m rmse_per_class: [0.104, 0.385, 0.05, 0.351, 0.055, 0.183, 0.277, 0.149, 0.269, 0.095]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4395 | Steps: 2 | Val loss: 0.3438 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6906 | Steps: 2 | Val loss: 0.5432 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5613 | Steps: 2 | Val loss: 0.4613 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4854 | Steps: 2 | Val loss: 0.3411 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=107141)[0m rmse: 0.17748869955539703
[2m[36m(func pid=107141)[0m mae:  0.13037683069705963
[2m[36m(func pid=107141)[0m rmse_per_class: [0.119, 0.264, 0.108, 0.335, 0.086, 0.188, 0.287, 0.136, 0.148, 0.104]
[2m[36m(func pid=107141)[0m 
== Status ==
Current time: 2024-01-07 05:39:20 (running for 00:38:27.02)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.525 |  0.192 |                   46 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.501 |  0.182 |                   44 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.691 |  0.181 |                   40 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.44  |  0.177 |                   23 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.18073444068431854
[2m[36m(func pid=102978)[0m mae:  0.1331484466791153
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.17877544462680817
[2m[36m(func pid=101124)[0m mae:  0.11947640031576157
[2m[36m(func pid=101124)[0m rmse_per_class: [0.102, 0.348, 0.058, 0.318, 0.055, 0.168, 0.27, 0.151, 0.222, 0.096]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.19093057513237
[2m[36m(func pid=101740)[0m mae:  0.11522205173969269
[2m[36m(func pid=101740)[0m rmse_per_class: [0.106, 0.442, 0.058, 0.367, 0.072, 0.19, 0.244, 0.14, 0.205, 0.086]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4454 | Steps: 2 | Val loss: 0.3416 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6885 | Steps: 2 | Val loss: 0.5394 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4669 | Steps: 2 | Val loss: 0.3171 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=107141)[0m rmse: 0.1773507297039032
[2m[36m(func pid=107141)[0m mae:  0.13025601208209991
[2m[36m(func pid=107141)[0m rmse_per_class: [0.12, 0.263, 0.108, 0.335, 0.085, 0.188, 0.286, 0.136, 0.148, 0.104]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4929 | Steps: 2 | Val loss: 0.4823 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 05:39:26 (running for 00:38:32.15)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.485 |  0.179 |                   47 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.561 |  0.191 |                   45 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.688 |  0.181 |                   41 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.445 |  0.177 |                   24 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.18069033324718475
[2m[36m(func pid=102978)[0m mae:  0.13311369717121124
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.16103732585906982
[2m[36m(func pid=101124)[0m mae:  0.10748666524887085
[2m[36m(func pid=101124)[0m rmse_per_class: [0.099, 0.266, 0.065, 0.281, 0.054, 0.16, 0.263, 0.152, 0.174, 0.096]
[2m[36m(func pid=101740)[0m rmse: 0.1878422349691391
[2m[36m(func pid=101740)[0m mae:  0.11663250625133514
[2m[36m(func pid=101740)[0m rmse_per_class: [0.099, 0.426, 0.052, 0.367, 0.053, 0.169, 0.242, 0.128, 0.237, 0.105]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4441 | Steps: 2 | Val loss: 0.3398 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6791 | Steps: 2 | Val loss: 0.5353 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4506 | Steps: 2 | Val loss: 0.3074 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5330 | Steps: 2 | Val loss: 0.4127 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=107141)[0m rmse: 0.17722374200820923
[2m[36m(func pid=107141)[0m mae:  0.13014797866344452
[2m[36m(func pid=107141)[0m rmse_per_class: [0.12, 0.263, 0.108, 0.335, 0.084, 0.188, 0.285, 0.136, 0.148, 0.104]
[2m[36m(func pid=107141)[0m 
== Status ==
Current time: 2024-01-07 05:39:31 (running for 00:38:37.21)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.467 |  0.161 |                   48 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.493 |  0.188 |                   46 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.679 |  0.181 |                   42 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.444 |  0.177 |                   25 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.1806638538837433
[2m[36m(func pid=102978)[0m mae:  0.13309352099895477
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.144, 0.143, 0.118]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.15447761118412018
[2m[36m(func pid=101124)[0m mae:  0.10142336785793304
[2m[36m(func pid=101124)[0m rmse_per_class: [0.096, 0.213, 0.057, 0.308, 0.053, 0.166, 0.262, 0.153, 0.142, 0.096]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.17992350459098816
[2m[36m(func pid=101740)[0m mae:  0.11022752523422241
[2m[36m(func pid=101740)[0m rmse_per_class: [0.161, 0.281, 0.057, 0.345, 0.05, 0.193, 0.24, 0.114, 0.197, 0.16]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4434 | Steps: 2 | Val loss: 0.3383 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6627 | Steps: 2 | Val loss: 0.5308 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4507 | Steps: 2 | Val loss: 0.3113 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5070 | Steps: 2 | Val loss: 0.4038 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=107141)[0m rmse: 0.177103191614151
[2m[36m(func pid=107141)[0m mae:  0.1300368756055832
[2m[36m(func pid=107141)[0m rmse_per_class: [0.121, 0.263, 0.108, 0.336, 0.083, 0.188, 0.285, 0.136, 0.148, 0.104]
[2m[36m(func pid=107141)[0m 
== Status ==
Current time: 2024-01-07 05:39:36 (running for 00:38:42.23)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.451 |  0.154 |                   49 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.533 |  0.18  |                   47 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.663 |  0.181 |                   43 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.443 |  0.177 |                   26 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.18067112565040588
[2m[36m(func pid=102978)[0m mae:  0.13309165835380554
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.329, 0.104, 0.191, 0.303, 0.145, 0.143, 0.117]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.1770905703306198
[2m[36m(func pid=101740)[0m mae:  0.10731397569179535
[2m[36m(func pid=101740)[0m rmse_per_class: [0.328, 0.217, 0.065, 0.293, 0.052, 0.194, 0.25, 0.108, 0.138, 0.127]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.1588108390569687
[2m[36m(func pid=101124)[0m mae:  0.10228253901004791
[2m[36m(func pid=101124)[0m rmse_per_class: [0.095, 0.224, 0.045, 0.348, 0.052, 0.178, 0.263, 0.153, 0.134, 0.096]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4424 | Steps: 2 | Val loss: 0.3369 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.6628 | Steps: 2 | Val loss: 0.5268 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4650 | Steps: 2 | Val loss: 0.3196 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5117 | Steps: 2 | Val loss: 0.4121 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=107141)[0m rmse: 0.17699523270130157
[2m[36m(func pid=107141)[0m mae:  0.12993481755256653
[2m[36m(func pid=107141)[0m rmse_per_class: [0.121, 0.263, 0.108, 0.336, 0.082, 0.188, 0.285, 0.136, 0.148, 0.103]
[2m[36m(func pid=107141)[0m 
== Status ==
Current time: 2024-01-07 05:39:41 (running for 00:38:47.24)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.451 |  0.159 |                   50 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.507 |  0.177 |                   48 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.663 |  0.181 |                   44 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.442 |  0.177 |                   27 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.18065539002418518
[2m[36m(func pid=102978)[0m mae:  0.13307245075702667
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.303, 0.145, 0.143, 0.117]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.17378613352775574
[2m[36m(func pid=101740)[0m mae:  0.10327331721782684
[2m[36m(func pid=101740)[0m rmse_per_class: [0.282, 0.254, 0.067, 0.287, 0.053, 0.177, 0.249, 0.131, 0.131, 0.107]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.16219396889209747
[2m[36m(func pid=101124)[0m mae:  0.10400497913360596
[2m[36m(func pid=101124)[0m rmse_per_class: [0.098, 0.246, 0.045, 0.344, 0.053, 0.193, 0.26, 0.153, 0.133, 0.096]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4411 | Steps: 2 | Val loss: 0.3359 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6652 | Steps: 2 | Val loss: 0.5229 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5551 | Steps: 2 | Val loss: 0.4081 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=107141)[0m rmse: 0.17693129181861877
[2m[36m(func pid=107141)[0m mae:  0.12985894083976746
[2m[36m(func pid=107141)[0m rmse_per_class: [0.122, 0.263, 0.109, 0.336, 0.081, 0.188, 0.284, 0.136, 0.147, 0.103]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4728 | Steps: 2 | Val loss: 0.3249 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 05:39:46 (running for 00:38:52.39)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.465 |  0.162 |                   51 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.512 |  0.174 |                   49 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.665 |  0.181 |                   45 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.441 |  0.177 |                   28 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=102978)[0m rmse: 0.1806076467037201
[2m[36m(func pid=102978)[0m mae:  0.13304618000984192
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.303, 0.144, 0.143, 0.117]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.17348788678646088
[2m[36m(func pid=101740)[0m mae:  0.10032875835895538
[2m[36m(func pid=101740)[0m rmse_per_class: [0.158, 0.275, 0.074, 0.347, 0.053, 0.153, 0.261, 0.181, 0.132, 0.102]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.16429413855075836
[2m[36m(func pid=101124)[0m mae:  0.10569597780704498
[2m[36m(func pid=101124)[0m rmse_per_class: [0.124, 0.261, 0.047, 0.305, 0.061, 0.21, 0.251, 0.153, 0.134, 0.096]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4400 | Steps: 2 | Val loss: 0.3349 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.6590 | Steps: 2 | Val loss: 0.5192 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5454 | Steps: 2 | Val loss: 0.4064 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=107141)[0m rmse: 0.1768721044063568
[2m[36m(func pid=107141)[0m mae:  0.12979336082935333
[2m[36m(func pid=107141)[0m rmse_per_class: [0.122, 0.263, 0.109, 0.336, 0.08, 0.188, 0.284, 0.136, 0.147, 0.103]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4844 | Steps: 2 | Val loss: 0.3313 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=102978)[0m rmse: 0.1805923879146576
[2m[36m(func pid=102978)[0m mae:  0.13303005695343018
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:39:52 (running for 00:38:58.24)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.473 |  0.164 |                   52 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.545 |  0.177 |                   51 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.659 |  0.181 |                   46 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.44  |  0.177 |                   29 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.17666015028953552
[2m[36m(func pid=101740)[0m mae:  0.10261225700378418
[2m[36m(func pid=101740)[0m rmse_per_class: [0.097, 0.278, 0.071, 0.321, 0.053, 0.236, 0.278, 0.205, 0.131, 0.096]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.17003896832466125
[2m[36m(func pid=101124)[0m mae:  0.10913433879613876
[2m[36m(func pid=101124)[0m rmse_per_class: [0.185, 0.27, 0.048, 0.272, 0.079, 0.22, 0.243, 0.153, 0.134, 0.096]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4369 | Steps: 2 | Val loss: 0.3340 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.6450 | Steps: 2 | Val loss: 0.5145 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5505 | Steps: 2 | Val loss: 0.4226 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=107141)[0m rmse: 0.17679999768733978
[2m[36m(func pid=107141)[0m mae:  0.1297149360179901
[2m[36m(func pid=107141)[0m rmse_per_class: [0.123, 0.263, 0.109, 0.336, 0.079, 0.188, 0.284, 0.136, 0.148, 0.103]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4850 | Steps: 2 | Val loss: 0.3418 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=102978)[0m rmse: 0.18057705461978912
[2m[36m(func pid=102978)[0m mae:  0.13301265239715576
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:39:57 (running for 00:39:03.49)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.484 |  0.17  |                   53 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.551 |  0.184 |                   52 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.645 |  0.181 |                   47 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.437 |  0.177 |                   30 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.18415433168411255
[2m[36m(func pid=101740)[0m mae:  0.11319024860858917
[2m[36m(func pid=101740)[0m rmse_per_class: [0.099, 0.274, 0.06, 0.292, 0.053, 0.366, 0.28, 0.127, 0.198, 0.093]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4381 | Steps: 2 | Val loss: 0.3333 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=101124)[0m rmse: 0.18020455539226532
[2m[36m(func pid=101124)[0m mae:  0.1146143302321434
[2m[36m(func pid=101124)[0m rmse_per_class: [0.25, 0.275, 0.049, 0.281, 0.109, 0.221, 0.235, 0.152, 0.134, 0.096]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.6551 | Steps: 2 | Val loss: 0.5117 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5434 | Steps: 2 | Val loss: 0.4204 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=107141)[0m rmse: 0.17674078047275543
[2m[36m(func pid=107141)[0m mae:  0.12965497374534607
[2m[36m(func pid=107141)[0m rmse_per_class: [0.123, 0.262, 0.109, 0.336, 0.078, 0.188, 0.283, 0.136, 0.148, 0.103]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4917 | Steps: 2 | Val loss: 0.3529 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=102978)[0m rmse: 0.18052956461906433
[2m[36m(func pid=102978)[0m mae:  0.13298189640045166
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.1874033510684967
[2m[36m(func pid=101740)[0m mae:  0.11656459420919418
[2m[36m(func pid=101740)[0m rmse_per_class: [0.102, 0.255, 0.053, 0.324, 0.051, 0.308, 0.274, 0.106, 0.306, 0.094]
== Status ==
Current time: 2024-01-07 05:40:02 (running for 00:39:08.81)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.485 |  0.18  |                   54 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.543 |  0.187 |                   53 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.655 |  0.181 |                   48 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.438 |  0.177 |                   31 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.18985138833522797
[2m[36m(func pid=101124)[0m mae:  0.12009932100772858
[2m[36m(func pid=101124)[0m rmse_per_class: [0.287, 0.277, 0.049, 0.313, 0.15, 0.212, 0.23, 0.151, 0.134, 0.096]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4362 | Steps: 2 | Val loss: 0.3326 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.6495 | Steps: 2 | Val loss: 0.5088 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4941 | Steps: 2 | Val loss: 0.3923 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=107141)[0m rmse: 0.1766619235277176
[2m[36m(func pid=107141)[0m mae:  0.12957945466041565
[2m[36m(func pid=107141)[0m rmse_per_class: [0.124, 0.262, 0.109, 0.336, 0.077, 0.189, 0.283, 0.136, 0.148, 0.103]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4864 | Steps: 2 | Val loss: 0.3621 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=102978)[0m rmse: 0.18048468232154846
[2m[36m(func pid=102978)[0m mae:  0.13295304775238037
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:40:07 (running for 00:39:13.83)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.492 |  0.19  |                   55 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.494 |  0.172 |                   54 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.65  |  0.18  |                   49 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.436 |  0.177 |                   32 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.17154256999492645
[2m[36m(func pid=101740)[0m mae:  0.10640140622854233
[2m[36m(func pid=101740)[0m rmse_per_class: [0.098, 0.221, 0.049, 0.334, 0.05, 0.178, 0.289, 0.111, 0.287, 0.098]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.19654282927513123
[2m[36m(func pid=101124)[0m mae:  0.12418671697378159
[2m[36m(func pid=101124)[0m rmse_per_class: [0.296, 0.276, 0.049, 0.34, 0.199, 0.197, 0.231, 0.149, 0.133, 0.095]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4340 | Steps: 2 | Val loss: 0.3319 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.6440 | Steps: 2 | Val loss: 0.5064 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4810 | Steps: 2 | Val loss: 0.3915 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=107141)[0m rmse: 0.1765912026166916
[2m[36m(func pid=107141)[0m mae:  0.1295175403356552
[2m[36m(func pid=107141)[0m rmse_per_class: [0.124, 0.262, 0.109, 0.336, 0.077, 0.189, 0.283, 0.136, 0.148, 0.103]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4749 | Steps: 2 | Val loss: 0.3671 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=102978)[0m rmse: 0.18046623468399048
[2m[36m(func pid=102978)[0m mae:  0.13294020295143127
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:40:13 (running for 00:39:19.34)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.486 |  0.197 |                   56 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.481 |  0.173 |                   55 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.644 |  0.18  |                   50 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.434 |  0.177 |                   33 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.1729779988527298
[2m[36m(func pid=101740)[0m mae:  0.10262350738048553
[2m[36m(func pid=101740)[0m rmse_per_class: [0.238, 0.245, 0.049, 0.338, 0.051, 0.156, 0.256, 0.118, 0.18, 0.098]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.19908449053764343
[2m[36m(func pid=101124)[0m mae:  0.1259775012731552
[2m[36m(func pid=101124)[0m rmse_per_class: [0.276, 0.273, 0.049, 0.355, 0.252, 0.181, 0.233, 0.145, 0.132, 0.094]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4347 | Steps: 2 | Val loss: 0.3315 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6287 | Steps: 2 | Val loss: 0.5025 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5687 | Steps: 2 | Val loss: 0.4482 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=107141)[0m rmse: 0.17653068900108337
[2m[36m(func pid=107141)[0m mae:  0.12945319712162018
[2m[36m(func pid=107141)[0m rmse_per_class: [0.124, 0.262, 0.109, 0.336, 0.076, 0.189, 0.282, 0.136, 0.148, 0.103]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4674 | Steps: 2 | Val loss: 0.3732 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=102978)[0m rmse: 0.18044358491897583
[2m[36m(func pid=102978)[0m mae:  0.1329154372215271
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:40:18 (running for 00:39:24.65)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.475 |  0.199 |                   57 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.569 |  0.18  |                   56 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.629 |  0.18  |                   51 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.435 |  0.177 |                   34 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.17958590388298035
[2m[36m(func pid=101740)[0m mae:  0.10547707229852676
[2m[36m(func pid=101740)[0m rmse_per_class: [0.261, 0.294, 0.049, 0.32, 0.065, 0.175, 0.247, 0.124, 0.149, 0.111]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.1992013156414032
[2m[36m(func pid=101124)[0m mae:  0.1268444061279297
[2m[36m(func pid=101124)[0m rmse_per_class: [0.238, 0.266, 0.049, 0.364, 0.3, 0.171, 0.24, 0.139, 0.133, 0.092]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4344 | Steps: 2 | Val loss: 0.3310 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6273 | Steps: 2 | Val loss: 0.5008 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5069 | Steps: 2 | Val loss: 0.4893 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=107141)[0m rmse: 0.1764761507511139
[2m[36m(func pid=107141)[0m mae:  0.1293855458498001
[2m[36m(func pid=107141)[0m rmse_per_class: [0.124, 0.262, 0.11, 0.336, 0.075, 0.188, 0.282, 0.136, 0.147, 0.103]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4397 | Steps: 2 | Val loss: 0.3783 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=102978)[0m rmse: 0.18044328689575195
[2m[36m(func pid=102978)[0m mae:  0.1329171359539032
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.116]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:40:23 (running for 00:39:29.71)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.467 |  0.199 |                   58 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.507 |  0.178 |                   57 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.627 |  0.18  |                   52 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.434 |  0.176 |                   35 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.17849227786064148
[2m[36m(func pid=101740)[0m mae:  0.10650648176670074
[2m[36m(func pid=101740)[0m rmse_per_class: [0.192, 0.296, 0.05, 0.293, 0.085, 0.192, 0.25, 0.129, 0.144, 0.153]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.1978537142276764
[2m[36m(func pid=101124)[0m mae:  0.12707528471946716
[2m[36m(func pid=101124)[0m rmse_per_class: [0.185, 0.256, 0.061, 0.37, 0.329, 0.17, 0.245, 0.128, 0.146, 0.089]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4297 | Steps: 2 | Val loss: 0.3303 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.6300 | Steps: 2 | Val loss: 0.4983 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5141 | Steps: 2 | Val loss: 0.4382 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=107141)[0m rmse: 0.1764189898967743
[2m[36m(func pid=107141)[0m mae:  0.1292872130870819
[2m[36m(func pid=107141)[0m rmse_per_class: [0.125, 0.262, 0.11, 0.337, 0.074, 0.188, 0.282, 0.136, 0.147, 0.103]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18041320145130157
[2m[36m(func pid=102978)[0m mae:  0.13289774954319
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.302, 0.143, 0.143, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4255 | Steps: 2 | Val loss: 0.3816 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 05:40:28 (running for 00:39:34.80)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.44  |  0.198 |                   59 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.514 |  0.174 |                   58 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.63  |  0.18  |                   53 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.43  |  0.176 |                   36 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.17369772493839264
[2m[36m(func pid=101740)[0m mae:  0.10426042973995209
[2m[36m(func pid=101740)[0m rmse_per_class: [0.132, 0.251, 0.044, 0.297, 0.106, 0.193, 0.247, 0.133, 0.139, 0.194]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.19650784134864807
[2m[36m(func pid=101124)[0m mae:  0.12773148715496063
[2m[36m(func pid=101124)[0m rmse_per_class: [0.131, 0.244, 0.069, 0.374, 0.331, 0.177, 0.245, 0.112, 0.195, 0.087]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4342 | Steps: 2 | Val loss: 0.3298 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.6278 | Steps: 2 | Val loss: 0.4956 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4738 | Steps: 2 | Val loss: 0.3947 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=107141)[0m rmse: 0.1763090044260025
[2m[36m(func pid=107141)[0m mae:  0.12918129563331604
[2m[36m(func pid=107141)[0m rmse_per_class: [0.125, 0.262, 0.11, 0.336, 0.074, 0.188, 0.282, 0.136, 0.147, 0.103]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18036995828151703
[2m[36m(func pid=102978)[0m mae:  0.13286849856376648
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.103, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4255 | Steps: 2 | Val loss: 0.3859 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 05:40:33 (running for 00:39:39.92)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.425 |  0.197 |                   60 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.474 |  0.171 |                   59 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.628 |  0.18  |                   54 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.434 |  0.176 |                   37 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.17120680212974548
[2m[36m(func pid=101740)[0m mae:  0.10205356031656265
[2m[36m(func pid=101740)[0m rmse_per_class: [0.098, 0.217, 0.063, 0.294, 0.136, 0.172, 0.242, 0.134, 0.153, 0.203]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.19860097765922546
[2m[36m(func pid=101124)[0m mae:  0.1303205043077469
[2m[36m(func pid=101124)[0m rmse_per_class: [0.104, 0.232, 0.071, 0.377, 0.304, 0.188, 0.255, 0.101, 0.265, 0.089]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.6245 | Steps: 2 | Val loss: 0.4930 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4319 | Steps: 2 | Val loss: 0.3293 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4657 | Steps: 2 | Val loss: 0.4124 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=102978)[0m rmse: 0.1803363859653473
[2m[36m(func pid=102978)[0m mae:  0.13284099102020264
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17618414759635925
[2m[36m(func pid=107141)[0m mae:  0.12906192243099213
[2m[36m(func pid=107141)[0m rmse_per_class: [0.125, 0.262, 0.11, 0.336, 0.073, 0.188, 0.281, 0.136, 0.147, 0.103]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4108 | Steps: 2 | Val loss: 0.3903 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 05:40:38 (running for 00:39:45.02)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.425 |  0.199 |                   61 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.466 |  0.176 |                   60 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.624 |  0.18  |                   55 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.432 |  0.176 |                   38 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.1757434904575348
[2m[36m(func pid=101740)[0m mae:  0.10602583736181259
[2m[36m(func pid=101740)[0m rmse_per_class: [0.097, 0.25, 0.063, 0.282, 0.172, 0.186, 0.241, 0.128, 0.182, 0.157]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.20009978115558624
[2m[36m(func pid=101124)[0m mae:  0.13278083503246307
[2m[36m(func pid=101124)[0m rmse_per_class: [0.098, 0.225, 0.065, 0.379, 0.251, 0.199, 0.273, 0.103, 0.304, 0.105]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.6229 | Steps: 2 | Val loss: 0.4904 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4283 | Steps: 2 | Val loss: 0.3287 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4707 | Steps: 2 | Val loss: 0.4462 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=107141)[0m rmse: 0.17605671286582947
[2m[36m(func pid=107141)[0m mae:  0.12891387939453125
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.262, 0.11, 0.336, 0.073, 0.188, 0.281, 0.136, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.18030588328838348
[2m[36m(func pid=102978)[0m mae:  0.13281996548175812
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4186 | Steps: 2 | Val loss: 0.3990 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 05:40:43 (running for 00:39:50.06)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.411 |  0.2   |                   62 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.471 |  0.189 |                   61 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.623 |  0.18  |                   56 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.428 |  0.176 |                   39 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.18906912207603455
[2m[36m(func pid=101740)[0m mae:  0.11392228305339813
[2m[36m(func pid=101740)[0m rmse_per_class: [0.096, 0.275, 0.071, 0.299, 0.219, 0.241, 0.249, 0.115, 0.197, 0.128]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4296 | Steps: 2 | Val loss: 0.3281 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=101124)[0m rmse: 0.20033606886863708
[2m[36m(func pid=101124)[0m mae:  0.13492551445960999
[2m[36m(func pid=101124)[0m rmse_per_class: [0.099, 0.223, 0.058, 0.38, 0.195, 0.207, 0.29, 0.116, 0.296, 0.139]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6163 | Steps: 2 | Val loss: 0.4874 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4967 | Steps: 2 | Val loss: 0.4692 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=102978)[0m rmse: 0.18028324842453003
[2m[36m(func pid=102978)[0m mae:  0.13279494643211365
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17590641975402832
[2m[36m(func pid=107141)[0m mae:  0.12878191471099854
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.262, 0.11, 0.336, 0.072, 0.188, 0.281, 0.136, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4305 | Steps: 2 | Val loss: 0.4097 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 05:40:49 (running for 00:39:55.18)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.419 |  0.2   |                   63 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.497 |  0.201 |                   62 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.616 |  0.18  |                   57 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.43  |  0.176 |                   40 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.20081022381782532
[2m[36m(func pid=101740)[0m mae:  0.11561518907546997
[2m[36m(func pid=101740)[0m rmse_per_class: [0.191, 0.283, 0.077, 0.314, 0.276, 0.18, 0.257, 0.12, 0.196, 0.115]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.19916677474975586
[2m[36m(func pid=101124)[0m mae:  0.13622446358203888
[2m[36m(func pid=101124)[0m rmse_per_class: [0.1, 0.228, 0.053, 0.381, 0.144, 0.212, 0.302, 0.133, 0.255, 0.183]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.6134 | Steps: 2 | Val loss: 0.4850 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4285 | Steps: 2 | Val loss: 0.3277 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5070 | Steps: 2 | Val loss: 0.4738 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=102978)[0m rmse: 0.18024948239326477
[2m[36m(func pid=102978)[0m mae:  0.13277065753936768
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17581385374069214
[2m[36m(func pid=107141)[0m mae:  0.128681942820549
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.261, 0.11, 0.335, 0.072, 0.188, 0.281, 0.136, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4372 | Steps: 2 | Val loss: 0.4178 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=101740)[0m rmse: 0.2078590840101242
[2m[36m(func pid=101740)[0m mae:  0.11830268800258636
[2m[36m(func pid=101740)[0m rmse_per_class: [0.291, 0.278, 0.055, 0.322, 0.285, 0.165, 0.268, 0.16, 0.152, 0.102]
== Status ==
Current time: 2024-01-07 05:40:54 (running for 00:40:00.25)
Memory usage on this node: 24.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.431 |  0.199 |                   64 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.507 |  0.208 |                   63 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.613 |  0.18  |                   58 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.428 |  0.176 |                   41 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.19713585078716278
[2m[36m(func pid=101124)[0m mae:  0.13650567829608917
[2m[36m(func pid=101124)[0m rmse_per_class: [0.1, 0.237, 0.048, 0.38, 0.107, 0.215, 0.308, 0.15, 0.204, 0.223]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6060 | Steps: 2 | Val loss: 0.4832 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4286 | Steps: 2 | Val loss: 0.3273 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4792 | Steps: 2 | Val loss: 0.4548 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=102978)[0m rmse: 0.1802332103252411
[2m[36m(func pid=102978)[0m mae:  0.1327572464942932
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4467 | Steps: 2 | Val loss: 0.4160 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=107141)[0m rmse: 0.17570939660072327
[2m[36m(func pid=107141)[0m mae:  0.1285768300294876
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.261, 0.11, 0.335, 0.071, 0.188, 0.28, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
== Status ==
Current time: 2024-01-07 05:40:59 (running for 00:40:05.33)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.437 |  0.197 |                   65 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.479 |  0.205 |                   64 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.606 |  0.18  |                   59 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.429 |  0.176 |                   42 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.20512959361076355
[2m[36m(func pid=101740)[0m mae:  0.11731298267841339
[2m[36m(func pid=101740)[0m rmse_per_class: [0.293, 0.266, 0.046, 0.317, 0.222, 0.179, 0.294, 0.199, 0.139, 0.096]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.19491614401340485
[2m[36m(func pid=101124)[0m mae:  0.13547223806381226
[2m[36m(func pid=101124)[0m rmse_per_class: [0.098, 0.246, 0.045, 0.377, 0.082, 0.216, 0.309, 0.164, 0.166, 0.246]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6036 | Steps: 2 | Val loss: 0.4814 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4635 | Steps: 2 | Val loss: 0.4081 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4304 | Steps: 2 | Val loss: 0.3270 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=102978)[0m rmse: 0.18023739755153656
[2m[36m(func pid=102978)[0m mae:  0.13275623321533203
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.301, 0.143, 0.143, 0.116]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:41:04 (running for 00:40:10.53)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.447 |  0.195 |                   66 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.464 |  0.187 |                   65 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.604 |  0.18  |                   60 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.429 |  0.176 |                   42 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4526 | Steps: 2 | Val loss: 0.3987 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=101740)[0m rmse: 0.1867777705192566
[2m[36m(func pid=101740)[0m mae:  0.10839823633432388
[2m[36m(func pid=101740)[0m rmse_per_class: [0.21, 0.236, 0.043, 0.296, 0.14, 0.184, 0.304, 0.217, 0.142, 0.098]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17559939622879028
[2m[36m(func pid=107141)[0m mae:  0.12849009037017822
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.261, 0.11, 0.335, 0.071, 0.188, 0.28, 0.136, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6051 | Steps: 2 | Val loss: 0.4784 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=101124)[0m rmse: 0.1915915459394455
[2m[36m(func pid=101124)[0m mae:  0.13249778747558594
[2m[36m(func pid=101124)[0m rmse_per_class: [0.097, 0.254, 0.046, 0.372, 0.066, 0.215, 0.305, 0.177, 0.145, 0.239]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4359 | Steps: 2 | Val loss: 0.3647 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4265 | Steps: 2 | Val loss: 0.3265 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=102978)[0m rmse: 0.18022139370441437
[2m[36m(func pid=102978)[0m mae:  0.13274399936199188
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.3, 0.143, 0.143, 0.116]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:41:09 (running for 00:40:15.66)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.453 |  0.192 |                   67 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.436 |  0.166 |                   66 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.605 |  0.18  |                   61 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.43  |  0.176 |                   43 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.16613230109214783
[2m[36m(func pid=101740)[0m mae:  0.09749988466501236
[2m[36m(func pid=101740)[0m rmse_per_class: [0.136, 0.208, 0.048, 0.285, 0.077, 0.171, 0.269, 0.209, 0.163, 0.096]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4409 | Steps: 2 | Val loss: 0.3685 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=107141)[0m rmse: 0.17547498643398285
[2m[36m(func pid=107141)[0m mae:  0.12840786576271057
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.261, 0.109, 0.335, 0.071, 0.188, 0.28, 0.136, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5974 | Steps: 2 | Val loss: 0.4764 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=101124)[0m rmse: 0.18610814213752747
[2m[36m(func pid=101124)[0m mae:  0.12723436951637268
[2m[36m(func pid=101124)[0m rmse_per_class: [0.103, 0.256, 0.05, 0.362, 0.057, 0.213, 0.298, 0.184, 0.136, 0.205]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4030 | Steps: 2 | Val loss: 0.3630 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4251 | Steps: 2 | Val loss: 0.3260 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=102978)[0m rmse: 0.18020078539848328
[2m[36m(func pid=102978)[0m mae:  0.13273224234580994
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.3, 0.143, 0.144, 0.116]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.16030044853687286
[2m[36m(func pid=101740)[0m mae:  0.09442368894815445
== Status ==
Current time: 2024-01-07 05:41:14 (running for 00:40:21.00)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.441 |  0.186 |                   68 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.403 |  0.16  |                   67 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.597 |  0.18  |                   62 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.426 |  0.175 |                   44 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=101740)[0m rmse_per_class: [0.106, 0.231, 0.053, 0.291, 0.053, 0.164, 0.267, 0.17, 0.176, 0.092]

[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4134 | Steps: 2 | Val loss: 0.3356 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17540571093559265
[2m[36m(func pid=107141)[0m mae:  0.12835630774497986
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.261, 0.109, 0.335, 0.071, 0.188, 0.28, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5974 | Steps: 2 | Val loss: 0.4742 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=101124)[0m rmse: 0.17944617569446564
[2m[36m(func pid=101124)[0m mae:  0.11992937326431274
[2m[36m(func pid=101124)[0m rmse_per_class: [0.139, 0.239, 0.053, 0.343, 0.053, 0.207, 0.284, 0.19, 0.132, 0.155]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4505 | Steps: 2 | Val loss: 0.3978 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4241 | Steps: 2 | Val loss: 0.3257 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=102978)[0m rmse: 0.18015098571777344
[2m[36m(func pid=102978)[0m mae:  0.13269828259944916
[2m[36m(func pid=102978)[0m rmse_per_class: [0.114, 0.264, 0.099, 0.33, 0.102, 0.191, 0.3, 0.143, 0.144, 0.115]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:41:20 (running for 00:40:26.29)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.413 |  0.179 |                   69 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.45  |  0.164 |                   68 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.597 |  0.18  |                   63 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.425 |  0.175 |                   45 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.16421076655387878
[2m[36m(func pid=101740)[0m mae:  0.09815472364425659
[2m[36m(func pid=101740)[0m rmse_per_class: [0.102, 0.283, 0.049, 0.288, 0.05, 0.211, 0.278, 0.122, 0.169, 0.09]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1753959357738495
[2m[36m(func pid=107141)[0m mae:  0.12830108404159546
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.261, 0.109, 0.335, 0.07, 0.188, 0.28, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3917 | Steps: 2 | Val loss: 0.3100 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5945 | Steps: 2 | Val loss: 0.4721 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=101124)[0m rmse: 0.17232170701026917
[2m[36m(func pid=101124)[0m mae:  0.11240182816982269
[2m[36m(func pid=101124)[0m rmse_per_class: [0.202, 0.215, 0.052, 0.315, 0.051, 0.199, 0.27, 0.171, 0.131, 0.116]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4939 | Steps: 2 | Val loss: 0.4165 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4253 | Steps: 2 | Val loss: 0.3254 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=102978)[0m rmse: 0.18012754619121552
[2m[36m(func pid=102978)[0m mae:  0.1326800137758255
[2m[36m(func pid=102978)[0m rmse_per_class: [0.115, 0.264, 0.099, 0.33, 0.101, 0.191, 0.3, 0.142, 0.144, 0.115]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:41:25 (running for 00:40:31.33)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.392 |  0.172 |                   70 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.494 |  0.169 |                   69 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.594 |  0.18  |                   64 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.424 |  0.175 |                   46 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.16888222098350525
[2m[36m(func pid=101740)[0m mae:  0.10087504237890244
[2m[36m(func pid=101740)[0m rmse_per_class: [0.115, 0.313, 0.049, 0.302, 0.051, 0.257, 0.257, 0.108, 0.148, 0.089]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17532901465892792
[2m[36m(func pid=107141)[0m mae:  0.12824860215187073
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.261, 0.109, 0.335, 0.07, 0.188, 0.28, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3754 | Steps: 2 | Val loss: 0.2949 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5914 | Steps: 2 | Val loss: 0.4702 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=101124)[0m rmse: 0.16466152667999268
[2m[36m(func pid=101124)[0m mae:  0.10606212913990021
[2m[36m(func pid=101124)[0m rmse_per_class: [0.246, 0.203, 0.048, 0.286, 0.051, 0.189, 0.256, 0.142, 0.132, 0.094]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4874 | Steps: 2 | Val loss: 0.4044 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4263 | Steps: 2 | Val loss: 0.3252 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=102978)[0m rmse: 0.18012604117393494
[2m[36m(func pid=102978)[0m mae:  0.13267633318901062
[2m[36m(func pid=102978)[0m rmse_per_class: [0.115, 0.264, 0.099, 0.33, 0.101, 0.191, 0.3, 0.142, 0.144, 0.115]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:41:30 (running for 00:40:36.62)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.375 |  0.165 |                   71 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.487 |  0.172 |                   70 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.591 |  0.18  |                   65 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.425 |  0.175 |                   47 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.1720888912677765
[2m[36m(func pid=101740)[0m mae:  0.10049057006835938
[2m[36m(func pid=101740)[0m rmse_per_class: [0.173, 0.303, 0.05, 0.331, 0.052, 0.24, 0.231, 0.116, 0.137, 0.088]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17527751624584198
[2m[36m(func pid=107141)[0m mae:  0.12819868326187134
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.261, 0.109, 0.335, 0.07, 0.188, 0.28, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3662 | Steps: 2 | Val loss: 0.2871 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.5913 | Steps: 2 | Val loss: 0.4674 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=101124)[0m rmse: 0.1577763855457306
[2m[36m(func pid=101124)[0m mae:  0.10106215626001358
[2m[36m(func pid=101124)[0m rmse_per_class: [0.244, 0.209, 0.044, 0.272, 0.052, 0.178, 0.245, 0.115, 0.133, 0.087]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4939 | Steps: 2 | Val loss: 0.3915 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4200 | Steps: 2 | Val loss: 0.3246 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=102978)[0m rmse: 0.18008241057395935
[2m[36m(func pid=102978)[0m mae:  0.13264347612857819
[2m[36m(func pid=102978)[0m rmse_per_class: [0.115, 0.264, 0.099, 0.33, 0.101, 0.191, 0.3, 0.142, 0.144, 0.115]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=101740)[0m rmse: 0.17412863671779633
[2m[36m(func pid=101740)[0m mae:  0.10022509098052979
[2m[36m(func pid=101740)[0m rmse_per_class: [0.229, 0.265, 0.047, 0.337, 0.052, 0.184, 0.237, 0.126, 0.144, 0.119]
== Status ==
Current time: 2024-01-07 05:41:35 (running for 00:40:41.68)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.366 |  0.158 |                   72 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.494 |  0.174 |                   71 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.591 |  0.18  |                   66 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.426 |  0.175 |                   48 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3730 | Steps: 2 | Val loss: 0.2859 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=107141)[0m rmse: 0.17517054080963135
[2m[36m(func pid=107141)[0m mae:  0.12813284993171692
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.261, 0.108, 0.335, 0.07, 0.188, 0.28, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.5919 | Steps: 2 | Val loss: 0.4649 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3922 | Steps: 2 | Val loss: 0.3969 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=101124)[0m rmse: 0.15397599339485168
[2m[36m(func pid=101124)[0m mae:  0.0980958342552185
[2m[36m(func pid=101124)[0m rmse_per_class: [0.217, 0.222, 0.043, 0.278, 0.053, 0.167, 0.238, 0.102, 0.134, 0.087]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4237 | Steps: 2 | Val loss: 0.3244 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=102978)[0m rmse: 0.18005862832069397
[2m[36m(func pid=102978)[0m mae:  0.13263113796710968
[2m[36m(func pid=102978)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.101, 0.191, 0.3, 0.142, 0.144, 0.115]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:41:40 (running for 00:40:46.78)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.373 |  0.154 |                   73 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.392 |  0.176 |                   72 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.592 |  0.18  |                   67 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.42  |  0.175 |                   49 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.1758839637041092
[2m[36m(func pid=101740)[0m mae:  0.10377182811498642
[2m[36m(func pid=101740)[0m rmse_per_class: [0.21, 0.23, 0.042, 0.309, 0.05, 0.163, 0.248, 0.13, 0.159, 0.216]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17516659200191498
[2m[36m(func pid=107141)[0m mae:  0.12809917330741882
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.261, 0.109, 0.335, 0.069, 0.188, 0.279, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3760 | Steps: 2 | Val loss: 0.2855 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.5871 | Steps: 2 | Val loss: 0.4629 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4394 | Steps: 2 | Val loss: 0.4145 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=101124)[0m rmse: 0.1517104059457779
[2m[36m(func pid=101124)[0m mae:  0.09602554142475128
[2m[36m(func pid=101124)[0m rmse_per_class: [0.178, 0.232, 0.043, 0.291, 0.053, 0.159, 0.233, 0.105, 0.134, 0.089]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4241 | Steps: 2 | Val loss: 0.3242 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=102978)[0m rmse: 0.18002799153327942
[2m[36m(func pid=102978)[0m mae:  0.132608100771904
[2m[36m(func pid=102978)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.101, 0.191, 0.3, 0.142, 0.144, 0.115]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:41:45 (running for 00:40:51.97)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.376 |  0.152 |                   74 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.439 |  0.177 |                   73 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.587 |  0.18  |                   68 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.424 |  0.175 |                   50 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.176728755235672
[2m[36m(func pid=101740)[0m mae:  0.10669051110744476
[2m[36m(func pid=101740)[0m rmse_per_class: [0.13, 0.235, 0.051, 0.295, 0.054, 0.165, 0.252, 0.128, 0.181, 0.277]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17514707148075104
[2m[36m(func pid=107141)[0m mae:  0.12805378437042236
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.261, 0.109, 0.335, 0.069, 0.188, 0.279, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4234 | Steps: 2 | Val loss: 0.2846 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.5826 | Steps: 2 | Val loss: 0.4607 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4578 | Steps: 2 | Val loss: 0.4133 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=101124)[0m rmse: 0.15049967169761658
[2m[36m(func pid=101124)[0m mae:  0.09471238404512405
[2m[36m(func pid=101124)[0m rmse_per_class: [0.144, 0.238, 0.044, 0.297, 0.054, 0.16, 0.232, 0.113, 0.133, 0.091]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4237 | Steps: 2 | Val loss: 0.3241 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=102978)[0m rmse: 0.18000292778015137
[2m[36m(func pid=102978)[0m mae:  0.13259223103523254
[2m[36m(func pid=102978)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.3, 0.142, 0.144, 0.115]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:41:50 (running for 00:40:57.04)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15649999678134918
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.423 |  0.15  |                   75 |
| train_5a6ec_00019 | RUNNING    | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.458 |  0.174 |                   74 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.583 |  0.18  |                   69 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.424 |  0.175 |                   51 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.17419223487377167
[2m[36m(func pid=101740)[0m mae:  0.10117536783218384
[2m[36m(func pid=101740)[0m rmse_per_class: [0.101, 0.249, 0.083, 0.323, 0.072, 0.161, 0.247, 0.122, 0.191, 0.192]
[2m[36m(func pid=101740)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1751100867986679
[2m[36m(func pid=107141)[0m mae:  0.12800538539886475
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.26, 0.109, 0.335, 0.069, 0.188, 0.279, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.4164 | Steps: 2 | Val loss: 0.2837 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.5769 | Steps: 2 | Val loss: 0.4582 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=101740)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.5097 | Steps: 2 | Val loss: 0.4006 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4204 | Steps: 2 | Val loss: 0.3239 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=101124)[0m rmse: 0.1504649668931961
[2m[36m(func pid=101124)[0m mae:  0.09454642981290817
[2m[36m(func pid=101124)[0m rmse_per_class: [0.121, 0.24, 0.046, 0.288, 0.054, 0.176, 0.234, 0.121, 0.132, 0.092]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.17995890974998474
[2m[36m(func pid=102978)[0m mae:  0.13255566358566284
[2m[36m(func pid=102978)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.3, 0.142, 0.144, 0.115]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:41:55 (running for 00:41:02.07)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 3 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.416 |  0.15  |                   76 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.577 |  0.18  |                   70 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.424 |  0.175 |                   52 |
| train_5a6ec_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101740)[0m rmse: 0.16846978664398193
[2m[36m(func pid=101740)[0m mae:  0.09726828336715698
[2m[36m(func pid=101740)[0m rmse_per_class: [0.095, 0.256, 0.084, 0.341, 0.096, 0.159, 0.242, 0.111, 0.174, 0.126]
[2m[36m(func pid=107141)[0m rmse: 0.17510101199150085
[2m[36m(func pid=107141)[0m mae:  0.1279669851064682
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.26, 0.11, 0.335, 0.068, 0.188, 0.279, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.4158 | Steps: 2 | Val loss: 0.2849 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.5767 | Steps: 2 | Val loss: 0.4561 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=101124)[0m rmse: 0.15168945491313934
[2m[36m(func pid=101124)[0m mae:  0.0955938845872879
[2m[36m(func pid=101124)[0m rmse_per_class: [0.108, 0.238, 0.049, 0.271, 0.055, 0.21, 0.237, 0.126, 0.131, 0.093]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4217 | Steps: 2 | Val loss: 0.3235 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=102978)[0m rmse: 0.1799088418483734
[2m[36m(func pid=102978)[0m mae:  0.1325155645608902
[2m[36m(func pid=102978)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1750200092792511
[2m[36m(func pid=107141)[0m mae:  0.12788349390029907
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.26, 0.11, 0.335, 0.068, 0.188, 0.279, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.4032 | Steps: 2 | Val loss: 0.2909 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.5751 | Steps: 2 | Val loss: 0.4546 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 05:42:02 (running for 00:41:08.59)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.416 |  0.152 |                   77 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.577 |  0.18  |                   71 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.422 |  0.175 |                   54 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=119084)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=119084)[0m Configuration completed!
[2m[36m(func pid=119084)[0m New optimizer parameters:
[2m[36m(func pid=119084)[0m SGD (
[2m[36m(func pid=119084)[0m Parameter Group 0
[2m[36m(func pid=119084)[0m     dampening: 0
[2m[36m(func pid=119084)[0m     differentiable: False
[2m[36m(func pid=119084)[0m     foreach: None
[2m[36m(func pid=119084)[0m     lr: 0.01
[2m[36m(func pid=119084)[0m     maximize: False
[2m[36m(func pid=119084)[0m     momentum: 0.9
[2m[36m(func pid=119084)[0m     nesterov: False
[2m[36m(func pid=119084)[0m     weight_decay: 1e-05
[2m[36m(func pid=119084)[0m )
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.1550358384847641
[2m[36m(func pid=101124)[0m mae:  0.09883548319339752
[2m[36m(func pid=101124)[0m rmse_per_class: [0.102, 0.232, 0.051, 0.262, 0.055, 0.252, 0.241, 0.13, 0.133, 0.093]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4208 | Steps: 2 | Val loss: 0.3233 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=102978)[0m rmse: 0.1798873245716095
[2m[36m(func pid=102978)[0m mae:  0.13250108063220978
[2m[36m(func pid=102978)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:42:07 (running for 00:41:13.88)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.403 |  0.155 |                   78 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.575 |  0.18  |                   72 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.421 |  0.175 |                   55 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107141)[0m rmse: 0.1750229448080063
[2m[36m(func pid=107141)[0m mae:  0.12783990800380707
[2m[36m(func pid=107141)[0m rmse_per_class: [0.127, 0.26, 0.11, 0.335, 0.068, 0.188, 0.279, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.4131 | Steps: 2 | Val loss: 0.3002 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.5719 | Steps: 2 | Val loss: 0.4528 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0034 | Steps: 2 | Val loss: 0.6610 | Batch size: 32 | lr: 0.01 | Duration: 4.68s
[2m[36m(func pid=101124)[0m rmse: 0.16072605550289154
[2m[36m(func pid=101124)[0m mae:  0.10378489643335342
[2m[36m(func pid=101124)[0m rmse_per_class: [0.096, 0.221, 0.052, 0.275, 0.055, 0.286, 0.245, 0.133, 0.152, 0.093]
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4212 | Steps: 2 | Val loss: 0.3232 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.17986640334129333
[2m[36m(func pid=102978)[0m mae:  0.1324821263551712
[2m[36m(func pid=102978)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.299, 0.142, 0.144, 0.114]
[2m[36m(func pid=102978)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.17973527312278748
[2m[36m(func pid=119084)[0m mae:  0.13224391639232635
[2m[36m(func pid=119084)[0m rmse_per_class: [0.115, 0.263, 0.098, 0.334, 0.099, 0.191, 0.297, 0.143, 0.141, 0.118]
[2m[36m(func pid=119084)[0m 
== Status ==
Current time: 2024-01-07 05:42:12 (running for 00:41:19.13)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.413 |  0.161 |                   79 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.572 |  0.18  |                   73 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.421 |  0.175 |                   56 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  1.003 |  0.18  |                    1 |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107141)[0m rmse: 0.1749800741672516
[2m[36m(func pid=107141)[0m mae:  0.12779846787452698
[2m[36m(func pid=107141)[0m rmse_per_class: [0.127, 0.26, 0.11, 0.335, 0.068, 0.188, 0.279, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.4050 | Steps: 2 | Val loss: 0.3103 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5728 | Steps: 2 | Val loss: 0.4509 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.7739 | Steps: 2 | Val loss: 0.5969 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4188 | Steps: 2 | Val loss: 0.3228 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=101124)[0m rmse: 0.1675659865140915
[2m[36m(func pid=101124)[0m mae:  0.10860252380371094
[2m[36m(func pid=101124)[0m rmse_per_class: [0.093, 0.21, 0.052, 0.298, 0.055, 0.291, 0.249, 0.134, 0.198, 0.093]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.17995861172676086
[2m[36m(func pid=119084)[0m mae:  0.13245585560798645
[2m[36m(func pid=119084)[0m rmse_per_class: [0.114, 0.264, 0.103, 0.334, 0.101, 0.191, 0.295, 0.141, 0.144, 0.113]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.17984409630298615
[2m[36m(func pid=102978)[0m mae:  0.13246652483940125
[2m[36m(func pid=102978)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=102978)[0m 
== Status ==
Current time: 2024-01-07 05:42:18 (running for 00:41:24.50)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.405 |  0.168 |                   80 |
| train_5a6ec_00020 | RUNNING    | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.573 |  0.18  |                   74 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.419 |  0.175 |                   57 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.774 |  0.18  |                    2 |
| train_5a6ec_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107141)[0m rmse: 0.1748681217432022
[2m[36m(func pid=107141)[0m mae:  0.12772217392921448
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.26, 0.11, 0.335, 0.068, 0.188, 0.279, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=102978)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.5671 | Steps: 2 | Val loss: 0.4497 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5627 | Steps: 2 | Val loss: 0.5093 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.4153 | Steps: 2 | Val loss: 0.3215 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4192 | Steps: 2 | Val loss: 0.3225 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=101124)[0m rmse: 0.17377059161663055
[2m[36m(func pid=101124)[0m mae:  0.1127895936369896
[2m[36m(func pid=101124)[0m rmse_per_class: [0.094, 0.202, 0.051, 0.322, 0.055, 0.271, 0.255, 0.135, 0.26, 0.093]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=102978)[0m rmse: 0.179855614900589
[2m[36m(func pid=102978)[0m mae:  0.1324693262577057
[2m[36m(func pid=102978)[0m rmse_per_class: [0.115, 0.264, 0.1, 0.33, 0.1, 0.19, 0.299, 0.142, 0.144, 0.114]
[2m[36m(func pid=119084)[0m rmse: 0.1796872615814209
[2m[36m(func pid=119084)[0m mae:  0.1322014182806015
[2m[36m(func pid=119084)[0m rmse_per_class: [0.114, 0.264, 0.106, 0.334, 0.1, 0.191, 0.293, 0.139, 0.146, 0.11]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17480012774467468
[2m[36m(func pid=107141)[0m mae:  0.12767401337623596
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.26, 0.109, 0.334, 0.067, 0.188, 0.279, 0.135, 0.148, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4613 | Steps: 2 | Val loss: 0.4280 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.4318 | Steps: 2 | Val loss: 0.3302 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=119084)[0m rmse: 0.17916202545166016
[2m[36m(func pid=119084)[0m mae:  0.13168945908546448
[2m[36m(func pid=119084)[0m rmse_per_class: [0.115, 0.264, 0.109, 0.336, 0.097, 0.19, 0.29, 0.137, 0.147, 0.106]
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4202 | Steps: 2 | Val loss: 0.3224 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=101124)[0m rmse: 0.17620253562927246
[2m[36m(func pid=101124)[0m mae:  0.11478189378976822
[2m[36m(func pid=101124)[0m rmse_per_class: [0.094, 0.199, 0.048, 0.339, 0.055, 0.234, 0.26, 0.134, 0.306, 0.092]
[2m[36m(func pid=107141)[0m rmse: 0.17485858500003815
[2m[36m(func pid=107141)[0m mae:  0.12764498591423035
[2m[36m(func pid=107141)[0m rmse_per_class: [0.127, 0.26, 0.11, 0.334, 0.067, 0.188, 0.279, 0.135, 0.147, 0.102]
== Status ==
Current time: 2024-01-07 05:42:23 (running for 00:41:29.79)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.415 |  0.174 |                   81 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.419 |  0.175 |                   58 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.563 |  0.18  |                    3 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 05:42:30 (running for 00:41:36.77)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.415 |  0.174 |                   81 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.419 |  0.175 |                   58 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.461 |  0.179 |                    4 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=120410)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=120410)[0m Configuration completed!
[2m[36m(func pid=120410)[0m New optimizer parameters:
[2m[36m(func pid=120410)[0m SGD (
[2m[36m(func pid=120410)[0m Parameter Group 0
[2m[36m(func pid=120410)[0m     dampening: 0
[2m[36m(func pid=120410)[0m     differentiable: False
[2m[36m(func pid=120410)[0m     foreach: None
[2m[36m(func pid=120410)[0m     lr: 0.1
[2m[36m(func pid=120410)[0m     maximize: False
[2m[36m(func pid=120410)[0m     momentum: 0.9
[2m[36m(func pid=120410)[0m     nesterov: False
[2m[36m(func pid=120410)[0m     weight_decay: 1e-05
[2m[36m(func pid=120410)[0m )
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.4069 | Steps: 2 | Val loss: 0.3352 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4178 | Steps: 2 | Val loss: 0.3220 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4474 | Steps: 2 | Val loss: 0.3720 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.7932 | Steps: 2 | Val loss: 0.4377 | Batch size: 32 | lr: 0.1 | Duration: 4.52s
== Status ==
Current time: 2024-01-07 05:42:35 (running for 00:41:41.79)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.432 |  0.176 |                   82 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.42  |  0.175 |                   59 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.461 |  0.179 |                    4 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.17545995116233826
[2m[36m(func pid=101124)[0m mae:  0.11480014026165009
[2m[36m(func pid=101124)[0m rmse_per_class: [0.095, 0.206, 0.046, 0.35, 0.055, 0.198, 0.26, 0.132, 0.321, 0.092]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1747426986694336
[2m[36m(func pid=107141)[0m mae:  0.12757046520709991
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.26, 0.109, 0.334, 0.067, 0.187, 0.278, 0.135, 0.147, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.17846503853797913
[2m[36m(func pid=119084)[0m mae:  0.13098137080669403
[2m[36m(func pid=119084)[0m rmse_per_class: [0.116, 0.264, 0.113, 0.337, 0.092, 0.19, 0.285, 0.136, 0.149, 0.103]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.17944152653217316
[2m[36m(func pid=120410)[0m mae:  0.13135318458080292
[2m[36m(func pid=120410)[0m rmse_per_class: [0.12, 0.264, 0.118, 0.342, 0.088, 0.191, 0.283, 0.139, 0.144, 0.105]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.4022 | Steps: 2 | Val loss: 0.3383 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4154 | Steps: 2 | Val loss: 0.3217 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4590 | Steps: 2 | Val loss: 0.3396 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.4801 | Steps: 2 | Val loss: 0.3665 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 05:42:41 (running for 00:41:47.26)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.402 |  0.173 |                   84 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.418 |  0.175 |                   60 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.447 |  0.178 |                    5 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.793 |  0.179 |                    1 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.1732948124408722
[2m[36m(func pid=101124)[0m mae:  0.1135329008102417
[2m[36m(func pid=101124)[0m rmse_per_class: [0.098, 0.229, 0.044, 0.358, 0.055, 0.172, 0.257, 0.128, 0.302, 0.091]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17476311326026917
[2m[36m(func pid=107141)[0m mae:  0.12752798199653625
[2m[36m(func pid=107141)[0m rmse_per_class: [0.127, 0.26, 0.11, 0.334, 0.067, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.17759159207344055
[2m[36m(func pid=119084)[0m mae:  0.13005974888801575
[2m[36m(func pid=119084)[0m rmse_per_class: [0.118, 0.264, 0.116, 0.339, 0.085, 0.189, 0.28, 0.135, 0.149, 0.1]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.17983728647232056
[2m[36m(func pid=120410)[0m mae:  0.13095715641975403
[2m[36m(func pid=120410)[0m rmse_per_class: [0.12, 0.264, 0.134, 0.345, 0.077, 0.19, 0.276, 0.139, 0.154, 0.098]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.4296 | Steps: 2 | Val loss: 0.3371 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4168 | Steps: 2 | Val loss: 0.3213 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4714 | Steps: 2 | Val loss: 0.3244 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.6391 | Steps: 2 | Val loss: 0.3313 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 05:42:46 (running for 00:41:52.50)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.402 |  0.173 |                   84 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.417 |  0.175 |                   62 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.459 |  0.178 |                    6 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.48  |  0.18  |                    2 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.17066900432109833
[2m[36m(func pid=101124)[0m mae:  0.11129876226186752
[2m[36m(func pid=101124)[0m rmse_per_class: [0.109, 0.26, 0.042, 0.362, 0.055, 0.159, 0.251, 0.122, 0.258, 0.089]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.174637570977211
[2m[36m(func pid=107141)[0m mae:  0.1274413764476776
[2m[36m(func pid=107141)[0m rmse_per_class: [0.127, 0.26, 0.109, 0.334, 0.067, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.1765185296535492
[2m[36m(func pid=119084)[0m mae:  0.12893564999103546
[2m[36m(func pid=119084)[0m rmse_per_class: [0.12, 0.263, 0.117, 0.34, 0.078, 0.188, 0.276, 0.135, 0.15, 0.098]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.17916719615459442
[2m[36m(func pid=120410)[0m mae:  0.12900134921073914
[2m[36m(func pid=120410)[0m rmse_per_class: [0.123, 0.264, 0.14, 0.352, 0.061, 0.188, 0.266, 0.14, 0.166, 0.092]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3849 | Steps: 2 | Val loss: 0.3366 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4170 | Steps: 2 | Val loss: 0.3210 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4823 | Steps: 2 | Val loss: 0.3184 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.7192 | Steps: 2 | Val loss: 0.3295 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=101124)[0m rmse: 0.16956421732902527
[2m[36m(func pid=101124)[0m mae:  0.10979652404785156
[2m[36m(func pid=101124)[0m rmse_per_class: [0.137, 0.286, 0.04, 0.362, 0.055, 0.155, 0.245, 0.116, 0.211, 0.088]
== Status ==
Current time: 2024-01-07 05:42:51 (running for 00:41:57.67)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.385 |  0.17  |                   86 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.417 |  0.175 |                   62 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.471 |  0.177 |                    7 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.639 |  0.179 |                    3 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.17551054060459137
[2m[36m(func pid=119084)[0m mae:  0.1278182715177536
[2m[36m(func pid=119084)[0m rmse_per_class: [0.123, 0.262, 0.117, 0.341, 0.072, 0.187, 0.273, 0.136, 0.149, 0.096]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17456719279289246
[2m[36m(func pid=107141)[0m mae:  0.12737932801246643
[2m[36m(func pid=107141)[0m rmse_per_class: [0.127, 0.26, 0.109, 0.333, 0.067, 0.187, 0.278, 0.135, 0.148, 0.101]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.17961479723453522
[2m[36m(func pid=120410)[0m mae:  0.12741006910800934
[2m[36m(func pid=120410)[0m rmse_per_class: [0.133, 0.262, 0.133, 0.358, 0.055, 0.187, 0.26, 0.144, 0.174, 0.091]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3856 | Steps: 2 | Val loss: 0.3353 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4141 | Steps: 2 | Val loss: 0.3207 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4904 | Steps: 2 | Val loss: 0.3164 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.6127 | Steps: 2 | Val loss: 0.3330 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 05:42:56 (running for 00:42:02.90)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.386 |  0.169 |                   87 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.417 |  0.175 |                   63 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.482 |  0.176 |                    8 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.719 |  0.18  |                    4 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.1688443124294281
[2m[36m(func pid=101124)[0m mae:  0.10826639086008072
[2m[36m(func pid=101124)[0m rmse_per_class: [0.166, 0.301, 0.04, 0.361, 0.054, 0.16, 0.239, 0.109, 0.172, 0.086]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17443886399269104
[2m[36m(func pid=107141)[0m mae:  0.12730064988136292
[2m[36m(func pid=107141)[0m rmse_per_class: [0.127, 0.26, 0.108, 0.333, 0.066, 0.187, 0.278, 0.135, 0.148, 0.101]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.17464536428451538
[2m[36m(func pid=119084)[0m mae:  0.1268104761838913
[2m[36m(func pid=119084)[0m rmse_per_class: [0.126, 0.261, 0.117, 0.341, 0.066, 0.187, 0.27, 0.136, 0.149, 0.094]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.17752590775489807
[2m[36m(func pid=120410)[0m mae:  0.12587140500545502
[2m[36m(func pid=120410)[0m rmse_per_class: [0.153, 0.255, 0.092, 0.358, 0.055, 0.185, 0.257, 0.147, 0.182, 0.091]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3814 | Steps: 2 | Val loss: 0.3317 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4175 | Steps: 2 | Val loss: 0.3205 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4947 | Steps: 2 | Val loss: 0.3160 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5613 | Steps: 2 | Val loss: 0.3345 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 05:43:01 (running for 00:42:08.11)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.386 |  0.169 |                   87 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.414 |  0.174 |                   64 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.495 |  0.174 |                   10 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.613 |  0.178 |                    5 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.17404140532016754
[2m[36m(func pid=119084)[0m mae:  0.1259825974702835
[2m[36m(func pid=119084)[0m rmse_per_class: [0.129, 0.26, 0.116, 0.341, 0.062, 0.186, 0.268, 0.136, 0.148, 0.093]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1744050830602646
[2m[36m(func pid=107141)[0m mae:  0.12726812064647675
[2m[36m(func pid=107141)[0m rmse_per_class: [0.127, 0.26, 0.108, 0.333, 0.066, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=101124)[0m rmse: 0.16819848120212555
[2m[36m(func pid=101124)[0m mae:  0.10693921893835068
[2m[36m(func pid=101124)[0m rmse_per_class: [0.188, 0.301, 0.04, 0.358, 0.054, 0.167, 0.234, 0.104, 0.149, 0.087]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.16982200741767883
[2m[36m(func pid=120410)[0m mae:  0.12151502072811127
[2m[36m(func pid=120410)[0m rmse_per_class: [0.124, 0.253, 0.052, 0.343, 0.056, 0.181, 0.261, 0.146, 0.19, 0.092]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4172 | Steps: 2 | Val loss: 0.3205 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4792 | Steps: 2 | Val loss: 0.3150 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3927 | Steps: 2 | Val loss: 0.3243 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5172 | Steps: 2 | Val loss: 0.3299 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 05:43:07 (running for 00:42:13.25)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.381 |  0.168 |                   88 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.417 |  0.174 |                   65 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.479 |  0.174 |                   11 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.561 |  0.17  |                    6 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.17351262271404266
[2m[36m(func pid=119084)[0m mae:  0.12527835369110107
[2m[36m(func pid=119084)[0m rmse_per_class: [0.132, 0.259, 0.114, 0.341, 0.059, 0.185, 0.266, 0.137, 0.149, 0.093]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17448723316192627
[2m[36m(func pid=107141)[0m mae:  0.12728135287761688
[2m[36m(func pid=107141)[0m rmse_per_class: [0.127, 0.26, 0.109, 0.333, 0.066, 0.187, 0.278, 0.135, 0.148, 0.101]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.16664156317710876
[2m[36m(func pid=101124)[0m mae:  0.10511646419763565
[2m[36m(func pid=101124)[0m rmse_per_class: [0.205, 0.284, 0.041, 0.351, 0.054, 0.175, 0.229, 0.099, 0.138, 0.091]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.16296400129795074
[2m[36m(func pid=120410)[0m mae:  0.11511200666427612
[2m[36m(func pid=120410)[0m rmse_per_class: [0.099, 0.258, 0.045, 0.304, 0.056, 0.192, 0.285, 0.14, 0.161, 0.091]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4732 | Steps: 2 | Val loss: 0.3133 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4165 | Steps: 2 | Val loss: 0.3204 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.4019 | Steps: 2 | Val loss: 0.3126 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4792 | Steps: 2 | Val loss: 0.3242 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 05:43:12 (running for 00:42:18.54)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.393 |  0.167 |                   89 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.417 |  0.174 |                   66 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.473 |  0.173 |                   12 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.517 |  0.163 |                    7 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.17308610677719116
[2m[36m(func pid=119084)[0m mae:  0.12467483431100845
[2m[36m(func pid=119084)[0m rmse_per_class: [0.136, 0.258, 0.112, 0.34, 0.057, 0.185, 0.265, 0.137, 0.149, 0.092]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17445801198482513
[2m[36m(func pid=107141)[0m mae:  0.1272432655096054
[2m[36m(func pid=107141)[0m rmse_per_class: [0.127, 0.26, 0.109, 0.333, 0.066, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.16388526558876038
[2m[36m(func pid=101124)[0m mae:  0.10261477530002594
[2m[36m(func pid=101124)[0m rmse_per_class: [0.202, 0.263, 0.043, 0.339, 0.053, 0.181, 0.224, 0.097, 0.133, 0.103]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.16240304708480835
[2m[36m(func pid=120410)[0m mae:  0.11350063979625702
[2m[36m(func pid=120410)[0m rmse_per_class: [0.104, 0.249, 0.046, 0.294, 0.056, 0.205, 0.297, 0.128, 0.135, 0.109]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4132 | Steps: 2 | Val loss: 0.3200 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4501 | Steps: 2 | Val loss: 0.3112 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3735 | Steps: 2 | Val loss: 0.2976 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4486 | Steps: 2 | Val loss: 0.3367 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 05:43:17 (running for 00:42:23.85)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.402 |  0.164 |                   90 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.413 |  0.174 |                   68 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.473 |  0.173 |                   12 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.479 |  0.162 |                    8 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107141)[0m rmse: 0.17434123158454895
[2m[36m(func pid=107141)[0m mae:  0.12717117369174957
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.26, 0.108, 0.333, 0.066, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.17272044718265533
[2m[36m(func pid=119084)[0m mae:  0.12420685589313507
[2m[36m(func pid=119084)[0m rmse_per_class: [0.139, 0.257, 0.11, 0.338, 0.056, 0.185, 0.264, 0.137, 0.149, 0.092]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.15962299704551697
[2m[36m(func pid=101124)[0m mae:  0.0996379479765892
[2m[36m(func pid=101124)[0m rmse_per_class: [0.182, 0.233, 0.045, 0.321, 0.053, 0.186, 0.22, 0.097, 0.132, 0.128]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.1782664656639099
[2m[36m(func pid=120410)[0m mae:  0.12370623648166656
[2m[36m(func pid=120410)[0m rmse_per_class: [0.101, 0.267, 0.047, 0.31, 0.056, 0.182, 0.293, 0.193, 0.133, 0.2]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4128 | Steps: 2 | Val loss: 0.3197 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4440 | Steps: 2 | Val loss: 0.3094 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3578 | Steps: 2 | Val loss: 0.2868 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4465 | Steps: 2 | Val loss: 0.3488 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 05:43:22 (running for 00:42:29.05)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.374 |  0.16  |                   91 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.413 |  0.174 |                   69 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.45  |  0.173 |                   13 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.449 |  0.178 |                    9 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107141)[0m rmse: 0.17424379289150238
[2m[36m(func pid=107141)[0m mae:  0.12710179388523102
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.26, 0.108, 0.333, 0.066, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.1723634898662567
[2m[36m(func pid=119084)[0m mae:  0.12375511974096298
[2m[36m(func pid=119084)[0m rmse_per_class: [0.142, 0.256, 0.108, 0.337, 0.055, 0.185, 0.264, 0.137, 0.149, 0.092]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.1559852659702301
[2m[36m(func pid=101124)[0m mae:  0.09810172021389008
[2m[36m(func pid=101124)[0m rmse_per_class: [0.148, 0.209, 0.048, 0.296, 0.052, 0.19, 0.22, 0.101, 0.131, 0.165]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.1883183717727661
[2m[36m(func pid=120410)[0m mae:  0.13041236996650696
[2m[36m(func pid=120410)[0m rmse_per_class: [0.098, 0.272, 0.045, 0.349, 0.055, 0.183, 0.278, 0.227, 0.133, 0.243]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4134 | Steps: 2 | Val loss: 0.3194 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4255 | Steps: 2 | Val loss: 0.3079 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3468 | Steps: 2 | Val loss: 0.2836 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 05:43:28 (running for 00:42:34.19)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.358 |  0.156 |                   92 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.413 |  0.174 |                   69 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.426 |  0.172 |                   15 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.447 |  0.188 |                   10 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.1717528998851776
[2m[36m(func pid=119084)[0m mae:  0.12324501574039459
[2m[36m(func pid=119084)[0m rmse_per_class: [0.142, 0.255, 0.105, 0.334, 0.055, 0.184, 0.264, 0.136, 0.15, 0.092]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4527 | Steps: 2 | Val loss: 0.3158 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=107141)[0m rmse: 0.17422783374786377
[2m[36m(func pid=107141)[0m mae:  0.12706267833709717
[2m[36m(func pid=107141)[0m rmse_per_class: [0.126, 0.26, 0.108, 0.333, 0.066, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.1553008109331131
[2m[36m(func pid=101124)[0m mae:  0.09916464239358902
[2m[36m(func pid=101124)[0m rmse_per_class: [0.113, 0.203, 0.053, 0.274, 0.052, 0.192, 0.226, 0.104, 0.131, 0.206]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.17408348619937897
[2m[36m(func pid=120410)[0m mae:  0.12270363420248032
[2m[36m(func pid=120410)[0m rmse_per_class: [0.118, 0.251, 0.044, 0.357, 0.054, 0.183, 0.261, 0.154, 0.14, 0.179]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4146 | Steps: 2 | Val loss: 0.3058 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4105 | Steps: 2 | Val loss: 0.3190 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3658 | Steps: 2 | Val loss: 0.2868 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 05:43:33 (running for 00:42:39.35)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.347 |  0.155 |                   93 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.413 |  0.174 |                   70 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.415 |  0.17  |                   16 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.453 |  0.174 |                   11 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.17028188705444336
[2m[36m(func pid=119084)[0m mae:  0.12231826782226562
[2m[36m(func pid=119084)[0m rmse_per_class: [0.139, 0.254, 0.097, 0.331, 0.055, 0.183, 0.264, 0.135, 0.151, 0.093]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.17407745122909546
[2m[36m(func pid=107141)[0m mae:  0.12697215378284454
[2m[36m(func pid=107141)[0m rmse_per_class: [0.125, 0.26, 0.107, 0.332, 0.066, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4051 | Steps: 2 | Val loss: 0.2979 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=101124)[0m rmse: 0.15759064257144928
[2m[36m(func pid=101124)[0m mae:  0.10166840255260468
[2m[36m(func pid=101124)[0m rmse_per_class: [0.093, 0.213, 0.054, 0.26, 0.051, 0.192, 0.235, 0.108, 0.131, 0.239]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.16341862082481384
[2m[36m(func pid=120410)[0m mae:  0.11662481725215912
[2m[36m(func pid=120410)[0m rmse_per_class: [0.096, 0.243, 0.046, 0.346, 0.058, 0.177, 0.258, 0.12, 0.185, 0.105]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4109 | Steps: 2 | Val loss: 0.3187 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4157 | Steps: 2 | Val loss: 0.3050 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3687 | Steps: 2 | Val loss: 0.2919 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 05:43:38 (running for 00:42:44.68)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.366 |  0.158 |                   94 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.411 |  0.174 |                   71 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.416 |  0.169 |                   17 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.405 |  0.163 |                   12 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16897282004356384
[2m[36m(func pid=119084)[0m mae:  0.12154026329517365
[2m[36m(func pid=119084)[0m rmse_per_class: [0.133, 0.253, 0.092, 0.329, 0.055, 0.183, 0.265, 0.134, 0.152, 0.093]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1740235984325409
[2m[36m(func pid=107141)[0m mae:  0.12692773342132568
[2m[36m(func pid=107141)[0m rmse_per_class: [0.125, 0.26, 0.107, 0.332, 0.066, 0.187, 0.278, 0.135, 0.149, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4000 | Steps: 2 | Val loss: 0.2915 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=101124)[0m rmse: 0.1600370556116104
[2m[36m(func pid=101124)[0m mae:  0.10356400907039642
[2m[36m(func pid=101124)[0m rmse_per_class: [0.091, 0.227, 0.051, 0.258, 0.05, 0.19, 0.245, 0.112, 0.131, 0.245]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.16385427117347717
[2m[36m(func pid=120410)[0m mae:  0.11438320577144623
[2m[36m(func pid=120410)[0m rmse_per_class: [0.1, 0.262, 0.047, 0.318, 0.085, 0.173, 0.268, 0.127, 0.168, 0.091]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4132 | Steps: 2 | Val loss: 0.3036 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.4063 | Steps: 2 | Val loss: 0.2948 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4128 | Steps: 2 | Val loss: 0.3187 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=119084)[0m rmse: 0.16727308928966522
[2m[36m(func pid=119084)[0m mae:  0.12054004520177841
[2m[36m(func pid=119084)[0m rmse_per_class: [0.125, 0.253, 0.085, 0.325, 0.054, 0.182, 0.267, 0.133, 0.153, 0.095]
[2m[36m(func pid=119084)[0m 
== Status ==
Current time: 2024-01-07 05:43:43 (running for 00:42:49.75)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.369 |  0.16  |                   95 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.411 |  0.174 |                   72 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.413 |  0.167 |                   18 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.4   |  0.164 |                   13 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107141)[0m rmse: 0.17405256628990173
[2m[36m(func pid=107141)[0m mae:  0.1269003450870514
[2m[36m(func pid=107141)[0m rmse_per_class: [0.125, 0.26, 0.108, 0.332, 0.065, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.16087020933628082
[2m[36m(func pid=101124)[0m mae:  0.10390754044055939
[2m[36m(func pid=101124)[0m rmse_per_class: [0.092, 0.236, 0.052, 0.264, 0.05, 0.184, 0.256, 0.115, 0.131, 0.227]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3965 | Steps: 2 | Val loss: 0.2847 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4037 | Steps: 2 | Val loss: 0.3022 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=120410)[0m rmse: 0.16092494130134583
[2m[36m(func pid=120410)[0m mae:  0.11017640680074692
[2m[36m(func pid=120410)[0m rmse_per_class: [0.097, 0.237, 0.047, 0.286, 0.137, 0.182, 0.269, 0.128, 0.137, 0.09]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3695 | Steps: 2 | Val loss: 0.2933 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4097 | Steps: 2 | Val loss: 0.3183 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 05:43:48 (running for 00:42:55.11)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.406 |  0.161 |                   96 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.413 |  0.174 |                   73 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.404 |  0.166 |                   19 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.397 |  0.161 |                   14 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16555722057819366
[2m[36m(func pid=119084)[0m mae:  0.119589664041996
[2m[36m(func pid=119084)[0m rmse_per_class: [0.117, 0.253, 0.077, 0.322, 0.054, 0.181, 0.27, 0.133, 0.153, 0.097]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=101124)[0m rmse: 0.15907388925552368
[2m[36m(func pid=101124)[0m mae:  0.1025523915886879
[2m[36m(func pid=101124)[0m rmse_per_class: [0.093, 0.241, 0.052, 0.272, 0.052, 0.176, 0.265, 0.119, 0.132, 0.189]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1739458292722702
[2m[36m(func pid=107141)[0m mae:  0.12682117521762848
[2m[36m(func pid=107141)[0m rmse_per_class: [0.125, 0.26, 0.108, 0.332, 0.066, 0.187, 0.278, 0.135, 0.148, 0.102]
[2m[36m(func pid=107141)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3858 | Steps: 2 | Val loss: 0.2981 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4020 | Steps: 2 | Val loss: 0.3014 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=120410)[0m rmse: 0.16777092218399048
[2m[36m(func pid=120410)[0m mae:  0.11424098163843155
[2m[36m(func pid=120410)[0m rmse_per_class: [0.101, 0.253, 0.045, 0.292, 0.18, 0.188, 0.272, 0.122, 0.133, 0.091]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3669 | Steps: 2 | Val loss: 0.2898 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=107141)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4101 | Steps: 2 | Val loss: 0.3182 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 05:43:54 (running for 00:43:00.42)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.37  |  0.159 |                   97 |
| train_5a6ec_00021 | RUNNING    | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.41  |  0.174 |                   74 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.402 |  0.164 |                   20 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.386 |  0.168 |                   15 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.15632204711437225
[2m[36m(func pid=101124)[0m mae:  0.10104044526815414
[2m[36m(func pid=101124)[0m rmse_per_class: [0.092, 0.243, 0.051, 0.272, 0.058, 0.166, 0.271, 0.123, 0.137, 0.15]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.16431939601898193
[2m[36m(func pid=119084)[0m mae:  0.11902850866317749
[2m[36m(func pid=119084)[0m rmse_per_class: [0.111, 0.253, 0.068, 0.32, 0.054, 0.18, 0.272, 0.132, 0.153, 0.099]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=107141)[0m rmse: 0.1739608347415924
[2m[36m(func pid=107141)[0m mae:  0.12678967416286469
[2m[36m(func pid=107141)[0m rmse_per_class: [0.125, 0.26, 0.108, 0.332, 0.065, 0.187, 0.277, 0.135, 0.148, 0.102]
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3713 | Steps: 2 | Val loss: 0.3094 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3575 | Steps: 2 | Val loss: 0.2869 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3948 | Steps: 2 | Val loss: 0.3008 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=120410)[0m rmse: 0.17284974455833435
[2m[36m(func pid=120410)[0m mae:  0.11986108869314194
[2m[36m(func pid=120410)[0m rmse_per_class: [0.123, 0.248, 0.045, 0.33, 0.179, 0.176, 0.274, 0.113, 0.137, 0.104]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:43:59 (running for 00:43:05.67)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00018 | RUNNING    | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.358 |  0.154 |                   99 |
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.402 |  0.164 |                   20 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.371 |  0.173 |                   16 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.15409070253372192
[2m[36m(func pid=101124)[0m mae:  0.100354865193367
[2m[36m(func pid=101124)[0m rmse_per_class: [0.091, 0.241, 0.049, 0.264, 0.067, 0.16, 0.276, 0.123, 0.151, 0.119]
[2m[36m(func pid=101124)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.16358448565006256
[2m[36m(func pid=119084)[0m mae:  0.11875222623348236
[2m[36m(func pid=119084)[0m rmse_per_class: [0.105, 0.253, 0.062, 0.318, 0.054, 0.18, 0.275, 0.133, 0.154, 0.104]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3643 | Steps: 2 | Val loss: 0.3140 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=101124)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3502 | Steps: 2 | Val loss: 0.2866 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3951 | Steps: 2 | Val loss: 0.3011 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=120410)[0m rmse: 0.17200878262519836
[2m[36m(func pid=120410)[0m mae:  0.12190262228250504
[2m[36m(func pid=120410)[0m rmse_per_class: [0.095, 0.231, 0.046, 0.35, 0.129, 0.176, 0.276, 0.122, 0.153, 0.142]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:44:05 (running for 00:43:11.18)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.395 |  0.164 |                   21 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.364 |  0.172 |                   17 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101124)[0m rmse: 0.1547722965478897
[2m[36m(func pid=101124)[0m mae:  0.10136464983224869
[2m[36m(func pid=101124)[0m rmse_per_class: [0.095, 0.236, 0.046, 0.257, 0.081, 0.162, 0.281, 0.117, 0.174, 0.099]
[2m[36m(func pid=119084)[0m rmse: 0.1637810915708542
[2m[36m(func pid=119084)[0m mae:  0.11896847188472748
[2m[36m(func pid=119084)[0m rmse_per_class: [0.102, 0.253, 0.06, 0.317, 0.054, 0.179, 0.277, 0.133, 0.153, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3626 | Steps: 2 | Val loss: 0.3032 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=120410)[0m rmse: 0.16717761754989624
[2m[36m(func pid=120410)[0m mae:  0.11838042736053467
[2m[36m(func pid=120410)[0m rmse_per_class: [0.095, 0.233, 0.046, 0.341, 0.089, 0.182, 0.272, 0.122, 0.145, 0.147]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3921 | Steps: 2 | Val loss: 0.3012 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 05:44:10 (running for 00:43:16.71)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.392 |  0.164 |                   23 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.363 |  0.167 |                   18 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.1641470193862915
[2m[36m(func pid=119084)[0m mae:  0.11924368143081665
[2m[36m(func pid=119084)[0m rmse_per_class: [0.101, 0.253, 0.059, 0.317, 0.055, 0.179, 0.278, 0.134, 0.153, 0.113]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3565 | Steps: 2 | Val loss: 0.2787 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=120410)[0m rmse: 0.15443778038024902
[2m[36m(func pid=120410)[0m mae:  0.10852134227752686
[2m[36m(func pid=120410)[0m rmse_per_class: [0.1, 0.228, 0.045, 0.306, 0.069, 0.177, 0.26, 0.109, 0.136, 0.115]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3906 | Steps: 2 | Val loss: 0.3014 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=119084)[0m rmse: 0.16487061977386475
[2m[36m(func pid=119084)[0m mae:  0.11971863359212875
[2m[36m(func pid=119084)[0m rmse_per_class: [0.101, 0.254, 0.059, 0.318, 0.055, 0.179, 0.279, 0.135, 0.152, 0.117]
== Status ==
Current time: 2024-01-07 05:44:15 (running for 00:43:21.99)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.391 |  0.165 |                   24 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.357 |  0.154 |                   19 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3457 | Steps: 2 | Val loss: 0.2689 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3896 | Steps: 2 | Val loss: 0.3017 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=120410)[0m rmse: 0.14903686940670013
[2m[36m(func pid=120410)[0m mae:  0.10306575149297714
[2m[36m(func pid=120410)[0m rmse_per_class: [0.124, 0.22, 0.042, 0.281, 0.062, 0.168, 0.254, 0.11, 0.135, 0.093]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:44:20 (running for 00:43:27.12)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.39  |  0.166 |                   25 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.346 |  0.149 |                   20 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16571535170078278
[2m[36m(func pid=119084)[0m mae:  0.1202029213309288
[2m[36m(func pid=119084)[0m rmse_per_class: [0.102, 0.254, 0.061, 0.32, 0.055, 0.179, 0.278, 0.136, 0.151, 0.121]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3493 | Steps: 2 | Val loss: 0.2681 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3857 | Steps: 2 | Val loss: 0.3015 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=120410)[0m rmse: 0.1487288773059845
[2m[36m(func pid=120410)[0m mae:  0.10253395140171051
[2m[36m(func pid=120410)[0m rmse_per_class: [0.115, 0.221, 0.044, 0.282, 0.061, 0.169, 0.25, 0.114, 0.142, 0.089]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:44:26 (running for 00:43:32.35)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.386 |  0.166 |                   26 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.349 |  0.149 |                   21 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16622768342494965
[2m[36m(func pid=119084)[0m mae:  0.12042564153671265
[2m[36m(func pid=119084)[0m rmse_per_class: [0.103, 0.254, 0.062, 0.321, 0.056, 0.179, 0.277, 0.136, 0.15, 0.124]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3463 | Steps: 2 | Val loss: 0.2749 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3882 | Steps: 2 | Val loss: 0.3011 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=120410)[0m rmse: 0.1524219959974289
[2m[36m(func pid=120410)[0m mae:  0.10538823902606964
[2m[36m(func pid=120410)[0m rmse_per_class: [0.095, 0.217, 0.048, 0.307, 0.066, 0.175, 0.25, 0.111, 0.166, 0.089]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:44:31 (running for 00:43:37.71)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.388 |  0.167 |                   27 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.346 |  0.152 |                   22 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16669343411922455
[2m[36m(func pid=119084)[0m mae:  0.12049101293087006
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.255, 0.065, 0.322, 0.056, 0.179, 0.276, 0.136, 0.148, 0.125]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3453 | Steps: 2 | Val loss: 0.2791 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3816 | Steps: 2 | Val loss: 0.3005 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=120410)[0m rmse: 0.15328803658485413
[2m[36m(func pid=120410)[0m mae:  0.10648196935653687
[2m[36m(func pid=120410)[0m rmse_per_class: [0.094, 0.217, 0.047, 0.325, 0.071, 0.171, 0.248, 0.105, 0.162, 0.092]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:44:36 (running for 00:43:42.90)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.382 |  0.167 |                   28 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.345 |  0.153 |                   23 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16684356331825256
[2m[36m(func pid=119084)[0m mae:  0.12041641771793365
[2m[36m(func pid=119084)[0m rmse_per_class: [0.106, 0.255, 0.068, 0.323, 0.057, 0.179, 0.274, 0.136, 0.147, 0.124]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3404 | Steps: 2 | Val loss: 0.2807 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3861 | Steps: 2 | Val loss: 0.2996 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=120410)[0m rmse: 0.15490299463272095
[2m[36m(func pid=120410)[0m mae:  0.10748982429504395
[2m[36m(func pid=120410)[0m rmse_per_class: [0.116, 0.219, 0.044, 0.328, 0.078, 0.168, 0.25, 0.106, 0.142, 0.099]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:44:42 (running for 00:43:48.21)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.386 |  0.167 |                   29 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.34  |  0.155 |                   24 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16681544482707977
[2m[36m(func pid=119084)[0m mae:  0.12013145536184311
[2m[36m(func pid=119084)[0m rmse_per_class: [0.107, 0.255, 0.071, 0.322, 0.058, 0.18, 0.272, 0.135, 0.146, 0.122]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3543 | Steps: 2 | Val loss: 0.2801 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3811 | Steps: 2 | Val loss: 0.2990 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=120410)[0m rmse: 0.15706762671470642
[2m[36m(func pid=120410)[0m mae:  0.10825761407613754
[2m[36m(func pid=120410)[0m rmse_per_class: [0.129, 0.219, 0.044, 0.308, 0.086, 0.172, 0.262, 0.107, 0.135, 0.109]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.16650423407554626
[2m[36m(func pid=119084)[0m mae:  0.11993153393268585
[2m[36m(func pid=119084)[0m rmse_per_class: [0.108, 0.254, 0.07, 0.323, 0.059, 0.18, 0.27, 0.134, 0.147, 0.119]
== Status ==
Current time: 2024-01-07 05:44:47 (running for 00:43:53.57)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.381 |  0.167 |                   30 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.354 |  0.157 |                   25 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3354 | Steps: 2 | Val loss: 0.2736 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3798 | Steps: 2 | Val loss: 0.2980 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=120410)[0m rmse: 0.15319296717643738
[2m[36m(func pid=120410)[0m mae:  0.10571831464767456
[2m[36m(func pid=120410)[0m rmse_per_class: [0.097, 0.221, 0.047, 0.286, 0.097, 0.173, 0.256, 0.103, 0.138, 0.114]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:44:52 (running for 00:43:58.85)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.38  |  0.166 |                   31 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.335 |  0.153 |                   26 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.1661001443862915
[2m[36m(func pid=119084)[0m mae:  0.11957985162734985
[2m[36m(func pid=119084)[0m rmse_per_class: [0.109, 0.253, 0.07, 0.323, 0.06, 0.18, 0.269, 0.133, 0.148, 0.116]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3364 | Steps: 2 | Val loss: 0.2700 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3783 | Steps: 2 | Val loss: 0.2974 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=120410)[0m rmse: 0.15163397789001465
[2m[36m(func pid=120410)[0m mae:  0.10325231403112411
[2m[36m(func pid=120410)[0m rmse_per_class: [0.092, 0.224, 0.06, 0.278, 0.098, 0.169, 0.249, 0.101, 0.146, 0.099]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:44:58 (running for 00:44:04.50)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.378 |  0.166 |                   32 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.336 |  0.152 |                   27 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.1657184511423111
[2m[36m(func pid=119084)[0m mae:  0.11930378526449203
[2m[36m(func pid=119084)[0m rmse_per_class: [0.109, 0.253, 0.069, 0.323, 0.062, 0.18, 0.267, 0.132, 0.149, 0.114]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3379 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=120410)[0m rmse: 0.15284785628318787
[2m[36m(func pid=120410)[0m mae:  0.1047806367278099
[2m[36m(func pid=120410)[0m rmse_per_class: [0.102, 0.218, 0.05, 0.295, 0.096, 0.166, 0.251, 0.103, 0.152, 0.095]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3764 | Steps: 2 | Val loss: 0.2965 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 05:45:03 (running for 00:44:10.07)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.376 |  0.165 |                   33 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.338 |  0.153 |                   28 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.165120929479599
[2m[36m(func pid=119084)[0m mae:  0.11892563104629517
[2m[36m(func pid=119084)[0m rmse_per_class: [0.109, 0.252, 0.067, 0.323, 0.063, 0.18, 0.266, 0.131, 0.149, 0.111]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3407 | Steps: 2 | Val loss: 0.2765 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=120410)[0m rmse: 0.15516415238380432
[2m[36m(func pid=120410)[0m mae:  0.10655238479375839
[2m[36m(func pid=120410)[0m rmse_per_class: [0.132, 0.216, 0.042, 0.304, 0.087, 0.165, 0.256, 0.105, 0.15, 0.095]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3829 | Steps: 2 | Val loss: 0.2958 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3362 | Steps: 2 | Val loss: 0.2688 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 05:45:09 (running for 00:44:15.67)
Memory usage on this node: 18.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.383 |  0.165 |                   34 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.341 |  0.155 |                   29 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16468921303749084
[2m[36m(func pid=119084)[0m mae:  0.11856091022491455
[2m[36m(func pid=119084)[0m rmse_per_class: [0.109, 0.251, 0.066, 0.323, 0.064, 0.18, 0.265, 0.131, 0.15, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.14927907288074493
[2m[36m(func pid=120410)[0m mae:  0.10234558582305908
[2m[36m(func pid=120410)[0m rmse_per_class: [0.112, 0.217, 0.041, 0.285, 0.079, 0.165, 0.255, 0.103, 0.138, 0.098]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3778 | Steps: 2 | Val loss: 0.2949 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 05:45:14 (running for 00:44:20.84)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.378 |  0.164 |                   35 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.336 |  0.149 |                   30 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.1640169322490692
[2m[36m(func pid=119084)[0m mae:  0.11808665096759796
[2m[36m(func pid=119084)[0m rmse_per_class: [0.108, 0.25, 0.064, 0.323, 0.065, 0.18, 0.264, 0.13, 0.15, 0.107]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3344 | Steps: 2 | Val loss: 0.2653 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3741 | Steps: 2 | Val loss: 0.2940 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=120410)[0m rmse: 0.14707499742507935
[2m[36m(func pid=120410)[0m mae:  0.10035349428653717
[2m[36m(func pid=120410)[0m rmse_per_class: [0.092, 0.216, 0.051, 0.276, 0.079, 0.165, 0.25, 0.101, 0.138, 0.103]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:45:19 (running for 00:44:25.97)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.374 |  0.163 |                   36 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.334 |  0.147 |                   31 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.163285493850708
[2m[36m(func pid=119084)[0m mae:  0.11768104881048203
[2m[36m(func pid=119084)[0m rmse_per_class: [0.106, 0.25, 0.059, 0.322, 0.066, 0.179, 0.264, 0.13, 0.151, 0.106]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3259 | Steps: 2 | Val loss: 0.2659 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=120410)[0m rmse: 0.147762268781662
[2m[36m(func pid=120410)[0m mae:  0.10095685720443726
[2m[36m(func pid=120410)[0m rmse_per_class: [0.094, 0.221, 0.049, 0.274, 0.083, 0.166, 0.248, 0.099, 0.143, 0.101]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3737 | Steps: 2 | Val loss: 0.2930 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 05:45:24 (running for 00:44:31.05)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.374 |  0.163 |                   37 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.326 |  0.148 |                   32 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16264495253562927
[2m[36m(func pid=119084)[0m mae:  0.11727788299322128
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.25, 0.056, 0.321, 0.066, 0.179, 0.264, 0.13, 0.151, 0.105]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3357 | Steps: 2 | Val loss: 0.2717 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=120410)[0m rmse: 0.15244312584400177
[2m[36m(func pid=120410)[0m mae:  0.1041240319609642
[2m[36m(func pid=120410)[0m rmse_per_class: [0.121, 0.218, 0.046, 0.29, 0.087, 0.167, 0.252, 0.1, 0.144, 0.1]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3772 | Steps: 2 | Val loss: 0.2924 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 05:45:30 (running for 00:44:36.37)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.377 |  0.162 |                   38 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.336 |  0.152 |                   33 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16243989765644073
[2m[36m(func pid=119084)[0m mae:  0.11702434718608856
[2m[36m(func pid=119084)[0m rmse_per_class: [0.103, 0.25, 0.057, 0.319, 0.066, 0.179, 0.264, 0.129, 0.151, 0.105]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3250 | Steps: 2 | Val loss: 0.2749 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3745 | Steps: 2 | Val loss: 0.2918 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=120410)[0m rmse: 0.15361179411411285
[2m[36m(func pid=120410)[0m mae:  0.10504402965307236
[2m[36m(func pid=120410)[0m rmse_per_class: [0.123, 0.212, 0.043, 0.308, 0.087, 0.167, 0.252, 0.101, 0.142, 0.102]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:45:35 (running for 00:44:41.63)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.374 |  0.162 |                   39 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.325 |  0.154 |                   34 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16217660903930664
[2m[36m(func pid=119084)[0m mae:  0.11681374162435532
[2m[36m(func pid=119084)[0m rmse_per_class: [0.103, 0.25, 0.057, 0.318, 0.067, 0.178, 0.265, 0.129, 0.15, 0.105]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3374 | Steps: 2 | Val loss: 0.2746 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3754 | Steps: 2 | Val loss: 0.2911 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=120410)[0m rmse: 0.15234073996543884
[2m[36m(func pid=120410)[0m mae:  0.10445906966924667
[2m[36m(func pid=120410)[0m rmse_per_class: [0.097, 0.215, 0.049, 0.311, 0.086, 0.165, 0.252, 0.099, 0.147, 0.102]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.16176369786262512
[2m[36m(func pid=119084)[0m mae:  0.11651799827814102
[2m[36m(func pid=119084)[0m rmse_per_class: [0.103, 0.25, 0.055, 0.316, 0.067, 0.178, 0.266, 0.129, 0.149, 0.105]
[2m[36m(func pid=119084)[0m 
== Status ==
Current time: 2024-01-07 05:45:40 (running for 00:44:47.09)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.375 |  0.162 |                   40 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.337 |  0.152 |                   35 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3212 | Steps: 2 | Val loss: 0.2726 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3732 | Steps: 2 | Val loss: 0.2909 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=120410)[0m rmse: 0.15217053890228271
[2m[36m(func pid=120410)[0m mae:  0.10352203994989395
[2m[36m(func pid=120410)[0m rmse_per_class: [0.098, 0.212, 0.06, 0.3, 0.091, 0.164, 0.252, 0.097, 0.148, 0.1]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.16175058484077454
[2m[36m(func pid=119084)[0m mae:  0.11650145053863525
[2m[36m(func pid=119084)[0m rmse_per_class: [0.103, 0.25, 0.055, 0.314, 0.067, 0.178, 0.267, 0.129, 0.148, 0.106]
== Status ==
Current time: 2024-01-07 05:45:46 (running for 00:44:52.47)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.373 |  0.162 |                   41 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.321 |  0.152 |                   36 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3329 | Steps: 2 | Val loss: 0.2710 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=120410)[0m rmse: 0.1520967334508896
[2m[36m(func pid=120410)[0m mae:  0.10317746549844742
[2m[36m(func pid=120410)[0m rmse_per_class: [0.12, 0.215, 0.05, 0.289, 0.091, 0.164, 0.251, 0.099, 0.145, 0.099]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3738 | Steps: 2 | Val loss: 0.2912 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 05:45:51 (running for 00:44:57.90)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.374 |  0.162 |                   42 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.333 |  0.152 |                   37 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16205495595932007
[2m[36m(func pid=119084)[0m mae:  0.11660407483577728
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.25, 0.057, 0.314, 0.067, 0.178, 0.267, 0.13, 0.147, 0.107]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3179 | Steps: 2 | Val loss: 0.2731 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=120410)[0m rmse: 0.15369537472724915
[2m[36m(func pid=120410)[0m mae:  0.10462917387485504
[2m[36m(func pid=120410)[0m rmse_per_class: [0.126, 0.215, 0.042, 0.297, 0.09, 0.164, 0.251, 0.101, 0.15, 0.102]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3715 | Steps: 2 | Val loss: 0.2916 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 05:45:57 (running for 00:45:03.35)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.371 |  0.162 |                   43 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.318 |  0.154 |                   38 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16232241690158844
[2m[36m(func pid=119084)[0m mae:  0.11677433550357819
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.25, 0.057, 0.315, 0.068, 0.178, 0.267, 0.13, 0.147, 0.107]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3177 | Steps: 2 | Val loss: 0.2742 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=120410)[0m rmse: 0.15354326367378235
[2m[36m(func pid=120410)[0m mae:  0.10481585562229156
[2m[36m(func pid=120410)[0m rmse_per_class: [0.121, 0.212, 0.042, 0.305, 0.086, 0.163, 0.25, 0.098, 0.148, 0.11]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3722 | Steps: 2 | Val loss: 0.2919 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 05:46:02 (running for 00:45:08.79)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.372 |  0.163 |                   44 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.318 |  0.154 |                   39 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16256515681743622
[2m[36m(func pid=119084)[0m mae:  0.11686400324106216
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.25, 0.058, 0.316, 0.068, 0.178, 0.267, 0.13, 0.146, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3157 | Steps: 2 | Val loss: 0.2728 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3722 | Steps: 2 | Val loss: 0.2924 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=120410)[0m rmse: 0.15195223689079285
[2m[36m(func pid=120410)[0m mae:  0.103814497590065
[2m[36m(func pid=120410)[0m rmse_per_class: [0.117, 0.211, 0.042, 0.302, 0.082, 0.164, 0.251, 0.097, 0.143, 0.112]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:46:07 (running for 00:45:14.11)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.372 |  0.163 |                   45 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.316 |  0.152 |                   40 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16291023790836334
[2m[36m(func pid=119084)[0m mae:  0.11700788885354996
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.25, 0.06, 0.318, 0.068, 0.178, 0.267, 0.13, 0.147, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3157 | Steps: 2 | Val loss: 0.2697 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3715 | Steps: 2 | Val loss: 0.2926 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=120410)[0m rmse: 0.14955294132232666
[2m[36m(func pid=120410)[0m mae:  0.10160961002111435
[2m[36m(func pid=120410)[0m rmse_per_class: [0.107, 0.212, 0.051, 0.298, 0.077, 0.165, 0.246, 0.097, 0.14, 0.104]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.1630258411169052
[2m[36m(func pid=119084)[0m mae:  0.11707593500614166
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.249, 0.061, 0.319, 0.068, 0.178, 0.266, 0.13, 0.147, 0.109]
[2m[36m(func pid=119084)[0m 
== Status ==
Current time: 2024-01-07 05:46:13 (running for 00:45:19.40)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.372 |  0.163 |                   46 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.316 |  0.15  |                   41 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3145 | Steps: 2 | Val loss: 0.2670 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=120410)[0m rmse: 0.14825522899627686
[2m[36m(func pid=120410)[0m mae:  0.10036532580852509
[2m[36m(func pid=120410)[0m rmse_per_class: [0.116, 0.21, 0.048, 0.288, 0.078, 0.164, 0.246, 0.097, 0.141, 0.095]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3698 | Steps: 2 | Val loss: 0.2928 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=119084)[0m rmse: 0.1631082445383072
[2m[36m(func pid=119084)[0m mae:  0.1171962171792984
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.249, 0.06, 0.32, 0.068, 0.178, 0.266, 0.129, 0.148, 0.11]
== Status ==
Current time: 2024-01-07 05:46:18 (running for 00:45:24.59)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.37  |  0.163 |                   47 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.314 |  0.148 |                   42 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3198 | Steps: 2 | Val loss: 0.2650 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3751 | Steps: 2 | Val loss: 0.2932 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=120410)[0m rmse: 0.14688420295715332
[2m[36m(func pid=120410)[0m mae:  0.09958218038082123
[2m[36m(func pid=120410)[0m rmse_per_class: [0.109, 0.211, 0.043, 0.28, 0.082, 0.162, 0.248, 0.097, 0.145, 0.091]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:46:23 (running for 00:45:29.83)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.375 |  0.163 |                   48 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.32  |  0.147 |                   43 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.1633281409740448
[2m[36m(func pid=119084)[0m mae:  0.11737005412578583
[2m[36m(func pid=119084)[0m rmse_per_class: [0.105, 0.249, 0.06, 0.321, 0.067, 0.177, 0.265, 0.129, 0.149, 0.111]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3135 | Steps: 2 | Val loss: 0.2676 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3708 | Steps: 2 | Val loss: 0.2929 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=120410)[0m rmse: 0.14924371242523193
[2m[36m(func pid=120410)[0m mae:  0.10066868364810944
[2m[36m(func pid=120410)[0m rmse_per_class: [0.121, 0.212, 0.044, 0.29, 0.085, 0.162, 0.246, 0.097, 0.143, 0.092]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:46:28 (running for 00:45:35.11)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.371 |  0.163 |                   49 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.314 |  0.149 |                   44 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16317783296108246
[2m[36m(func pid=119084)[0m mae:  0.1172560453414917
[2m[36m(func pid=119084)[0m rmse_per_class: [0.105, 0.249, 0.059, 0.321, 0.067, 0.177, 0.264, 0.128, 0.149, 0.112]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3171 | Steps: 2 | Val loss: 0.2697 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3694 | Steps: 2 | Val loss: 0.2922 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=120410)[0m rmse: 0.15047404170036316
[2m[36m(func pid=120410)[0m mae:  0.10144808143377304
[2m[36m(func pid=120410)[0m rmse_per_class: [0.124, 0.211, 0.044, 0.298, 0.086, 0.162, 0.246, 0.097, 0.141, 0.095]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:46:34 (running for 00:45:40.75)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.369 |  0.163 |                   50 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.317 |  0.15  |                   45 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16271451115608215
[2m[36m(func pid=119084)[0m mae:  0.1169409528374672
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.248, 0.057, 0.32, 0.067, 0.177, 0.263, 0.128, 0.148, 0.114]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3119 | Steps: 2 | Val loss: 0.2725 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3697 | Steps: 2 | Val loss: 0.2917 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=120410)[0m rmse: 0.15169498324394226
[2m[36m(func pid=120410)[0m mae:  0.10301635414361954
[2m[36m(func pid=120410)[0m rmse_per_class: [0.116, 0.211, 0.043, 0.304, 0.086, 0.162, 0.249, 0.096, 0.146, 0.106]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:46:39 (running for 00:45:46.01)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.37  |  0.162 |                   51 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.312 |  0.152 |                   46 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.1623694747686386
[2m[36m(func pid=119084)[0m mae:  0.1167469471693039
[2m[36m(func pid=119084)[0m rmse_per_class: [0.103, 0.248, 0.055, 0.32, 0.067, 0.177, 0.263, 0.127, 0.148, 0.115]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3231 | Steps: 2 | Val loss: 0.2726 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3704 | Steps: 2 | Val loss: 0.2915 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=120410)[0m rmse: 0.15120509266853333
[2m[36m(func pid=120410)[0m mae:  0.1029784232378006
[2m[36m(func pid=120410)[0m rmse_per_class: [0.105, 0.212, 0.043, 0.306, 0.084, 0.163, 0.246, 0.096, 0.149, 0.107]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.16228720545768738
[2m[36m(func pid=119084)[0m mae:  0.11665777862071991
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.247, 0.055, 0.32, 0.067, 0.177, 0.263, 0.127, 0.148, 0.115]
[2m[36m(func pid=119084)[0m 
== Status ==
Current time: 2024-01-07 05:46:45 (running for 00:45:51.27)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.37  |  0.162 |                   52 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.323 |  0.151 |                   47 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3563 | Steps: 2 | Val loss: 0.2684 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3721 | Steps: 2 | Val loss: 0.2911 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=120410)[0m rmse: 0.14880432188510895
[2m[36m(func pid=120410)[0m mae:  0.1006733775138855
[2m[36m(func pid=120410)[0m rmse_per_class: [0.107, 0.211, 0.041, 0.3, 0.086, 0.163, 0.24, 0.096, 0.142, 0.102]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.16218465566635132
[2m[36m(func pid=119084)[0m mae:  0.1165432333946228
[2m[36m(func pid=119084)[0m rmse_per_class: [0.105, 0.247, 0.054, 0.319, 0.067, 0.177, 0.263, 0.126, 0.147, 0.116]
== Status ==
Current time: 2024-01-07 05:46:50 (running for 00:45:56.44)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.372 |  0.162 |                   53 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.356 |  0.149 |                   48 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3124 | Steps: 2 | Val loss: 0.2662 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3692 | Steps: 2 | Val loss: 0.2905 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=120410)[0m rmse: 0.148061603307724
[2m[36m(func pid=120410)[0m mae:  0.09917902946472168
[2m[36m(func pid=120410)[0m rmse_per_class: [0.12, 0.209, 0.046, 0.28, 0.089, 0.163, 0.247, 0.096, 0.137, 0.093]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:46:55 (running for 00:46:01.81)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.369 |  0.162 |                   54 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.312 |  0.148 |                   49 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16189752519130707
[2m[36m(func pid=119084)[0m mae:  0.11631309986114502
[2m[36m(func pid=119084)[0m rmse_per_class: [0.107, 0.247, 0.053, 0.318, 0.066, 0.177, 0.264, 0.126, 0.147, 0.114]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3267 | Steps: 2 | Val loss: 0.2680 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3676 | Steps: 2 | Val loss: 0.2897 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=120410)[0m rmse: 0.15005160868167877
[2m[36m(func pid=120410)[0m mae:  0.09948160499334335
[2m[36m(func pid=120410)[0m rmse_per_class: [0.122, 0.209, 0.061, 0.275, 0.098, 0.162, 0.252, 0.096, 0.136, 0.089]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:47:00 (running for 00:46:07.03)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.368 |  0.161 |                   55 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.327 |  0.15  |                   50 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.1613481044769287
[2m[36m(func pid=119084)[0m mae:  0.11591044813394547
[2m[36m(func pid=119084)[0m rmse_per_class: [0.108, 0.246, 0.051, 0.317, 0.066, 0.177, 0.264, 0.126, 0.146, 0.113]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3206 | Steps: 2 | Val loss: 0.2668 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3719 | Steps: 2 | Val loss: 0.2891 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=120410)[0m rmse: 0.14945979416370392
[2m[36m(func pid=120410)[0m mae:  0.09961937367916107
[2m[36m(func pid=120410)[0m rmse_per_class: [0.103, 0.211, 0.057, 0.283, 0.105, 0.162, 0.245, 0.096, 0.146, 0.088]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.16096560657024384
[2m[36m(func pid=119084)[0m mae:  0.11557513475418091
[2m[36m(func pid=119084)[0m rmse_per_class: [0.107, 0.246, 0.051, 0.316, 0.066, 0.176, 0.263, 0.125, 0.146, 0.112]
[2m[36m(func pid=119084)[0m 
== Status ==
Current time: 2024-01-07 05:47:06 (running for 00:46:12.41)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.372 |  0.161 |                   56 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.321 |  0.149 |                   51 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3234 | Steps: 2 | Val loss: 0.2710 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3686 | Steps: 2 | Val loss: 0.2885 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=120410)[0m rmse: 0.1503828912973404
[2m[36m(func pid=120410)[0m mae:  0.10097502171993256
[2m[36m(func pid=120410)[0m rmse_per_class: [0.096, 0.21, 0.05, 0.304, 0.104, 0.163, 0.24, 0.098, 0.147, 0.094]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:47:11 (running for 00:46:17.52)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.369 |  0.16  |                   57 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.323 |  0.15  |                   52 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16045178472995758
[2m[36m(func pid=119084)[0m mae:  0.11524634063243866
[2m[36m(func pid=119084)[0m rmse_per_class: [0.106, 0.246, 0.049, 0.316, 0.066, 0.176, 0.263, 0.125, 0.145, 0.112]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3127 | Steps: 2 | Val loss: 0.2678 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3650 | Steps: 2 | Val loss: 0.2881 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=120410)[0m rmse: 0.14897778630256653
[2m[36m(func pid=120410)[0m mae:  0.10007953643798828
[2m[36m(func pid=120410)[0m rmse_per_class: [0.108, 0.209, 0.042, 0.295, 0.095, 0.162, 0.243, 0.098, 0.136, 0.1]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:47:16 (running for 00:46:22.80)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.365 |  0.16  |                   58 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.313 |  0.149 |                   53 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16019774973392487
[2m[36m(func pid=119084)[0m mae:  0.11504100263118744
[2m[36m(func pid=119084)[0m rmse_per_class: [0.105, 0.245, 0.049, 0.316, 0.066, 0.176, 0.263, 0.125, 0.145, 0.112]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3107 | Steps: 2 | Val loss: 0.2672 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3687 | Steps: 2 | Val loss: 0.2877 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=120410)[0m rmse: 0.1490987241268158
[2m[36m(func pid=120410)[0m mae:  0.10014835745096207
[2m[36m(func pid=120410)[0m rmse_per_class: [0.124, 0.207, 0.045, 0.285, 0.085, 0.162, 0.251, 0.096, 0.137, 0.1]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:47:21 (running for 00:46:27.96)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.369 |  0.16  |                   59 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.311 |  0.149 |                   54 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15996521711349487
[2m[36m(func pid=119084)[0m mae:  0.11483509838581085
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.245, 0.049, 0.316, 0.067, 0.176, 0.262, 0.125, 0.144, 0.111]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3072 | Steps: 2 | Val loss: 0.2709 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3643 | Steps: 2 | Val loss: 0.2876 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=120410)[0m rmse: 0.15024833381175995
[2m[36m(func pid=120410)[0m mae:  0.10168337821960449
[2m[36m(func pid=120410)[0m rmse_per_class: [0.116, 0.206, 0.051, 0.292, 0.078, 0.163, 0.255, 0.095, 0.145, 0.102]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:47:27 (running for 00:46:33.28)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.364 |  0.16  |                   60 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.307 |  0.15  |                   55 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16002748906612396
[2m[36m(func pid=119084)[0m mae:  0.1147649735212326
[2m[36m(func pid=119084)[0m rmse_per_class: [0.103, 0.245, 0.052, 0.315, 0.067, 0.176, 0.263, 0.124, 0.145, 0.111]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3140 | Steps: 2 | Val loss: 0.2715 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3629 | Steps: 2 | Val loss: 0.2872 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=120410)[0m rmse: 0.14992737770080566
[2m[36m(func pid=120410)[0m mae:  0.1017233282327652
[2m[36m(func pid=120410)[0m rmse_per_class: [0.104, 0.207, 0.049, 0.3, 0.075, 0.163, 0.248, 0.095, 0.157, 0.101]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:47:32 (running for 00:46:38.68)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.363 |  0.16  |                   61 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.314 |  0.15  |                   56 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.1599688082933426
[2m[36m(func pid=119084)[0m mae:  0.11459078639745712
[2m[36m(func pid=119084)[0m rmse_per_class: [0.103, 0.245, 0.053, 0.313, 0.068, 0.176, 0.262, 0.124, 0.145, 0.11]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3101 | Steps: 2 | Val loss: 0.2690 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3640 | Steps: 2 | Val loss: 0.2866 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=120410)[0m rmse: 0.14841647446155548
[2m[36m(func pid=120410)[0m mae:  0.10044930130243301
[2m[36m(func pid=120410)[0m rmse_per_class: [0.108, 0.205, 0.045, 0.297, 0.076, 0.162, 0.241, 0.096, 0.159, 0.096]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.1597018539905548
[2m[36m(func pid=119084)[0m mae:  0.114314004778862
[2m[36m(func pid=119084)[0m rmse_per_class: [0.102, 0.245, 0.053, 0.311, 0.068, 0.176, 0.262, 0.124, 0.145, 0.11]
[2m[36m(func pid=119084)[0m 
== Status ==
Current time: 2024-01-07 05:47:37 (running for 00:46:43.98)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.364 |  0.16  |                   62 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.31  |  0.148 |                   57 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3073 | Steps: 2 | Val loss: 0.2667 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3654 | Steps: 2 | Val loss: 0.2866 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=120410)[0m rmse: 0.14750251173973083
[2m[36m(func pid=120410)[0m mae:  0.09922372549772263
[2m[36m(func pid=120410)[0m rmse_per_class: [0.12, 0.204, 0.043, 0.294, 0.077, 0.16, 0.239, 0.096, 0.149, 0.092]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:47:43 (running for 00:46:49.25)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.365 |  0.16  |                   63 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.307 |  0.148 |                   58 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.1597980558872223
[2m[36m(func pid=119084)[0m mae:  0.11421249806880951
[2m[36m(func pid=119084)[0m rmse_per_class: [0.102, 0.245, 0.056, 0.311, 0.068, 0.176, 0.261, 0.124, 0.145, 0.11]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3077 | Steps: 2 | Val loss: 0.2661 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3639 | Steps: 2 | Val loss: 0.2866 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=120410)[0m rmse: 0.1478681117296219
[2m[36m(func pid=120410)[0m mae:  0.09879597276449203
[2m[36m(func pid=120410)[0m rmse_per_class: [0.127, 0.206, 0.043, 0.293, 0.082, 0.16, 0.24, 0.096, 0.141, 0.091]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:47:48 (running for 00:46:54.76)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.364 |  0.16  |                   64 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.308 |  0.148 |                   59 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.1598607301712036
[2m[36m(func pid=119084)[0m mae:  0.11425236612558365
[2m[36m(func pid=119084)[0m rmse_per_class: [0.103, 0.245, 0.056, 0.311, 0.068, 0.176, 0.261, 0.124, 0.145, 0.11]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3207 | Steps: 2 | Val loss: 0.2680 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3688 | Steps: 2 | Val loss: 0.2867 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=120410)[0m rmse: 0.14948995411396027
[2m[36m(func pid=120410)[0m mae:  0.10011768341064453
[2m[36m(func pid=120410)[0m rmse_per_class: [0.126, 0.207, 0.041, 0.295, 0.09, 0.161, 0.246, 0.095, 0.144, 0.09]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:47:53 (running for 00:47:00.00)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.369 |  0.16  |                   65 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.321 |  0.149 |                   60 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.16002428531646729
[2m[36m(func pid=119084)[0m mae:  0.1142820343375206
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.245, 0.057, 0.311, 0.068, 0.176, 0.262, 0.124, 0.145, 0.11]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3063 | Steps: 2 | Val loss: 0.2710 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3644 | Steps: 2 | Val loss: 0.2863 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=120410)[0m rmse: 0.15058931708335876
[2m[36m(func pid=120410)[0m mae:  0.10162520408630371
[2m[36m(func pid=120410)[0m rmse_per_class: [0.106, 0.209, 0.04, 0.302, 0.099, 0.161, 0.251, 0.097, 0.151, 0.091]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:47:59 (running for 00:47:05.32)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.364 |  0.16  |                   66 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.306 |  0.151 |                   61 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15969505906105042
[2m[36m(func pid=119084)[0m mae:  0.11413417011499405
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.244, 0.055, 0.31, 0.068, 0.176, 0.262, 0.124, 0.145, 0.109]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3092 | Steps: 2 | Val loss: 0.2721 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3635 | Steps: 2 | Val loss: 0.2864 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=120410)[0m rmse: 0.15153104066848755
[2m[36m(func pid=120410)[0m mae:  0.10250002145767212
[2m[36m(func pid=120410)[0m rmse_per_class: [0.106, 0.209, 0.041, 0.293, 0.103, 0.162, 0.259, 0.096, 0.153, 0.093]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:48:04 (running for 00:47:10.89)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.363 |  0.16  |                   67 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.309 |  0.152 |                   62 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15975013375282288
[2m[36m(func pid=119084)[0m mae:  0.11421482264995575
[2m[36m(func pid=119084)[0m rmse_per_class: [0.105, 0.244, 0.054, 0.311, 0.068, 0.176, 0.262, 0.124, 0.146, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3081 | Steps: 2 | Val loss: 0.2720 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3616 | Steps: 2 | Val loss: 0.2865 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=120410)[0m rmse: 0.152486652135849
[2m[36m(func pid=120410)[0m mae:  0.10219082981348038
[2m[36m(func pid=120410)[0m rmse_per_class: [0.118, 0.207, 0.052, 0.286, 0.102, 0.164, 0.258, 0.096, 0.144, 0.098]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:48:09 (running for 00:47:15.97)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.362 |  0.16  |                   68 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.308 |  0.152 |                   63 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15974389016628265
[2m[36m(func pid=119084)[0m mae:  0.11432834714651108
[2m[36m(func pid=119084)[0m rmse_per_class: [0.105, 0.244, 0.053, 0.312, 0.068, 0.176, 0.262, 0.123, 0.147, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3106 | Steps: 2 | Val loss: 0.2672 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3655 | Steps: 2 | Val loss: 0.2865 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=120410)[0m rmse: 0.1493452489376068
[2m[36m(func pid=120410)[0m mae:  0.09967736154794693
[2m[36m(func pid=120410)[0m rmse_per_class: [0.119, 0.208, 0.046, 0.282, 0.096, 0.162, 0.245, 0.095, 0.14, 0.099]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:48:15 (running for 00:47:21.37)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.366 |  0.16  |                   69 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.311 |  0.149 |                   64 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15970847010612488
[2m[36m(func pid=119084)[0m mae:  0.11424809694290161
[2m[36m(func pid=119084)[0m rmse_per_class: [0.105, 0.244, 0.053, 0.313, 0.067, 0.176, 0.261, 0.123, 0.148, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3026 | Steps: 2 | Val loss: 0.2629 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3624 | Steps: 2 | Val loss: 0.2863 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=120410)[0m rmse: 0.1460084617137909
[2m[36m(func pid=120410)[0m mae:  0.09760777652263641
[2m[36m(func pid=120410)[0m rmse_per_class: [0.117, 0.208, 0.04, 0.277, 0.087, 0.16, 0.237, 0.096, 0.144, 0.093]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.1595315933227539
[2m[36m(func pid=119084)[0m mae:  0.11415894329547882
[2m[36m(func pid=119084)[0m rmse_per_class: [0.105, 0.243, 0.052, 0.313, 0.067, 0.176, 0.261, 0.123, 0.148, 0.108]
[2m[36m(func pid=119084)[0m 
== Status ==
Current time: 2024-01-07 05:48:20 (running for 00:47:26.46)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.362 |  0.16  |                   70 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.303 |  0.146 |                   65 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3295 | Steps: 2 | Val loss: 0.2633 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3604 | Steps: 2 | Val loss: 0.2865 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=120410)[0m rmse: 0.145657017827034
[2m[36m(func pid=120410)[0m mae:  0.09782635420560837
[2m[36m(func pid=120410)[0m rmse_per_class: [0.113, 0.204, 0.039, 0.285, 0.081, 0.16, 0.238, 0.096, 0.151, 0.09]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:48:25 (running for 00:47:31.72)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.36  |  0.16  |                   71 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.33  |  0.146 |                   66 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.1595364660024643
[2m[36m(func pid=119084)[0m mae:  0.11427439749240875
[2m[36m(func pid=119084)[0m rmse_per_class: [0.105, 0.242, 0.051, 0.314, 0.067, 0.176, 0.261, 0.123, 0.148, 0.109]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3093 | Steps: 2 | Val loss: 0.2663 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3617 | Steps: 2 | Val loss: 0.2867 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=120410)[0m rmse: 0.14636239409446716
[2m[36m(func pid=120410)[0m mae:  0.09863686561584473
[2m[36m(func pid=120410)[0m rmse_per_class: [0.106, 0.206, 0.041, 0.293, 0.08, 0.159, 0.248, 0.096, 0.148, 0.088]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.15963047742843628
[2m[36m(func pid=119084)[0m mae:  0.1143069863319397
[2m[36m(func pid=119084)[0m rmse_per_class: [0.106, 0.242, 0.051, 0.315, 0.067, 0.176, 0.261, 0.122, 0.148, 0.109]
[2m[36m(func pid=119084)[0m 
== Status ==
Current time: 2024-01-07 05:48:30 (running for 00:47:36.86)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.362 |  0.16  |                   72 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.309 |  0.146 |                   67 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3046 | Steps: 2 | Val loss: 0.2673 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3631 | Steps: 2 | Val loss: 0.2869 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=120410)[0m rmse: 0.1469735950231552
[2m[36m(func pid=120410)[0m mae:  0.09883473813533783
[2m[36m(func pid=120410)[0m rmse_per_class: [0.103, 0.207, 0.045, 0.287, 0.082, 0.159, 0.259, 0.099, 0.143, 0.087]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:48:35 (running for 00:47:41.98)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.363 |  0.16  |                   73 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.305 |  0.147 |                   68 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15970811247825623
[2m[36m(func pid=119084)[0m mae:  0.11435536295175552
[2m[36m(func pid=119084)[0m rmse_per_class: [0.107, 0.241, 0.05, 0.316, 0.067, 0.175, 0.261, 0.122, 0.147, 0.109]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2996 | Steps: 2 | Val loss: 0.2690 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3605 | Steps: 2 | Val loss: 0.2872 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=120410)[0m rmse: 0.14995789527893066
[2m[36m(func pid=120410)[0m mae:  0.09987666457891464
[2m[36m(func pid=120410)[0m rmse_per_class: [0.13, 0.205, 0.045, 0.285, 0.09, 0.159, 0.258, 0.097, 0.14, 0.09]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:48:40 (running for 00:47:47.02)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15924999490380287
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.361 |  0.16  |                   74 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.3   |  0.15  |                   69 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15969082713127136
[2m[36m(func pid=119084)[0m mae:  0.11441805213689804
[2m[36m(func pid=119084)[0m rmse_per_class: [0.107, 0.241, 0.049, 0.318, 0.068, 0.175, 0.26, 0.122, 0.147, 0.11]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3031 | Steps: 2 | Val loss: 0.2693 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3593 | Steps: 2 | Val loss: 0.2870 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=120410)[0m rmse: 0.15042750537395477
[2m[36m(func pid=120410)[0m mae:  0.09964369237422943
[2m[36m(func pid=120410)[0m rmse_per_class: [0.136, 0.211, 0.044, 0.281, 0.094, 0.159, 0.246, 0.094, 0.138, 0.1]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:48:46 (running for 00:47:52.19)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.359 |  0.159 |                   75 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.303 |  0.15  |                   70 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15946267545223236
[2m[36m(func pid=119084)[0m mae:  0.11423834413290024
[2m[36m(func pid=119084)[0m rmse_per_class: [0.105, 0.24, 0.049, 0.319, 0.068, 0.175, 0.26, 0.122, 0.146, 0.11]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3065 | Steps: 2 | Val loss: 0.2648 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3625 | Steps: 2 | Val loss: 0.2869 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=120410)[0m rmse: 0.14734385907649994
[2m[36m(func pid=120410)[0m mae:  0.09771649539470673
[2m[36m(func pid=120410)[0m rmse_per_class: [0.125, 0.209, 0.041, 0.274, 0.092, 0.161, 0.236, 0.097, 0.139, 0.1]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.15940925478935242
[2m[36m(func pid=119084)[0m mae:  0.11409244686365128
[2m[36m(func pid=119084)[0m rmse_per_class: [0.105, 0.24, 0.05, 0.319, 0.069, 0.175, 0.259, 0.122, 0.145, 0.11]
== Status ==
Current time: 2024-01-07 05:48:51 (running for 00:47:57.51)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.363 |  0.159 |                   76 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.307 |  0.147 |                   71 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3131 | Steps: 2 | Val loss: 0.2625 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3593 | Steps: 2 | Val loss: 0.2864 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=120410)[0m rmse: 0.14520959556102753
[2m[36m(func pid=120410)[0m mae:  0.0971614271402359
[2m[36m(func pid=120410)[0m rmse_per_class: [0.109, 0.204, 0.039, 0.277, 0.087, 0.161, 0.235, 0.099, 0.146, 0.094]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:48:56 (running for 00:48:03.01)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.359 |  0.159 |                   77 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.313 |  0.145 |                   72 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15911003947257996
[2m[36m(func pid=119084)[0m mae:  0.11382003873586655
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.24, 0.049, 0.319, 0.069, 0.175, 0.259, 0.121, 0.144, 0.11]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2995 | Steps: 2 | Val loss: 0.2627 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3579 | Steps: 2 | Val loss: 0.2861 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=120410)[0m rmse: 0.14440904557704926
[2m[36m(func pid=120410)[0m mae:  0.0969056636095047
[2m[36m(func pid=120410)[0m rmse_per_class: [0.105, 0.204, 0.038, 0.278, 0.086, 0.159, 0.242, 0.095, 0.146, 0.09]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:49:02 (running for 00:48:08.36)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.358 |  0.159 |                   78 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.3   |  0.144 |                   73 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15902334451675415
[2m[36m(func pid=119084)[0m mae:  0.11365362256765366
[2m[36m(func pid=119084)[0m rmse_per_class: [0.105, 0.24, 0.05, 0.318, 0.069, 0.175, 0.258, 0.121, 0.144, 0.11]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3012 | Steps: 2 | Val loss: 0.2680 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3637 | Steps: 2 | Val loss: 0.2855 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=120410)[0m rmse: 0.14872637391090393
[2m[36m(func pid=120410)[0m mae:  0.09966126084327698
[2m[36m(func pid=120410)[0m rmse_per_class: [0.12, 0.203, 0.041, 0.287, 0.09, 0.158, 0.254, 0.094, 0.149, 0.09]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:49:07 (running for 00:48:13.56)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1589999943971634
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.364 |  0.159 |                   79 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.301 |  0.149 |                   74 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15874436497688293
[2m[36m(func pid=119084)[0m mae:  0.11337126791477203
[2m[36m(func pid=119084)[0m rmse_per_class: [0.105, 0.24, 0.05, 0.317, 0.07, 0.175, 0.258, 0.121, 0.143, 0.109]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2970 | Steps: 2 | Val loss: 0.2765 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3575 | Steps: 2 | Val loss: 0.2851 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=120410)[0m rmse: 0.15421895682811737
[2m[36m(func pid=120410)[0m mae:  0.10319993644952774
[2m[36m(func pid=120410)[0m rmse_per_class: [0.125, 0.205, 0.05, 0.302, 0.094, 0.159, 0.261, 0.096, 0.152, 0.098]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:49:12 (running for 00:48:18.69)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.358 |  0.159 |                   80 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.297 |  0.154 |                   75 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15868552029132843
[2m[36m(func pid=119084)[0m mae:  0.11323859542608261
[2m[36m(func pid=119084)[0m rmse_per_class: [0.106, 0.24, 0.051, 0.315, 0.069, 0.175, 0.258, 0.121, 0.144, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3043 | Steps: 2 | Val loss: 0.2755 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3594 | Steps: 2 | Val loss: 0.2850 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=120410)[0m rmse: 0.1531447470188141
[2m[36m(func pid=120410)[0m mae:  0.1021108403801918
[2m[36m(func pid=120410)[0m rmse_per_class: [0.116, 0.205, 0.057, 0.303, 0.092, 0.159, 0.251, 0.096, 0.149, 0.103]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:49:17 (running for 00:48:23.95)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.359 |  0.159 |                   81 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.304 |  0.153 |                   76 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15877462923526764
[2m[36m(func pid=119084)[0m mae:  0.11320199072360992
[2m[36m(func pid=119084)[0m rmse_per_class: [0.106, 0.241, 0.052, 0.314, 0.069, 0.175, 0.258, 0.121, 0.145, 0.107]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2982 | Steps: 2 | Val loss: 0.2663 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3603 | Steps: 2 | Val loss: 0.2849 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=120410)[0m rmse: 0.14738380908966064
[2m[36m(func pid=120410)[0m mae:  0.0982990488409996
[2m[36m(func pid=120410)[0m rmse_per_class: [0.121, 0.203, 0.04, 0.291, 0.085, 0.158, 0.239, 0.094, 0.144, 0.098]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:49:23 (running for 00:48:29.15)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.36  |  0.159 |                   82 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.298 |  0.147 |                   77 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15893833339214325
[2m[36m(func pid=119084)[0m mae:  0.11323815584182739
[2m[36m(func pid=119084)[0m rmse_per_class: [0.107, 0.241, 0.053, 0.312, 0.07, 0.175, 0.258, 0.12, 0.146, 0.106]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3216 | Steps: 2 | Val loss: 0.2638 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3583 | Steps: 2 | Val loss: 0.2848 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=120410)[0m rmse: 0.14529529213905334
[2m[36m(func pid=120410)[0m mae:  0.09709500521421432
[2m[36m(func pid=120410)[0m rmse_per_class: [0.115, 0.204, 0.038, 0.285, 0.083, 0.16, 0.237, 0.096, 0.143, 0.092]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:49:28 (running for 00:48:34.29)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.358 |  0.159 |                   83 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.322 |  0.145 |                   78 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15893253684043884
[2m[36m(func pid=119084)[0m mae:  0.11324682086706161
[2m[36m(func pid=119084)[0m rmse_per_class: [0.108, 0.241, 0.053, 0.312, 0.07, 0.175, 0.259, 0.12, 0.146, 0.106]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2985 | Steps: 2 | Val loss: 0.2672 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3558 | Steps: 2 | Val loss: 0.2845 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=120410)[0m rmse: 0.14665351808071136
[2m[36m(func pid=120410)[0m mae:  0.09871677309274673
[2m[36m(func pid=120410)[0m rmse_per_class: [0.103, 0.21, 0.038, 0.287, 0.087, 0.159, 0.25, 0.094, 0.147, 0.092]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:49:33 (running for 00:48:39.77)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.356 |  0.159 |                   84 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.299 |  0.147 |                   79 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15872099995613098
[2m[36m(func pid=119084)[0m mae:  0.11319825798273087
[2m[36m(func pid=119084)[0m rmse_per_class: [0.109, 0.24, 0.05, 0.311, 0.07, 0.175, 0.259, 0.12, 0.146, 0.107]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3089 | Steps: 2 | Val loss: 0.2696 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3576 | Steps: 2 | Val loss: 0.2840 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=120410)[0m rmse: 0.1493636667728424
[2m[36m(func pid=120410)[0m mae:  0.10008110851049423
[2m[36m(func pid=120410)[0m rmse_per_class: [0.105, 0.205, 0.042, 0.291, 0.1, 0.158, 0.257, 0.095, 0.148, 0.092]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:49:39 (running for 00:48:45.23)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.358 |  0.158 |                   85 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.309 |  0.149 |                   80 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15836304426193237
[2m[36m(func pid=119084)[0m mae:  0.11296814680099487
[2m[36m(func pid=119084)[0m rmse_per_class: [0.108, 0.24, 0.049, 0.31, 0.07, 0.175, 0.26, 0.12, 0.146, 0.106]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3082 | Steps: 2 | Val loss: 0.2744 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3560 | Steps: 2 | Val loss: 0.2839 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=120410)[0m rmse: 0.15445519983768463
[2m[36m(func pid=120410)[0m mae:  0.10252448171377182
[2m[36m(func pid=120410)[0m rmse_per_class: [0.115, 0.208, 0.061, 0.286, 0.11, 0.159, 0.263, 0.094, 0.157, 0.091]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:49:44 (running for 00:48:50.61)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.356 |  0.158 |                   86 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.308 |  0.154 |                   81 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15831701457500458
[2m[36m(func pid=119084)[0m mae:  0.11289523541927338
[2m[36m(func pid=119084)[0m rmse_per_class: [0.107, 0.239, 0.05, 0.31, 0.07, 0.175, 0.26, 0.119, 0.145, 0.107]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2973 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3572 | Steps: 2 | Val loss: 0.2841 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=120410)[0m rmse: 0.15401986241340637
[2m[36m(func pid=120410)[0m mae:  0.10235317051410675
[2m[36m(func pid=120410)[0m rmse_per_class: [0.121, 0.215, 0.047, 0.28, 0.114, 0.159, 0.255, 0.094, 0.165, 0.09]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:49:49 (running for 00:48:55.66)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.357 |  0.158 |                   87 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.297 |  0.154 |                   82 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15828587114810944
[2m[36m(func pid=119084)[0m mae:  0.11299894005060196
[2m[36m(func pid=119084)[0m rmse_per_class: [0.107, 0.239, 0.049, 0.311, 0.07, 0.175, 0.26, 0.119, 0.146, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3024 | Steps: 2 | Val loss: 0.2643 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3543 | Steps: 2 | Val loss: 0.2844 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=120410)[0m rmse: 0.14739330112934113
[2m[36m(func pid=120410)[0m mae:  0.0975109189748764
[2m[36m(func pid=120410)[0m rmse_per_class: [0.115, 0.209, 0.039, 0.277, 0.107, 0.159, 0.236, 0.095, 0.145, 0.093]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.15842494368553162
[2m[36m(func pid=119084)[0m mae:  0.11316186189651489
[2m[36m(func pid=119084)[0m rmse_per_class: [0.107, 0.239, 0.048, 0.312, 0.07, 0.175, 0.26, 0.119, 0.146, 0.108]
[2m[36m(func pid=119084)[0m 
== Status ==
Current time: 2024-01-07 05:49:54 (running for 00:49:00.75)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.354 |  0.158 |                   88 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.302 |  0.147 |                   83 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2973 | Steps: 2 | Val loss: 0.2625 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3535 | Steps: 2 | Val loss: 0.2847 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=120410)[0m rmse: 0.14517180621623993
[2m[36m(func pid=120410)[0m mae:  0.0956936851143837
[2m[36m(func pid=120410)[0m rmse_per_class: [0.117, 0.204, 0.037, 0.281, 0.094, 0.159, 0.235, 0.095, 0.134, 0.096]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:49:59 (running for 00:49:05.99)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.353 |  0.159 |                   89 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.297 |  0.145 |                   84 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.158512145280838
[2m[36m(func pid=119084)[0m mae:  0.1132362112402916
[2m[36m(func pid=119084)[0m rmse_per_class: [0.107, 0.238, 0.048, 0.314, 0.07, 0.175, 0.26, 0.119, 0.146, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3009 | Steps: 2 | Val loss: 0.2698 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3544 | Steps: 2 | Val loss: 0.2849 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=120410)[0m rmse: 0.1491669863462448
[2m[36m(func pid=120410)[0m mae:  0.09809841960668564
[2m[36m(func pid=120410)[0m rmse_per_class: [0.129, 0.21, 0.042, 0.298, 0.09, 0.159, 0.241, 0.093, 0.133, 0.095]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:50:05 (running for 00:49:11.21)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.354 |  0.159 |                   90 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.301 |  0.149 |                   85 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15856659412384033
[2m[36m(func pid=119084)[0m mae:  0.11320523917675018
[2m[36m(func pid=119084)[0m rmse_per_class: [0.108, 0.238, 0.049, 0.315, 0.069, 0.175, 0.259, 0.119, 0.146, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2964 | Steps: 2 | Val loss: 0.2734 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3557 | Steps: 2 | Val loss: 0.2846 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=120410)[0m rmse: 0.15156976878643036
[2m[36m(func pid=120410)[0m mae:  0.10031916201114655
[2m[36m(func pid=120410)[0m rmse_per_class: [0.131, 0.203, 0.052, 0.3, 0.09, 0.158, 0.251, 0.093, 0.144, 0.093]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:50:10 (running for 00:49:16.44)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.356 |  0.158 |                   91 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.296 |  0.152 |                   86 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15833285450935364
[2m[36m(func pid=119084)[0m mae:  0.11296729743480682
[2m[36m(func pid=119084)[0m rmse_per_class: [0.107, 0.238, 0.049, 0.316, 0.07, 0.175, 0.258, 0.119, 0.145, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3015 | Steps: 2 | Val loss: 0.2700 | Batch size: 32 | lr: 0.1 | Duration: 3.26s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3535 | Steps: 2 | Val loss: 0.2844 | Batch size: 32 | lr: 0.01 | Duration: 3.24s
[2m[36m(func pid=120410)[0m rmse: 0.15009278059005737
[2m[36m(func pid=120410)[0m mae:  0.1002819687128067
[2m[36m(func pid=120410)[0m rmse_per_class: [0.118, 0.202, 0.046, 0.286, 0.087, 0.158, 0.252, 0.094, 0.168, 0.091]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:50:15 (running for 00:49:21.97)
Memory usage on this node: 19.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.353 |  0.158 |                   92 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.302 |  0.15  |                   87 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.1581641435623169
[2m[36m(func pid=119084)[0m mae:  0.11279778182506561
[2m[36m(func pid=119084)[0m rmse_per_class: [0.106, 0.238, 0.049, 0.315, 0.07, 0.174, 0.258, 0.119, 0.145, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3022 | Steps: 2 | Val loss: 0.2636 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3573 | Steps: 2 | Val loss: 0.2838 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=120410)[0m rmse: 0.1456238031387329
[2m[36m(func pid=120410)[0m mae:  0.0972779244184494
[2m[36m(func pid=120410)[0m rmse_per_class: [0.113, 0.205, 0.039, 0.273, 0.082, 0.157, 0.25, 0.094, 0.152, 0.092]
[2m[36m(func pid=120410)[0m 
== Status ==
Current time: 2024-01-07 05:50:21 (running for 00:49:27.24)
Memory usage on this node: 19.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.357 |  0.158 |                   93 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.302 |  0.146 |                   88 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15782637894153595
[2m[36m(func pid=119084)[0m mae:  0.11254167556762695
[2m[36m(func pid=119084)[0m rmse_per_class: [0.106, 0.237, 0.049, 0.314, 0.07, 0.174, 0.258, 0.118, 0.144, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2968 | Steps: 2 | Val loss: 0.2598 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3538 | Steps: 2 | Val loss: 0.2835 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=120410)[0m rmse: 0.14302584528923035
[2m[36m(func pid=120410)[0m mae:  0.09450825303792953
[2m[36m(func pid=120410)[0m rmse_per_class: [0.118, 0.202, 0.037, 0.273, 0.081, 0.157, 0.243, 0.094, 0.135, 0.091]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.15772955119609833
[2m[36m(func pid=119084)[0m mae:  0.11238658428192139
[2m[36m(func pid=119084)[0m rmse_per_class: [0.106, 0.237, 0.05, 0.313, 0.069, 0.174, 0.258, 0.118, 0.145, 0.107]
[2m[36m(func pid=119084)[0m 
== Status ==
Current time: 2024-01-07 05:50:26 (running for 00:49:32.61)
Memory usage on this node: 19.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.354 |  0.158 |                   94 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.297 |  0.143 |                   89 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2952 | Steps: 2 | Val loss: 0.2601 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3534 | Steps: 2 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=120410)[0m rmse: 0.14243772625923157
[2m[36m(func pid=120410)[0m mae:  0.09425038844347
[2m[36m(func pid=120410)[0m rmse_per_class: [0.107, 0.203, 0.037, 0.278, 0.083, 0.157, 0.241, 0.094, 0.133, 0.091]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.1577223241329193
[2m[36m(func pid=119084)[0m mae:  0.11235512793064117
[2m[36m(func pid=119084)[0m rmse_per_class: [0.106, 0.238, 0.05, 0.312, 0.069, 0.174, 0.258, 0.118, 0.146, 0.107]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3036 | Steps: 2 | Val loss: 0.2679 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3563 | Steps: 2 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 05:50:36 (running for 00:49:42.59)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.353 |  0.158 |                   95 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.295 |  0.142 |                   90 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.1578553169965744
[2m[36m(func pid=119084)[0m mae:  0.11242488771677017
[2m[36m(func pid=119084)[0m rmse_per_class: [0.106, 0.238, 0.05, 0.311, 0.069, 0.174, 0.258, 0.118, 0.147, 0.107]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.14834186434745789
[2m[36m(func pid=120410)[0m mae:  0.09881460666656494
[2m[36m(func pid=120410)[0m rmse_per_class: [0.104, 0.203, 0.047, 0.291, 0.094, 0.158, 0.246, 0.093, 0.152, 0.096]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3551 | Steps: 2 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2958 | Steps: 2 | Val loss: 0.2767 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 05:50:41 (running for 00:49:47.91)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.356 |  0.158 |                   96 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.304 |  0.148 |                   91 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15795817971229553
[2m[36m(func pid=119084)[0m mae:  0.11252337694168091
[2m[36m(func pid=119084)[0m rmse_per_class: [0.106, 0.238, 0.05, 0.31, 0.069, 0.174, 0.258, 0.118, 0.148, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.15483538806438446
[2m[36m(func pid=120410)[0m mae:  0.10319037735462189
[2m[36m(func pid=120410)[0m rmse_per_class: [0.121, 0.204, 0.043, 0.304, 0.107, 0.158, 0.248, 0.093, 0.173, 0.097]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3561 | Steps: 2 | Val loss: 0.2834 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2969 | Steps: 2 | Val loss: 0.2750 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 05:50:47 (running for 00:49:53.40)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.355 |  0.158 |                   97 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.296 |  0.155 |                   92 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m rmse: 0.15507817268371582
[2m[36m(func pid=120410)[0m mae:  0.10223327577114105
[2m[36m(func pid=120410)[0m rmse_per_class: [0.152, 0.206, 0.043, 0.296, 0.109, 0.158, 0.247, 0.093, 0.152, 0.095]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.158058300614357
[2m[36m(func pid=119084)[0m mae:  0.11254646629095078
[2m[36m(func pid=119084)[0m rmse_per_class: [0.106, 0.238, 0.051, 0.311, 0.069, 0.174, 0.258, 0.118, 0.148, 0.108]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3539 | Steps: 2 | Val loss: 0.2836 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2883 | Steps: 2 | Val loss: 0.2659 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 05:50:52 (running for 00:49:58.81)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.356 |  0.158 |                   98 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.297 |  0.155 |                   93 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119084)[0m rmse: 0.15809018909931183
[2m[36m(func pid=119084)[0m mae:  0.11259351670742035
[2m[36m(func pid=119084)[0m rmse_per_class: [0.106, 0.238, 0.051, 0.312, 0.069, 0.174, 0.257, 0.117, 0.149, 0.109]
[2m[36m(func pid=119084)[0m 
[2m[36m(func pid=120410)[0m rmse: 0.14817558228969574
[2m[36m(func pid=120410)[0m mae:  0.09765084087848663
[2m[36m(func pid=120410)[0m rmse_per_class: [0.13, 0.203, 0.039, 0.276, 0.103, 0.157, 0.248, 0.094, 0.143, 0.089]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2898 | Steps: 2 | Val loss: 0.2615 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=119084)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3603 | Steps: 2 | Val loss: 0.2829 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 05:50:58 (running for 00:50:04.21)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00022 | RUNNING    | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.354 |  0.158 |                   99 |
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.288 |  0.148 |                   94 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m rmse: 0.1433112621307373
[2m[36m(func pid=120410)[0m mae:  0.0951298326253891
[2m[36m(func pid=120410)[0m rmse_per_class: [0.094, 0.202, 0.042, 0.268, 0.094, 0.156, 0.247, 0.094, 0.146, 0.09]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=119084)[0m rmse: 0.157536119222641
[2m[36m(func pid=119084)[0m mae:  0.11223925650119781
[2m[36m(func pid=119084)[0m rmse_per_class: [0.104, 0.237, 0.049, 0.312, 0.069, 0.174, 0.257, 0.117, 0.148, 0.109]
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3029 | Steps: 2 | Val loss: 0.2638 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 05:51:03 (running for 00:50:09.64)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.29  |  0.143 |                   95 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
| train_5a6ec_00018 | TERMINATED | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.35  |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m rmse: 0.14505556225776672
[2m[36m(func pid=120410)[0m mae:  0.09684433043003082
[2m[36m(func pid=120410)[0m rmse_per_class: [0.093, 0.202, 0.041, 0.278, 0.091, 0.157, 0.244, 0.093, 0.155, 0.096]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3055 | Steps: 2 | Val loss: 0.2711 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 05:51:08 (running for 00:50:15.09)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.303 |  0.145 |                   96 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
| train_5a6ec_00018 | TERMINATED | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.35  |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m rmse: 0.15085816383361816
[2m[36m(func pid=120410)[0m mae:  0.10036027431488037
[2m[36m(func pid=120410)[0m rmse_per_class: [0.13, 0.202, 0.041, 0.298, 0.091, 0.157, 0.244, 0.093, 0.156, 0.096]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2906 | Steps: 2 | Val loss: 0.2735 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 05:51:14 (running for 00:50:20.56)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.305 |  0.151 |                   97 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
| train_5a6ec_00018 | TERMINATED | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.35  |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m rmse: 0.15346185863018036
[2m[36m(func pid=120410)[0m mae:  0.10062297433614731
[2m[36m(func pid=120410)[0m rmse_per_class: [0.172, 0.201, 0.043, 0.296, 0.093, 0.157, 0.247, 0.094, 0.139, 0.094]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2932 | Steps: 2 | Val loss: 0.2637 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 05:51:20 (running for 00:50:26.20)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.291 |  0.153 |                   98 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
| train_5a6ec_00018 | TERMINATED | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.35  |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=120410)[0m rmse: 0.14564864337444305
[2m[36m(func pid=120410)[0m mae:  0.09522879868745804
[2m[36m(func pid=120410)[0m rmse_per_class: [0.119, 0.201, 0.046, 0.278, 0.095, 0.156, 0.242, 0.094, 0.135, 0.091]
[2m[36m(func pid=120410)[0m 
[2m[36m(func pid=120410)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2931 | Steps: 2 | Val loss: 0.2614 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 05:51:25 (running for 00:50:31.80)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00023 | RUNNING    | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.293 |  0.146 |                   99 |
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
| train_5a6ec_00018 | TERMINATED | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.35  |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 05:51:26 (running for 00:50:32.39)
Memory usage on this node: 16.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: -0.1577499955892563
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/120.08 GiB heap, 0.0/55.45 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_5a6ec_00000 | TERMINATED | 192.168.7.53:14462  | 0.0001 |       0.99 |         0      |  0.481 |  0.178 |                   75 |
| train_5a6ec_00001 | TERMINATED | 192.168.7.53:14835  | 0.001  |       0.99 |         0      |  0.441 |  0.196 |                  100 |
| train_5a6ec_00002 | TERMINATED | 192.168.7.53:15258  | 0.01   |       0.99 |         0      |  0.358 |  0.156 |                  100 |
| train_5a6ec_00003 | TERMINATED | 192.168.7.53:15692  | 0.1    |       0.99 |         0      |  0.418 |  0.173 |                   75 |
| train_5a6ec_00004 | TERMINATED | 192.168.7.53:32174  | 0.0001 |       0.9  |         0      |  0.567 |  0.18  |                   75 |
| train_5a6ec_00005 | TERMINATED | 192.168.7.53:32277  | 0.001  |       0.9  |         0      |  0.413 |  0.174 |                   75 |
| train_5a6ec_00006 | TERMINATED | 192.168.7.53:37799  | 0.01   |       0.9  |         0      |  0.353 |  0.158 |                  100 |
| train_5a6ec_00007 | TERMINATED | 192.168.7.53:38408  | 0.1    |       0.9  |         0      |  0.291 |  0.145 |                  100 |
| train_5a6ec_00008 | TERMINATED | 192.168.7.53:49621  | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   75 |
| train_5a6ec_00009 | TERMINATED | 192.168.7.53:50153  | 0.001  |       0.99 |         0.0001 |  0.465 |  0.164 |                   75 |
| train_5a6ec_00010 | TERMINATED | 192.168.7.53:60723  | 0.01   |       0.99 |         0.0001 |  0.357 |  0.157 |                  100 |
| train_5a6ec_00011 | TERMINATED | 192.168.7.53:60725  | 0.1    |       0.99 |         0.0001 |  0.425 |  0.17  |                   75 |
| train_5a6ec_00012 | TERMINATED | 192.168.7.53:67138  | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   75 |
| train_5a6ec_00013 | TERMINATED | 192.168.7.53:67747  | 0.001  |       0.9  |         0.0001 |  0.413 |  0.175 |                   75 |
| train_5a6ec_00014 | TERMINATED | 192.168.7.53:78481  | 0.01   |       0.9  |         0.0001 |  0.352 |  0.158 |                  100 |
| train_5a6ec_00015 | TERMINATED | 192.168.7.53:83767  | 0.1    |       0.9  |         0.0001 |  0.289 |  0.145 |                  100 |
| train_5a6ec_00016 | TERMINATED | 192.168.7.53:84699  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   75 |
| train_5a6ec_00017 | TERMINATED | 192.168.7.53:85321  | 0.001  |       0.99 |         1e-05  |  0.458 |  0.162 |                   75 |
| train_5a6ec_00018 | TERMINATED | 192.168.7.53:101124 | 0.01   |       0.99 |         1e-05  |  0.35  |  0.155 |                  100 |
| train_5a6ec_00019 | TERMINATED | 192.168.7.53:101740 | 0.1    |       0.99 |         1e-05  |  0.51  |  0.168 |                   75 |
| train_5a6ec_00020 | TERMINATED | 192.168.7.53:102978 | 0.0001 |       0.9  |         1e-05  |  0.567 |  0.18  |                   75 |
| train_5a6ec_00021 | TERMINATED | 192.168.7.53:107141 | 0.001  |       0.9  |         1e-05  |  0.41  |  0.174 |                   75 |
| train_5a6ec_00022 | TERMINATED | 192.168.7.53:119084 | 0.01   |       0.9  |         1e-05  |  0.36  |  0.158 |                  100 |
| train_5a6ec_00023 | TERMINATED | 192.168.7.53:120410 | 0.1    |       0.9  |         1e-05  |  0.293 |  0.143 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+


[2m[36m(func pid=120410)[0m rmse: 0.14271649718284607
[2m[36m(func pid=120410)[0m mae:  0.09380804747343063
[2m[36m(func pid=120410)[0m rmse_per_class: [0.096, 0.201, 0.045, 0.276, 0.094, 0.156, 0.239, 0.094, 0.137, 0.09]
2024-01-07 05:51:26,264	INFO tune.py:798 -- Total run time: 3033.44 seconds (3032.37 seconds for the tuning loop).
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341339.1 ON aap04 CANCELLED AT 2024-01-07T05:51:33 ***
srun: error: aap04: task 0: Exited with exit code 1

IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 15:28:02,037	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 15:28:02,038	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 15:28:04,559	SUCC scripts.py:747 -- --------------------
2024-01-07 15:28:04,559	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 15:28:04,560	SUCC scripts.py:749 -- --------------------
2024-01-07 15:28:04,560	INFO scripts.py:751 -- Next steps
2024-01-07 15:28:04,560	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 15:28:04,560	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 15:28:04,560	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 15:28:04,560	INFO scripts.py:773 -- import ray
2024-01-07 15:28:04,560	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 15:28:04,560	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 15:28:04,560	INFO scripts.py:791 --   ray status
2024-01-07 15:28:04,560	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 15:28:04,560	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 15:28:04,560	INFO scripts.py:810 --   ray stop
2024-01-07 15:28:04,560	INFO scripts.py:891 -- --block
2024-01-07 15:28:04,560	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 15:28:04,560	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              12628881343549080806
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7f7bfe8df0a0>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          Supervised
task_name:           multilabel
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          10
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         random
seed:                42
dropout:             None
transfer_learning:   LP
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1


Supervised model resnet18 with random weights
Old final fully-connected layer: Linear(in_features=512, out_features=1000, bias=True)
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Linear probing adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 15:28:45,461	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 15:28:45,479	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 15:29:06,297	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-07 15:29:06 (running for 00:00:20.33)
Memory usage on this node: 13.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |
| train_10f5e_00001 | PENDING  |                     | 0.001  |       0.99 |         0      |
| train_10f5e_00002 | PENDING  |                     | 0.01   |       0.99 |         0      |
| train_10f5e_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138220)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138220)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=138220)[0m Configuration completed!
[2m[36m(func pid=138220)[0m New optimizer parameters:
[2m[36m(func pid=138220)[0m SGD (
[2m[36m(func pid=138220)[0m Parameter Group 0
[2m[36m(func pid=138220)[0m     dampening: 0
[2m[36m(func pid=138220)[0m     differentiable: False
[2m[36m(func pid=138220)[0m     foreach: None
[2m[36m(func pid=138220)[0m     lr: 0.0001
[2m[36m(func pid=138220)[0m     maximize: False
[2m[36m(func pid=138220)[0m     momentum: 0.99
[2m[36m(func pid=138220)[0m     nesterov: False
[2m[36m(func pid=138220)[0m     weight_decay: 0
[2m[36m(func pid=138220)[0m )
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0355 | Steps: 4 | Val loss: 0.7189 | Batch size: 32 | lr: 0.0001 | Duration: 5.86s
[2m[36m(func pid=138220)[0m rmse: 0.1804104745388031
[2m[36m(func pid=138220)[0m mae:  0.13286635279655457
[2m[36m(func pid=138220)[0m rmse_per_class: [0.114, 0.264, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
== Status ==
Current time: 2024-01-07 15:29:16 (running for 00:00:30.17)
Memory usage on this node: 15.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |
| train_10f5e_00002 | PENDING  |                     | 0.01   |       0.99 |         0      |
| train_10f5e_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138601)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=138601)[0m Configuration completed!
[2m[36m(func pid=138601)[0m New optimizer parameters:
[2m[36m(func pid=138601)[0m SGD (
[2m[36m(func pid=138601)[0m Parameter Group 0
[2m[36m(func pid=138601)[0m     dampening: 0
[2m[36m(func pid=138601)[0m     differentiable: False
[2m[36m(func pid=138601)[0m     foreach: None
[2m[36m(func pid=138601)[0m     lr: 0.001
[2m[36m(func pid=138601)[0m     maximize: False
[2m[36m(func pid=138601)[0m     momentum: 0.99
[2m[36m(func pid=138601)[0m     nesterov: False
[2m[36m(func pid=138601)[0m     weight_decay: 0
[2m[36m(func pid=138601)[0m )
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0227 | Steps: 4 | Val loss: 0.7033 | Batch size: 32 | lr: 0.001 | Duration: 4.53s
[2m[36m(func pid=138601)[0m rmse: 0.18037651479244232
[2m[36m(func pid=138601)[0m mae:  0.13282674551010132
[2m[36m(func pid=138601)[0m rmse_per_class: [0.114, 0.264, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
== Status ==
Current time: 2024-01-07 15:29:27 (running for 00:00:41.23)
Memory usage on this node: 17.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |
| train_10f5e_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139034)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=139034)[0m Configuration completed!
[2m[36m(func pid=139034)[0m New optimizer parameters:
[2m[36m(func pid=139034)[0m SGD (
[2m[36m(func pid=139034)[0m Parameter Group 0
[2m[36m(func pid=139034)[0m     dampening: 0
[2m[36m(func pid=139034)[0m     differentiable: False
[2m[36m(func pid=139034)[0m     foreach: None
[2m[36m(func pid=139034)[0m     lr: 0.01
[2m[36m(func pid=139034)[0m     maximize: False
[2m[36m(func pid=139034)[0m     momentum: 0.99
[2m[36m(func pid=139034)[0m     nesterov: False
[2m[36m(func pid=139034)[0m     weight_decay: 0
[2m[36m(func pid=139034)[0m )
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8861 | Steps: 4 | Val loss: 0.5902 | Batch size: 32 | lr: 0.01 | Duration: 4.84s
[2m[36m(func pid=139034)[0m rmse: 0.1800946295261383
[2m[36m(func pid=139034)[0m mae:  0.13252054154872894
[2m[36m(func pid=139034)[0m rmse_per_class: [0.114, 0.264, 0.105, 0.335, 0.101, 0.191, 0.294, 0.141, 0.144, 0.113]
== Status ==
Current time: 2024-01-07 15:29:36 (running for 00:00:49.84)
Memory usage on this node: 20.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139456)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=139456)[0m Configuration completed!
[2m[36m(func pid=139456)[0m New optimizer parameters:
[2m[36m(func pid=139456)[0m SGD (
[2m[36m(func pid=139456)[0m Parameter Group 0
[2m[36m(func pid=139456)[0m     dampening: 0
[2m[36m(func pid=139456)[0m     differentiable: False
[2m[36m(func pid=139456)[0m     foreach: None
[2m[36m(func pid=139456)[0m     lr: 0.1
[2m[36m(func pid=139456)[0m     maximize: False
[2m[36m(func pid=139456)[0m     momentum: 0.99
[2m[36m(func pid=139456)[0m     nesterov: False
[2m[36m(func pid=139456)[0m     weight_decay: 0
[2m[36m(func pid=139456)[0m )
[2m[36m(func pid=139456)[0m 
== Status ==
Current time: 2024-01-07 15:29:44 (running for 00:00:58.03)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |        |        |                      |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |        |        |                      |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.886 |   0.18 |                    1 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |        |        |                      |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.4909 | Steps: 4 | Val loss: 0.3942 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.9068 | Steps: 4 | Val loss: 0.6623 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0310 | Steps: 4 | Val loss: 0.7257 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.6547 | Steps: 4 | Val loss: 0.3536 | Batch size: 32 | lr: 0.1 | Duration: 4.60s
[2m[36m(func pid=139034)[0m rmse: 0.17926420271396637
[2m[36m(func pid=139034)[0m mae:  0.13142898678779602
[2m[36m(func pid=139034)[0m rmse_per_class: [0.114, 0.266, 0.118, 0.338, 0.093, 0.189, 0.285, 0.137, 0.147, 0.104]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:29:49 (running for 00:01:03.27)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  1.035 |  0.18  |                    1 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.907 |  0.181 |                    2 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.491 |  0.179 |                    2 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |        |        |                      |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.18063142895698547
[2m[36m(func pid=138601)[0m mae:  0.13307514786720276
[2m[36m(func pid=138601)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.105, 0.191, 0.3, 0.141, 0.143, 0.114]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.18083703517913818
[2m[36m(func pid=138220)[0m mae:  0.13326117396354675
[2m[36m(func pid=138220)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.106, 0.191, 0.302, 0.142, 0.143, 0.115]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.17922478914260864
[2m[36m(func pid=139456)[0m mae:  0.13008655607700348
[2m[36m(func pid=139456)[0m rmse_per_class: [0.138, 0.265, 0.118, 0.35, 0.074, 0.188, 0.273, 0.14, 0.149, 0.097]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5040 | Steps: 4 | Val loss: 0.3226 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7428 | Steps: 4 | Val loss: 0.5795 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0035 | Steps: 4 | Val loss: 0.7256 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.7872 | Steps: 4 | Val loss: 0.3723 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=139034)[0m rmse: 0.17742660641670227
[2m[36m(func pid=139034)[0m mae:  0.12833696603775024
[2m[36m(func pid=139034)[0m rmse_per_class: [0.116, 0.268, 0.136, 0.346, 0.072, 0.187, 0.269, 0.137, 0.15, 0.094]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:29:54 (running for 00:01:08.57)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  1.031 |  0.181 |                    2 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.743 |  0.18  |                    3 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.504 |  0.177 |                    3 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.655 |  0.179 |                    1 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.1804521381855011
[2m[36m(func pid=138601)[0m mae:  0.13291873037815094
[2m[36m(func pid=138601)[0m rmse_per_class: [0.114, 0.264, 0.103, 0.332, 0.105, 0.191, 0.3, 0.14, 0.144, 0.113]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.18106193840503693
[2m[36m(func pid=138220)[0m mae:  0.13345660269260406
[2m[36m(func pid=138220)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.108, 0.191, 0.304, 0.142, 0.143, 0.116]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.19235087931156158
[2m[36m(func pid=139456)[0m mae:  0.1313198059797287
[2m[36m(func pid=139456)[0m rmse_per_class: [0.319, 0.272, 0.063, 0.375, 0.055, 0.189, 0.267, 0.151, 0.14, 0.093]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6427 | Steps: 4 | Val loss: 0.3568 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.5872 | Steps: 4 | Val loss: 0.4793 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.9676 | Steps: 4 | Val loss: 0.7142 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=139034)[0m rmse: 0.177241712808609
[2m[36m(func pid=139034)[0m mae:  0.12404750287532806
[2m[36m(func pid=139034)[0m rmse_per_class: [0.114, 0.272, 0.129, 0.355, 0.057, 0.183, 0.282, 0.142, 0.148, 0.091]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8605 | Steps: 4 | Val loss: 0.3838 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 15:30:00 (running for 00:01:13.70)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  1.004 |  0.181 |                    3 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.587 |  0.18  |                    4 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.643 |  0.177 |                    4 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.787 |  0.192 |                    2 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.1799270212650299
[2m[36m(func pid=138601)[0m mae:  0.13244900107383728
[2m[36m(func pid=138601)[0m rmse_per_class: [0.115, 0.264, 0.105, 0.333, 0.103, 0.19, 0.297, 0.138, 0.145, 0.11]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.18112437427043915
[2m[36m(func pid=138220)[0m mae:  0.13352224230766296
[2m[36m(func pid=138220)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.108, 0.191, 0.305, 0.142, 0.142, 0.116]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.17259737849235535
[2m[36m(func pid=139456)[0m mae:  0.11775550991296768
[2m[36m(func pid=139456)[0m rmse_per_class: [0.102, 0.244, 0.046, 0.384, 0.056, 0.186, 0.311, 0.155, 0.146, 0.096]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.7200 | Steps: 4 | Val loss: 0.4110 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5013 | Steps: 4 | Val loss: 0.4004 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.9279 | Steps: 4 | Val loss: 0.6949 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=139034)[0m rmse: 0.18272969126701355
[2m[36m(func pid=139034)[0m mae:  0.1231796145439148
[2m[36m(func pid=139034)[0m rmse_per_class: [0.112, 0.275, 0.11, 0.366, 0.055, 0.181, 0.347, 0.148, 0.14, 0.093]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:30:05 (running for 00:01:18.78)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.968 |  0.181 |                    4 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.501 |  0.179 |                    5 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.72  |  0.183 |                    5 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.861 |  0.173 |                    3 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.17925173044204712
[2m[36m(func pid=138601)[0m mae:  0.13175912201404572
[2m[36m(func pid=138601)[0m rmse_per_class: [0.116, 0.264, 0.108, 0.335, 0.099, 0.189, 0.291, 0.137, 0.147, 0.107]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8088 | Steps: 4 | Val loss: 0.4265 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=138220)[0m rmse: 0.18105824291706085
[2m[36m(func pid=138220)[0m mae:  0.133457213640213
[2m[36m(func pid=138220)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.108, 0.191, 0.306, 0.143, 0.142, 0.116]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8061 | Steps: 4 | Val loss: 0.4479 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4562 | Steps: 4 | Val loss: 0.3483 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=139456)[0m rmse: 0.1930452287197113
[2m[36m(func pid=139456)[0m mae:  0.13287334144115448
[2m[36m(func pid=139456)[0m rmse_per_class: [0.109, 0.302, 0.06, 0.366, 0.056, 0.329, 0.31, 0.155, 0.147, 0.096]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8791 | Steps: 4 | Val loss: 0.6674 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=139034)[0m rmse: 0.18953049182891846
[2m[36m(func pid=139034)[0m mae:  0.12459419667720795
[2m[36m(func pid=139034)[0m rmse_per_class: [0.111, 0.278, 0.088, 0.376, 0.056, 0.181, 0.424, 0.152, 0.135, 0.095]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:30:10 (running for 00:01:24.05)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.928 |  0.181 |                    5 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.456 |  0.178 |                    6 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.806 |  0.19  |                    6 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.809 |  0.193 |                    4 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.17843982577323914
[2m[36m(func pid=138601)[0m mae:  0.1308739334344864
[2m[36m(func pid=138601)[0m rmse_per_class: [0.117, 0.265, 0.11, 0.338, 0.093, 0.187, 0.284, 0.136, 0.149, 0.104]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.7502 | Steps: 4 | Val loss: 0.3637 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=138220)[0m rmse: 0.18098950386047363
[2m[36m(func pid=138220)[0m mae:  0.13336968421936035
[2m[36m(func pid=138220)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.109, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8248 | Steps: 4 | Val loss: 0.4546 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4443 | Steps: 4 | Val loss: 0.3278 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=139456)[0m rmse: 0.17900073528289795
[2m[36m(func pid=139456)[0m mae:  0.1152910590171814
[2m[36m(func pid=139456)[0m rmse_per_class: [0.105, 0.29, 0.046, 0.432, 0.056, 0.179, 0.281, 0.127, 0.137, 0.136]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.19359037280082703
[2m[36m(func pid=139034)[0m mae:  0.12598881125450134
[2m[36m(func pid=139034)[0m rmse_per_class: [0.115, 0.279, 0.067, 0.381, 0.056, 0.181, 0.472, 0.154, 0.134, 0.096]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8235 | Steps: 4 | Val loss: 0.6343 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 15:30:15 (running for 00:01:29.11)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.879 |  0.181 |                    6 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.444 |  0.178 |                    7 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.825 |  0.194 |                    7 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.75  |  0.179 |                    5 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.17764848470687866
[2m[36m(func pid=138601)[0m mae:  0.1298125684261322
[2m[36m(func pid=138601)[0m rmse_per_class: [0.119, 0.265, 0.113, 0.344, 0.085, 0.186, 0.277, 0.136, 0.151, 0.1]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8944 | Steps: 4 | Val loss: 0.5359 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=138220)[0m rmse: 0.18088769912719727
[2m[36m(func pid=138220)[0m mae:  0.13328693807125092
[2m[36m(func pid=138220)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.108, 0.19, 0.305, 0.144, 0.142, 0.116]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7182 | Steps: 4 | Val loss: 0.4322 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4706 | Steps: 4 | Val loss: 0.3301 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=139456)[0m rmse: 0.2210659682750702
[2m[36m(func pid=139456)[0m mae:  0.14917030930519104
[2m[36m(func pid=139456)[0m rmse_per_class: [0.121, 0.299, 0.049, 0.353, 0.056, 0.217, 0.279, 0.418, 0.138, 0.279]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.19222931563854218
[2m[36m(func pid=139034)[0m mae:  0.1252545714378357
[2m[36m(func pid=139034)[0m rmse_per_class: [0.137, 0.277, 0.058, 0.382, 0.056, 0.179, 0.448, 0.155, 0.134, 0.097]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7780 | Steps: 4 | Val loss: 0.6026 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 15:30:20 (running for 00:01:34.14)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.823 |  0.181 |                    7 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.471 |  0.177 |                    8 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.718 |  0.192 |                    8 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.894 |  0.221 |                    6 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.1773746907711029
[2m[36m(func pid=138601)[0m mae:  0.12900346517562866
[2m[36m(func pid=138601)[0m rmse_per_class: [0.12, 0.267, 0.113, 0.349, 0.077, 0.185, 0.273, 0.137, 0.155, 0.098]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8622 | Steps: 4 | Val loss: 0.4646 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6821 | Steps: 4 | Val loss: 0.4054 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=138220)[0m rmse: 0.18090786039829254
[2m[36m(func pid=138220)[0m mae:  0.13328848779201508
[2m[36m(func pid=138220)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.108, 0.19, 0.305, 0.143, 0.143, 0.116]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4957 | Steps: 4 | Val loss: 0.3453 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=139456)[0m rmse: 0.1958504319190979
[2m[36m(func pid=139456)[0m mae:  0.13213665783405304
[2m[36m(func pid=139456)[0m rmse_per_class: [0.109, 0.205, 0.049, 0.387, 0.056, 0.175, 0.317, 0.222, 0.346, 0.092]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.18565665185451508
[2m[36m(func pid=139034)[0m mae:  0.12276222556829453
[2m[36m(func pid=139034)[0m rmse_per_class: [0.183, 0.266, 0.066, 0.38, 0.056, 0.173, 0.347, 0.155, 0.134, 0.097]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.17748188972473145
[2m[36m(func pid=138601)[0m mae:  0.12819445133209229
[2m[36m(func pid=138601)[0m rmse_per_class: [0.123, 0.268, 0.112, 0.354, 0.069, 0.185, 0.272, 0.139, 0.157, 0.096]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:30:25 (running for 00:01:39.60)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.778 |  0.181 |                    8 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.496 |  0.177 |                    9 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.682 |  0.186 |                    9 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.862 |  0.196 |                    7 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7218 | Steps: 4 | Val loss: 0.5678 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8569 | Steps: 4 | Val loss: 0.6612 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.6601 | Steps: 4 | Val loss: 0.3742 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=138220)[0m rmse: 0.18074931204319
[2m[36m(func pid=138220)[0m mae:  0.13314664363861084
[2m[36m(func pid=138220)[0m rmse_per_class: [0.113, 0.265, 0.099, 0.329, 0.107, 0.19, 0.303, 0.143, 0.143, 0.116]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5331 | Steps: 4 | Val loss: 0.3651 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=139456)[0m rmse: 0.20025673508644104
[2m[36m(func pid=139456)[0m mae:  0.13727807998657227
[2m[36m(func pid=139456)[0m rmse_per_class: [0.111, 0.384, 0.049, 0.389, 0.056, 0.277, 0.315, 0.153, 0.171, 0.097]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.17565783858299255
[2m[36m(func pid=139034)[0m mae:  0.12080534547567368
[2m[36m(func pid=139034)[0m rmse_per_class: [0.162, 0.245, 0.081, 0.373, 0.056, 0.188, 0.251, 0.155, 0.149, 0.097]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:30:31 (running for 00:01:44.73)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.722 |  0.181 |                    9 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.533 |  0.178 |                   10 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.66  |  0.176 |                   10 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.857 |  0.2   |                    8 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.17793512344360352
[2m[36m(func pid=138601)[0m mae:  0.12754777073860168
[2m[36m(func pid=138601)[0m rmse_per_class: [0.126, 0.269, 0.108, 0.359, 0.064, 0.185, 0.275, 0.14, 0.159, 0.094]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.6729 | Steps: 4 | Val loss: 0.5307 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7573 | Steps: 4 | Val loss: 0.4736 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7010 | Steps: 4 | Val loss: 0.3722 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5689 | Steps: 4 | Val loss: 0.3940 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=138220)[0m rmse: 0.18046876788139343
[2m[36m(func pid=138220)[0m mae:  0.1329163908958435
[2m[36m(func pid=138220)[0m rmse_per_class: [0.114, 0.265, 0.099, 0.329, 0.106, 0.19, 0.302, 0.143, 0.143, 0.115]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.17927932739257812
[2m[36m(func pid=139456)[0m mae:  0.11208905279636383
[2m[36m(func pid=139456)[0m rmse_per_class: [0.138, 0.227, 0.051, 0.388, 0.055, 0.199, 0.352, 0.155, 0.135, 0.091]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.1805102527141571
[2m[36m(func pid=139034)[0m mae:  0.126066192984581
[2m[36m(func pid=139034)[0m rmse_per_class: [0.097, 0.249, 0.065, 0.359, 0.056, 0.218, 0.275, 0.154, 0.235, 0.097]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:30:36 (running for 00:01:49.85)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.673 |  0.18  |                   10 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.569 |  0.179 |                   11 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.701 |  0.181 |                   11 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.757 |  0.179 |                    9 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.17946913838386536
[2m[36m(func pid=138601)[0m mae:  0.12733343243598938
[2m[36m(func pid=138601)[0m rmse_per_class: [0.13, 0.27, 0.105, 0.364, 0.059, 0.186, 0.286, 0.142, 0.16, 0.093]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6263 | Steps: 4 | Val loss: 0.4936 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8285 | Steps: 4 | Val loss: 0.5186 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.6757 | Steps: 4 | Val loss: 0.3794 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5949 | Steps: 4 | Val loss: 0.4166 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=138220)[0m rmse: 0.18018607795238495
[2m[36m(func pid=138220)[0m mae:  0.13268908858299255
[2m[36m(func pid=138220)[0m rmse_per_class: [0.114, 0.265, 0.099, 0.33, 0.104, 0.19, 0.3, 0.142, 0.144, 0.115]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.1811983287334442
[2m[36m(func pid=139034)[0m mae:  0.12567515671253204
[2m[36m(func pid=139034)[0m rmse_per_class: [0.104, 0.291, 0.047, 0.323, 0.056, 0.225, 0.313, 0.153, 0.204, 0.096]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.24015513062477112
[2m[36m(func pid=139456)[0m mae:  0.15978729724884033
[2m[36m(func pid=139456)[0m rmse_per_class: [0.108, 0.3, 0.094, 0.357, 0.117, 0.215, 0.286, 0.152, 0.135, 0.636]
[2m[36m(func pid=139456)[0m 
== Status ==
Current time: 2024-01-07 15:30:41 (running for 00:01:55.04)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.626 |  0.18  |                   11 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.595 |  0.181 |                   12 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.676 |  0.181 |                   12 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.829 |  0.24  |                   10 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.18070410192012787
[2m[36m(func pid=138601)[0m mae:  0.1269722282886505
[2m[36m(func pid=138601)[0m rmse_per_class: [0.132, 0.271, 0.099, 0.367, 0.056, 0.187, 0.298, 0.144, 0.159, 0.093]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5821 | Steps: 4 | Val loss: 0.4604 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.6254 | Steps: 4 | Val loss: 0.3473 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7090 | Steps: 4 | Val loss: 0.5100 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5993 | Steps: 4 | Val loss: 0.4376 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=138220)[0m rmse: 0.17989149689674377
[2m[36m(func pid=138220)[0m mae:  0.1324431151151657
[2m[36m(func pid=138220)[0m rmse_per_class: [0.114, 0.265, 0.099, 0.33, 0.102, 0.19, 0.299, 0.142, 0.144, 0.114]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.1694086641073227
[2m[36m(func pid=139034)[0m mae:  0.11607994139194489
[2m[36m(func pid=139034)[0m rmse_per_class: [0.108, 0.274, 0.041, 0.292, 0.056, 0.226, 0.321, 0.146, 0.137, 0.092]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.20615199208259583
[2m[36m(func pid=139456)[0m mae:  0.13456787168979645
[2m[36m(func pid=139456)[0m rmse_per_class: [0.108, 0.297, 0.04, 0.353, 0.37, 0.222, 0.307, 0.122, 0.155, 0.088]
[2m[36m(func pid=139456)[0m 
== Status ==
Current time: 2024-01-07 15:30:46 (running for 00:02:00.10)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.582 |  0.18  |                   12 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.599 |  0.182 |                   13 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.625 |  0.169 |                   13 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.709 |  0.206 |                   11 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.1821739226579666
[2m[36m(func pid=138601)[0m mae:  0.12680880725383759
[2m[36m(func pid=138601)[0m rmse_per_class: [0.135, 0.272, 0.093, 0.37, 0.055, 0.188, 0.312, 0.146, 0.159, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5516 | Steps: 4 | Val loss: 0.4328 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5487 | Steps: 4 | Val loss: 0.3229 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.6114 | Steps: 4 | Val loss: 0.4504 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.6256 | Steps: 4 | Val loss: 0.4502 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=138220)[0m rmse: 0.1796150952577591
[2m[36m(func pid=138220)[0m mae:  0.13218313455581665
[2m[36m(func pid=138220)[0m rmse_per_class: [0.114, 0.265, 0.101, 0.331, 0.101, 0.19, 0.297, 0.141, 0.145, 0.113]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.16585476696491241
[2m[36m(func pid=139034)[0m mae:  0.11096149682998657
[2m[36m(func pid=139034)[0m rmse_per_class: [0.108, 0.24, 0.044, 0.333, 0.056, 0.194, 0.316, 0.125, 0.135, 0.107]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.1802586168050766
[2m[36m(func pid=139456)[0m mae:  0.11532182991504669
[2m[36m(func pid=139456)[0m rmse_per_class: [0.138, 0.211, 0.049, 0.375, 0.238, 0.207, 0.239, 0.118, 0.131, 0.097]
[2m[36m(func pid=139456)[0m 
== Status ==
Current time: 2024-01-07 15:30:51 (running for 00:02:05.44)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.552 |  0.18  |                   13 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.626 |  0.183 |                   14 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.549 |  0.166 |                   14 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.611 |  0.18  |                   12 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.1833222508430481
[2m[36m(func pid=138601)[0m mae:  0.12666282057762146
[2m[36m(func pid=138601)[0m rmse_per_class: [0.138, 0.273, 0.087, 0.372, 0.055, 0.188, 0.324, 0.147, 0.158, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5317 | Steps: 4 | Val loss: 0.4128 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5364 | Steps: 4 | Val loss: 0.3565 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.6906 | Steps: 4 | Val loss: 0.4956 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.6392 | Steps: 4 | Val loss: 0.4649 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=138220)[0m rmse: 0.1793631613254547
[2m[36m(func pid=138220)[0m mae:  0.1319570541381836
[2m[36m(func pid=138220)[0m rmse_per_class: [0.114, 0.265, 0.102, 0.332, 0.1, 0.19, 0.295, 0.14, 0.145, 0.111]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.18801717460155487
[2m[36m(func pid=139034)[0m mae:  0.1276106983423233
[2m[36m(func pid=139034)[0m rmse_per_class: [0.105, 0.27, 0.046, 0.279, 0.056, 0.175, 0.305, 0.17, 0.139, 0.336]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.20856218039989471
[2m[36m(func pid=139456)[0m mae:  0.12767110764980316
[2m[36m(func pid=139456)[0m rmse_per_class: [0.101, 0.473, 0.049, 0.385, 0.077, 0.181, 0.297, 0.288, 0.139, 0.096]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.18510687351226807
[2m[36m(func pid=138601)[0m mae:  0.12687748670578003
[2m[36m(func pid=138601)[0m rmse_per_class: [0.142, 0.274, 0.081, 0.375, 0.054, 0.19, 0.339, 0.149, 0.156, 0.092]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:30:56 (running for 00:02:10.49)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.532 |  0.179 |                   14 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.639 |  0.185 |                   15 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.536 |  0.188 |                   15 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.691 |  0.209 |                   13 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5422 | Steps: 4 | Val loss: 0.4323 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5078 | Steps: 4 | Val loss: 0.3926 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7269 | Steps: 4 | Val loss: 0.4397 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6589 | Steps: 4 | Val loss: 0.4764 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=139034)[0m rmse: 0.21537403762340546
[2m[36m(func pid=139034)[0m mae:  0.14512112736701965
[2m[36m(func pid=139034)[0m rmse_per_class: [0.102, 0.289, 0.048, 0.341, 0.056, 0.198, 0.283, 0.27, 0.139, 0.426]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17903152108192444
[2m[36m(func pid=138220)[0m mae:  0.13165085017681122
[2m[36m(func pid=138220)[0m rmse_per_class: [0.115, 0.265, 0.103, 0.333, 0.098, 0.189, 0.292, 0.139, 0.146, 0.11]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.18675318360328674
[2m[36m(func pid=138601)[0m mae:  0.12710508704185486
[2m[36m(func pid=138601)[0m rmse_per_class: [0.148, 0.275, 0.075, 0.377, 0.055, 0.191, 0.35, 0.15, 0.155, 0.091]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:31:02 (running for 00:02:15.69)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.508 |  0.179 |                   15 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.659 |  0.187 |                   16 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.542 |  0.215 |                   16 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.727 |  0.171 |                   14 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.17061516642570496
[2m[36m(func pid=139456)[0m mae:  0.10514543205499649
[2m[36m(func pid=139456)[0m rmse_per_class: [0.097, 0.202, 0.049, 0.278, 0.053, 0.223, 0.289, 0.129, 0.138, 0.249]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6219 | Steps: 4 | Val loss: 0.4485 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4853 | Steps: 4 | Val loss: 0.3748 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7164 | Steps: 4 | Val loss: 0.4910 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7137 | Steps: 4 | Val loss: 0.4601 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=139034)[0m rmse: 0.20858637988567352
[2m[36m(func pid=139034)[0m mae:  0.13885685801506042
[2m[36m(func pid=139034)[0m rmse_per_class: [0.131, 0.295, 0.048, 0.368, 0.056, 0.21, 0.263, 0.347, 0.14, 0.228]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17874762415885925
[2m[36m(func pid=138220)[0m mae:  0.13137799501419067
[2m[36m(func pid=138220)[0m rmse_per_class: [0.115, 0.265, 0.105, 0.335, 0.095, 0.189, 0.29, 0.138, 0.147, 0.108]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.18886442482471466
[2m[36m(func pid=138601)[0m mae:  0.12788110971450806
[2m[36m(func pid=138601)[0m rmse_per_class: [0.157, 0.277, 0.07, 0.379, 0.055, 0.193, 0.36, 0.151, 0.155, 0.092]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:31:07 (running for 00:02:21.24)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.485 |  0.179 |                   16 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.716 |  0.189 |                   17 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.622 |  0.209 |                   17 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.714 |  0.18  |                   15 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.18026328086853027
[2m[36m(func pid=139456)[0m mae:  0.11318202316761017
[2m[36m(func pid=139456)[0m rmse_per_class: [0.103, 0.291, 0.039, 0.307, 0.056, 0.173, 0.241, 0.15, 0.138, 0.304]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.5983 | Steps: 4 | Val loss: 0.4395 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4727 | Steps: 4 | Val loss: 0.3635 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6773 | Steps: 4 | Val loss: 0.4760 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6866 | Steps: 4 | Val loss: 0.4759 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=139034)[0m rmse: 0.2045234739780426
[2m[36m(func pid=139034)[0m mae:  0.13144133985042572
[2m[36m(func pid=139034)[0m rmse_per_class: [0.181, 0.294, 0.047, 0.374, 0.056, 0.209, 0.287, 0.352, 0.14, 0.106]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17848578095436096
[2m[36m(func pid=138220)[0m mae:  0.1310908943414688
[2m[36m(func pid=138220)[0m rmse_per_class: [0.115, 0.265, 0.106, 0.335, 0.094, 0.189, 0.288, 0.137, 0.147, 0.107]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.18881352245807648
[2m[36m(func pid=138601)[0m mae:  0.12759563326835632
[2m[36m(func pid=138601)[0m rmse_per_class: [0.157, 0.277, 0.066, 0.379, 0.055, 0.193, 0.361, 0.151, 0.157, 0.092]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:31:13 (running for 00:02:26.71)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.473 |  0.178 |                   17 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.677 |  0.189 |                   18 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.598 |  0.205 |                   18 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.687 |  0.198 |                   16 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.19814710319042206
[2m[36m(func pid=139456)[0m mae:  0.1315097063779831
[2m[36m(func pid=139456)[0m rmse_per_class: [0.237, 0.256, 0.054, 0.335, 0.056, 0.191, 0.269, 0.153, 0.338, 0.092]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6323 | Steps: 4 | Val loss: 0.4073 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4658 | Steps: 4 | Val loss: 0.3538 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6495 | Steps: 4 | Val loss: 0.4692 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7838 | Steps: 4 | Val loss: 0.5678 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=139034)[0m rmse: 0.19906753301620483
[2m[36m(func pid=139034)[0m mae:  0.12481653690338135
[2m[36m(func pid=139034)[0m rmse_per_class: [0.253, 0.288, 0.041, 0.367, 0.056, 0.194, 0.284, 0.276, 0.139, 0.091]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17828220129013062
[2m[36m(func pid=138220)[0m mae:  0.13084232807159424
[2m[36m(func pid=138220)[0m rmse_per_class: [0.116, 0.265, 0.108, 0.336, 0.092, 0.189, 0.286, 0.137, 0.148, 0.106]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.18939104676246643
[2m[36m(func pid=138601)[0m mae:  0.1277424395084381
[2m[36m(func pid=138601)[0m rmse_per_class: [0.166, 0.277, 0.063, 0.38, 0.055, 0.193, 0.358, 0.152, 0.158, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.22285588085651398
[2m[36m(func pid=139456)[0m mae:  0.12787652015686035
[2m[36m(func pid=139456)[0m rmse_per_class: [0.14, 0.463, 0.229, 0.386, 0.056, 0.315, 0.311, 0.104, 0.131, 0.093]
== Status ==
Current time: 2024-01-07 15:31:18 (running for 00:02:32.16)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.466 |  0.178 |                   18 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.65  |  0.189 |                   19 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.632 |  0.199 |                   19 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.784 |  0.223 |                   17 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5658 | Steps: 4 | Val loss: 0.3804 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4616 | Steps: 4 | Val loss: 0.3471 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6825 | Steps: 4 | Val loss: 0.4728 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=139034)[0m rmse: 0.18001605570316315
[2m[36m(func pid=139034)[0m mae:  0.11168422549962997
[2m[36m(func pid=139034)[0m rmse_per_class: [0.264, 0.269, 0.049, 0.341, 0.056, 0.175, 0.254, 0.163, 0.138, 0.091]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8999 | Steps: 4 | Val loss: 0.6370 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=138601)[0m rmse: 0.191054567694664
[2m[36m(func pid=138601)[0m mae:  0.12827841937541962
[2m[36m(func pid=138601)[0m rmse_per_class: [0.182, 0.278, 0.059, 0.381, 0.056, 0.194, 0.357, 0.152, 0.159, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.1780441850423813
[2m[36m(func pid=138220)[0m mae:  0.13057094812393188
[2m[36m(func pid=138220)[0m rmse_per_class: [0.116, 0.265, 0.109, 0.338, 0.091, 0.189, 0.284, 0.136, 0.148, 0.104]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5583 | Steps: 4 | Val loss: 0.3659 | Batch size: 32 | lr: 0.01 | Duration: 2.62s
== Status ==
Current time: 2024-01-07 15:31:24 (running for 00:02:37.70)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.462 |  0.178 |                   19 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.682 |  0.191 |                   20 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.566 |  0.18  |                   20 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.9   |  0.224 |                   18 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.22364971041679382
[2m[36m(func pid=139456)[0m mae:  0.1341921091079712
[2m[36m(func pid=139456)[0m rmse_per_class: [0.108, 0.603, 0.092, 0.388, 0.056, 0.17, 0.323, 0.262, 0.14, 0.095]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6500 | Steps: 4 | Val loss: 0.4708 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4529 | Steps: 4 | Val loss: 0.3412 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=139034)[0m rmse: 0.16954545676708221
[2m[36m(func pid=139034)[0m mae:  0.10359841585159302
[2m[36m(func pid=139034)[0m rmse_per_class: [0.16, 0.239, 0.128, 0.295, 0.055, 0.243, 0.231, 0.117, 0.135, 0.092]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.7468 | Steps: 4 | Val loss: 0.4158 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=138601)[0m rmse: 0.19245082139968872
[2m[36m(func pid=138601)[0m mae:  0.12874910235404968
[2m[36m(func pid=138601)[0m rmse_per_class: [0.199, 0.278, 0.057, 0.382, 0.056, 0.195, 0.352, 0.153, 0.16, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17782218754291534
[2m[36m(func pid=138220)[0m mae:  0.13030992448329926
[2m[36m(func pid=138220)[0m rmse_per_class: [0.117, 0.265, 0.11, 0.339, 0.089, 0.188, 0.282, 0.136, 0.149, 0.103]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5331 | Steps: 4 | Val loss: 0.3459 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 15:31:29 (running for 00:02:43.00)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.453 |  0.178 |                   20 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.65  |  0.192 |                   21 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.558 |  0.17  |                   21 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.747 |  0.167 |                   19 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.16710537672042847
[2m[36m(func pid=139456)[0m mae:  0.09702345728874207
[2m[36m(func pid=139456)[0m rmse_per_class: [0.227, 0.227, 0.037, 0.291, 0.053, 0.222, 0.228, 0.154, 0.14, 0.093]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6550 | Steps: 4 | Val loss: 0.4641 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4489 | Steps: 4 | Val loss: 0.3372 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=139034)[0m rmse: 0.17571528255939484
[2m[36m(func pid=139034)[0m mae:  0.10833778232336044
[2m[36m(func pid=139034)[0m rmse_per_class: [0.096, 0.219, 0.177, 0.274, 0.053, 0.3, 0.272, 0.14, 0.132, 0.094]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.9998 | Steps: 4 | Val loss: 0.4490 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=138601)[0m rmse: 0.19341132044792175
[2m[36m(func pid=138601)[0m mae:  0.12914207577705383
[2m[36m(func pid=138601)[0m rmse_per_class: [0.218, 0.278, 0.056, 0.383, 0.056, 0.195, 0.342, 0.153, 0.162, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17763561010360718
[2m[36m(func pid=138220)[0m mae:  0.13006356358528137
[2m[36m(func pid=138220)[0m rmse_per_class: [0.117, 0.265, 0.11, 0.34, 0.087, 0.188, 0.28, 0.136, 0.15, 0.102]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5012 | Steps: 4 | Val loss: 0.3277 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 15:31:34 (running for 00:02:48.52)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.449 |  0.178 |                   21 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.655 |  0.193 |                   22 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.533 |  0.176 |                   22 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  1     |  0.18  |                   20 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.18044114112854004
[2m[36m(func pid=139456)[0m mae:  0.10580109059810638
[2m[36m(func pid=139456)[0m rmse_per_class: [0.136, 0.285, 0.049, 0.31, 0.153, 0.164, 0.243, 0.107, 0.14, 0.217]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.6588 | Steps: 4 | Val loss: 0.4581 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4422 | Steps: 4 | Val loss: 0.3331 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=139034)[0m rmse: 0.17152287065982819
[2m[36m(func pid=139034)[0m mae:  0.11335863918066025
[2m[36m(func pid=139034)[0m rmse_per_class: [0.105, 0.22, 0.093, 0.273, 0.052, 0.262, 0.306, 0.151, 0.158, 0.095]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.19451414048671722
[2m[36m(func pid=138601)[0m mae:  0.12969450652599335
[2m[36m(func pid=138601)[0m rmse_per_class: [0.243, 0.277, 0.054, 0.383, 0.056, 0.194, 0.327, 0.153, 0.164, 0.093]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6137 | Steps: 4 | Val loss: 0.5936 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=138220)[0m rmse: 0.17747317254543304
[2m[36m(func pid=138220)[0m mae:  0.1298777312040329
[2m[36m(func pid=138220)[0m rmse_per_class: [0.117, 0.266, 0.11, 0.34, 0.086, 0.188, 0.279, 0.136, 0.15, 0.102]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5318 | Steps: 4 | Val loss: 0.3637 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 15:31:40 (running for 00:02:53.83)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.442 |  0.177 |                   22 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.659 |  0.195 |                   23 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.501 |  0.172 |                   23 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.614 |  0.233 |                   21 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.2332298755645752
[2m[36m(func pid=139456)[0m mae:  0.15066267549991608
[2m[36m(func pid=139456)[0m rmse_per_class: [0.118, 0.241, 0.049, 0.382, 0.311, 0.194, 0.329, 0.118, 0.4, 0.19]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.6069 | Steps: 4 | Val loss: 0.4397 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=139034)[0m rmse: 0.1717107594013214
[2m[36m(func pid=139034)[0m mae:  0.11923973262310028
[2m[36m(func pid=139034)[0m rmse_per_class: [0.108, 0.242, 0.045, 0.281, 0.054, 0.193, 0.312, 0.154, 0.233, 0.095]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4535 | Steps: 4 | Val loss: 0.3318 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=138601)[0m rmse: 0.19341321289539337
[2m[36m(func pid=138601)[0m mae:  0.12944403290748596
[2m[36m(func pid=138601)[0m rmse_per_class: [0.244, 0.275, 0.054, 0.383, 0.056, 0.193, 0.315, 0.153, 0.169, 0.093]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8558 | Steps: 4 | Val loss: 0.6362 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=138220)[0m rmse: 0.17730927467346191
[2m[36m(func pid=138220)[0m mae:  0.12969882786273956
[2m[36m(func pid=138220)[0m rmse_per_class: [0.117, 0.265, 0.111, 0.341, 0.085, 0.188, 0.278, 0.136, 0.15, 0.101]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5535 | Steps: 4 | Val loss: 0.3577 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 15:31:45 (running for 00:02:59.44)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.454 |  0.177 |                   23 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.607 |  0.193 |                   24 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.532 |  0.172 |                   24 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.856 |  0.225 |                   22 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.22472040355205536
[2m[36m(func pid=139456)[0m mae:  0.14019562304019928
[2m[36m(func pid=139456)[0m rmse_per_class: [0.162, 0.349, 0.034, 0.389, 0.211, 0.174, 0.319, 0.137, 0.385, 0.087]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5873 | Steps: 4 | Val loss: 0.4284 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=139034)[0m rmse: 0.17089001834392548
[2m[36m(func pid=139034)[0m mae:  0.11762911081314087
[2m[36m(func pid=139034)[0m rmse_per_class: [0.104, 0.252, 0.039, 0.292, 0.08, 0.178, 0.286, 0.154, 0.229, 0.094]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4416 | Steps: 4 | Val loss: 0.3305 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=138601)[0m rmse: 0.19256015121936798
[2m[36m(func pid=138601)[0m mae:  0.12932845950126648
[2m[36m(func pid=138601)[0m rmse_per_class: [0.251, 0.272, 0.053, 0.383, 0.056, 0.19, 0.3, 0.153, 0.174, 0.093]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.7145 | Steps: 4 | Val loss: 0.5246 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=138220)[0m rmse: 0.17711444199085236
[2m[36m(func pid=138220)[0m mae:  0.12942174077033997
[2m[36m(func pid=138220)[0m rmse_per_class: [0.118, 0.265, 0.112, 0.342, 0.083, 0.188, 0.276, 0.136, 0.151, 0.1]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5155 | Steps: 4 | Val loss: 0.3146 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 15:31:50 (running for 00:03:04.65)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.442 |  0.177 |                   24 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.587 |  0.193 |                   25 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.553 |  0.171 |                   25 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.715 |  0.185 |                   23 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.18450751900672913
[2m[36m(func pid=139456)[0m mae:  0.10713846981525421
[2m[36m(func pid=139456)[0m rmse_per_class: [0.103, 0.298, 0.097, 0.386, 0.067, 0.243, 0.269, 0.15, 0.14, 0.092]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.6276 | Steps: 4 | Val loss: 0.4225 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=139034)[0m rmse: 0.16345173120498657
[2m[36m(func pid=139034)[0m mae:  0.10828803479671478
[2m[36m(func pid=139034)[0m rmse_per_class: [0.098, 0.24, 0.039, 0.319, 0.153, 0.169, 0.225, 0.154, 0.146, 0.09]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4458 | Steps: 4 | Val loss: 0.3317 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=138601)[0m rmse: 0.1916891634464264
[2m[36m(func pid=138601)[0m mae:  0.12932053208351135
[2m[36m(func pid=138601)[0m rmse_per_class: [0.254, 0.269, 0.053, 0.383, 0.056, 0.189, 0.288, 0.153, 0.179, 0.093]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.8502 | Steps: 4 | Val loss: 0.4342 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=138220)[0m rmse: 0.17710989713668823
[2m[36m(func pid=138220)[0m mae:  0.1292455941438675
[2m[36m(func pid=138220)[0m rmse_per_class: [0.119, 0.265, 0.113, 0.344, 0.08, 0.188, 0.275, 0.136, 0.151, 0.099]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4424 | Steps: 4 | Val loss: 0.3453 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5522 | Steps: 4 | Val loss: 0.4115 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 15:31:56 (running for 00:03:10.08)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.446 |  0.177 |                   25 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.628 |  0.192 |                   26 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.515 |  0.163 |                   26 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.85  |  0.18  |                   24 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.18024078011512756
[2m[36m(func pid=139456)[0m mae:  0.10041388124227524
[2m[36m(func pid=139456)[0m rmse_per_class: [0.099, 0.208, 0.242, 0.296, 0.052, 0.291, 0.239, 0.139, 0.14, 0.097]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.18386732041835785
[2m[36m(func pid=139034)[0m mae:  0.11595497280359268
[2m[36m(func pid=139034)[0m rmse_per_class: [0.193, 0.229, 0.041, 0.35, 0.238, 0.175, 0.231, 0.153, 0.134, 0.095]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4521 | Steps: 4 | Val loss: 0.3350 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=138601)[0m rmse: 0.18922559916973114
[2m[36m(func pid=138601)[0m mae:  0.12863001227378845
[2m[36m(func pid=138601)[0m rmse_per_class: [0.244, 0.264, 0.053, 0.383, 0.056, 0.186, 0.276, 0.153, 0.186, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.6755 | Steps: 4 | Val loss: 0.4445 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=138220)[0m rmse: 0.17703144252300262
[2m[36m(func pid=138220)[0m mae:  0.12897448241710663
[2m[36m(func pid=138220)[0m rmse_per_class: [0.12, 0.265, 0.114, 0.346, 0.077, 0.187, 0.275, 0.136, 0.151, 0.098]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4562 | Steps: 4 | Val loss: 0.3908 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5440 | Steps: 4 | Val loss: 0.3983 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 15:32:01 (running for 00:03:15.61)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.452 |  0.177 |                   26 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.552 |  0.189 |                   27 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.442 |  0.184 |                   27 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.675 |  0.186 |                   25 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.18557748198509216
[2m[36m(func pid=139456)[0m mae:  0.11427770555019379
[2m[36m(func pid=139456)[0m rmse_per_class: [0.104, 0.248, 0.029, 0.496, 0.055, 0.169, 0.319, 0.119, 0.138, 0.179]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.2015388458967209
[2m[36m(func pid=139034)[0m mae:  0.1300411969423294
[2m[36m(func pid=139034)[0m rmse_per_class: [0.193, 0.264, 0.041, 0.362, 0.267, 0.189, 0.227, 0.15, 0.138, 0.184]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4511 | Steps: 4 | Val loss: 0.3325 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=138601)[0m rmse: 0.18538376688957214
[2m[36m(func pid=138601)[0m mae:  0.12716175615787506
[2m[36m(func pid=138601)[0m rmse_per_class: [0.221, 0.257, 0.052, 0.381, 0.056, 0.182, 0.265, 0.153, 0.194, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5730 | Steps: 4 | Val loss: 0.4596 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5653 | Steps: 4 | Val loss: 0.4156 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=138220)[0m rmse: 0.17684635519981384
[2m[36m(func pid=138220)[0m mae:  0.1288403570652008
[2m[36m(func pid=138220)[0m rmse_per_class: [0.12, 0.265, 0.114, 0.346, 0.077, 0.187, 0.274, 0.136, 0.151, 0.098]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5321 | Steps: 4 | Val loss: 0.3829 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 15:32:07 (running for 00:03:21.06)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.451 |  0.177 |                   27 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.544 |  0.185 |                   28 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.456 |  0.202 |                   28 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.573 |  0.19  |                   26 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.18957427144050598
[2m[36m(func pid=139456)[0m mae:  0.12112345546483994
[2m[36m(func pid=139456)[0m rmse_per_class: [0.102, 0.223, 0.049, 0.317, 0.055, 0.183, 0.269, 0.145, 0.364, 0.19]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.2140043079853058
[2m[36m(func pid=139034)[0m mae:  0.14110687375068665
[2m[36m(func pid=139034)[0m rmse_per_class: [0.138, 0.279, 0.038, 0.364, 0.257, 0.196, 0.233, 0.143, 0.139, 0.352]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.18101009726524353
[2m[36m(func pid=138601)[0m mae:  0.12551714479923248
[2m[36m(func pid=138601)[0m rmse_per_class: [0.189, 0.251, 0.052, 0.38, 0.056, 0.179, 0.256, 0.153, 0.202, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4593 | Steps: 4 | Val loss: 0.3342 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.6939 | Steps: 4 | Val loss: 0.4123 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5015 | Steps: 4 | Val loss: 0.3802 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=138220)[0m rmse: 0.17672649025917053
[2m[36m(func pid=138220)[0m mae:  0.12860983610153198
[2m[36m(func pid=138220)[0m rmse_per_class: [0.12, 0.266, 0.114, 0.347, 0.075, 0.187, 0.273, 0.136, 0.152, 0.098]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4927 | Steps: 4 | Val loss: 0.3707 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 15:32:12 (running for 00:03:26.58)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.459 |  0.177 |                   28 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.532 |  0.181 |                   29 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.565 |  0.214 |                   29 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.694 |  0.171 |                   27 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.17054513096809387
[2m[36m(func pid=139456)[0m mae:  0.09972180426120758
[2m[36m(func pid=139456)[0m rmse_per_class: [0.094, 0.279, 0.049, 0.373, 0.054, 0.245, 0.256, 0.106, 0.136, 0.114]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.20615394413471222
[2m[36m(func pid=139034)[0m mae:  0.13507820665836334
[2m[36m(func pid=139034)[0m rmse_per_class: [0.108, 0.28, 0.043, 0.347, 0.248, 0.186, 0.242, 0.11, 0.139, 0.358]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.1770191192626953
[2m[36m(func pid=138601)[0m mae:  0.12419763952493668
[2m[36m(func pid=138601)[0m rmse_per_class: [0.161, 0.246, 0.053, 0.378, 0.056, 0.176, 0.248, 0.152, 0.207, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4568 | Steps: 4 | Val loss: 0.3366 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6123 | Steps: 4 | Val loss: 0.4387 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5360 | Steps: 4 | Val loss: 0.3507 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=138220)[0m rmse: 0.17671184241771698
[2m[36m(func pid=138220)[0m mae:  0.1284526288509369
[2m[36m(func pid=138220)[0m rmse_per_class: [0.121, 0.265, 0.115, 0.348, 0.074, 0.187, 0.272, 0.136, 0.152, 0.097]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5006 | Steps: 4 | Val loss: 0.3633 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 15:32:18 (running for 00:03:32.11)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.457 |  0.177 |                   29 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.493 |  0.177 |                   30 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.501 |  0.206 |                   30 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.612 |  0.195 |                   28 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.19456733763217926
[2m[36m(func pid=139456)[0m mae:  0.11373839527368546
[2m[36m(func pid=139456)[0m rmse_per_class: [0.177, 0.327, 0.049, 0.317, 0.049, 0.334, 0.296, 0.124, 0.139, 0.134]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.18617306649684906
[2m[36m(func pid=139034)[0m mae:  0.117223359644413
[2m[36m(func pid=139034)[0m rmse_per_class: [0.112, 0.275, 0.098, 0.302, 0.188, 0.174, 0.254, 0.139, 0.139, 0.18]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.17422713339328766
[2m[36m(func pid=138601)[0m mae:  0.12322358787059784
[2m[36m(func pid=138601)[0m rmse_per_class: [0.135, 0.247, 0.057, 0.376, 0.056, 0.174, 0.247, 0.152, 0.206, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4580 | Steps: 4 | Val loss: 0.3335 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4773 | Steps: 4 | Val loss: 0.3470 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5724 | Steps: 4 | Val loss: 0.4388 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4687 | Steps: 4 | Val loss: 0.3552 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=138220)[0m rmse: 0.17632852494716644
[2m[36m(func pid=138220)[0m mae:  0.12817969918251038
[2m[36m(func pid=138220)[0m rmse_per_class: [0.121, 0.266, 0.113, 0.347, 0.073, 0.187, 0.271, 0.136, 0.152, 0.097]
[2m[36m(func pid=138220)[0m 
== Status ==
Current time: 2024-01-07 15:32:23 (running for 00:03:37.37)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.458 |  0.176 |                   30 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.501 |  0.174 |                   31 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.477 |  0.185 |                   32 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.612 |  0.195 |                   28 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.18509551882743835
[2m[36m(func pid=139034)[0m mae:  0.11580070108175278
[2m[36m(func pid=139034)[0m rmse_per_class: [0.109, 0.264, 0.132, 0.276, 0.1, 0.177, 0.291, 0.274, 0.138, 0.09]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.18842656910419464
[2m[36m(func pid=139456)[0m mae:  0.11355428397655487
[2m[36m(func pid=139456)[0m rmse_per_class: [0.28, 0.23, 0.042, 0.279, 0.077, 0.179, 0.291, 0.138, 0.139, 0.23]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.1730751395225525
[2m[36m(func pid=138601)[0m mae:  0.12307874858379364
[2m[36m(func pid=138601)[0m rmse_per_class: [0.113, 0.253, 0.067, 0.373, 0.056, 0.175, 0.251, 0.151, 0.199, 0.093]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4729 | Steps: 4 | Val loss: 0.3369 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4896 | Steps: 4 | Val loss: 0.3353 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.6024 | Steps: 4 | Val loss: 0.3955 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4817 | Steps: 4 | Val loss: 0.3495 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=138220)[0m rmse: 0.17633798718452454
[2m[36m(func pid=138220)[0m mae:  0.12800157070159912
[2m[36m(func pid=138220)[0m rmse_per_class: [0.123, 0.266, 0.113, 0.348, 0.071, 0.186, 0.271, 0.136, 0.152, 0.097]
[2m[36m(func pid=138220)[0m 
== Status ==
Current time: 2024-01-07 15:32:28 (running for 00:03:42.57)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.473 |  0.176 |                   31 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.469 |  0.173 |                   32 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.49  |  0.181 |                   33 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.572 |  0.188 |                   29 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.1812179684638977
[2m[36m(func pid=139034)[0m mae:  0.11611256748437881
[2m[36m(func pid=139034)[0m rmse_per_class: [0.106, 0.245, 0.107, 0.272, 0.056, 0.173, 0.307, 0.318, 0.137, 0.09]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.17299337685108185
[2m[36m(func pid=139456)[0m mae:  0.10297022014856339
[2m[36m(func pid=139456)[0m rmse_per_class: [0.097, 0.241, 0.097, 0.292, 0.133, 0.163, 0.273, 0.134, 0.131, 0.168]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.17330196499824524
[2m[36m(func pid=138601)[0m mae:  0.12313340604305267
[2m[36m(func pid=138601)[0m rmse_per_class: [0.101, 0.263, 0.079, 0.368, 0.056, 0.177, 0.259, 0.149, 0.185, 0.096]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4737 | Steps: 4 | Val loss: 0.3360 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4712 | Steps: 4 | Val loss: 0.3192 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4594 | Steps: 4 | Val loss: 0.3428 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5878 | Steps: 4 | Val loss: 0.5543 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=138220)[0m rmse: 0.17614363133907318
[2m[36m(func pid=138220)[0m mae:  0.1278398185968399
[2m[36m(func pid=138220)[0m rmse_per_class: [0.123, 0.266, 0.112, 0.348, 0.071, 0.186, 0.27, 0.137, 0.152, 0.096]
[2m[36m(func pid=138220)[0m 
== Status ==
Current time: 2024-01-07 15:32:34 (running for 00:03:47.74)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.474 |  0.176 |                   32 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.482 |  0.173 |                   33 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.471 |  0.173 |                   34 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.602 |  0.173 |                   30 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.17272213101387024
[2m[36m(func pid=139034)[0m mae:  0.11137690395116806
[2m[36m(func pid=139034)[0m rmse_per_class: [0.136, 0.223, 0.083, 0.271, 0.052, 0.17, 0.305, 0.261, 0.135, 0.093]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.17354151606559753
[2m[36m(func pid=138601)[0m mae:  0.12283644825220108
[2m[36m(func pid=138601)[0m rmse_per_class: [0.096, 0.272, 0.085, 0.362, 0.056, 0.182, 0.267, 0.148, 0.167, 0.1]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.215276837348938
[2m[36m(func pid=139456)[0m mae:  0.13460633158683777
[2m[36m(func pid=139456)[0m rmse_per_class: [0.1, 0.239, 0.241, 0.379, 0.222, 0.189, 0.315, 0.122, 0.229, 0.118]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4738 | Steps: 4 | Val loss: 0.3390 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4504 | Steps: 4 | Val loss: 0.3046 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4746 | Steps: 4 | Val loss: 0.3418 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5466 | Steps: 4 | Val loss: 0.4768 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=138220)[0m rmse: 0.1761554628610611
[2m[36m(func pid=138220)[0m mae:  0.12768661975860596
[2m[36m(func pid=138220)[0m rmse_per_class: [0.124, 0.265, 0.112, 0.35, 0.069, 0.186, 0.27, 0.137, 0.152, 0.096]
[2m[36m(func pid=138220)[0m 
== Status ==
Current time: 2024-01-07 15:32:39 (running for 00:03:53.21)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.474 |  0.176 |                   33 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.459 |  0.174 |                   34 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.45  |  0.162 |                   35 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.588 |  0.215 |                   31 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.1623016893863678
[2m[36m(func pid=139034)[0m mae:  0.10659795999526978
[2m[36m(func pid=139034)[0m rmse_per_class: [0.181, 0.205, 0.054, 0.279, 0.054, 0.165, 0.288, 0.162, 0.141, 0.094]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.17518474161624908
[2m[36m(func pid=138601)[0m mae:  0.12329816818237305
[2m[36m(func pid=138601)[0m rmse_per_class: [0.097, 0.276, 0.098, 0.355, 0.056, 0.186, 0.277, 0.146, 0.153, 0.108]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.21769919991493225
[2m[36m(func pid=139456)[0m mae:  0.12581929564476013
[2m[36m(func pid=139456)[0m rmse_per_class: [0.362, 0.241, 0.029, 0.376, 0.237, 0.173, 0.276, 0.252, 0.136, 0.095]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4838 | Steps: 4 | Val loss: 0.3419 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4382 | Steps: 4 | Val loss: 0.3152 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4670 | Steps: 4 | Val loss: 0.3358 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.5243 | Steps: 4 | Val loss: 0.4098 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=138220)[0m rmse: 0.1759651005268097
[2m[36m(func pid=138220)[0m mae:  0.12736202776432037
[2m[36m(func pid=138220)[0m rmse_per_class: [0.124, 0.266, 0.111, 0.35, 0.068, 0.186, 0.269, 0.137, 0.153, 0.096]
[2m[36m(func pid=138220)[0m 
== Status ==
Current time: 2024-01-07 15:32:44 (running for 00:03:58.54)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.484 |  0.176 |                   34 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.475 |  0.175 |                   35 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.438 |  0.157 |                   36 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.547 |  0.218 |                   32 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.15657654404640198
[2m[36m(func pid=139034)[0m mae:  0.10594364255666733
[2m[36m(func pid=139034)[0m rmse_per_class: [0.128, 0.21, 0.042, 0.3, 0.055, 0.168, 0.256, 0.102, 0.21, 0.094]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.17549017071723938
[2m[36m(func pid=138601)[0m mae:  0.12295593321323395
[2m[36m(func pid=138601)[0m rmse_per_class: [0.098, 0.272, 0.1, 0.345, 0.056, 0.193, 0.285, 0.143, 0.142, 0.121]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.17199485003948212
[2m[36m(func pid=139456)[0m mae:  0.0994315892457962
[2m[36m(func pid=139456)[0m rmse_per_class: [0.122, 0.224, 0.049, 0.298, 0.094, 0.171, 0.309, 0.202, 0.139, 0.11]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4849 | Steps: 4 | Val loss: 0.3444 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4819 | Steps: 4 | Val loss: 0.3362 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4950 | Steps: 4 | Val loss: 0.3329 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.6973 | Steps: 4 | Val loss: 0.3857 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 15:32:50 (running for 00:04:03.70)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.485 |  0.176 |                   35 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.467 |  0.175 |                   36 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.438 |  0.157 |                   36 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.524 |  0.172 |                   33 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138220)[0m rmse: 0.17603138089179993
[2m[36m(func pid=138220)[0m mae:  0.12729385495185852
[2m[36m(func pid=138220)[0m rmse_per_class: [0.125, 0.265, 0.111, 0.351, 0.067, 0.186, 0.269, 0.137, 0.153, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.1605747789144516
[2m[36m(func pid=139034)[0m mae:  0.10839134454727173
[2m[36m(func pid=139034)[0m rmse_per_class: [0.095, 0.23, 0.037, 0.314, 0.056, 0.176, 0.236, 0.107, 0.261, 0.094]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.176212340593338
[2m[36m(func pid=138601)[0m mae:  0.12265489250421524
[2m[36m(func pid=138601)[0m rmse_per_class: [0.1, 0.264, 0.112, 0.334, 0.056, 0.195, 0.291, 0.139, 0.136, 0.136]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.17113256454467773
[2m[36m(func pid=139456)[0m mae:  0.09826096892356873
[2m[36m(func pid=139456)[0m rmse_per_class: [0.106, 0.22, 0.049, 0.349, 0.049, 0.182, 0.276, 0.169, 0.13, 0.18]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4886 | Steps: 4 | Val loss: 0.3422 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4652 | Steps: 4 | Val loss: 0.3270 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4641 | Steps: 4 | Val loss: 0.2952 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.6278 | Steps: 4 | Val loss: 0.4847 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 15:32:55 (running for 00:04:09.22)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.485 |  0.176 |                   35 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.465 |  0.175 |                   38 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.482 |  0.161 |                   37 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.697 |  0.171 |                   34 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.1747356355190277
[2m[36m(func pid=138601)[0m mae:  0.12133920192718506
[2m[36m(func pid=138601)[0m rmse_per_class: [0.101, 0.254, 0.104, 0.318, 0.056, 0.195, 0.296, 0.134, 0.133, 0.155]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.1758238971233368
[2m[36m(func pid=138220)[0m mae:  0.12719881534576416
[2m[36m(func pid=138220)[0m rmse_per_class: [0.126, 0.265, 0.11, 0.351, 0.066, 0.186, 0.268, 0.137, 0.153, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.15660445392131805
[2m[36m(func pid=139034)[0m mae:  0.10299724340438843
[2m[36m(func pid=139034)[0m rmse_per_class: [0.092, 0.238, 0.04, 0.31, 0.056, 0.173, 0.225, 0.12, 0.223, 0.09]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.201986625790596
[2m[36m(func pid=139456)[0m mae:  0.1284000426530838
[2m[36m(func pid=139456)[0m rmse_per_class: [0.092, 0.247, 0.049, 0.377, 0.053, 0.221, 0.308, 0.12, 0.453, 0.1]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4515 | Steps: 4 | Val loss: 0.3259 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4979 | Steps: 4 | Val loss: 0.3452 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4021 | Steps: 4 | Val loss: 0.2845 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6005 | Steps: 4 | Val loss: 0.5279 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 15:33:00 (running for 00:04:14.46)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.489 |  0.176 |                   36 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.451 |  0.175 |                   39 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.464 |  0.157 |                   38 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.628 |  0.202 |                   35 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.17451712489128113
[2m[36m(func pid=138601)[0m mae:  0.12056130170822144
[2m[36m(func pid=138601)[0m rmse_per_class: [0.102, 0.247, 0.107, 0.306, 0.056, 0.191, 0.3, 0.131, 0.132, 0.172]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.15025214850902557
[2m[36m(func pid=139034)[0m mae:  0.09667925536632538
[2m[36m(func pid=139034)[0m rmse_per_class: [0.093, 0.227, 0.056, 0.294, 0.056, 0.163, 0.229, 0.131, 0.156, 0.097]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17585304379463196
[2m[36m(func pid=138220)[0m mae:  0.1270485371351242
[2m[36m(func pid=138220)[0m rmse_per_class: [0.128, 0.265, 0.11, 0.352, 0.065, 0.186, 0.268, 0.138, 0.153, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.17935259640216827
[2m[36m(func pid=139456)[0m mae:  0.10412247478961945
[2m[36m(func pid=139456)[0m rmse_per_class: [0.236, 0.308, 0.029, 0.387, 0.054, 0.163, 0.264, 0.126, 0.133, 0.093]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4542 | Steps: 4 | Val loss: 0.3262 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4024 | Steps: 4 | Val loss: 0.3115 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4974 | Steps: 4 | Val loss: 0.3512 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7101 | Steps: 4 | Val loss: 0.5073 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 15:33:05 (running for 00:04:19.66)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.498 |  0.176 |                   37 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.454 |  0.173 |                   40 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.402 |  0.15  |                   39 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.6   |  0.179 |                   36 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.1733579784631729
[2m[36m(func pid=138601)[0m mae:  0.11989233642816544
[2m[36m(func pid=138601)[0m rmse_per_class: [0.103, 0.248, 0.088, 0.296, 0.056, 0.188, 0.303, 0.128, 0.133, 0.191]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.1609688252210617
[2m[36m(func pid=139034)[0m mae:  0.10359853506088257
[2m[36m(func pid=139034)[0m rmse_per_class: [0.103, 0.203, 0.071, 0.279, 0.056, 0.164, 0.241, 0.138, 0.134, 0.22]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17621764540672302
[2m[36m(func pid=138220)[0m mae:  0.12706485390663147
[2m[36m(func pid=138220)[0m rmse_per_class: [0.13, 0.265, 0.11, 0.353, 0.063, 0.186, 0.269, 0.138, 0.153, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.19368493556976318
[2m[36m(func pid=139456)[0m mae:  0.11229096353054047
[2m[36m(func pid=139456)[0m rmse_per_class: [0.096, 0.221, 0.388, 0.356, 0.053, 0.199, 0.249, 0.135, 0.139, 0.103]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4697 | Steps: 4 | Val loss: 0.3279 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5120 | Steps: 4 | Val loss: 0.3502 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4970 | Steps: 4 | Val loss: 0.3491 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6038 | Steps: 4 | Val loss: 0.4441 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=138601)[0m rmse: 0.17301228642463684
[2m[36m(func pid=138601)[0m mae:  0.11969643831253052
[2m[36m(func pid=138601)[0m rmse_per_class: [0.103, 0.253, 0.075, 0.29, 0.056, 0.184, 0.304, 0.131, 0.133, 0.201]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:33:11 (running for 00:04:25.22)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.497 |  0.176 |                   38 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.47  |  0.173 |                   41 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.512 |  0.187 |                   41 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.71  |  0.194 |                   37 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.18721123039722443
[2m[36m(func pid=139034)[0m mae:  0.12432285398244858
[2m[36m(func pid=139034)[0m rmse_per_class: [0.099, 0.23, 0.04, 0.28, 0.056, 0.171, 0.279, 0.144, 0.136, 0.438]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17590363323688507
[2m[36m(func pid=138220)[0m mae:  0.12687797844409943
[2m[36m(func pid=138220)[0m rmse_per_class: [0.13, 0.265, 0.109, 0.353, 0.063, 0.186, 0.268, 0.138, 0.153, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.17739710211753845
[2m[36m(func pid=139456)[0m mae:  0.110764279961586
[2m[36m(func pid=139456)[0m rmse_per_class: [0.104, 0.236, 0.038, 0.458, 0.051, 0.207, 0.307, 0.123, 0.138, 0.112]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4522 | Steps: 4 | Val loss: 0.3292 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4705 | Steps: 4 | Val loss: 0.3631 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4999 | Steps: 4 | Val loss: 0.3513 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.5687 | Steps: 4 | Val loss: 0.3728 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=138601)[0m rmse: 0.17317667603492737
[2m[36m(func pid=138601)[0m mae:  0.11986593902111053
[2m[36m(func pid=138601)[0m rmse_per_class: [0.102, 0.259, 0.063, 0.288, 0.056, 0.18, 0.304, 0.141, 0.133, 0.205]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:33:16 (running for 00:04:30.44)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.497 |  0.176 |                   39 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.452 |  0.173 |                   42 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.471 |  0.192 |                   42 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.604 |  0.177 |                   38 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.1917530745267868
[2m[36m(func pid=139034)[0m mae:  0.1282767951488495
[2m[36m(func pid=139034)[0m rmse_per_class: [0.109, 0.255, 0.036, 0.279, 0.056, 0.168, 0.288, 0.143, 0.137, 0.445]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17592762410640717
[2m[36m(func pid=138220)[0m mae:  0.1267632395029068
[2m[36m(func pid=138220)[0m rmse_per_class: [0.131, 0.265, 0.108, 0.353, 0.062, 0.186, 0.268, 0.138, 0.153, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.16512349247932434
[2m[36m(func pid=139456)[0m mae:  0.10191398859024048
[2m[36m(func pid=139456)[0m rmse_per_class: [0.101, 0.215, 0.049, 0.301, 0.066, 0.167, 0.249, 0.113, 0.199, 0.193]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4805 | Steps: 4 | Val loss: 0.3294 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.5216 | Steps: 4 | Val loss: 0.3425 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5032 | Steps: 4 | Val loss: 0.3549 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.5357 | Steps: 4 | Val loss: 0.5336 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=138601)[0m rmse: 0.1737099587917328
[2m[36m(func pid=138601)[0m mae:  0.12010069936513901
[2m[36m(func pid=138601)[0m rmse_per_class: [0.102, 0.265, 0.054, 0.287, 0.056, 0.178, 0.303, 0.16, 0.134, 0.2]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:33:21 (running for 00:04:35.58)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.5   |  0.176 |                   40 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.481 |  0.174 |                   43 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.522 |  0.181 |                   43 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.569 |  0.165 |                   39 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.18111446499824524
[2m[36m(func pid=139034)[0m mae:  0.11815043538808823
[2m[36m(func pid=139034)[0m rmse_per_class: [0.189, 0.255, 0.04, 0.268, 0.056, 0.17, 0.28, 0.142, 0.138, 0.273]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17613878846168518
[2m[36m(func pid=138220)[0m mae:  0.12675458192825317
[2m[36m(func pid=138220)[0m rmse_per_class: [0.133, 0.265, 0.108, 0.354, 0.061, 0.186, 0.269, 0.138, 0.153, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.21185632050037384
[2m[36m(func pid=139456)[0m mae:  0.12408604472875595
[2m[36m(func pid=139456)[0m rmse_per_class: [0.18, 0.271, 0.049, 0.385, 0.241, 0.188, 0.269, 0.239, 0.201, 0.096]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4500 | Steps: 4 | Val loss: 0.3287 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4937 | Steps: 4 | Val loss: 0.3189 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5003 | Steps: 4 | Val loss: 0.3556 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=138601)[0m rmse: 0.17459748685359955
[2m[36m(func pid=138601)[0m mae:  0.1205243244767189
[2m[36m(func pid=138601)[0m rmse_per_class: [0.1, 0.269, 0.048, 0.286, 0.056, 0.178, 0.3, 0.186, 0.134, 0.19]
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6395 | Steps: 4 | Val loss: 0.6509 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:33:27 (running for 00:04:40.74)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.503 |  0.176 |                   41 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.45  |  0.175 |                   44 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.494 |  0.171 |                   44 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.536 |  0.212 |                   40 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.17136070132255554
[2m[36m(func pid=139034)[0m mae:  0.10967105627059937
[2m[36m(func pid=139034)[0m rmse_per_class: [0.239, 0.243, 0.037, 0.266, 0.055, 0.187, 0.264, 0.142, 0.137, 0.144]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17602476477622986
[2m[36m(func pid=138220)[0m mae:  0.12657403945922852
[2m[36m(func pid=138220)[0m rmse_per_class: [0.134, 0.265, 0.108, 0.354, 0.06, 0.185, 0.268, 0.138, 0.152, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.2237071692943573
[2m[36m(func pid=139456)[0m mae:  0.13388487696647644
[2m[36m(func pid=139456)[0m rmse_per_class: [0.105, 0.415, 0.049, 0.389, 0.287, 0.201, 0.307, 0.268, 0.13, 0.088]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4516 | Steps: 4 | Val loss: 0.3258 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4322 | Steps: 4 | Val loss: 0.2864 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4843 | Steps: 4 | Val loss: 0.3508 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=138601)[0m rmse: 0.1750319004058838
[2m[36m(func pid=138601)[0m mae:  0.1206224337220192
[2m[36m(func pid=138601)[0m rmse_per_class: [0.098, 0.271, 0.046, 0.285, 0.056, 0.177, 0.298, 0.206, 0.134, 0.179]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6797 | Steps: 4 | Val loss: 0.4976 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=139034)[0m rmse: 0.1542445719242096
[2m[36m(func pid=139034)[0m mae:  0.09772536903619766
[2m[36m(func pid=139034)[0m rmse_per_class: [0.172, 0.218, 0.037, 0.262, 0.055, 0.197, 0.237, 0.136, 0.133, 0.095]
== Status ==
Current time: 2024-01-07 15:33:32 (running for 00:04:45.86)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.5   |  0.176 |                   42 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.452 |  0.175 |                   45 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.432 |  0.154 |                   45 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.639 |  0.224 |                   41 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.1754179298877716
[2m[36m(func pid=138220)[0m mae:  0.12621571123600006
[2m[36m(func pid=138220)[0m rmse_per_class: [0.134, 0.265, 0.106, 0.353, 0.06, 0.185, 0.266, 0.139, 0.152, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4902 | Steps: 4 | Val loss: 0.3237 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=139456)[0m rmse: 0.18016831576824188
[2m[36m(func pid=139456)[0m mae:  0.11041662842035294
[2m[36m(func pid=139456)[0m rmse_per_class: [0.11, 0.201, 0.036, 0.387, 0.181, 0.266, 0.28, 0.118, 0.13, 0.092]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3999 | Steps: 4 | Val loss: 0.2683 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4997 | Steps: 4 | Val loss: 0.3498 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=138601)[0m rmse: 0.1764272153377533
[2m[36m(func pid=138601)[0m mae:  0.1210210770368576
[2m[36m(func pid=138601)[0m rmse_per_class: [0.104, 0.273, 0.044, 0.285, 0.056, 0.179, 0.294, 0.232, 0.134, 0.163]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.141927570104599
[2m[36m(func pid=139034)[0m mae:  0.09100411087274551
[2m[36m(func pid=139034)[0m rmse_per_class: [0.1, 0.201, 0.042, 0.27, 0.053, 0.182, 0.223, 0.126, 0.133, 0.089]
== Status ==
Current time: 2024-01-07 15:33:37 (running for 00:04:51.02)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.484 |  0.175 |                   43 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.49  |  0.176 |                   46 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.4   |  0.142 |                   46 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.68  |  0.18  |                   42 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4719 | Steps: 4 | Val loss: 0.3995 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=138220)[0m rmse: 0.17534711956977844
[2m[36m(func pid=138220)[0m mae:  0.1260862946510315
[2m[36m(func pid=138220)[0m rmse_per_class: [0.135, 0.265, 0.104, 0.353, 0.059, 0.185, 0.266, 0.139, 0.152, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4614 | Steps: 4 | Val loss: 0.3219 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=139456)[0m rmse: 0.1787835657596588
[2m[36m(func pid=139456)[0m mae:  0.10905157029628754
[2m[36m(func pid=139456)[0m rmse_per_class: [0.191, 0.212, 0.039, 0.347, 0.058, 0.166, 0.241, 0.114, 0.228, 0.191]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4387 | Steps: 4 | Val loss: 0.2775 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5033 | Steps: 4 | Val loss: 0.3533 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=138601)[0m rmse: 0.1784079670906067
[2m[36m(func pid=138601)[0m mae:  0.12152369320392609
[2m[36m(func pid=138601)[0m rmse_per_class: [0.115, 0.274, 0.042, 0.29, 0.056, 0.18, 0.288, 0.26, 0.133, 0.146]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:33:42 (running for 00:04:56.18)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.5   |  0.175 |                   44 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.461 |  0.178 |                   47 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.439 |  0.146 |                   47 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.472 |  0.179 |                   43 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.14589634537696838
[2m[36m(func pid=139034)[0m mae:  0.09462472051382065
[2m[36m(func pid=139034)[0m rmse_per_class: [0.093, 0.202, 0.042, 0.303, 0.052, 0.16, 0.22, 0.106, 0.192, 0.09]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.6137 | Steps: 4 | Val loss: 0.4333 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=138220)[0m rmse: 0.17561405897140503
[2m[36m(func pid=138220)[0m mae:  0.1261037439107895
[2m[36m(func pid=138220)[0m rmse_per_class: [0.139, 0.265, 0.103, 0.354, 0.059, 0.185, 0.266, 0.139, 0.152, 0.094]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4406 | Steps: 4 | Val loss: 0.3195 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3904 | Steps: 4 | Val loss: 0.2970 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=139456)[0m rmse: 0.1823578029870987
[2m[36m(func pid=139456)[0m mae:  0.1107727512717247
[2m[36m(func pid=139456)[0m rmse_per_class: [0.127, 0.196, 0.113, 0.37, 0.056, 0.169, 0.316, 0.131, 0.151, 0.195]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5009 | Steps: 4 | Val loss: 0.3548 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=138601)[0m rmse: 0.18006932735443115
[2m[36m(func pid=138601)[0m mae:  0.12174252420663834
[2m[36m(func pid=138601)[0m rmse_per_class: [0.128, 0.272, 0.043, 0.298, 0.056, 0.181, 0.281, 0.275, 0.133, 0.132]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:33:47 (running for 00:05:01.32)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.503 |  0.176 |                   45 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.441 |  0.18  |                   48 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.39  |  0.155 |                   48 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.614 |  0.182 |                   44 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.1553359180688858
[2m[36m(func pid=139034)[0m mae:  0.10062868893146515
[2m[36m(func pid=139034)[0m rmse_per_class: [0.097, 0.249, 0.038, 0.323, 0.052, 0.16, 0.228, 0.096, 0.221, 0.09]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6466 | Steps: 4 | Val loss: 0.3742 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=138220)[0m rmse: 0.17581060528755188
[2m[36m(func pid=138220)[0m mae:  0.12612642347812653
[2m[36m(func pid=138220)[0m rmse_per_class: [0.141, 0.265, 0.103, 0.354, 0.058, 0.185, 0.266, 0.139, 0.152, 0.094]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4596 | Steps: 4 | Val loss: 0.3191 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4342 | Steps: 4 | Val loss: 0.3039 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=139456)[0m rmse: 0.15665367245674133
[2m[36m(func pid=139456)[0m mae:  0.09348712116479874
[2m[36m(func pid=139456)[0m rmse_per_class: [0.11, 0.235, 0.072, 0.283, 0.056, 0.188, 0.238, 0.139, 0.156, 0.089]
[2m[36m(func pid=139456)[0m 
== Status ==
Current time: 2024-01-07 15:33:52 (running for 00:05:06.34)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.501 |  0.176 |                   46 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.46  |  0.182 |                   49 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.39  |  0.155 |                   48 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.647 |  0.157 |                   45 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.1820863038301468
[2m[36m(func pid=138601)[0m mae:  0.12203945219516754
[2m[36m(func pid=138601)[0m rmse_per_class: [0.154, 0.271, 0.044, 0.309, 0.056, 0.182, 0.274, 0.277, 0.133, 0.12]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4969 | Steps: 4 | Val loss: 0.3532 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=139034)[0m rmse: 0.16292056441307068
[2m[36m(func pid=139034)[0m mae:  0.10497163236141205
[2m[36m(func pid=139034)[0m rmse_per_class: [0.093, 0.294, 0.036, 0.323, 0.064, 0.166, 0.261, 0.127, 0.178, 0.088]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5932 | Steps: 4 | Val loss: 0.4220 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4367 | Steps: 4 | Val loss: 0.3217 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=138220)[0m rmse: 0.17583206295967102
[2m[36m(func pid=138220)[0m mae:  0.1261572688817978
[2m[36m(func pid=138220)[0m rmse_per_class: [0.142, 0.265, 0.102, 0.354, 0.058, 0.185, 0.266, 0.139, 0.152, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3765 | Steps: 4 | Val loss: 0.2997 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=139456)[0m rmse: 0.1714240163564682
[2m[36m(func pid=139456)[0m mae:  0.10605140030384064
[2m[36m(func pid=139456)[0m rmse_per_class: [0.106, 0.198, 0.032, 0.37, 0.056, 0.266, 0.326, 0.124, 0.147, 0.089]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.18431882560253143
[2m[36m(func pid=138601)[0m mae:  0.12255030870437622
[2m[36m(func pid=138601)[0m rmse_per_class: [0.179, 0.27, 0.045, 0.321, 0.056, 0.184, 0.267, 0.277, 0.132, 0.112]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:33:58 (running for 00:05:11.90)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.497 |  0.176 |                   47 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.437 |  0.184 |                   50 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.376 |  0.166 |                   50 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.593 |  0.171 |                   46 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.1661994606256485
[2m[36m(func pid=139034)[0m mae:  0.10604789108037949
[2m[36m(func pid=139034)[0m rmse_per_class: [0.099, 0.248, 0.034, 0.306, 0.112, 0.168, 0.289, 0.187, 0.131, 0.088]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4930 | Steps: 4 | Val loss: 0.3515 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5506 | Steps: 4 | Val loss: 0.5801 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4470 | Steps: 4 | Val loss: 0.3241 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=138220)[0m rmse: 0.1756349503993988
[2m[36m(func pid=138220)[0m mae:  0.12601760029792786
[2m[36m(func pid=138220)[0m rmse_per_class: [0.144, 0.264, 0.101, 0.354, 0.058, 0.185, 0.265, 0.139, 0.152, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3828 | Steps: 4 | Val loss: 0.3115 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=139456)[0m rmse: 0.2227240800857544
[2m[36m(func pid=139456)[0m mae:  0.14099907875061035
[2m[36m(func pid=139456)[0m rmse_per_class: [0.524, 0.237, 0.049, 0.387, 0.056, 0.193, 0.329, 0.189, 0.159, 0.104]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.18424519896507263
[2m[36m(func pid=138601)[0m mae:  0.12222814559936523
[2m[36m(func pid=138601)[0m rmse_per_class: [0.201, 0.267, 0.045, 0.332, 0.055, 0.184, 0.261, 0.258, 0.133, 0.106]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:34:03 (running for 00:05:17.35)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.493 |  0.176 |                   48 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.447 |  0.184 |                   51 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.383 |  0.176 |                   51 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.551 |  0.223 |                   47 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.1764664649963379
[2m[36m(func pid=139034)[0m mae:  0.11050842702388763
[2m[36m(func pid=139034)[0m rmse_per_class: [0.166, 0.203, 0.03, 0.284, 0.176, 0.17, 0.305, 0.202, 0.133, 0.097]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4972 | Steps: 4 | Val loss: 0.3520 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.6369 | Steps: 4 | Val loss: 0.5880 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4284 | Steps: 4 | Val loss: 0.3248 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=138220)[0m rmse: 0.1757277548313141
[2m[36m(func pid=138220)[0m mae:  0.1259789764881134
[2m[36m(func pid=138220)[0m rmse_per_class: [0.146, 0.264, 0.1, 0.355, 0.057, 0.185, 0.264, 0.14, 0.152, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3962 | Steps: 4 | Val loss: 0.3316 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=139456)[0m rmse: 0.22319066524505615
[2m[36m(func pid=139456)[0m mae:  0.14044760167598724
[2m[36m(func pid=139456)[0m rmse_per_class: [0.448, 0.222, 0.048, 0.388, 0.055, 0.167, 0.334, 0.242, 0.144, 0.185]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.18186071515083313
[2m[36m(func pid=138601)[0m mae:  0.12116897106170654
[2m[36m(func pid=138601)[0m rmse_per_class: [0.206, 0.264, 0.045, 0.341, 0.055, 0.184, 0.255, 0.232, 0.134, 0.102]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:34:09 (running for 00:05:22.69)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.497 |  0.176 |                   49 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.428 |  0.182 |                   52 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.396 |  0.185 |                   52 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.637 |  0.223 |                   48 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.1854066103696823
[2m[36m(func pid=139034)[0m mae:  0.11626994609832764
[2m[36m(func pid=139034)[0m rmse_per_class: [0.162, 0.238, 0.034, 0.28, 0.252, 0.169, 0.305, 0.156, 0.137, 0.121]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4930 | Steps: 4 | Val loss: 0.3533 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.6448 | Steps: 4 | Val loss: 0.4315 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4497 | Steps: 4 | Val loss: 0.3293 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=138220)[0m rmse: 0.17602552473545074
[2m[36m(func pid=138220)[0m mae:  0.1260509490966797
[2m[36m(func pid=138220)[0m rmse_per_class: [0.149, 0.264, 0.1, 0.355, 0.057, 0.185, 0.264, 0.14, 0.152, 0.094]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4127 | Steps: 4 | Val loss: 0.3477 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=138601)[0m rmse: 0.1796129047870636
[2m[36m(func pid=138601)[0m mae:  0.12024232000112534
[2m[36m(func pid=138601)[0m rmse_per_class: [0.212, 0.261, 0.044, 0.35, 0.055, 0.184, 0.25, 0.203, 0.138, 0.099]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.16643992066383362
[2m[36m(func pid=139456)[0m mae:  0.10127951204776764
[2m[36m(func pid=139456)[0m rmse_per_class: [0.11, 0.293, 0.034, 0.381, 0.053, 0.186, 0.263, 0.119, 0.13, 0.095]
[2m[36m(func pid=139456)[0m 
== Status ==
Current time: 2024-01-07 15:34:14 (running for 00:05:27.88)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.493 |  0.176 |                   50 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.45  |  0.18  |                   53 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.413 |  0.186 |                   53 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.645 |  0.166 |                   49 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.1859309822320938
[2m[36m(func pid=139034)[0m mae:  0.11970742791891098
[2m[36m(func pid=139034)[0m rmse_per_class: [0.129, 0.262, 0.034, 0.282, 0.269, 0.173, 0.293, 0.103, 0.138, 0.178]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4870 | Steps: 4 | Val loss: 0.3556 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4232 | Steps: 4 | Val loss: 0.3275 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5110 | Steps: 4 | Val loss: 0.3769 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=138220)[0m rmse: 0.17645865678787231
[2m[36m(func pid=138220)[0m mae:  0.12618064880371094
[2m[36m(func pid=138220)[0m rmse_per_class: [0.154, 0.263, 0.1, 0.356, 0.056, 0.186, 0.264, 0.14, 0.152, 0.094]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4183 | Steps: 4 | Val loss: 0.3442 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=138601)[0m rmse: 0.17556315660476685
[2m[36m(func pid=138601)[0m mae:  0.11866822093725204
[2m[36m(func pid=138601)[0m rmse_per_class: [0.196, 0.256, 0.044, 0.355, 0.054, 0.182, 0.249, 0.178, 0.145, 0.097]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.1592930257320404
[2m[36m(func pid=139456)[0m mae:  0.09789390116930008
[2m[36m(func pid=139456)[0m rmse_per_class: [0.108, 0.191, 0.029, 0.32, 0.069, 0.197, 0.302, 0.107, 0.18, 0.09]
[2m[36m(func pid=139456)[0m 
== Status ==
Current time: 2024-01-07 15:34:19 (running for 00:05:33.18)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.487 |  0.176 |                   51 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.423 |  0.176 |                   54 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.418 |  0.181 |                   54 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.511 |  0.159 |                   50 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.18053176999092102
[2m[36m(func pid=139034)[0m mae:  0.11693120002746582
[2m[36m(func pid=139034)[0m rmse_per_class: [0.092, 0.27, 0.042, 0.278, 0.262, 0.174, 0.255, 0.096, 0.138, 0.199]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5033 | Steps: 4 | Val loss: 0.3570 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4164 | Steps: 4 | Val loss: 0.3274 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5088 | Steps: 4 | Val loss: 0.4471 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4029 | Steps: 4 | Val loss: 0.3258 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=138220)[0m rmse: 0.17670291662216187
[2m[36m(func pid=138220)[0m mae:  0.12616313993930817
[2m[36m(func pid=138220)[0m rmse_per_class: [0.158, 0.263, 0.099, 0.357, 0.056, 0.185, 0.264, 0.14, 0.151, 0.094]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.1722532957792282
[2m[36m(func pid=138601)[0m mae:  0.11752281337976456
[2m[36m(func pid=138601)[0m rmse_per_class: [0.175, 0.252, 0.044, 0.359, 0.054, 0.181, 0.247, 0.158, 0.157, 0.095]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.21312406659126282
[2m[36m(func pid=139456)[0m mae:  0.12699314951896667
[2m[36m(func pid=139456)[0m rmse_per_class: [0.228, 0.263, 0.057, 0.305, 0.35, 0.194, 0.309, 0.116, 0.221, 0.088]
[2m[36m(func pid=139456)[0m 
== Status ==
Current time: 2024-01-07 15:34:24 (running for 00:05:38.23)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.503 |  0.177 |                   52 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.416 |  0.172 |                   55 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.403 |  0.173 |                   55 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.509 |  0.213 |                   51 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.17339175939559937
[2m[36m(func pid=139034)[0m mae:  0.11009503901004791
[2m[36m(func pid=139034)[0m rmse_per_class: [0.094, 0.265, 0.057, 0.275, 0.244, 0.172, 0.228, 0.11, 0.137, 0.152]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4858 | Steps: 4 | Val loss: 0.3504 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4240 | Steps: 4 | Val loss: 0.3262 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7057 | Steps: 4 | Val loss: 0.6249 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4096 | Steps: 4 | Val loss: 0.3038 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=138220)[0m rmse: 0.1760278195142746
[2m[36m(func pid=138220)[0m mae:  0.1258586347103119
[2m[36m(func pid=138220)[0m rmse_per_class: [0.155, 0.263, 0.097, 0.355, 0.056, 0.185, 0.262, 0.14, 0.151, 0.094]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.1692294031381607
[2m[36m(func pid=138601)[0m mae:  0.11654514074325562
[2m[36m(func pid=138601)[0m rmse_per_class: [0.147, 0.247, 0.044, 0.362, 0.053, 0.179, 0.247, 0.143, 0.176, 0.094]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.22206878662109375
[2m[36m(func pid=139456)[0m mae:  0.13174353539943695
[2m[36m(func pid=139456)[0m rmse_per_class: [0.216, 0.263, 0.103, 0.389, 0.425, 0.197, 0.259, 0.128, 0.138, 0.103]
[2m[36m(func pid=139456)[0m 
== Status ==
Current time: 2024-01-07 15:34:29 (running for 00:05:43.34)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.486 |  0.176 |                   53 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.424 |  0.169 |                   56 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.41  |  0.167 |                   56 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.706 |  0.222 |                   52 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.16747811436653137
[2m[36m(func pid=139034)[0m mae:  0.10393579304218292
[2m[36m(func pid=139034)[0m rmse_per_class: [0.095, 0.251, 0.068, 0.276, 0.207, 0.168, 0.243, 0.122, 0.134, 0.11]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4885 | Steps: 4 | Val loss: 0.3478 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4478 | Steps: 4 | Val loss: 0.3268 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5763 | Steps: 4 | Val loss: 0.7153 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4030 | Steps: 4 | Val loss: 0.2835 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=138220)[0m rmse: 0.17602016031742096
[2m[36m(func pid=138220)[0m mae:  0.12581029534339905
[2m[36m(func pid=138220)[0m rmse_per_class: [0.157, 0.263, 0.097, 0.355, 0.056, 0.185, 0.261, 0.14, 0.151, 0.095]
[2m[36m(func pid=138601)[0m rmse: 0.16782207787036896
[2m[36m(func pid=138601)[0m mae:  0.11624745279550552
[2m[36m(func pid=138601)[0m rmse_per_class: [0.128, 0.242, 0.043, 0.364, 0.053, 0.178, 0.244, 0.134, 0.199, 0.093]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.17733250558376312
[2m[36m(func pid=139456)[0m mae:  0.11038337647914886
[2m[36m(func pid=139456)[0m rmse_per_class: [0.109, 0.207, 0.028, 0.389, 0.176, 0.234, 0.246, 0.104, 0.132, 0.147]
[2m[36m(func pid=139456)[0m 
== Status ==
Current time: 2024-01-07 15:34:34 (running for 00:05:48.49)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.489 |  0.176 |                   54 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.448 |  0.168 |                   57 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.403 |  0.156 |                   57 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.576 |  0.177 |                   53 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.15631583333015442
[2m[36m(func pid=139034)[0m mae:  0.09841777384281158
[2m[36m(func pid=139034)[0m rmse_per_class: [0.091, 0.222, 0.07, 0.285, 0.157, 0.167, 0.227, 0.12, 0.131, 0.092]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4830 | Steps: 4 | Val loss: 0.3452 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4160 | Steps: 4 | Val loss: 0.3199 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.6110 | Steps: 4 | Val loss: 0.7873 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3686 | Steps: 4 | Val loss: 0.2902 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=138220)[0m rmse: 0.17579083144664764
[2m[36m(func pid=138220)[0m mae:  0.12570598721504211
[2m[36m(func pid=138220)[0m rmse_per_class: [0.158, 0.262, 0.095, 0.355, 0.056, 0.186, 0.261, 0.14, 0.151, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.16512562334537506
[2m[36m(func pid=138601)[0m mae:  0.1150258332490921
[2m[36m(func pid=138601)[0m rmse_per_class: [0.107, 0.235, 0.043, 0.363, 0.054, 0.177, 0.242, 0.128, 0.21, 0.092]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:34:39 (running for 00:05:53.52)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.483 |  0.176 |                   55 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.416 |  0.165 |                   58 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.403 |  0.156 |                   57 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.611 |  0.207 |                   54 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.20729851722717285
[2m[36m(func pid=139456)[0m mae:  0.12953665852546692
[2m[36m(func pid=139456)[0m rmse_per_class: [0.104, 0.34, 0.035, 0.389, 0.084, 0.174, 0.328, 0.218, 0.257, 0.144]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.1569247543811798
[2m[36m(func pid=139034)[0m mae:  0.10296761989593506
[2m[36m(func pid=139034)[0m rmse_per_class: [0.096, 0.201, 0.051, 0.301, 0.098, 0.17, 0.237, 0.127, 0.199, 0.088]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4165 | Steps: 4 | Val loss: 0.3204 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4834 | Steps: 4 | Val loss: 0.3424 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3747 | Steps: 4 | Val loss: 0.3173 | Batch size: 32 | lr: 0.01 | Duration: 2.64s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.6772 | Steps: 4 | Val loss: 0.8364 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=138601)[0m rmse: 0.1649310439825058
[2m[36m(func pid=138601)[0m mae:  0.11525356769561768
[2m[36m(func pid=138601)[0m rmse_per_class: [0.101, 0.23, 0.043, 0.364, 0.054, 0.175, 0.238, 0.125, 0.227, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17550969123840332
[2m[36m(func pid=138220)[0m mae:  0.12556330859661102
[2m[36m(func pid=138220)[0m rmse_per_class: [0.157, 0.262, 0.094, 0.354, 0.055, 0.185, 0.261, 0.14, 0.151, 0.095]
[2m[36m(func pid=138220)[0m 
== Status ==
Current time: 2024-01-07 15:34:45 (running for 00:05:58.70)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.483 |  0.176 |                   56 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.416 |  0.165 |                   59 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.375 |  0.165 |                   59 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.611 |  0.207 |                   54 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.16526998579502106
[2m[36m(func pid=139034)[0m mae:  0.10991036891937256
[2m[36m(func pid=139034)[0m rmse_per_class: [0.107, 0.233, 0.05, 0.321, 0.062, 0.168, 0.272, 0.121, 0.232, 0.087]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.23302297294139862
[2m[36m(func pid=139456)[0m mae:  0.14420171082019806
[2m[36m(func pid=139456)[0m rmse_per_class: [0.332, 0.229, 0.049, 0.389, 0.063, 0.181, 0.339, 0.247, 0.361, 0.139]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4026 | Steps: 4 | Val loss: 0.3151 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4851 | Steps: 4 | Val loss: 0.3429 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4265 | Steps: 4 | Val loss: 0.3181 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.7484 | Steps: 4 | Val loss: 0.6584 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=138601)[0m rmse: 0.16352908313274384
[2m[36m(func pid=138601)[0m mae:  0.11481411755084991
[2m[36m(func pid=138601)[0m rmse_per_class: [0.095, 0.227, 0.043, 0.363, 0.056, 0.173, 0.239, 0.123, 0.225, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.1758824586868286
[2m[36m(func pid=138220)[0m mae:  0.12572219967842102
[2m[36m(func pid=138220)[0m rmse_per_class: [0.161, 0.262, 0.094, 0.355, 0.055, 0.185, 0.261, 0.14, 0.151, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.1641659438610077
[2m[36m(func pid=139034)[0m mae:  0.10862773656845093
[2m[36m(func pid=139034)[0m rmse_per_class: [0.12, 0.252, 0.051, 0.32, 0.051, 0.166, 0.281, 0.107, 0.205, 0.088]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:34:50 (running for 00:06:04.46)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.485 |  0.176 |                   57 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.403 |  0.164 |                   60 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.426 |  0.164 |                   60 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.748 |  0.192 |                   56 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.19159935414791107
[2m[36m(func pid=139456)[0m mae:  0.11897222697734833
[2m[36m(func pid=139456)[0m rmse_per_class: [0.15, 0.248, 0.045, 0.388, 0.05, 0.34, 0.269, 0.11, 0.183, 0.134]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4108 | Steps: 4 | Val loss: 0.3170 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5072 | Steps: 4 | Val loss: 0.3451 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3631 | Steps: 4 | Val loss: 0.2943 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6420 | Steps: 4 | Val loss: 0.4442 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=138601)[0m rmse: 0.16442030668258667
[2m[36m(func pid=138601)[0m mae:  0.11539556086063385
[2m[36m(func pid=138601)[0m rmse_per_class: [0.095, 0.228, 0.046, 0.363, 0.058, 0.172, 0.24, 0.124, 0.228, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17644724249839783
[2m[36m(func pid=138220)[0m mae:  0.12585607171058655
[2m[36m(func pid=138220)[0m rmse_per_class: [0.167, 0.262, 0.094, 0.356, 0.055, 0.186, 0.26, 0.14, 0.151, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.15525639057159424
[2m[36m(func pid=139034)[0m mae:  0.10166491568088531
[2m[36m(func pid=139034)[0m rmse_per_class: [0.133, 0.225, 0.055, 0.289, 0.05, 0.173, 0.279, 0.1, 0.157, 0.09]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:34:56 (running for 00:06:09.83)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.507 |  0.176 |                   58 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.411 |  0.164 |                   61 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.363 |  0.155 |                   61 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.642 |  0.174 |                   57 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.17373833060264587
[2m[36m(func pid=139456)[0m mae:  0.10637303441762924
[2m[36m(func pid=139456)[0m rmse_per_class: [0.111, 0.234, 0.037, 0.328, 0.055, 0.229, 0.373, 0.14, 0.133, 0.098]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4050 | Steps: 4 | Val loss: 0.3124 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3678 | Steps: 4 | Val loss: 0.2851 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4795 | Steps: 4 | Val loss: 0.3399 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.6338 | Steps: 4 | Val loss: 0.5717 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=138601)[0m rmse: 0.16336852312088013
[2m[36m(func pid=138601)[0m mae:  0.11468996107578278
[2m[36m(func pid=138601)[0m rmse_per_class: [0.095, 0.231, 0.048, 0.362, 0.061, 0.172, 0.241, 0.124, 0.208, 0.092]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.15070697665214539
[2m[36m(func pid=139034)[0m mae:  0.09766093641519547
[2m[36m(func pid=139034)[0m rmse_per_class: [0.112, 0.197, 0.103, 0.264, 0.05, 0.175, 0.271, 0.095, 0.135, 0.104]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17579326033592224
[2m[36m(func pid=138220)[0m mae:  0.12558449804782867
[2m[36m(func pid=138220)[0m rmse_per_class: [0.165, 0.262, 0.092, 0.355, 0.055, 0.185, 0.259, 0.14, 0.15, 0.095]
[2m[36m(func pid=138220)[0m 
== Status ==
Current time: 2024-01-07 15:35:01 (running for 00:06:15.26)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.48  |  0.176 |                   59 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.405 |  0.163 |                   62 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.368 |  0.151 |                   62 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.634 |  0.181 |                   58 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.18051452934741974
[2m[36m(func pid=139456)[0m mae:  0.11527415364980698
[2m[36m(func pid=139456)[0m rmse_per_class: [0.099, 0.224, 0.046, 0.46, 0.054, 0.175, 0.327, 0.132, 0.133, 0.154]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4251 | Steps: 4 | Val loss: 0.3140 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3700 | Steps: 4 | Val loss: 0.2849 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4762 | Steps: 4 | Val loss: 0.3355 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=138601)[0m rmse: 0.16487185657024384
[2m[36m(func pid=138601)[0m mae:  0.11541427671909332
[2m[36m(func pid=138601)[0m rmse_per_class: [0.095, 0.238, 0.06, 0.361, 0.065, 0.172, 0.245, 0.126, 0.195, 0.093]
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6211 | Steps: 4 | Val loss: 0.5790 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.1556364744901657
[2m[36m(func pid=139034)[0m mae:  0.0996454507112503
[2m[36m(func pid=139034)[0m rmse_per_class: [0.09, 0.212, 0.148, 0.257, 0.051, 0.169, 0.26, 0.094, 0.131, 0.143]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17516948282718658
[2m[36m(func pid=138220)[0m mae:  0.12521958351135254
[2m[36m(func pid=138220)[0m rmse_per_class: [0.162, 0.262, 0.091, 0.354, 0.055, 0.185, 0.259, 0.14, 0.15, 0.095]
[2m[36m(func pid=138220)[0m 
== Status ==
Current time: 2024-01-07 15:35:06 (running for 00:06:20.47)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.476 |  0.175 |                   60 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.425 |  0.165 |                   63 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.37  |  0.156 |                   63 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.621 |  0.196 |                   59 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.19644878804683685
[2m[36m(func pid=139456)[0m mae:  0.12723328173160553
[2m[36m(func pid=139456)[0m rmse_per_class: [0.132, 0.221, 0.133, 0.333, 0.049, 0.184, 0.343, 0.118, 0.327, 0.124]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4000 | Steps: 4 | Val loss: 0.3129 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3643 | Steps: 4 | Val loss: 0.2879 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4722 | Steps: 4 | Val loss: 0.3379 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=138601)[0m rmse: 0.16588418185710907
[2m[36m(func pid=138601)[0m mae:  0.11544033139944077
[2m[36m(func pid=138601)[0m rmse_per_class: [0.096, 0.246, 0.074, 0.359, 0.069, 0.172, 0.247, 0.128, 0.175, 0.093]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6269 | Steps: 4 | Val loss: 0.5644 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=139034)[0m rmse: 0.1604883223772049
[2m[36m(func pid=139034)[0m mae:  0.10303745418787003
[2m[36m(func pid=139034)[0m rmse_per_class: [0.092, 0.233, 0.128, 0.262, 0.052, 0.164, 0.245, 0.093, 0.132, 0.204]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17609617114067078
[2m[36m(func pid=138220)[0m mae:  0.1256711781024933
[2m[36m(func pid=138220)[0m rmse_per_class: [0.169, 0.261, 0.092, 0.355, 0.055, 0.185, 0.259, 0.14, 0.15, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3967 | Steps: 4 | Val loss: 0.3136 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 15:35:12 (running for 00:06:26.05)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.472 |  0.176 |                   61 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.4   |  0.166 |                   64 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.364 |  0.16  |                   64 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.627 |  0.219 |                   60 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.21949610114097595
[2m[36m(func pid=139456)[0m mae:  0.12900224328041077
[2m[36m(func pid=139456)[0m rmse_per_class: [0.139, 0.218, 0.201, 0.384, 0.218, 0.25, 0.291, 0.23, 0.177, 0.088]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4018 | Steps: 4 | Val loss: 0.2940 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4831 | Steps: 4 | Val loss: 0.3416 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=138601)[0m rmse: 0.16839255392551422
[2m[36m(func pid=138601)[0m mae:  0.11618753522634506
[2m[36m(func pid=138601)[0m rmse_per_class: [0.096, 0.258, 0.091, 0.357, 0.076, 0.173, 0.254, 0.129, 0.156, 0.094]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6191 | Steps: 4 | Val loss: 0.7686 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=139034)[0m rmse: 0.1598394215106964
[2m[36m(func pid=139034)[0m mae:  0.10384170711040497
[2m[36m(func pid=139034)[0m rmse_per_class: [0.091, 0.247, 0.084, 0.273, 0.052, 0.161, 0.23, 0.097, 0.131, 0.233]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.1771729737520218
[2m[36m(func pid=138220)[0m mae:  0.1260959655046463
[2m[36m(func pid=138220)[0m rmse_per_class: [0.179, 0.261, 0.092, 0.356, 0.055, 0.185, 0.259, 0.14, 0.15, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3849 | Steps: 4 | Val loss: 0.3099 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 15:35:17 (running for 00:06:31.39)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.483 |  0.177 |                   62 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.397 |  0.168 |                   65 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.402 |  0.16  |                   65 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.619 |  0.217 |                   61 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.21728332340717316
[2m[36m(func pid=139456)[0m mae:  0.13303634524345398
[2m[36m(func pid=139456)[0m rmse_per_class: [0.111, 0.283, 0.033, 0.389, 0.279, 0.2, 0.369, 0.286, 0.135, 0.087]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3769 | Steps: 4 | Val loss: 0.2922 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4724 | Steps: 4 | Val loss: 0.3424 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=138601)[0m rmse: 0.16904477775096893
[2m[36m(func pid=138601)[0m mae:  0.11632256209850311
[2m[36m(func pid=138601)[0m rmse_per_class: [0.096, 0.263, 0.092, 0.353, 0.084, 0.174, 0.258, 0.129, 0.145, 0.095]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7677 | Steps: 4 | Val loss: 0.7604 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=139034)[0m rmse: 0.15332381427288055
[2m[36m(func pid=139034)[0m mae:  0.10075227916240692
[2m[36m(func pid=139034)[0m rmse_per_class: [0.096, 0.246, 0.054, 0.283, 0.052, 0.16, 0.227, 0.109, 0.135, 0.17]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17772537469863892
[2m[36m(func pid=138220)[0m mae:  0.12630656361579895
[2m[36m(func pid=138220)[0m rmse_per_class: [0.184, 0.261, 0.092, 0.357, 0.055, 0.185, 0.258, 0.14, 0.15, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3969 | Steps: 4 | Val loss: 0.3070 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 15:35:23 (running for 00:06:36.91)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.472 |  0.178 |                   63 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.385 |  0.169 |                   66 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.377 |  0.153 |                   66 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.768 |  0.24  |                   62 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.23953363299369812
[2m[36m(func pid=139456)[0m mae:  0.1468975692987442
[2m[36m(func pid=139456)[0m rmse_per_class: [0.097, 0.264, 0.046, 0.389, 0.387, 0.332, 0.323, 0.267, 0.144, 0.146]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3725 | Steps: 4 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=138601)[0m rmse: 0.16965916752815247
[2m[36m(func pid=138601)[0m mae:  0.11649902164936066
[2m[36m(func pid=138601)[0m rmse_per_class: [0.096, 0.267, 0.089, 0.348, 0.093, 0.175, 0.264, 0.129, 0.138, 0.097]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4730 | Steps: 4 | Val loss: 0.3453 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=139034)[0m rmse: 0.15001340210437775
[2m[36m(func pid=139034)[0m mae:  0.0987882912158966
[2m[36m(func pid=139034)[0m rmse_per_class: [0.124, 0.227, 0.045, 0.279, 0.052, 0.159, 0.232, 0.108, 0.162, 0.113]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.6858 | Steps: 4 | Val loss: 0.6498 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4030 | Steps: 4 | Val loss: 0.3020 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=138220)[0m rmse: 0.17870770394802094
[2m[36m(func pid=138220)[0m mae:  0.1266644448041916
[2m[36m(func pid=138220)[0m rmse_per_class: [0.193, 0.261, 0.093, 0.358, 0.054, 0.186, 0.258, 0.14, 0.15, 0.095]
[2m[36m(func pid=138220)[0m 
== Status ==
Current time: 2024-01-07 15:35:28 (running for 00:06:42.35)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.473 |  0.179 |                   64 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.397 |  0.17  |                   67 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.372 |  0.15  |                   67 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.686 |  0.207 |                   63 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.20741064846515656
[2m[36m(func pid=139456)[0m mae:  0.1339302957057953
[2m[36m(func pid=139456)[0m rmse_per_class: [0.262, 0.234, 0.049, 0.387, 0.155, 0.196, 0.329, 0.119, 0.154, 0.187]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3609 | Steps: 4 | Val loss: 0.2805 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=138601)[0m rmse: 0.16842125356197357
[2m[36m(func pid=138601)[0m mae:  0.11593743413686752
[2m[36m(func pid=138601)[0m rmse_per_class: [0.096, 0.263, 0.074, 0.341, 0.104, 0.178, 0.267, 0.128, 0.134, 0.099]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4637 | Steps: 4 | Val loss: 0.3434 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=139034)[0m rmse: 0.15098385512828827
[2m[36m(func pid=139034)[0m mae:  0.09953206032514572
[2m[36m(func pid=139034)[0m rmse_per_class: [0.14, 0.206, 0.044, 0.276, 0.051, 0.157, 0.248, 0.103, 0.194, 0.091]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5886 | Steps: 4 | Val loss: 0.3546 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4051 | Steps: 4 | Val loss: 0.2984 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=138220)[0m rmse: 0.17866337299346924
[2m[36m(func pid=138220)[0m mae:  0.12657047808170319
[2m[36m(func pid=138220)[0m rmse_per_class: [0.194, 0.261, 0.092, 0.358, 0.054, 0.185, 0.257, 0.141, 0.15, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3467 | Steps: 4 | Val loss: 0.2758 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=139456)[0m rmse: 0.15010994672775269
[2m[36m(func pid=139456)[0m mae:  0.09072266519069672
[2m[36m(func pid=139456)[0m rmse_per_class: [0.11, 0.216, 0.049, 0.29, 0.061, 0.163, 0.239, 0.128, 0.151, 0.095]
== Status ==
Current time: 2024-01-07 15:35:34 (running for 00:06:47.90)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.464 |  0.179 |                   65 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.403 |  0.168 |                   68 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.361 |  0.151 |                   68 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.589 |  0.15  |                   64 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.16716602444648743
[2m[36m(func pid=138601)[0m mae:  0.11522410809993744
[2m[36m(func pid=138601)[0m rmse_per_class: [0.095, 0.256, 0.067, 0.334, 0.109, 0.179, 0.269, 0.127, 0.133, 0.102]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4582 | Steps: 4 | Val loss: 0.3361 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=139034)[0m rmse: 0.14866352081298828
[2m[36m(func pid=139034)[0m mae:  0.0975758358836174
[2m[36m(func pid=139034)[0m rmse_per_class: [0.137, 0.2, 0.038, 0.274, 0.05, 0.157, 0.263, 0.098, 0.183, 0.088]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5973 | Steps: 4 | Val loss: 0.4858 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3840 | Steps: 4 | Val loss: 0.2964 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=138220)[0m rmse: 0.17730474472045898
[2m[36m(func pid=138220)[0m mae:  0.12610527873039246
[2m[36m(func pid=138220)[0m rmse_per_class: [0.184, 0.26, 0.091, 0.356, 0.054, 0.185, 0.257, 0.14, 0.151, 0.095]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3526 | Steps: 4 | Val loss: 0.2821 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 15:35:39 (running for 00:06:53.22)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.458 |  0.177 |                   66 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.405 |  0.167 |                   69 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.347 |  0.149 |                   69 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.597 |  0.173 |                   65 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.17319825291633606
[2m[36m(func pid=139456)[0m mae:  0.10332535207271576
[2m[36m(func pid=139456)[0m rmse_per_class: [0.109, 0.284, 0.049, 0.467, 0.05, 0.169, 0.237, 0.131, 0.133, 0.104]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.16669367253780365
[2m[36m(func pid=138601)[0m mae:  0.1150558814406395
[2m[36m(func pid=138601)[0m rmse_per_class: [0.093, 0.247, 0.065, 0.326, 0.117, 0.18, 0.273, 0.126, 0.132, 0.106]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.15008161962032318
[2m[36m(func pid=139034)[0m mae:  0.0974220260977745
[2m[36m(func pid=139034)[0m rmse_per_class: [0.119, 0.24, 0.046, 0.28, 0.05, 0.16, 0.27, 0.097, 0.152, 0.087]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4623 | Steps: 4 | Val loss: 0.3373 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.6270 | Steps: 4 | Val loss: 0.4857 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3941 | Steps: 4 | Val loss: 0.2960 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=138220)[0m rmse: 0.17803364992141724
[2m[36m(func pid=138220)[0m mae:  0.12638448178768158
[2m[36m(func pid=138220)[0m rmse_per_class: [0.191, 0.26, 0.091, 0.356, 0.054, 0.185, 0.257, 0.14, 0.151, 0.095]
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3586 | Steps: 4 | Val loss: 0.2857 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=138220)[0m 
== Status ==
Current time: 2024-01-07 15:35:45 (running for 00:06:58.80)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.462 |  0.178 |                   67 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.384 |  0.167 |                   70 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.353 |  0.15  |                   70 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.627 |  0.211 |                   66 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.21059541404247284
[2m[36m(func pid=139456)[0m mae:  0.12329284846782684
[2m[36m(func pid=139456)[0m rmse_per_class: [0.303, 0.288, 0.035, 0.317, 0.079, 0.248, 0.31, 0.217, 0.133, 0.176]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.16677246987819672
[2m[36m(func pid=138601)[0m mae:  0.11510820686817169
[2m[36m(func pid=138601)[0m rmse_per_class: [0.094, 0.24, 0.065, 0.318, 0.123, 0.18, 0.278, 0.126, 0.132, 0.111]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.15045949816703796
[2m[36m(func pid=139034)[0m mae:  0.09623823314905167
[2m[36m(func pid=139034)[0m rmse_per_class: [0.093, 0.27, 0.066, 0.28, 0.051, 0.164, 0.263, 0.096, 0.135, 0.087]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4532 | Steps: 4 | Val loss: 0.3324 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.5592 | Steps: 4 | Val loss: 0.6796 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3957 | Steps: 4 | Val loss: 0.2955 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3624 | Steps: 4 | Val loss: 0.2763 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=138220)[0m rmse: 0.1771913766860962
[2m[36m(func pid=138220)[0m mae:  0.12603752315044403
[2m[36m(func pid=138220)[0m rmse_per_class: [0.186, 0.26, 0.089, 0.355, 0.054, 0.185, 0.256, 0.14, 0.15, 0.096]
[2m[36m(func pid=138220)[0m 
== Status ==
Current time: 2024-01-07 15:35:50 (running for 00:07:04.34)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.453 |  0.177 |                   68 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.394 |  0.167 |                   71 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.359 |  0.15  |                   71 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.559 |  0.227 |                   67 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.2274431735277176
[2m[36m(func pid=139456)[0m mae:  0.14459016919136047
[2m[36m(func pid=139456)[0m rmse_per_class: [0.264, 0.285, 0.057, 0.389, 0.06, 0.186, 0.323, 0.298, 0.208, 0.205]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.16616016626358032
[2m[36m(func pid=138601)[0m mae:  0.11537282168865204
[2m[36m(func pid=138601)[0m rmse_per_class: [0.095, 0.234, 0.052, 0.309, 0.135, 0.18, 0.283, 0.125, 0.132, 0.116]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.14681464433670044
[2m[36m(func pid=139034)[0m mae:  0.09259644150733948
[2m[36m(func pid=139034)[0m rmse_per_class: [0.091, 0.239, 0.088, 0.271, 0.056, 0.162, 0.246, 0.096, 0.132, 0.087]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4522 | Steps: 4 | Val loss: 0.3319 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7050 | Steps: 4 | Val loss: 0.7402 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3998 | Steps: 4 | Val loss: 0.2957 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4079 | Steps: 4 | Val loss: 0.2728 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=138220)[0m rmse: 0.17741923034191132
[2m[36m(func pid=138220)[0m mae:  0.1261470466852188
[2m[36m(func pid=138220)[0m rmse_per_class: [0.189, 0.259, 0.089, 0.355, 0.054, 0.185, 0.256, 0.14, 0.151, 0.096]
[2m[36m(func pid=138220)[0m 
== Status ==
Current time: 2024-01-07 15:35:56 (running for 00:07:09.69)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.452 |  0.177 |                   69 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.396 |  0.166 |                   72 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.362 |  0.147 |                   72 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.705 |  0.188 |                   68 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139456)[0m rmse: 0.18812477588653564
[2m[36m(func pid=139456)[0m mae:  0.11780855804681778
[2m[36m(func pid=139456)[0m rmse_per_class: [0.111, 0.216, 0.195, 0.389, 0.062, 0.2, 0.239, 0.154, 0.151, 0.164]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.16604290902614594
[2m[36m(func pid=138601)[0m mae:  0.11536810547113419
[2m[36m(func pid=138601)[0m rmse_per_class: [0.098, 0.233, 0.048, 0.301, 0.139, 0.179, 0.285, 0.125, 0.132, 0.12]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.14744655787944794
[2m[36m(func pid=139034)[0m mae:  0.09290654957294464
[2m[36m(func pid=139034)[0m rmse_per_class: [0.091, 0.211, 0.098, 0.267, 0.069, 0.174, 0.238, 0.106, 0.132, 0.089]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4517 | Steps: 4 | Val loss: 0.3310 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7982 | Steps: 4 | Val loss: 0.7023 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3945 | Steps: 4 | Val loss: 0.2957 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3373 | Steps: 4 | Val loss: 0.2690 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=138220)[0m rmse: 0.17765328288078308
[2m[36m(func pid=138220)[0m mae:  0.12625090777873993
[2m[36m(func pid=138220)[0m rmse_per_class: [0.192, 0.259, 0.089, 0.355, 0.054, 0.185, 0.256, 0.14, 0.15, 0.096]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.16585195064544678
[2m[36m(func pid=138601)[0m mae:  0.11515561491250992
[2m[36m(func pid=138601)[0m rmse_per_class: [0.104, 0.233, 0.046, 0.295, 0.139, 0.177, 0.286, 0.124, 0.132, 0.123]
== Status ==
Current time: 2024-01-07 15:36:01 (running for 00:07:15.02)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.452 |  0.178 |                   70 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.394 |  0.166 |                   74 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.408 |  0.147 |                   73 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.705 |  0.188 |                   68 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.1892111748456955
[2m[36m(func pid=139456)[0m mae:  0.10983236879110336
[2m[36m(func pid=139456)[0m rmse_per_class: [0.111, 0.314, 0.101, 0.389, 0.142, 0.189, 0.279, 0.136, 0.14, 0.091]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.14643146097660065
[2m[36m(func pid=139034)[0m mae:  0.09373031556606293
[2m[36m(func pid=139034)[0m rmse_per_class: [0.09, 0.198, 0.071, 0.271, 0.093, 0.177, 0.229, 0.108, 0.132, 0.094]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4523 | Steps: 4 | Val loss: 0.3317 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3880 | Steps: 4 | Val loss: 0.2963 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6968 | Steps: 4 | Val loss: 0.6811 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3491 | Steps: 4 | Val loss: 0.2810 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 15:36:06 (running for 00:07:20.10)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: -0.16599999368190765
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.452 |  0.178 |                   70 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.388 |  0.166 |                   75 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.337 |  0.146 |                   74 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.798 |  0.189 |                   69 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138220)[0m rmse: 0.178135484457016
[2m[36m(func pid=138220)[0m mae:  0.12644992768764496
[2m[36m(func pid=138220)[0m rmse_per_class: [0.196, 0.259, 0.089, 0.355, 0.054, 0.185, 0.256, 0.14, 0.151, 0.096]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.1661759465932846
[2m[36m(func pid=138601)[0m mae:  0.11497262865304947
[2m[36m(func pid=138601)[0m rmse_per_class: [0.111, 0.237, 0.045, 0.289, 0.139, 0.176, 0.286, 0.122, 0.132, 0.125]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.22160804271697998
[2m[36m(func pid=139456)[0m mae:  0.12904156744480133
[2m[36m(func pid=139456)[0m rmse_per_class: [0.229, 0.248, 0.034, 0.387, 0.435, 0.245, 0.266, 0.136, 0.14, 0.097]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.1536121666431427
[2m[36m(func pid=139034)[0m mae:  0.10033418238162994
[2m[36m(func pid=139034)[0m rmse_per_class: [0.097, 0.212, 0.034, 0.277, 0.133, 0.181, 0.237, 0.119, 0.134, 0.113]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.4009 | Steps: 4 | Val loss: 0.2952 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4454 | Steps: 4 | Val loss: 0.3306 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6774 | Steps: 4 | Val loss: 0.5277 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3335 | Steps: 4 | Val loss: 0.2960 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=138601)[0m rmse: 0.16554692387580872
[2m[36m(func pid=138601)[0m mae:  0.11464253813028336
[2m[36m(func pid=138601)[0m rmse_per_class: [0.11, 0.241, 0.041, 0.287, 0.142, 0.175, 0.283, 0.119, 0.132, 0.125]
== Status ==
Current time: 2024-01-07 15:36:11 (running for 00:07:25.53)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: -0.15699999779462814
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.452 |  0.178 |                   71 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.401 |  0.166 |                   76 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.349 |  0.154 |                   75 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.697 |  0.222 |                   70 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.17817643284797668
[2m[36m(func pid=138220)[0m mae:  0.1264764815568924
[2m[36m(func pid=138220)[0m rmse_per_class: [0.197, 0.259, 0.089, 0.355, 0.054, 0.185, 0.256, 0.14, 0.151, 0.096]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.2176758348941803
[2m[36m(func pid=139456)[0m mae:  0.12510384619235992
[2m[36m(func pid=139456)[0m rmse_per_class: [0.328, 0.248, 0.049, 0.323, 0.371, 0.225, 0.271, 0.13, 0.14, 0.092]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.16266676783561707
[2m[36m(func pid=139034)[0m mae:  0.10649985074996948
[2m[36m(func pid=139034)[0m rmse_per_class: [0.121, 0.229, 0.032, 0.285, 0.161, 0.171, 0.241, 0.111, 0.139, 0.137]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.4014 | Steps: 4 | Val loss: 0.2943 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4469 | Steps: 4 | Val loss: 0.3298 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.9083 | Steps: 4 | Val loss: 0.4293 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3655 | Steps: 4 | Val loss: 0.3125 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=138601)[0m rmse: 0.16522561013698578
[2m[36m(func pid=138601)[0m mae:  0.11432995647192001
[2m[36m(func pid=138601)[0m rmse_per_class: [0.112, 0.245, 0.04, 0.286, 0.14, 0.174, 0.28, 0.116, 0.132, 0.126]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:36:17 (running for 00:07:30.72)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: -0.15699999779462814
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.445 |  0.178 |                   72 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.401 |  0.165 |                   77 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.333 |  0.163 |                   76 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.677 |  0.218 |                   71 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138220)[0m rmse: 0.17832227051258087
[2m[36m(func pid=138220)[0m mae:  0.1266368180513382
[2m[36m(func pid=138220)[0m rmse_per_class: [0.198, 0.258, 0.09, 0.355, 0.054, 0.185, 0.256, 0.14, 0.151, 0.096]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.16290061175823212
[2m[36m(func pid=139456)[0m mae:  0.09529506415128708
[2m[36m(func pid=139456)[0m rmse_per_class: [0.099, 0.189, 0.049, 0.306, 0.146, 0.206, 0.241, 0.16, 0.137, 0.096]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.1717006415128708
[2m[36m(func pid=139034)[0m mae:  0.1123940721154213
[2m[36m(func pid=139034)[0m rmse_per_class: [0.15, 0.244, 0.035, 0.297, 0.183, 0.163, 0.251, 0.096, 0.151, 0.148]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3893 | Steps: 4 | Val loss: 0.2936 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4344 | Steps: 4 | Val loss: 0.3306 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3564 | Steps: 4 | Val loss: 0.3164 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.6465 | Steps: 4 | Val loss: 0.5922 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 15:36:22 (running for 00:07:35.89)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: -0.15699999779462814
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.447 |  0.178 |                   73 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.389 |  0.165 |                   78 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.366 |  0.172 |                   77 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.908 |  0.163 |                   72 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.1648557186126709
[2m[36m(func pid=138601)[0m mae:  0.11418215930461884
[2m[36m(func pid=138601)[0m rmse_per_class: [0.114, 0.249, 0.04, 0.286, 0.139, 0.174, 0.276, 0.114, 0.133, 0.125]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.1789039820432663
[2m[36m(func pid=138220)[0m mae:  0.12692299485206604
[2m[36m(func pid=138220)[0m rmse_per_class: [0.202, 0.258, 0.09, 0.355, 0.054, 0.186, 0.256, 0.14, 0.151, 0.097]
[2m[36m(func pid=138220)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.17654278874397278
[2m[36m(func pid=139034)[0m mae:  0.11559908092021942
[2m[36m(func pid=139034)[0m rmse_per_class: [0.153, 0.236, 0.036, 0.296, 0.202, 0.168, 0.272, 0.096, 0.157, 0.151]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=139456)[0m rmse: 0.18946805596351624
[2m[36m(func pid=139456)[0m mae:  0.12435358762741089
[2m[36m(func pid=139456)[0m rmse_per_class: [0.112, 0.221, 0.049, 0.375, 0.065, 0.175, 0.295, 0.157, 0.329, 0.115]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3874 | Steps: 4 | Val loss: 0.2914 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3397 | Steps: 4 | Val loss: 0.3126 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=138220)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4289 | Steps: 4 | Val loss: 0.3280 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.9853 | Steps: 4 | Val loss: 0.6334 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 15:36:27 (running for 00:07:41.28)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: -0.15699999779462814
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | RUNNING  | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.434 |  0.179 |                   74 |
| train_10f5e_00001 | RUNNING  | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.387 |  0.163 |                   79 |
| train_10f5e_00002 | RUNNING  | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.356 |  0.177 |                   78 |
| train_10f5e_00003 | RUNNING  | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.647 |  0.189 |                   73 |
| train_10f5e_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.16348500549793243
[2m[36m(func pid=138601)[0m mae:  0.1135319247841835
[2m[36m(func pid=138601)[0m rmse_per_class: [0.109, 0.253, 0.041, 0.289, 0.134, 0.173, 0.268, 0.113, 0.135, 0.12]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.17655304074287415
[2m[36m(func pid=139034)[0m mae:  0.11422157287597656
[2m[36m(func pid=139034)[0m rmse_per_class: [0.12, 0.225, 0.051, 0.296, 0.215, 0.167, 0.282, 0.106, 0.167, 0.135]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138220)[0m rmse: 0.1783400923013687
[2m[36m(func pid=138220)[0m mae:  0.12666088342666626
[2m[36m(func pid=138220)[0m rmse_per_class: [0.198, 0.257, 0.09, 0.354, 0.054, 0.185, 0.257, 0.14, 0.151, 0.097]
[2m[36m(func pid=139456)[0m rmse: 0.20109061896800995
[2m[36m(func pid=139456)[0m mae:  0.11916301399469376
[2m[36m(func pid=139456)[0m rmse_per_class: [0.112, 0.297, 0.049, 0.388, 0.068, 0.251, 0.298, 0.269, 0.16, 0.118]
[2m[36m(func pid=139456)[0m 
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3745 | Steps: 4 | Val loss: 0.2908 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3448 | Steps: 4 | Val loss: 0.3012 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=139456)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.9683 | Steps: 4 | Val loss: 0.5888 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=138601)[0m rmse: 0.1630067527294159
[2m[36m(func pid=138601)[0m mae:  0.11362774670124054
[2m[36m(func pid=138601)[0m rmse_per_class: [0.106, 0.255, 0.04, 0.292, 0.129, 0.174, 0.263, 0.113, 0.141, 0.117]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.17036986351013184
[2m[36m(func pid=139034)[0m mae:  0.10835884511470795
[2m[36m(func pid=139034)[0m rmse_per_class: [0.093, 0.204, 0.069, 0.29, 0.205, 0.166, 0.278, 0.125, 0.172, 0.101]
[2m[36m(func pid=139456)[0m rmse: 0.1799010932445526
[2m[36m(func pid=139456)[0m mae:  0.10693023353815079
[2m[36m(func pid=139456)[0m rmse_per_class: [0.111, 0.217, 0.034, 0.382, 0.057, 0.205, 0.425, 0.126, 0.139, 0.103]
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3686 | Steps: 4 | Val loss: 0.2913 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
== Status ==
Current time: 2024-01-07 15:36:32 (running for 00:07:46.38)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: -0.1599999964237213
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.163 |                   80 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.34  |  0.177 |                   79 |
| train_10f5e_00003 | RUNNING    | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.985 |  0.201 |                   74 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=157242)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=157242)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=157242)[0m Configuration completed!
[2m[36m(func pid=157242)[0m New optimizer parameters:
[2m[36m(func pid=157242)[0m SGD (
[2m[36m(func pid=157242)[0m Parameter Group 0
[2m[36m(func pid=157242)[0m     dampening: 0
[2m[36m(func pid=157242)[0m     differentiable: False
[2m[36m(func pid=157242)[0m     foreach: None
[2m[36m(func pid=157242)[0m     lr: 0.0001
[2m[36m(func pid=157242)[0m     maximize: False
[2m[36m(func pid=157242)[0m     momentum: 0.9
[2m[36m(func pid=157242)[0m     nesterov: False
[2m[36m(func pid=157242)[0m     weight_decay: 0
[2m[36m(func pid=157242)[0m )
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:36:37 (running for 00:07:51.54)
Memory usage on this node: 21.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 3 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.369 |  0.163 |                   81 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.345 |  0.17  |                   80 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.16302189230918884
[2m[36m(func pid=138601)[0m mae:  0.11394987255334854
[2m[36m(func pid=138601)[0m rmse_per_class: [0.1, 0.257, 0.041, 0.298, 0.127, 0.175, 0.259, 0.115, 0.149, 0.111]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3466 | Steps: 4 | Val loss: 0.2930 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3807 | Steps: 4 | Val loss: 0.2915 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0524 | Steps: 4 | Val loss: 0.7203 | Batch size: 32 | lr: 0.0001 | Duration: 4.33s
[2m[36m(func pid=139034)[0m rmse: 0.1652936041355133
[2m[36m(func pid=139034)[0m mae:  0.10434380918741226
[2m[36m(func pid=139034)[0m rmse_per_class: [0.091, 0.206, 0.061, 0.292, 0.171, 0.165, 0.27, 0.135, 0.17, 0.093]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.16325369477272034
[2m[36m(func pid=138601)[0m mae:  0.11412952095270157
[2m[36m(func pid=138601)[0m rmse_per_class: [0.096, 0.256, 0.041, 0.304, 0.121, 0.175, 0.256, 0.118, 0.159, 0.106]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18042460083961487
[2m[36m(func pid=157242)[0m mae:  0.13288068771362305
[2m[36m(func pid=157242)[0m rmse_per_class: [0.114, 0.264, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3365 | Steps: 4 | Val loss: 0.2830 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3815 | Steps: 4 | Val loss: 0.2930 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 15:36:43 (running for 00:07:56.67)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.381 |  0.163 |                   82 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.347 |  0.165 |                   81 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |        |        |                      |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=157825)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157825)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=157825)[0m Configuration completed!
[2m[36m(func pid=157825)[0m New optimizer parameters:
[2m[36m(func pid=157825)[0m SGD (
[2m[36m(func pid=157825)[0m Parameter Group 0
[2m[36m(func pid=157825)[0m     dampening: 0
[2m[36m(func pid=157825)[0m     differentiable: False
[2m[36m(func pid=157825)[0m     foreach: None
[2m[36m(func pid=157825)[0m     lr: 0.001
[2m[36m(func pid=157825)[0m     maximize: False
[2m[36m(func pid=157825)[0m     momentum: 0.9
[2m[36m(func pid=157825)[0m     nesterov: False
[2m[36m(func pid=157825)[0m     weight_decay: 0
[2m[36m(func pid=157825)[0m )
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.156539648771286
[2m[36m(func pid=139034)[0m mae:  0.0995010957121849
[2m[36m(func pid=139034)[0m rmse_per_class: [0.09, 0.228, 0.04, 0.297, 0.124, 0.162, 0.254, 0.125, 0.154, 0.091]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.16402491927146912
[2m[36m(func pid=138601)[0m mae:  0.11417271196842194
[2m[36m(func pid=138601)[0m rmse_per_class: [0.094, 0.257, 0.041, 0.312, 0.113, 0.175, 0.25, 0.128, 0.168, 0.102]
[2m[36m(func pid=138601)[0m 
== Status ==
Current time: 2024-01-07 15:36:48 (running for 00:08:01.92)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.381 |  0.164 |                   83 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.337 |  0.157 |                   82 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  1.052 |  0.18  |                    1 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |        |        |                      |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0259 | Steps: 4 | Val loss: 0.7284 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0283 | Steps: 4 | Val loss: 0.7057 | Batch size: 32 | lr: 0.001 | Duration: 4.38s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3286 | Steps: 4 | Val loss: 0.2728 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3766 | Steps: 4 | Val loss: 0.2953 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=157242)[0m rmse: 0.18082472681999207
[2m[36m(func pid=157242)[0m mae:  0.13323691487312317
[2m[36m(func pid=157242)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.106, 0.192, 0.302, 0.142, 0.143, 0.116]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.18042699992656708
[2m[36m(func pid=157825)[0m mae:  0.13285912573337555
[2m[36m(func pid=157825)[0m rmse_per_class: [0.114, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.14531175792217255
[2m[36m(func pid=139034)[0m mae:  0.09270228445529938
[2m[36m(func pid=139034)[0m rmse_per_class: [0.096, 0.233, 0.03, 0.287, 0.09, 0.16, 0.234, 0.102, 0.135, 0.088]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:36:53 (running for 00:08:07.15)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.377 |  0.165 |                   84 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.329 |  0.145 |                   83 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  1.026 |  0.181 |                    2 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  1.028 |  0.18  |                    1 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.16537150740623474
[2m[36m(func pid=138601)[0m mae:  0.1149718165397644
[2m[36m(func pid=138601)[0m rmse_per_class: [0.094, 0.256, 0.04, 0.32, 0.108, 0.176, 0.25, 0.132, 0.181, 0.099]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0124 | Steps: 4 | Val loss: 0.7322 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3429 | Steps: 4 | Val loss: 0.2638 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.9241 | Steps: 4 | Val loss: 0.6732 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3915 | Steps: 4 | Val loss: 0.2974 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=157242)[0m rmse: 0.18094661831855774
[2m[36m(func pid=157242)[0m mae:  0.13334760069847107
[2m[36m(func pid=157242)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.33, 0.108, 0.191, 0.304, 0.142, 0.142, 0.116]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.13801082968711853
[2m[36m(func pid=139034)[0m mae:  0.08878114819526672
[2m[36m(func pid=139034)[0m rmse_per_class: [0.107, 0.206, 0.027, 0.273, 0.071, 0.157, 0.225, 0.093, 0.132, 0.089]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.18065282702445984
[2m[36m(func pid=157825)[0m mae:  0.1330665647983551
[2m[36m(func pid=157825)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.331, 0.106, 0.192, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=157825)[0m 
== Status ==
Current time: 2024-01-07 15:36:58 (running for 00:08:12.19)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.391 |  0.167 |                   85 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.343 |  0.138 |                   84 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  1.012 |  0.181 |                    3 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.924 |  0.181 |                    2 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.16678741574287415
[2m[36m(func pid=138601)[0m mae:  0.11538781970739365
[2m[36m(func pid=138601)[0m rmse_per_class: [0.094, 0.254, 0.04, 0.327, 0.104, 0.176, 0.247, 0.145, 0.185, 0.096]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.9890 | Steps: 4 | Val loss: 0.7289 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3250 | Steps: 4 | Val loss: 0.2617 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7890 | Steps: 4 | Val loss: 0.6150 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3866 | Steps: 4 | Val loss: 0.2989 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=157242)[0m rmse: 0.18098950386047363
[2m[36m(func pid=157242)[0m mae:  0.13339869678020477
[2m[36m(func pid=157242)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.108, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1805519163608551
[2m[36m(func pid=157825)[0m mae:  0.1329602599143982
[2m[36m(func pid=157825)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.331, 0.106, 0.191, 0.301, 0.141, 0.143, 0.114]
[2m[36m(func pid=157825)[0m 
== Status ==
Current time: 2024-01-07 15:37:03 (running for 00:08:17.20)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.387 |  0.167 |                   86 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.325 |  0.138 |                   85 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.989 |  0.181 |                    4 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.789 |  0.181 |                    3 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.16735239326953888
[2m[36m(func pid=138601)[0m mae:  0.11580910533666611
[2m[36m(func pid=138601)[0m rmse_per_class: [0.094, 0.251, 0.04, 0.332, 0.097, 0.177, 0.247, 0.151, 0.192, 0.094]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.13824094831943512
[2m[36m(func pid=139034)[0m mae:  0.08930756151676178
[2m[36m(func pid=139034)[0m rmse_per_class: [0.131, 0.195, 0.027, 0.264, 0.06, 0.158, 0.226, 0.096, 0.131, 0.095]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.9704 | Steps: 4 | Val loss: 0.7208 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3291 | Steps: 4 | Val loss: 0.2663 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3894 | Steps: 4 | Val loss: 0.2988 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6713 | Steps: 4 | Val loss: 0.5474 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=157242)[0m rmse: 0.18109871447086334
[2m[36m(func pid=157242)[0m mae:  0.13346882164478302
[2m[36m(func pid=157242)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.109, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:37:08 (running for 00:08:22.33)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.387 |  0.167 |                   86 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.329 |  0.144 |                   86 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.97  |  0.181 |                    5 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.789 |  0.181 |                    3 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.16705255210399628
[2m[36m(func pid=138601)[0m mae:  0.11583548784255981
[2m[36m(func pid=138601)[0m rmse_per_class: [0.094, 0.246, 0.04, 0.335, 0.092, 0.177, 0.247, 0.155, 0.191, 0.094]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.14400675892829895
[2m[36m(func pid=139034)[0m mae:  0.09350915253162384
[2m[36m(func pid=139034)[0m rmse_per_class: [0.148, 0.202, 0.028, 0.264, 0.054, 0.166, 0.236, 0.099, 0.134, 0.109]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1801607459783554
[2m[36m(func pid=157825)[0m mae:  0.13258200883865356
[2m[36m(func pid=157825)[0m rmse_per_class: [0.114, 0.264, 0.103, 0.33, 0.106, 0.19, 0.3, 0.14, 0.142, 0.112]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.9435 | Steps: 4 | Val loss: 0.7107 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3131 | Steps: 4 | Val loss: 0.2794 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3672 | Steps: 4 | Val loss: 0.2978 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5856 | Steps: 4 | Val loss: 0.4848 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=157242)[0m rmse: 0.18113751709461212
[2m[36m(func pid=157242)[0m mae:  0.13348443806171417
[2m[36m(func pid=157242)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.116]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:37:13 (running for 00:08:27.46)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.389 |  0.167 |                   87 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.313 |  0.153 |                   87 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.943 |  0.181 |                    6 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.671 |  0.18  |                    4 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.1527862846851349
[2m[36m(func pid=139034)[0m mae:  0.10132255405187607
[2m[36m(func pid=139034)[0m rmse_per_class: [0.129, 0.215, 0.03, 0.267, 0.052, 0.175, 0.259, 0.102, 0.153, 0.146]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.16581153869628906
[2m[36m(func pid=138601)[0m mae:  0.11546020209789276
[2m[36m(func pid=138601)[0m rmse_per_class: [0.094, 0.239, 0.04, 0.337, 0.086, 0.178, 0.248, 0.154, 0.187, 0.094]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1796119213104248
[2m[36m(func pid=157825)[0m mae:  0.13208141922950745
[2m[36m(func pid=157825)[0m rmse_per_class: [0.114, 0.264, 0.104, 0.329, 0.105, 0.189, 0.298, 0.14, 0.143, 0.11]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.9115 | Steps: 4 | Val loss: 0.6968 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3459 | Steps: 4 | Val loss: 0.2879 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3859 | Steps: 4 | Val loss: 0.2969 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5281 | Steps: 4 | Val loss: 0.4367 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=157242)[0m rmse: 0.1810614913702011
[2m[36m(func pid=157242)[0m mae:  0.1334252655506134
[2m[36m(func pid=157242)[0m rmse_per_class: [0.113, 0.263, 0.098, 0.328, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.15670199692249298
[2m[36m(func pid=139034)[0m mae:  0.10517017543315887
[2m[36m(func pid=139034)[0m rmse_per_class: [0.096, 0.22, 0.029, 0.272, 0.054, 0.18, 0.269, 0.101, 0.168, 0.18]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:37:18 (running for 00:08:32.62)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.386 |  0.164 |                   89 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.346 |  0.157 |                   88 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.911 |  0.181 |                    7 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.586 |  0.18  |                    5 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.16439804434776306
[2m[36m(func pid=138601)[0m mae:  0.11482949554920197
[2m[36m(func pid=138601)[0m rmse_per_class: [0.095, 0.233, 0.044, 0.339, 0.08, 0.178, 0.249, 0.152, 0.181, 0.094]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17903825640678406
[2m[36m(func pid=157825)[0m mae:  0.13159652054309845
[2m[36m(func pid=157825)[0m rmse_per_class: [0.115, 0.264, 0.104, 0.33, 0.103, 0.188, 0.296, 0.139, 0.143, 0.108]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8981 | Steps: 4 | Val loss: 0.6842 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3459 | Steps: 4 | Val loss: 0.2991 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3587 | Steps: 4 | Val loss: 0.2949 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4973 | Steps: 4 | Val loss: 0.4052 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=157242)[0m rmse: 0.1811000406742096
[2m[36m(func pid=157242)[0m mae:  0.13346917927265167
[2m[36m(func pid=157242)[0m rmse_per_class: [0.113, 0.264, 0.097, 0.328, 0.108, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.1626264601945877
[2m[36m(func pid=139034)[0m mae:  0.10962969064712524
[2m[36m(func pid=139034)[0m rmse_per_class: [0.093, 0.216, 0.036, 0.279, 0.053, 0.172, 0.275, 0.106, 0.201, 0.196]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:37:24 (running for 00:08:37.85)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.386 |  0.164 |                   89 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.346 |  0.163 |                   89 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.898 |  0.181 |                    8 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.497 |  0.179 |                    7 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.1785808503627777
[2m[36m(func pid=157825)[0m mae:  0.13126540184020996
[2m[36m(func pid=157825)[0m rmse_per_class: [0.115, 0.264, 0.103, 0.331, 0.1, 0.188, 0.294, 0.139, 0.144, 0.108]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.1625264436006546
[2m[36m(func pid=138601)[0m mae:  0.11392156034708023
[2m[36m(func pid=138601)[0m rmse_per_class: [0.096, 0.228, 0.046, 0.339, 0.077, 0.177, 0.25, 0.15, 0.168, 0.095]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8685 | Steps: 4 | Val loss: 0.6707 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3285 | Steps: 4 | Val loss: 0.2905 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3742 | Steps: 4 | Val loss: 0.2952 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4773 | Steps: 4 | Val loss: 0.3807 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=157242)[0m rmse: 0.18115337193012238
[2m[36m(func pid=157242)[0m mae:  0.13352221250534058
[2m[36m(func pid=157242)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.328, 0.108, 0.19, 0.306, 0.144, 0.142, 0.118]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.15618905425071716
[2m[36m(func pid=139034)[0m mae:  0.10421083122491837
[2m[36m(func pid=139034)[0m rmse_per_class: [0.091, 0.21, 0.042, 0.284, 0.055, 0.164, 0.255, 0.096, 0.195, 0.171]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:37:29 (running for 00:08:43.02)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.162 |                   91 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.329 |  0.156 |                   90 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.869 |  0.181 |                    9 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.497 |  0.179 |                    7 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.1619260162115097
[2m[36m(func pid=138601)[0m mae:  0.11326805502176285
[2m[36m(func pid=138601)[0m rmse_per_class: [0.101, 0.223, 0.058, 0.34, 0.07, 0.176, 0.25, 0.146, 0.159, 0.095]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1781526505947113
[2m[36m(func pid=157825)[0m mae:  0.13095518946647644
[2m[36m(func pid=157825)[0m rmse_per_class: [0.116, 0.264, 0.103, 0.332, 0.097, 0.187, 0.292, 0.138, 0.145, 0.107]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8446 | Steps: 4 | Val loss: 0.6565 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3486 | Steps: 4 | Val loss: 0.2801 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3649 | Steps: 4 | Val loss: 0.2943 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4571 | Steps: 4 | Val loss: 0.3628 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=157242)[0m rmse: 0.18111561238765717
[2m[36m(func pid=157242)[0m mae:  0.13349829614162445
[2m[36m(func pid=157242)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.328, 0.108, 0.19, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.14997178316116333
[2m[36m(func pid=139034)[0m mae:  0.09840494394302368
[2m[36m(func pid=139034)[0m rmse_per_class: [0.095, 0.198, 0.057, 0.283, 0.054, 0.16, 0.238, 0.094, 0.175, 0.145]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:37:34 (running for 00:08:48.25)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.162 |                   91 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.349 |  0.15  |                   91 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.845 |  0.181 |                   10 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.457 |  0.178 |                    9 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.1613815575838089
[2m[36m(func pid=138601)[0m mae:  0.11245381832122803
[2m[36m(func pid=138601)[0m rmse_per_class: [0.106, 0.22, 0.068, 0.34, 0.067, 0.175, 0.252, 0.14, 0.15, 0.095]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17788687348365784
[2m[36m(func pid=157825)[0m mae:  0.13081876933574677
[2m[36m(func pid=157825)[0m rmse_per_class: [0.117, 0.263, 0.102, 0.334, 0.093, 0.188, 0.29, 0.138, 0.146, 0.107]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8253 | Steps: 4 | Val loss: 0.6430 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3303 | Steps: 4 | Val loss: 0.2683 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3653 | Steps: 4 | Val loss: 0.2935 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4607 | Steps: 4 | Val loss: 0.3546 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=157242)[0m rmse: 0.1811542809009552
[2m[36m(func pid=157242)[0m mae:  0.133502796292305
[2m[36m(func pid=157242)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.328, 0.108, 0.19, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.14288999140262604
[2m[36m(func pid=139034)[0m mae:  0.0925997868180275
[2m[36m(func pid=139034)[0m rmse_per_class: [0.105, 0.198, 0.045, 0.29, 0.055, 0.159, 0.22, 0.091, 0.151, 0.113]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:37:39 (running for 00:08:53.28)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.365 |  0.161 |                   93 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.33  |  0.143 |                   92 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.825 |  0.181 |                   11 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.457 |  0.178 |                    9 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.1607903689146042
[2m[36m(func pid=138601)[0m mae:  0.11208822578191757
[2m[36m(func pid=138601)[0m rmse_per_class: [0.115, 0.219, 0.072, 0.339, 0.064, 0.175, 0.256, 0.13, 0.143, 0.096]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17769429087638855
[2m[36m(func pid=157825)[0m mae:  0.1306791603565216
[2m[36m(func pid=157825)[0m rmse_per_class: [0.117, 0.264, 0.101, 0.335, 0.092, 0.187, 0.289, 0.137, 0.147, 0.107]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8065 | Steps: 4 | Val loss: 0.6295 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3209 | Steps: 4 | Val loss: 0.2694 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3801 | Steps: 4 | Val loss: 0.2940 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4536 | Steps: 4 | Val loss: 0.3493 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=157242)[0m rmse: 0.181123286485672
[2m[36m(func pid=157242)[0m mae:  0.13346531987190247
[2m[36m(func pid=157242)[0m rmse_per_class: [0.113, 0.265, 0.096, 0.328, 0.107, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.14344307780265808
[2m[36m(func pid=139034)[0m mae:  0.0922146737575531
[2m[36m(func pid=139034)[0m rmse_per_class: [0.127, 0.201, 0.029, 0.305, 0.057, 0.16, 0.22, 0.096, 0.142, 0.099]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:37:44 (running for 00:08:58.38)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.38  |  0.161 |                   94 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.321 |  0.143 |                   93 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.807 |  0.181 |                   12 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.461 |  0.178 |                   10 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.16117757558822632
[2m[36m(func pid=138601)[0m mae:  0.11190067231655121
[2m[36m(func pid=138601)[0m rmse_per_class: [0.128, 0.22, 0.077, 0.338, 0.06, 0.174, 0.259, 0.121, 0.138, 0.096]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1775435209274292
[2m[36m(func pid=157825)[0m mae:  0.1305270493030548
[2m[36m(func pid=157825)[0m rmse_per_class: [0.117, 0.265, 0.101, 0.336, 0.091, 0.188, 0.287, 0.137, 0.147, 0.107]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7871 | Steps: 4 | Val loss: 0.6184 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3325 | Steps: 4 | Val loss: 0.2715 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3703 | Steps: 4 | Val loss: 0.2934 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4472 | Steps: 4 | Val loss: 0.3435 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=157242)[0m rmse: 0.18105311691761017
[2m[36m(func pid=157242)[0m mae:  0.13340839743614197
[2m[36m(func pid=157242)[0m rmse_per_class: [0.113, 0.265, 0.096, 0.329, 0.107, 0.191, 0.305, 0.145, 0.143, 0.118]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=139034)[0m rmse: 0.1431187391281128
[2m[36m(func pid=139034)[0m mae:  0.09157420694828033
[2m[36m(func pid=139034)[0m rmse_per_class: [0.133, 0.205, 0.028, 0.302, 0.061, 0.158, 0.221, 0.097, 0.136, 0.09]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:37:49 (running for 00:09:03.60)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.37  |  0.161 |                   95 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.333 |  0.143 |                   94 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.787 |  0.181 |                   13 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.454 |  0.178 |                   11 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.16109700500965118
[2m[36m(func pid=138601)[0m mae:  0.11146388202905655
[2m[36m(func pid=138601)[0m rmse_per_class: [0.135, 0.222, 0.077, 0.336, 0.059, 0.173, 0.262, 0.115, 0.136, 0.096]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17730531096458435
[2m[36m(func pid=157825)[0m mae:  0.13035091757774353
[2m[36m(func pid=157825)[0m rmse_per_class: [0.118, 0.264, 0.101, 0.337, 0.088, 0.188, 0.286, 0.137, 0.147, 0.107]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3573 | Steps: 4 | Val loss: 0.2708 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7718 | Steps: 4 | Val loss: 0.6060 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3788 | Steps: 4 | Val loss: 0.2893 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4525 | Steps: 4 | Val loss: 0.3408 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=139034)[0m rmse: 0.14468131959438324
[2m[36m(func pid=139034)[0m mae:  0.09339258074760437
[2m[36m(func pid=139034)[0m rmse_per_class: [0.116, 0.203, 0.028, 0.296, 0.071, 0.156, 0.233, 0.113, 0.142, 0.089]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18097904324531555
[2m[36m(func pid=157242)[0m mae:  0.13337267935276031
[2m[36m(func pid=157242)[0m rmse_per_class: [0.113, 0.265, 0.095, 0.329, 0.106, 0.19, 0.305, 0.145, 0.143, 0.118]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:37:54 (running for 00:09:08.63)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.379 |  0.159 |                   96 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.357 |  0.145 |                   95 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.772 |  0.181 |                   14 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.447 |  0.177 |                   12 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.15897594392299652
[2m[36m(func pid=138601)[0m mae:  0.11000949144363403
[2m[36m(func pid=138601)[0m rmse_per_class: [0.134, 0.224, 0.064, 0.334, 0.057, 0.171, 0.26, 0.115, 0.134, 0.097]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1770896166563034
[2m[36m(func pid=157825)[0m mae:  0.1301875114440918
[2m[36m(func pid=157825)[0m rmse_per_class: [0.119, 0.263, 0.101, 0.338, 0.086, 0.188, 0.285, 0.137, 0.147, 0.107]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3237 | Steps: 4 | Val loss: 0.2744 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7543 | Steps: 4 | Val loss: 0.5942 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3698 | Steps: 4 | Val loss: 0.2908 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4511 | Steps: 4 | Val loss: 0.3401 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=139034)[0m rmse: 0.1492973268032074
[2m[36m(func pid=139034)[0m mae:  0.09764789044857025
[2m[36m(func pid=139034)[0m rmse_per_class: [0.099, 0.202, 0.028, 0.285, 0.088, 0.163, 0.256, 0.123, 0.158, 0.09]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:37:59 (running for 00:09:13.64)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.379 |  0.159 |                   96 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.324 |  0.149 |                   96 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.754 |  0.181 |                   15 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.453 |  0.177 |                   13 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.15993532538414001
[2m[36m(func pid=138601)[0m mae:  0.11026610434055328
[2m[36m(func pid=138601)[0m rmse_per_class: [0.142, 0.228, 0.068, 0.332, 0.056, 0.171, 0.263, 0.108, 0.133, 0.098]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18103456497192383
[2m[36m(func pid=157242)[0m mae:  0.13340428471565247
[2m[36m(func pid=157242)[0m rmse_per_class: [0.113, 0.265, 0.095, 0.329, 0.106, 0.191, 0.305, 0.145, 0.143, 0.118]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17700543999671936
[2m[36m(func pid=157825)[0m mae:  0.13008809089660645
[2m[36m(func pid=157825)[0m rmse_per_class: [0.12, 0.263, 0.102, 0.339, 0.084, 0.188, 0.284, 0.137, 0.147, 0.107]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3298 | Steps: 4 | Val loss: 0.2797 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3755 | Steps: 4 | Val loss: 0.2861 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7373 | Steps: 4 | Val loss: 0.5831 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4398 | Steps: 4 | Val loss: 0.3374 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=139034)[0m rmse: 0.15435653924942017
[2m[36m(func pid=139034)[0m mae:  0.101303830742836
[2m[36m(func pid=139034)[0m rmse_per_class: [0.089, 0.209, 0.037, 0.279, 0.103, 0.174, 0.268, 0.108, 0.177, 0.099]
[2m[36m(func pid=139034)[0m 
== Status ==
Current time: 2024-01-07 15:38:05 (running for 00:09:19.04)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.376 |  0.158 |                   98 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.33  |  0.154 |                   97 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.754 |  0.181 |                   15 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.451 |  0.177 |                   14 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.15758521854877472
[2m[36m(func pid=138601)[0m mae:  0.10894741863012314
[2m[36m(func pid=138601)[0m rmse_per_class: [0.137, 0.227, 0.058, 0.327, 0.056, 0.169, 0.261, 0.108, 0.133, 0.1]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18096785247325897
[2m[36m(func pid=157242)[0m mae:  0.13335393369197845
[2m[36m(func pid=157242)[0m rmse_per_class: [0.114, 0.265, 0.095, 0.329, 0.106, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17682014405727386
[2m[36m(func pid=157825)[0m mae:  0.12991507351398468
[2m[36m(func pid=157825)[0m rmse_per_class: [0.121, 0.263, 0.101, 0.34, 0.082, 0.188, 0.284, 0.137, 0.146, 0.107]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3558 | Steps: 4 | Val loss: 0.2776 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3666 | Steps: 4 | Val loss: 0.2831 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7289 | Steps: 4 | Val loss: 0.5723 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4329 | Steps: 4 | Val loss: 0.3353 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 15:38:10 (running for 00:09:24.04)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.376 |  0.158 |                   98 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.356 |  0.154 |                   98 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.737 |  0.181 |                   16 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.44  |  0.177 |                   15 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=139034)[0m rmse: 0.15426787734031677
[2m[36m(func pid=139034)[0m mae:  0.10041096061468124
[2m[36m(func pid=139034)[0m rmse_per_class: [0.09, 0.208, 0.049, 0.265, 0.126, 0.174, 0.267, 0.092, 0.162, 0.111]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=138601)[0m rmse: 0.1563001424074173
[2m[36m(func pid=138601)[0m mae:  0.10843177139759064
[2m[36m(func pid=138601)[0m rmse_per_class: [0.132, 0.229, 0.054, 0.321, 0.056, 0.169, 0.262, 0.106, 0.133, 0.102]
[2m[36m(func pid=138601)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18089960515499115
[2m[36m(func pid=157242)[0m mae:  0.13329145312309265
[2m[36m(func pid=157242)[0m rmse_per_class: [0.114, 0.265, 0.096, 0.329, 0.105, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1766812801361084
[2m[36m(func pid=157825)[0m mae:  0.12979379296302795
[2m[36m(func pid=157825)[0m rmse_per_class: [0.122, 0.263, 0.1, 0.34, 0.081, 0.188, 0.283, 0.137, 0.146, 0.107]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3309 | Steps: 4 | Val loss: 0.2799 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=138601)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3744 | Steps: 4 | Val loss: 0.2794 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.7076 | Steps: 4 | Val loss: 0.5628 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4324 | Steps: 4 | Val loss: 0.3338 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 15:38:15 (running for 00:09:29.25)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00001 | RUNNING    | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.367 |  0.156 |                   99 |
| train_10f5e_00002 | RUNNING    | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.331 |  0.156 |                   99 |
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.729 |  0.181 |                   17 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.433 |  0.177 |                   16 |
| train_10f5e_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=138601)[0m rmse: 0.15437102317810059
[2m[36m(func pid=138601)[0m mae:  0.10692097246646881
[2m[36m(func pid=138601)[0m rmse_per_class: [0.124, 0.227, 0.054, 0.314, 0.055, 0.169, 0.259, 0.106, 0.133, 0.103]
[2m[36m(func pid=139034)[0m rmse: 0.15637609362602234
[2m[36m(func pid=139034)[0m mae:  0.1007462590932846
[2m[36m(func pid=139034)[0m rmse_per_class: [0.09, 0.216, 0.054, 0.265, 0.149, 0.167, 0.252, 0.092, 0.148, 0.132]
[2m[36m(func pid=139034)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18085439503192902
[2m[36m(func pid=157242)[0m mae:  0.1332438439130783
[2m[36m(func pid=157242)[0m rmse_per_class: [0.114, 0.265, 0.096, 0.329, 0.105, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17656829953193665
[2m[36m(func pid=157825)[0m mae:  0.12968400120735168
[2m[36m(func pid=157825)[0m rmse_per_class: [0.124, 0.263, 0.1, 0.341, 0.079, 0.187, 0.282, 0.136, 0.146, 0.107]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=139034)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3231 | Steps: 4 | Val loss: 0.2802 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6915 | Steps: 4 | Val loss: 0.5512 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4367 | Steps: 4 | Val loss: 0.3328 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=139034)[0m rmse: 0.1562070995569229
[2m[36m(func pid=139034)[0m mae:  0.0999901294708252
[2m[36m(func pid=139034)[0m rmse_per_class: [0.091, 0.213, 0.048, 0.27, 0.169, 0.164, 0.233, 0.098, 0.138, 0.137]
[2m[36m(func pid=157242)[0m rmse: 0.18087908625602722
[2m[36m(func pid=157242)[0m mae:  0.13324615359306335
[2m[36m(func pid=157242)[0m rmse_per_class: [0.114, 0.265, 0.096, 0.329, 0.104, 0.191, 0.303, 0.145, 0.143, 0.119]
[2m[36m(func pid=157825)[0m rmse: 0.17640048265457153
[2m[36m(func pid=157825)[0m mae:  0.12963120639324188
[2m[36m(func pid=157825)[0m rmse_per_class: [0.123, 0.263, 0.099, 0.34, 0.079, 0.187, 0.282, 0.136, 0.147, 0.107]
== Status ==
Current time: 2024-01-07 15:38:20 (running for 00:09:34.59)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 3 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.708 |  0.181 |                   18 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.432 |  0.177 |                   17 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=162590)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=162590)[0m Configuration completed!
[2m[36m(func pid=162590)[0m New optimizer parameters:
[2m[36m(func pid=162590)[0m SGD (
[2m[36m(func pid=162590)[0m Parameter Group 0
[2m[36m(func pid=162590)[0m     dampening: 0
[2m[36m(func pid=162590)[0m     differentiable: False
[2m[36m(func pid=162590)[0m     foreach: None
[2m[36m(func pid=162590)[0m     lr: 0.01
[2m[36m(func pid=162590)[0m     maximize: False
[2m[36m(func pid=162590)[0m     momentum: 0.9
[2m[36m(func pid=162590)[0m     nesterov: False
[2m[36m(func pid=162590)[0m     weight_decay: 0
[2m[36m(func pid=162590)[0m )
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6872 | Steps: 4 | Val loss: 0.5432 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4310 | Steps: 4 | Val loss: 0.3315 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8862 | Steps: 4 | Val loss: 0.5958 | Batch size: 32 | lr: 0.01 | Duration: 4.50s
[2m[36m(func pid=157242)[0m rmse: 0.18082664906978607
[2m[36m(func pid=157242)[0m mae:  0.13319876790046692
[2m[36m(func pid=157242)[0m rmse_per_class: [0.114, 0.265, 0.096, 0.329, 0.104, 0.191, 0.303, 0.145, 0.143, 0.119]
[2m[36m(func pid=157825)[0m rmse: 0.17614814639091492
[2m[36m(func pid=157825)[0m mae:  0.12939080595970154
[2m[36m(func pid=157825)[0m rmse_per_class: [0.123, 0.263, 0.099, 0.34, 0.077, 0.187, 0.282, 0.136, 0.147, 0.106]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.18002758920192719
[2m[36m(func pid=162590)[0m mae:  0.13248813152313232
[2m[36m(func pid=162590)[0m rmse_per_class: [0.114, 0.263, 0.104, 0.334, 0.101, 0.191, 0.295, 0.141, 0.143, 0.113]
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4327 | Steps: 4 | Val loss: 0.3306 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 15:38:29 (running for 00:09:43.16)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.691 |  0.181 |                   19 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.431 |  0.176 |                   19 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |        |        |                      |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=163121)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=163121)[0m Configuration completed!
[2m[36m(func pid=163121)[0m New optimizer parameters:
[2m[36m(func pid=163121)[0m SGD (
[2m[36m(func pid=163121)[0m Parameter Group 0
[2m[36m(func pid=163121)[0m     dampening: 0
[2m[36m(func pid=163121)[0m     differentiable: False
[2m[36m(func pid=163121)[0m     foreach: None
[2m[36m(func pid=163121)[0m     lr: 0.1
[2m[36m(func pid=163121)[0m     maximize: False
[2m[36m(func pid=163121)[0m     momentum: 0.9
[2m[36m(func pid=163121)[0m     nesterov: False
[2m[36m(func pid=163121)[0m     weight_decay: 0
[2m[36m(func pid=163121)[0m )
[2m[36m(func pid=163121)[0m 
== Status ==
Current time: 2024-01-07 15:38:34 (running for 00:09:48.46)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.687 |  0.181 |                   20 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.433 |  0.176 |                   20 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.886 |  0.18  |                    1 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |        |        |                      |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.17588552832603455
[2m[36m(func pid=157825)[0m mae:  0.1290556639432907
[2m[36m(func pid=157825)[0m rmse_per_class: [0.125, 0.263, 0.1, 0.34, 0.075, 0.187, 0.281, 0.136, 0.147, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6708 | Steps: 4 | Val loss: 0.5337 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5084 | Steps: 4 | Val loss: 0.4242 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.6434 | Steps: 4 | Val loss: 0.3648 | Batch size: 32 | lr: 0.1 | Duration: 4.76s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4297 | Steps: 4 | Val loss: 0.3289 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=157242)[0m rmse: 0.18077769875526428
[2m[36m(func pid=157242)[0m mae:  0.13315211236476898
[2m[36m(func pid=157242)[0m rmse_per_class: [0.114, 0.265, 0.096, 0.329, 0.104, 0.191, 0.303, 0.145, 0.143, 0.119]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.17830201983451843
[2m[36m(func pid=162590)[0m mae:  0.13098427653312683
[2m[36m(func pid=162590)[0m rmse_per_class: [0.114, 0.264, 0.107, 0.334, 0.096, 0.19, 0.29, 0.137, 0.145, 0.106]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.17777366936206818
[2m[36m(func pid=163121)[0m mae:  0.12912617623806
[2m[36m(func pid=163121)[0m rmse_per_class: [0.117, 0.266, 0.119, 0.344, 0.078, 0.189, 0.279, 0.138, 0.147, 0.1]
[2m[36m(func pid=163121)[0m 
== Status ==
Current time: 2024-01-07 15:38:40 (running for 00:09:53.74)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.671 |  0.181 |                   21 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.43  |  0.175 |                   21 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.508 |  0.178 |                    2 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.643 |  0.178 |                    1 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.17543925344944
[2m[36m(func pid=157825)[0m mae:  0.12869778275489807
[2m[36m(func pid=157825)[0m rmse_per_class: [0.125, 0.263, 0.099, 0.339, 0.074, 0.187, 0.28, 0.136, 0.147, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6675 | Steps: 4 | Val loss: 0.5307 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4557 | Steps: 4 | Val loss: 0.3355 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.6825 | Steps: 4 | Val loss: 0.3181 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4379 | Steps: 4 | Val loss: 0.3283 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=157242)[0m rmse: 0.18068605661392212
[2m[36m(func pid=157242)[0m mae:  0.1331048458814621
[2m[36m(func pid=157242)[0m rmse_per_class: [0.114, 0.265, 0.096, 0.329, 0.104, 0.19, 0.303, 0.144, 0.143, 0.119]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.17564183473587036
[2m[36m(func pid=162590)[0m mae:  0.12865839898586273
[2m[36m(func pid=162590)[0m rmse_per_class: [0.119, 0.263, 0.105, 0.335, 0.083, 0.188, 0.281, 0.135, 0.147, 0.101]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.17325042188167572
[2m[36m(func pid=163121)[0m mae:  0.12105892598628998
[2m[36m(func pid=163121)[0m rmse_per_class: [0.122, 0.271, 0.091, 0.349, 0.055, 0.183, 0.284, 0.141, 0.145, 0.091]
[2m[36m(func pid=163121)[0m 
== Status ==
Current time: 2024-01-07 15:38:45 (running for 00:09:59.07)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.668 |  0.181 |                   22 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.438 |  0.175 |                   22 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.456 |  0.176 |                    3 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.682 |  0.173 |                    2 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.1751723289489746
[2m[36m(func pid=157825)[0m mae:  0.12844860553741455
[2m[36m(func pid=157825)[0m rmse_per_class: [0.126, 0.262, 0.098, 0.338, 0.073, 0.187, 0.28, 0.136, 0.147, 0.104]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4914 | Steps: 4 | Val loss: 0.3146 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.6503 | Steps: 4 | Val loss: 0.5225 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.6490 | Steps: 4 | Val loss: 0.3188 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=162590)[0m rmse: 0.17307859659194946
[2m[36m(func pid=162590)[0m mae:  0.1264548897743225
[2m[36m(func pid=162590)[0m rmse_per_class: [0.123, 0.261, 0.095, 0.337, 0.071, 0.185, 0.275, 0.136, 0.15, 0.097]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4242 | Steps: 4 | Val loss: 0.3270 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=157242)[0m rmse: 0.1807040274143219
[2m[36m(func pid=157242)[0m mae:  0.13311247527599335
[2m[36m(func pid=157242)[0m rmse_per_class: [0.114, 0.265, 0.096, 0.329, 0.104, 0.191, 0.303, 0.144, 0.143, 0.118]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.16920658946037292
[2m[36m(func pid=163121)[0m mae:  0.12102631479501724
[2m[36m(func pid=163121)[0m rmse_per_class: [0.128, 0.252, 0.083, 0.322, 0.056, 0.19, 0.277, 0.141, 0.15, 0.092]
[2m[36m(func pid=163121)[0m 
== Status ==
Current time: 2024-01-07 15:38:50 (running for 00:10:04.44)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.65  |  0.181 |                   23 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.424 |  0.175 |                   23 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.491 |  0.173 |                    4 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.649 |  0.169 |                    3 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.17484593391418457
[2m[36m(func pid=157825)[0m mae:  0.12821921706199646
[2m[36m(func pid=157825)[0m rmse_per_class: [0.125, 0.262, 0.097, 0.338, 0.073, 0.187, 0.28, 0.136, 0.146, 0.104]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4923 | Steps: 4 | Val loss: 0.3095 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.6459 | Steps: 4 | Val loss: 0.5160 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.5064 | Steps: 4 | Val loss: 0.3591 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4319 | Steps: 4 | Val loss: 0.3269 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=162590)[0m rmse: 0.1707790344953537
[2m[36m(func pid=162590)[0m mae:  0.12440039217472076
[2m[36m(func pid=162590)[0m rmse_per_class: [0.127, 0.259, 0.085, 0.331, 0.062, 0.184, 0.273, 0.138, 0.156, 0.095]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18063654005527496
[2m[36m(func pid=157242)[0m mae:  0.13304714858531952
[2m[36m(func pid=157242)[0m rmse_per_class: [0.114, 0.265, 0.097, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.118]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.17466208338737488
[2m[36m(func pid=163121)[0m mae:  0.12402548640966415
[2m[36m(func pid=163121)[0m rmse_per_class: [0.103, 0.247, 0.045, 0.333, 0.056, 0.185, 0.298, 0.143, 0.133, 0.205]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1749432533979416== Status ==
Current time: 2024-01-07 15:38:56 (running for 00:10:09.70)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.646 |  0.181 |                   24 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.432 |  0.175 |                   24 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.492 |  0.171 |                    5 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.506 |  0.175 |                    4 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=157825)[0m mae:  0.12820670008659363
[2m[36m(func pid=157825)[0m rmse_per_class: [0.127, 0.261, 0.098, 0.338, 0.072, 0.187, 0.28, 0.136, 0.146, 0.104]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4696 | Steps: 4 | Val loss: 0.3079 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.6368 | Steps: 4 | Val loss: 0.5085 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4957 | Steps: 4 | Val loss: 0.3498 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4306 | Steps: 4 | Val loss: 0.3263 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=162590)[0m rmse: 0.17084677517414093
[2m[36m(func pid=162590)[0m mae:  0.12395290285348892
[2m[36m(func pid=162590)[0m rmse_per_class: [0.142, 0.257, 0.076, 0.328, 0.057, 0.182, 0.272, 0.139, 0.161, 0.094]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18061242997646332
[2m[36m(func pid=157242)[0m mae:  0.133014515042305
[2m[36m(func pid=157242)[0m rmse_per_class: [0.114, 0.265, 0.097, 0.329, 0.103, 0.191, 0.302, 0.144, 0.143, 0.118]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.17953290045261383
[2m[36m(func pid=163121)[0m mae:  0.125896617770195
[2m[36m(func pid=163121)[0m rmse_per_class: [0.104, 0.256, 0.048, 0.379, 0.055, 0.189, 0.277, 0.227, 0.134, 0.126]
[2m[36m(func pid=163121)[0m 
== Status ==
Current time: 2024-01-07 15:39:01 (running for 00:10:14.90)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.637 |  0.181 |                   25 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.431 |  0.175 |                   25 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.47  |  0.171 |                    6 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.496 |  0.18  |                    5 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.1749274581670761
[2m[36m(func pid=157825)[0m mae:  0.12808457016944885
[2m[36m(func pid=157825)[0m rmse_per_class: [0.129, 0.261, 0.098, 0.338, 0.071, 0.187, 0.279, 0.136, 0.146, 0.104]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4468 | Steps: 4 | Val loss: 0.3092 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.6308 | Steps: 4 | Val loss: 0.5019 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4996 | Steps: 4 | Val loss: 0.3152 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4279 | Steps: 4 | Val loss: 0.3257 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=162590)[0m rmse: 0.17265023291110992
[2m[36m(func pid=162590)[0m mae:  0.12468115240335464
[2m[36m(func pid=162590)[0m rmse_per_class: [0.164, 0.257, 0.07, 0.329, 0.055, 0.182, 0.271, 0.14, 0.164, 0.094]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18056467175483704
[2m[36m(func pid=157242)[0m mae:  0.13297536969184875
[2m[36m(func pid=157242)[0m rmse_per_class: [0.114, 0.265, 0.097, 0.329, 0.103, 0.19, 0.302, 0.144, 0.143, 0.118]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.17144903540611267
[2m[36m(func pid=163121)[0m mae:  0.12020997703075409
[2m[36m(func pid=163121)[0m rmse_per_class: [0.097, 0.237, 0.043, 0.355, 0.056, 0.232, 0.27, 0.128, 0.205, 0.092]
== Status ==
Current time: 2024-01-07 15:39:06 (running for 00:10:19.94)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.631 |  0.181 |                   26 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.431 |  0.175 |                   25 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.447 |  0.173 |                    7 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.5   |  0.171 |                    6 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17477616667747498
[2m[36m(func pid=157825)[0m mae:  0.12782783806324005
[2m[36m(func pid=157825)[0m rmse_per_class: [0.131, 0.261, 0.097, 0.338, 0.07, 0.187, 0.279, 0.136, 0.145, 0.104]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4468 | Steps: 4 | Val loss: 0.3090 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.6214 | Steps: 4 | Val loss: 0.4931 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4058 | Steps: 4 | Val loss: 0.3057 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4213 | Steps: 4 | Val loss: 0.3240 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=162590)[0m rmse: 0.17086736857891083
[2m[36m(func pid=162590)[0m mae:  0.12371426820755005
[2m[36m(func pid=162590)[0m rmse_per_class: [0.154, 0.254, 0.069, 0.326, 0.055, 0.182, 0.272, 0.139, 0.161, 0.098]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18046702444553375
[2m[36m(func pid=157242)[0m mae:  0.13290533423423767
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.264, 0.097, 0.33, 0.102, 0.19, 0.301, 0.144, 0.143, 0.118]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:39:11 (running for 00:10:25.28)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.621 |  0.18  |                   27 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.428 |  0.175 |                   26 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.447 |  0.171 |                    8 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.406 |  0.16  |                    7 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.15975823998451233
[2m[36m(func pid=163121)[0m mae:  0.10836587101221085
[2m[36m(func pid=163121)[0m rmse_per_class: [0.097, 0.238, 0.045, 0.287, 0.124, 0.178, 0.258, 0.145, 0.135, 0.091]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17415814101696014
[2m[36m(func pid=157825)[0m mae:  0.12746790051460266
[2m[36m(func pid=157825)[0m rmse_per_class: [0.127, 0.262, 0.096, 0.337, 0.07, 0.187, 0.278, 0.136, 0.145, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4465 | Steps: 4 | Val loss: 0.3075 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6121 | Steps: 4 | Val loss: 0.4878 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4228 | Steps: 4 | Val loss: 0.3230 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4303 | Steps: 4 | Val loss: 0.3200 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=162590)[0m rmse: 0.16800831258296967
[2m[36m(func pid=162590)[0m mae:  0.12227402627468109
[2m[36m(func pid=162590)[0m rmse_per_class: [0.12, 0.253, 0.071, 0.326, 0.054, 0.181, 0.271, 0.136, 0.164, 0.104]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18036773800849915
[2m[36m(func pid=157242)[0m mae:  0.13285483419895172
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.264, 0.097, 0.33, 0.101, 0.19, 0.301, 0.144, 0.144, 0.118]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:39:16 (running for 00:10:30.46)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.612 |  0.18  |                   28 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.421 |  0.174 |                   27 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.447 |  0.168 |                    9 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.43  |  0.178 |                    8 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.1782955825328827
[2m[36m(func pid=163121)[0m mae:  0.12477515637874603
[2m[36m(func pid=163121)[0m rmse_per_class: [0.095, 0.243, 0.05, 0.329, 0.205, 0.181, 0.301, 0.111, 0.135, 0.133]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1737152636051178
[2m[36m(func pid=157825)[0m mae:  0.12728950381278992
[2m[36m(func pid=157825)[0m rmse_per_class: [0.124, 0.262, 0.094, 0.336, 0.07, 0.187, 0.278, 0.136, 0.145, 0.106]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4114 | Steps: 4 | Val loss: 0.3074 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.6106 | Steps: 4 | Val loss: 0.4865 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4159 | Steps: 4 | Val loss: 0.3256 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4222 | Steps: 4 | Val loss: 0.3226 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=162590)[0m rmse: 0.16678163409233093
[2m[36m(func pid=162590)[0m mae:  0.1213701143860817
[2m[36m(func pid=162590)[0m rmse_per_class: [0.101, 0.253, 0.077, 0.328, 0.054, 0.179, 0.272, 0.132, 0.156, 0.115]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.1803455650806427
[2m[36m(func pid=157242)[0m mae:  0.13285408914089203
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.265, 0.097, 0.33, 0.102, 0.19, 0.301, 0.143, 0.144, 0.117]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:39:22 (running for 00:10:35.87)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.611 |  0.18  |                   29 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.423 |  0.174 |                   28 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.411 |  0.167 |                   10 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.416 |  0.174 |                    9 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.17360755801200867
[2m[36m(func pid=157825)[0m mae:  0.12721817195415497
[2m[36m(func pid=157825)[0m rmse_per_class: [0.122, 0.262, 0.095, 0.336, 0.071, 0.186, 0.278, 0.136, 0.145, 0.106]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.17413082718849182
[2m[36m(func pid=163121)[0m mae:  0.12261499464511871
[2m[36m(func pid=163121)[0m rmse_per_class: [0.104, 0.243, 0.053, 0.366, 0.07, 0.17, 0.271, 0.203, 0.16, 0.101]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4075 | Steps: 4 | Val loss: 0.3080 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.6052 | Steps: 4 | Val loss: 0.4810 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=162590)[0m rmse: 0.16750501096248627
[2m[36m(func pid=162590)[0m mae:  0.12132035195827484
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.252, 0.083, 0.33, 0.055, 0.179, 0.275, 0.133, 0.143, 0.127]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3942 | Steps: 4 | Val loss: 0.2787 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4202 | Steps: 4 | Val loss: 0.3229 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=157242)[0m rmse: 0.1802966147661209
[2m[36m(func pid=157242)[0m mae:  0.13280434906482697
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.265, 0.097, 0.33, 0.102, 0.19, 0.301, 0.143, 0.144, 0.117]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:39:27 (running for 00:10:41.22)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.605 |  0.18  |                   30 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.422 |  0.174 |                   29 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.408 |  0.168 |                   11 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.394 |  0.153 |                   10 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.15287306904792786
[2m[36m(func pid=163121)[0m mae:  0.10437504947185516
[2m[36m(func pid=163121)[0m rmse_per_class: [0.094, 0.233, 0.043, 0.307, 0.053, 0.178, 0.253, 0.109, 0.134, 0.125]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17389631271362305
[2m[36m(func pid=157825)[0m mae:  0.12730616331100464
[2m[36m(func pid=157825)[0m rmse_per_class: [0.124, 0.262, 0.098, 0.337, 0.069, 0.186, 0.277, 0.136, 0.145, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3998 | Steps: 4 | Val loss: 0.3036 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5935 | Steps: 4 | Val loss: 0.4758 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4014 | Steps: 4 | Val loss: 0.2824 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4221 | Steps: 4 | Val loss: 0.3228 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=162590)[0m rmse: 0.16644714772701263
[2m[36m(func pid=162590)[0m mae:  0.1197989359498024
[2m[36m(func pid=162590)[0m rmse_per_class: [0.097, 0.251, 0.082, 0.324, 0.055, 0.179, 0.276, 0.137, 0.136, 0.127]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18026851117610931
[2m[36m(func pid=157242)[0m mae:  0.13278022408485413
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.265, 0.097, 0.33, 0.102, 0.19, 0.3, 0.143, 0.144, 0.117]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:39:32 (running for 00:10:46.44)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.594 |  0.18  |                   31 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.42  |  0.174 |                   30 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.4   |  0.166 |                   12 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.401 |  0.158 |                   11 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.17376652359962463
[2m[36m(func pid=157825)[0m mae:  0.12717773020267487
[2m[36m(func pid=157825)[0m rmse_per_class: [0.123, 0.262, 0.098, 0.338, 0.069, 0.186, 0.276, 0.136, 0.145, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15755099058151245
[2m[36m(func pid=163121)[0m mae:  0.1094464659690857
[2m[36m(func pid=163121)[0m rmse_per_class: [0.148, 0.227, 0.048, 0.301, 0.053, 0.173, 0.27, 0.106, 0.144, 0.106]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3962 | Steps: 4 | Val loss: 0.2984 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5896 | Steps: 4 | Val loss: 0.4750 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4204 | Steps: 4 | Val loss: 0.3221 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=162590)[0m rmse: 0.16453717648983002
[2m[36m(func pid=162590)[0m mae:  0.11819269508123398
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.253, 0.072, 0.312, 0.056, 0.179, 0.274, 0.143, 0.134, 0.123]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4215 | Steps: 4 | Val loss: 0.3294 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=157242)[0m rmse: 0.18022407591342926
[2m[36m(func pid=157242)[0m mae:  0.13276877999305725
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.265, 0.097, 0.33, 0.102, 0.19, 0.3, 0.142, 0.144, 0.116]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:39:38 (running for 00:10:51.71)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.59  |  0.18  |                   32 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.42  |  0.174 |                   32 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.396 |  0.165 |                   13 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.401 |  0.158 |                   11 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.1736096292734146
[2m[36m(func pid=157825)[0m mae:  0.12704071402549744
[2m[36m(func pid=157825)[0m rmse_per_class: [0.123, 0.262, 0.097, 0.338, 0.068, 0.186, 0.276, 0.136, 0.145, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.16152001917362213
[2m[36m(func pid=163121)[0m mae:  0.11279672384262085
[2m[36m(func pid=163121)[0m rmse_per_class: [0.094, 0.228, 0.051, 0.378, 0.073, 0.17, 0.238, 0.11, 0.176, 0.098]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4075 | Steps: 4 | Val loss: 0.2960 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.5817 | Steps: 4 | Val loss: 0.4700 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4258 | Steps: 4 | Val loss: 0.3223 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=162590)[0m rmse: 0.16416668891906738
[2m[36m(func pid=162590)[0m mae:  0.11785416305065155
[2m[36m(func pid=162590)[0m rmse_per_class: [0.11, 0.256, 0.063, 0.308, 0.058, 0.181, 0.271, 0.144, 0.135, 0.116]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4092 | Steps: 4 | Val loss: 0.3250 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=157242)[0m rmse: 0.18016771972179413
[2m[36m(func pid=157242)[0m mae:  0.13271570205688477
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.265, 0.097, 0.33, 0.102, 0.19, 0.3, 0.142, 0.144, 0.116]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:39:43 (running for 00:10:56.98)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.582 |  0.18  |                   33 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   33 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.408 |  0.164 |                   14 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.421 |  0.162 |                   12 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.1734745353460312
[2m[36m(func pid=157825)[0m mae:  0.12700480222702026
[2m[36m(func pid=157825)[0m rmse_per_class: [0.122, 0.262, 0.097, 0.337, 0.069, 0.186, 0.276, 0.136, 0.145, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.1722656488418579
[2m[36m(func pid=163121)[0m mae:  0.11876281350851059
[2m[36m(func pid=163121)[0m rmse_per_class: [0.091, 0.228, 0.049, 0.367, 0.19, 0.187, 0.263, 0.103, 0.135, 0.11]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3966 | Steps: 4 | Val loss: 0.3007 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5835 | Steps: 4 | Val loss: 0.4647 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4240 | Steps: 4 | Val loss: 0.3218 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3933 | Steps: 4 | Val loss: 0.2885 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=162590)[0m rmse: 0.1675812155008316
[2m[36m(func pid=162590)[0m mae:  0.12096185982227325
[2m[36m(func pid=162590)[0m rmse_per_class: [0.121, 0.258, 0.061, 0.318, 0.061, 0.183, 0.271, 0.139, 0.151, 0.112]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18006737530231476
[2m[36m(func pid=157242)[0m mae:  0.13265252113342285
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.265, 0.097, 0.33, 0.101, 0.19, 0.3, 0.142, 0.144, 0.116]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:39:48 (running for 00:11:02.32)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.583 |  0.18  |                   34 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.424 |  0.173 |                   34 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.397 |  0.168 |                   15 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.409 |  0.172 |                   13 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.1734616458415985
[2m[36m(func pid=157825)[0m mae:  0.12691311538219452
[2m[36m(func pid=157825)[0m rmse_per_class: [0.123, 0.262, 0.097, 0.337, 0.067, 0.186, 0.276, 0.136, 0.146, 0.104]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.16086357831954956
[2m[36m(func pid=163121)[0m mae:  0.11135198920965195
[2m[36m(func pid=163121)[0m rmse_per_class: [0.094, 0.241, 0.053, 0.269, 0.12, 0.179, 0.284, 0.102, 0.152, 0.113]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3914 | Steps: 4 | Val loss: 0.3086 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.5698 | Steps: 4 | Val loss: 0.4643 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3874 | Steps: 4 | Val loss: 0.2755 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4166 | Steps: 4 | Val loss: 0.3215 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=162590)[0m rmse: 0.1716407984495163
[2m[36m(func pid=162590)[0m mae:  0.1243722066283226
[2m[36m(func pid=162590)[0m rmse_per_class: [0.113, 0.259, 0.059, 0.332, 0.065, 0.184, 0.271, 0.137, 0.188, 0.108]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18011972308158875
[2m[36m(func pid=157242)[0m mae:  0.1326773315668106
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.265, 0.097, 0.33, 0.102, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:39:53 (running for 00:11:07.37)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.57  |  0.18  |                   35 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.424 |  0.173 |                   34 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.391 |  0.172 |                   16 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.387 |  0.152 |                   15 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.15200814604759216
[2m[36m(func pid=163121)[0m mae:  0.10341773182153702
[2m[36m(func pid=163121)[0m rmse_per_class: [0.102, 0.213, 0.055, 0.303, 0.062, 0.18, 0.224, 0.104, 0.153, 0.124]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1733187735080719
[2m[36m(func pid=157825)[0m mae:  0.12683182954788208
[2m[36m(func pid=157825)[0m rmse_per_class: [0.123, 0.262, 0.096, 0.337, 0.067, 0.186, 0.276, 0.135, 0.147, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3976 | Steps: 4 | Val loss: 0.3084 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.5687 | Steps: 4 | Val loss: 0.4578 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3830 | Steps: 4 | Val loss: 0.2894 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=162590)[0m rmse: 0.17117483913898468
[2m[36m(func pid=162590)[0m mae:  0.12397757917642593
[2m[36m(func pid=162590)[0m rmse_per_class: [0.116, 0.253, 0.068, 0.338, 0.068, 0.183, 0.269, 0.134, 0.178, 0.105]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4237 | Steps: 4 | Val loss: 0.3207 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=157242)[0m rmse: 0.18003123998641968
[2m[36m(func pid=157242)[0m mae:  0.13260963559150696
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.33, 0.101, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:39:58 (running for 00:11:12.65)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.569 |  0.18  |                   36 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.417 |  0.173 |                   35 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.398 |  0.171 |                   17 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.383 |  0.159 |                   16 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.15918657183647156
[2m[36m(func pid=163121)[0m mae:  0.10992138087749481
[2m[36m(func pid=163121)[0m rmse_per_class: [0.126, 0.211, 0.049, 0.322, 0.054, 0.176, 0.284, 0.1, 0.134, 0.136]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17301107943058014
[2m[36m(func pid=157825)[0m mae:  0.126695916056633
[2m[36m(func pid=157825)[0m rmse_per_class: [0.121, 0.262, 0.094, 0.336, 0.067, 0.186, 0.276, 0.135, 0.147, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3862 | Steps: 4 | Val loss: 0.2980 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5603 | Steps: 4 | Val loss: 0.4502 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3651 | Steps: 4 | Val loss: 0.2733 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4142 | Steps: 4 | Val loss: 0.3200 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=162590)[0m rmse: 0.16519775986671448
[2m[36m(func pid=162590)[0m mae:  0.11931290477514267
[2m[36m(func pid=162590)[0m rmse_per_class: [0.109, 0.247, 0.066, 0.333, 0.071, 0.181, 0.263, 0.132, 0.145, 0.104]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.17997439205646515
[2m[36m(func pid=157242)[0m mae:  0.1325685977935791
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.33, 0.1, 0.19, 0.299, 0.142, 0.145, 0.115]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:40:04 (running for 00:11:17.91)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.56  |  0.18  |                   37 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.414 |  0.173 |                   37 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.386 |  0.165 |                   18 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.383 |  0.159 |                   16 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.17282500863075256
[2m[36m(func pid=157825)[0m mae:  0.12664788961410522
[2m[36m(func pid=157825)[0m rmse_per_class: [0.12, 0.262, 0.092, 0.336, 0.067, 0.186, 0.276, 0.135, 0.148, 0.106]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15127745270729065
[2m[36m(func pid=163121)[0m mae:  0.10260602086782455
[2m[36m(func pid=163121)[0m rmse_per_class: [0.114, 0.211, 0.035, 0.29, 0.068, 0.167, 0.264, 0.135, 0.132, 0.095]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3868 | Steps: 4 | Val loss: 0.2940 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.5568 | Steps: 4 | Val loss: 0.4475 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4202 | Steps: 4 | Val loss: 0.3198 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3613 | Steps: 4 | Val loss: 0.2731 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=162590)[0m rmse: 0.1630505472421646
[2m[36m(func pid=162590)[0m mae:  0.11701725423336029
[2m[36m(func pid=162590)[0m rmse_per_class: [0.108, 0.246, 0.064, 0.33, 0.07, 0.18, 0.258, 0.129, 0.136, 0.108]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.18003804981708527
[2m[36m(func pid=157242)[0m mae:  0.1326266974210739
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.33, 0.1, 0.19, 0.299, 0.142, 0.145, 0.115]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:40:09 (running for 00:11:23.13)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.557 |  0.18  |                   38 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.42  |  0.173 |                   38 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.387 |  0.163 |                   19 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.365 |  0.151 |                   17 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.17307163774967194
[2m[36m(func pid=157825)[0m mae:  0.12671491503715515
[2m[36m(func pid=157825)[0m rmse_per_class: [0.123, 0.262, 0.094, 0.336, 0.066, 0.186, 0.276, 0.135, 0.148, 0.106]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15103964507579803
[2m[36m(func pid=163121)[0m mae:  0.10245521366596222
[2m[36m(func pid=163121)[0m rmse_per_class: [0.093, 0.21, 0.057, 0.307, 0.099, 0.173, 0.226, 0.1, 0.157, 0.088]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3888 | Steps: 4 | Val loss: 0.2937 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.5533 | Steps: 4 | Val loss: 0.4435 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4124 | Steps: 4 | Val loss: 0.3190 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=162590)[0m rmse: 0.16257251799106598
[2m[36m(func pid=162590)[0m mae:  0.11694838851690292
[2m[36m(func pid=162590)[0m rmse_per_class: [0.105, 0.245, 0.061, 0.331, 0.069, 0.179, 0.258, 0.128, 0.138, 0.112]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3615 | Steps: 4 | Val loss: 0.2808 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=157242)[0m rmse: 0.17993022501468658
[2m[36m(func pid=157242)[0m mae:  0.1325419396162033
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.33, 0.1, 0.19, 0.299, 0.142, 0.145, 0.115]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:40:14 (running for 00:11:28.51)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.553 |  0.18  |                   39 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.412 |  0.173 |                   39 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.389 |  0.163 |                   20 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.361 |  0.151 |                   18 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.17281505465507507
[2m[36m(func pid=157825)[0m mae:  0.12651129066944122
[2m[36m(func pid=157825)[0m rmse_per_class: [0.122, 0.262, 0.094, 0.335, 0.066, 0.186, 0.275, 0.135, 0.148, 0.106]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15766718983650208
[2m[36m(func pid=163121)[0m mae:  0.1079811081290245
[2m[36m(func pid=163121)[0m rmse_per_class: [0.11, 0.229, 0.036, 0.293, 0.095, 0.193, 0.269, 0.101, 0.136, 0.114]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3909 | Steps: 4 | Val loss: 0.2956 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.5552 | Steps: 4 | Val loss: 0.4410 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4189 | Steps: 4 | Val loss: 0.3185 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=162590)[0m rmse: 0.1638994812965393
[2m[36m(func pid=162590)[0m mae:  0.11834622919559479
[2m[36m(func pid=162590)[0m rmse_per_class: [0.11, 0.244, 0.057, 0.331, 0.069, 0.177, 0.263, 0.127, 0.147, 0.112]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3619 | Steps: 4 | Val loss: 0.2637 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=157242)[0m rmse: 0.17981012165546417
[2m[36m(func pid=157242)[0m mae:  0.13245239853858948
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.265, 0.097, 0.331, 0.1, 0.19, 0.298, 0.142, 0.145, 0.115]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:40:20 (running for 00:11:33.76)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.555 |  0.18  |                   40 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.419 |  0.173 |                   40 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.391 |  0.164 |                   21 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.361 |  0.158 |                   19 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.17272745072841644
[2m[36m(func pid=157825)[0m mae:  0.126327782869339
[2m[36m(func pid=157825)[0m rmse_per_class: [0.122, 0.261, 0.095, 0.335, 0.066, 0.186, 0.275, 0.135, 0.148, 0.106]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.14501067996025085
[2m[36m(func pid=163121)[0m mae:  0.09742375463247299
[2m[36m(func pid=163121)[0m rmse_per_class: [0.095, 0.208, 0.064, 0.293, 0.072, 0.163, 0.227, 0.098, 0.132, 0.099]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3850 | Steps: 4 | Val loss: 0.2957 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5540 | Steps: 4 | Val loss: 0.4406 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4156 | Steps: 4 | Val loss: 0.3178 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=162590)[0m rmse: 0.16384883224964142
[2m[36m(func pid=162590)[0m mae:  0.11864831298589706
[2m[36m(func pid=162590)[0m rmse_per_class: [0.102, 0.246, 0.053, 0.33, 0.073, 0.177, 0.266, 0.128, 0.154, 0.111]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3713 | Steps: 4 | Val loss: 0.2821 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=157242)[0m rmse: 0.17978055775165558
[2m[36m(func pid=157242)[0m mae:  0.13244101405143738
[2m[36m(func pid=157242)[0m rmse_per_class: [0.115, 0.265, 0.097, 0.331, 0.1, 0.19, 0.298, 0.141, 0.145, 0.115]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:40:25 (running for 00:11:39.20)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.554 |  0.18  |                   41 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.416 |  0.173 |                   41 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.385 |  0.164 |                   22 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.362 |  0.145 |                   20 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.17251214385032654
[2m[36m(func pid=157825)[0m mae:  0.12608714401721954
[2m[36m(func pid=157825)[0m rmse_per_class: [0.122, 0.261, 0.095, 0.334, 0.065, 0.186, 0.274, 0.135, 0.148, 0.106]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15397457778453827
[2m[36m(func pid=163121)[0m mae:  0.10644707828760147
[2m[36m(func pid=163121)[0m rmse_per_class: [0.1, 0.209, 0.035, 0.325, 0.07, 0.167, 0.24, 0.135, 0.152, 0.107]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3962 | Steps: 4 | Val loss: 0.2992 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5431 | Steps: 4 | Val loss: 0.4356 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4113 | Steps: 4 | Val loss: 0.3176 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=162590)[0m rmse: 0.16629573702812195
[2m[36m(func pid=162590)[0m mae:  0.12013516575098038
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.249, 0.057, 0.331, 0.072, 0.176, 0.268, 0.128, 0.169, 0.114]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3574 | Steps: 4 | Val loss: 0.2771 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=157242)[0m rmse: 0.17964491248130798
[2m[36m(func pid=157242)[0m mae:  0.13232439756393433
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.265, 0.097, 0.331, 0.1, 0.19, 0.298, 0.141, 0.145, 0.114]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:40:30 (running for 00:11:44.32)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.543 |  0.18  |                   42 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.411 |  0.173 |                   42 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.396 |  0.166 |                   23 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.371 |  0.154 |                   21 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.17271307110786438
[2m[36m(func pid=157825)[0m mae:  0.126095250248909
[2m[36m(func pid=157825)[0m rmse_per_class: [0.123, 0.261, 0.096, 0.335, 0.064, 0.186, 0.273, 0.135, 0.148, 0.106]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.1505715548992157
[2m[36m(func pid=163121)[0m mae:  0.10421840846538544
[2m[36m(func pid=163121)[0m rmse_per_class: [0.099, 0.212, 0.04, 0.276, 0.068, 0.174, 0.291, 0.105, 0.134, 0.107]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3765 | Steps: 4 | Val loss: 0.3028 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.5353 | Steps: 4 | Val loss: 0.4304 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4128 | Steps: 4 | Val loss: 0.3165 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=162590)[0m rmse: 0.16895107924938202
[2m[36m(func pid=162590)[0m mae:  0.12170187383890152
[2m[36m(func pid=162590)[0m rmse_per_class: [0.097, 0.252, 0.065, 0.333, 0.074, 0.176, 0.27, 0.127, 0.153, 0.141]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3455 | Steps: 4 | Val loss: 0.2718 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=157242)[0m rmse: 0.1795998066663742
[2m[36m(func pid=157242)[0m mae:  0.1322822868824005
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.265, 0.097, 0.331, 0.099, 0.19, 0.298, 0.141, 0.145, 0.115]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:40:35 (running for 00:11:49.43)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.535 |  0.18  |                   43 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.413 |  0.172 |                   43 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.376 |  0.169 |                   24 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.357 |  0.151 |                   22 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.1721068024635315
[2m[36m(func pid=157825)[0m mae:  0.12575897574424744
[2m[36m(func pid=157825)[0m rmse_per_class: [0.119, 0.261, 0.094, 0.334, 0.065, 0.185, 0.273, 0.135, 0.149, 0.106]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.14844244718551636
[2m[36m(func pid=163121)[0m mae:  0.09976501762866974
[2m[36m(func pid=163121)[0m rmse_per_class: [0.097, 0.225, 0.045, 0.313, 0.086, 0.163, 0.232, 0.103, 0.133, 0.088]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3809 | Steps: 4 | Val loss: 0.3032 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5346 | Steps: 4 | Val loss: 0.4279 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=162590)[0m rmse: 0.17034412920475006
[2m[36m(func pid=162590)[0m mae:  0.12157651036977768
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.253, 0.08, 0.332, 0.074, 0.176, 0.269, 0.128, 0.139, 0.156]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4112 | Steps: 4 | Val loss: 0.3168 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3381 | Steps: 4 | Val loss: 0.2916 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=157242)[0m rmse: 0.17962488532066345
[2m[36m(func pid=157242)[0m mae:  0.13227562606334686
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.265, 0.098, 0.331, 0.099, 0.19, 0.297, 0.141, 0.145, 0.115]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1721898317337036
[2m[36m(func pid=157825)[0m mae:  0.12581846117973328
[2m[36m(func pid=157825)[0m rmse_per_class: [0.119, 0.261, 0.095, 0.334, 0.064, 0.185, 0.273, 0.135, 0.149, 0.106]== Status ==
Current time: 2024-01-07 15:40:41 (running for 00:11:54.95)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.535 |  0.18  |                   44 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.413 |  0.172 |                   43 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.381 |  0.17  |                   25 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.338 |  0.157 |                   24 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=163121)[0m rmse: 0.15666291117668152
[2m[36m(func pid=163121)[0m mae:  0.10653521120548248
[2m[36m(func pid=163121)[0m rmse_per_class: [0.117, 0.212, 0.039, 0.338, 0.099, 0.182, 0.22, 0.108, 0.149, 0.102]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3859 | Steps: 4 | Val loss: 0.2975 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5400 | Steps: 4 | Val loss: 0.4258 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=162590)[0m rmse: 0.16736379265785217
[2m[36m(func pid=162590)[0m mae:  0.1189633458852768
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.253, 0.075, 0.323, 0.072, 0.176, 0.265, 0.129, 0.135, 0.147]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4185 | Steps: 4 | Val loss: 0.3171 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3419 | Steps: 4 | Val loss: 0.2761 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=157242)[0m rmse: 0.1795443594455719
[2m[36m(func pid=157242)[0m mae:  0.13221243023872375
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.264, 0.098, 0.331, 0.099, 0.19, 0.297, 0.141, 0.144, 0.114]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:40:46 (running for 00:12:00.20)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.54  |  0.18  |                   45 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.411 |  0.172 |                   44 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.386 |  0.167 |                   26 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.342 |  0.151 |                   25 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.15147042274475098
[2m[36m(func pid=163121)[0m mae:  0.10305985063314438
[2m[36m(func pid=163121)[0m rmse_per_class: [0.101, 0.214, 0.048, 0.31, 0.086, 0.165, 0.248, 0.094, 0.132, 0.116]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17263561487197876
[2m[36m(func pid=157825)[0m mae:  0.12601952254772186
[2m[36m(func pid=157825)[0m rmse_per_class: [0.123, 0.261, 0.096, 0.335, 0.064, 0.185, 0.273, 0.135, 0.149, 0.106]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3798 | Steps: 4 | Val loss: 0.2917 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5329 | Steps: 4 | Val loss: 0.4245 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4105 | Steps: 4 | Val loss: 0.3168 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=162590)[0m rmse: 0.16308459639549255
[2m[36m(func pid=162590)[0m mae:  0.11598106473684311
[2m[36m(func pid=162590)[0m rmse_per_class: [0.1, 0.249, 0.068, 0.321, 0.071, 0.176, 0.259, 0.131, 0.135, 0.12]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3518 | Steps: 4 | Val loss: 0.2742 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=157242)[0m rmse: 0.17949822545051575
[2m[36m(func pid=157242)[0m mae:  0.1321791112422943
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.264, 0.098, 0.331, 0.098, 0.19, 0.297, 0.141, 0.145, 0.114]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17262002825737
[2m[36m(func pid=157825)[0m mae:  0.12597277760505676
[2m[36m(func pid=157825)[0m rmse_per_class: [0.123, 0.261, 0.096, 0.336, 0.064, 0.185, 0.273, 0.134, 0.149, 0.106]
== Status ==
Current time: 2024-01-07 15:40:51 (running for 00:12:05.39)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.533 |  0.179 |                   46 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.411 |  0.173 |                   46 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.38  |  0.163 |                   27 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.342 |  0.151 |                   25 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15153822302818298
[2m[36m(func pid=163121)[0m mae:  0.10245255380868912
[2m[36m(func pid=163121)[0m rmse_per_class: [0.112, 0.215, 0.069, 0.292, 0.069, 0.169, 0.259, 0.1, 0.134, 0.095]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3761 | Steps: 4 | Val loss: 0.2884 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5238 | Steps: 4 | Val loss: 0.4207 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4081 | Steps: 4 | Val loss: 0.3170 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3668 | Steps: 4 | Val loss: 0.2706 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=162590)[0m rmse: 0.16053323447704315
[2m[36m(func pid=162590)[0m mae:  0.11417683213949203
[2m[36m(func pid=162590)[0m rmse_per_class: [0.102, 0.245, 0.063, 0.321, 0.072, 0.177, 0.254, 0.129, 0.137, 0.105]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.17947988212108612
[2m[36m(func pid=157242)[0m mae:  0.1321568787097931
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.264, 0.098, 0.332, 0.098, 0.19, 0.297, 0.141, 0.145, 0.114]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:40:56 (running for 00:12:10.63)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.524 |  0.179 |                   47 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.408 |  0.173 |                   47 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.376 |  0.161 |                   28 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.352 |  0.152 |                   26 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.17253391444683075
[2m[36m(func pid=157825)[0m mae:  0.1258971393108368
[2m[36m(func pid=157825)[0m rmse_per_class: [0.124, 0.26, 0.095, 0.336, 0.064, 0.185, 0.273, 0.134, 0.148, 0.106]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.14863167703151703
[2m[36m(func pid=163121)[0m mae:  0.10091602802276611
[2m[36m(func pid=163121)[0m rmse_per_class: [0.091, 0.207, 0.04, 0.298, 0.092, 0.165, 0.235, 0.094, 0.158, 0.106]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3708 | Steps: 4 | Val loss: 0.2864 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5250 | Steps: 4 | Val loss: 0.4203 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=162590)[0m rmse: 0.15840110182762146
[2m[36m(func pid=162590)[0m mae:  0.1127716526389122
[2m[36m(func pid=162590)[0m rmse_per_class: [0.1, 0.241, 0.053, 0.324, 0.073, 0.178, 0.251, 0.128, 0.139, 0.098]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3845 | Steps: 4 | Val loss: 0.2661 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4124 | Steps: 4 | Val loss: 0.3167 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=157242)[0m rmse: 0.17940451204776764
[2m[36m(func pid=157242)[0m mae:  0.1320994645357132
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.264, 0.098, 0.331, 0.098, 0.19, 0.296, 0.141, 0.145, 0.114]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:41:02 (running for 00:12:15.90)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.525 |  0.179 |                   48 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.408 |  0.173 |                   47 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.371 |  0.158 |                   29 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.385 |  0.147 |                   28 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.17248305678367615
[2m[36m(func pid=157825)[0m mae:  0.12578581273555756
[2m[36m(func pid=157825)[0m rmse_per_class: [0.125, 0.26, 0.094, 0.336, 0.064, 0.185, 0.273, 0.134, 0.148, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.14658616483211517
[2m[36m(func pid=163121)[0m mae:  0.097531259059906
[2m[36m(func pid=163121)[0m rmse_per_class: [0.09, 0.211, 0.045, 0.295, 0.104, 0.162, 0.24, 0.093, 0.132, 0.095]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3844 | Steps: 4 | Val loss: 0.2870 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5214 | Steps: 4 | Val loss: 0.4162 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=162590)[0m rmse: 0.15860463678836823
[2m[36m(func pid=162590)[0m mae:  0.11294510215520859
[2m[36m(func pid=162590)[0m rmse_per_class: [0.099, 0.241, 0.054, 0.325, 0.071, 0.178, 0.251, 0.129, 0.143, 0.096]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3551 | Steps: 4 | Val loss: 0.2947 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4122 | Steps: 4 | Val loss: 0.3161 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=157242)[0m rmse: 0.17937776446342468
[2m[36m(func pid=157242)[0m mae:  0.13207612931728363
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.264, 0.099, 0.331, 0.097, 0.19, 0.296, 0.141, 0.145, 0.114]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:41:07 (running for 00:12:21.15)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.521 |  0.179 |                   49 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.412 |  0.172 |                   48 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.384 |  0.159 |                   30 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.355 |  0.164 |                   29 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.1643473356962204
[2m[36m(func pid=163121)[0m mae:  0.10911709070205688
[2m[36m(func pid=163121)[0m rmse_per_class: [0.111, 0.248, 0.06, 0.299, 0.094, 0.174, 0.262, 0.17, 0.133, 0.092]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1720491349697113
[2m[36m(func pid=157825)[0m mae:  0.125589981675148
[2m[36m(func pid=157825)[0m rmse_per_class: [0.123, 0.26, 0.092, 0.337, 0.063, 0.185, 0.273, 0.135, 0.148, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3919 | Steps: 4 | Val loss: 0.2881 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5188 | Steps: 4 | Val loss: 0.4145 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=162590)[0m rmse: 0.15920715034008026
[2m[36m(func pid=162590)[0m mae:  0.11356142908334732
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.241, 0.051, 0.328, 0.07, 0.176, 0.253, 0.137, 0.142, 0.097]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4104 | Steps: 4 | Val loss: 0.3164 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3856 | Steps: 4 | Val loss: 0.2872 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=157242)[0m rmse: 0.17932358384132385
[2m[36m(func pid=157242)[0m mae:  0.13204798102378845
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.264, 0.098, 0.332, 0.097, 0.19, 0.296, 0.141, 0.145, 0.114]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:41:12 (running for 00:12:26.45)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.519 |  0.179 |                   50 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.412 |  0.172 |                   49 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.392 |  0.159 |                   31 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.386 |  0.158 |                   30 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.15828202664852142
[2m[36m(func pid=163121)[0m mae:  0.10744760185480118
[2m[36m(func pid=163121)[0m rmse_per_class: [0.094, 0.209, 0.073, 0.324, 0.108, 0.167, 0.242, 0.094, 0.167, 0.106]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17236831784248352
[2m[36m(func pid=157825)[0m mae:  0.12568721175193787
[2m[36m(func pid=157825)[0m rmse_per_class: [0.126, 0.259, 0.093, 0.337, 0.063, 0.185, 0.273, 0.134, 0.148, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3783 | Steps: 4 | Val loss: 0.2896 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5236 | Steps: 4 | Val loss: 0.4130 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=162590)[0m rmse: 0.16072037816047668
[2m[36m(func pid=162590)[0m mae:  0.11519038677215576
[2m[36m(func pid=162590)[0m rmse_per_class: [0.105, 0.24, 0.056, 0.324, 0.069, 0.176, 0.264, 0.133, 0.14, 0.101]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3712 | Steps: 4 | Val loss: 0.2804 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4046 | Steps: 4 | Val loss: 0.3154 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=157242)[0m rmse: 0.17923584580421448
[2m[36m(func pid=157242)[0m mae:  0.13198956847190857
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.264, 0.098, 0.332, 0.097, 0.19, 0.296, 0.141, 0.145, 0.114]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:41:18 (running for 00:12:31.74)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.524 |  0.179 |                   51 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.41  |  0.172 |                   50 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.378 |  0.161 |                   32 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.371 |  0.155 |                   31 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.15537390112876892
[2m[36m(func pid=163121)[0m mae:  0.10408864170312881
[2m[36m(func pid=163121)[0m rmse_per_class: [0.142, 0.207, 0.035, 0.296, 0.101, 0.163, 0.238, 0.133, 0.136, 0.105]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17180053889751434
[2m[36m(func pid=157825)[0m mae:  0.1254129409790039
[2m[36m(func pid=157825)[0m rmse_per_class: [0.123, 0.259, 0.092, 0.337, 0.063, 0.184, 0.273, 0.135, 0.148, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3812 | Steps: 4 | Val loss: 0.2927 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5117 | Steps: 4 | Val loss: 0.4122 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=162590)[0m rmse: 0.16276384890079498
[2m[36m(func pid=162590)[0m mae:  0.11685963720083237
[2m[36m(func pid=162590)[0m rmse_per_class: [0.118, 0.239, 0.061, 0.323, 0.066, 0.175, 0.273, 0.127, 0.141, 0.105]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4081 | Steps: 4 | Val loss: 0.3147 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3610 | Steps: 4 | Val loss: 0.2778 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=157242)[0m rmse: 0.17916633188724518
[2m[36m(func pid=157242)[0m mae:  0.1319340318441391
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.264, 0.098, 0.332, 0.097, 0.19, 0.296, 0.14, 0.145, 0.113]
[2m[36m(func pid=157242)[0m 
== Status ==
Current time: 2024-01-07 15:41:23 (running for 00:12:37.06)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.512 |  0.179 |                   52 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.405 |  0.172 |                   51 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.381 |  0.163 |                   33 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.361 |  0.156 |                   32 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3845 | Steps: 4 | Val loss: 0.2938 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=163121)[0m rmse: 0.15623123943805695
[2m[36m(func pid=163121)[0m mae:  0.10295110940933228
[2m[36m(func pid=163121)[0m rmse_per_class: [0.14, 0.229, 0.063, 0.285, 0.093, 0.173, 0.236, 0.109, 0.132, 0.101]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17145594954490662
[2m[36m(func pid=157825)[0m mae:  0.12519574165344238
[2m[36m(func pid=157825)[0m rmse_per_class: [0.121, 0.259, 0.09, 0.337, 0.063, 0.184, 0.273, 0.135, 0.148, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.16319087147712708
[2m[36m(func pid=162590)[0m mae:  0.11752090603113174
[2m[36m(func pid=162590)[0m rmse_per_class: [0.115, 0.237, 0.067, 0.323, 0.065, 0.174, 0.277, 0.122, 0.145, 0.108]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5124 | Steps: 4 | Val loss: 0.4097 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3589 | Steps: 4 | Val loss: 0.2773 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4149 | Steps: 4 | Val loss: 0.3150 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 15:41:28 (running for 00:12:42.10)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.512 |  0.179 |                   53 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.408 |  0.171 |                   52 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.384 |  0.163 |                   34 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.361 |  0.156 |                   32 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157242)[0m rmse: 0.17919322848320007
[2m[36m(func pid=157242)[0m mae:  0.13196614384651184
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.264, 0.098, 0.332, 0.097, 0.19, 0.296, 0.141, 0.145, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3743 | Steps: 4 | Val loss: 0.2873 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=157825)[0m rmse: 0.17155598104000092
[2m[36m(func pid=157825)[0m mae:  0.12524880468845367
[2m[36m(func pid=157825)[0m rmse_per_class: [0.121, 0.259, 0.091, 0.337, 0.063, 0.184, 0.273, 0.135, 0.148, 0.104]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15418373048305511
[2m[36m(func pid=163121)[0m mae:  0.10440902411937714
[2m[36m(func pid=163121)[0m rmse_per_class: [0.093, 0.206, 0.035, 0.297, 0.098, 0.161, 0.262, 0.117, 0.181, 0.091]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.1598939746618271
[2m[36m(func pid=162590)[0m mae:  0.11487932503223419
[2m[36m(func pid=162590)[0m rmse_per_class: [0.103, 0.238, 0.061, 0.313, 0.065, 0.174, 0.269, 0.121, 0.144, 0.111]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5000 | Steps: 4 | Val loss: 0.4047 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4092 | Steps: 4 | Val loss: 0.3148 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3574 | Steps: 4 | Val loss: 0.2808 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 15:41:33 (running for 00:12:47.54)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.5   |  0.179 |                   54 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.415 |  0.172 |                   53 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.374 |  0.16  |                   35 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.359 |  0.154 |                   33 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157242)[0m rmse: 0.17913444340229034
[2m[36m(func pid=157242)[0m mae:  0.1318964958190918
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.264, 0.098, 0.332, 0.096, 0.19, 0.296, 0.141, 0.145, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15538665652275085
[2m[36m(func pid=163121)[0m mae:  0.10489951074123383
[2m[36m(func pid=163121)[0m rmse_per_class: [0.104, 0.204, 0.044, 0.315, 0.072, 0.167, 0.252, 0.119, 0.131, 0.145]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3752 | Steps: 4 | Val loss: 0.2842 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=157825)[0m rmse: 0.17176657915115356
[2m[36m(func pid=157825)[0m mae:  0.12527494132518768
[2m[36m(func pid=157825)[0m rmse_per_class: [0.123, 0.259, 0.092, 0.337, 0.063, 0.184, 0.272, 0.134, 0.149, 0.104]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15821446478366852
[2m[36m(func pid=162590)[0m mae:  0.11330656707286835
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.24, 0.053, 0.312, 0.065, 0.174, 0.259, 0.121, 0.142, 0.118]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5095 | Steps: 4 | Val loss: 0.4033 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3458 | Steps: 4 | Val loss: 0.2674 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4096 | Steps: 4 | Val loss: 0.3153 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 15:41:39 (running for 00:12:53.02)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.5   |  0.179 |                   54 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.41  |  0.172 |                   55 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.375 |  0.158 |                   36 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.357 |  0.155 |                   34 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157242)[0m rmse: 0.17913995683193207
[2m[36m(func pid=157242)[0m mae:  0.1318962275981903
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.265, 0.098, 0.332, 0.096, 0.19, 0.295, 0.141, 0.145, 0.114]
[2m[36m(func pid=157825)[0m rmse: 0.17187745869159698
[2m[36m(func pid=157825)[0m mae:  0.12539242208003998
[2m[36m(func pid=157825)[0m rmse_per_class: [0.122, 0.258, 0.092, 0.338, 0.063, 0.184, 0.273, 0.134, 0.15, 0.104]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3743 | Steps: 4 | Val loss: 0.2825 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=163121)[0m rmse: 0.14801950752735138
[2m[36m(func pid=163121)[0m mae:  0.09761818498373032
[2m[36m(func pid=163121)[0m rmse_per_class: [0.094, 0.201, 0.077, 0.3, 0.062, 0.184, 0.242, 0.094, 0.131, 0.095]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15661367774009705
[2m[36m(func pid=162590)[0m mae:  0.11129812896251678
[2m[36m(func pid=162590)[0m rmse_per_class: [0.096, 0.242, 0.047, 0.318, 0.064, 0.174, 0.248, 0.121, 0.14, 0.118]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3537 | Steps: 4 | Val loss: 0.2703 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4108 | Steps: 4 | Val loss: 0.3152 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5157 | Steps: 4 | Val loss: 0.4046 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 15:41:44 (running for 00:12:58.27)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.51  |  0.179 |                   55 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.41  |  0.172 |                   55 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.374 |  0.157 |                   37 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.354 |  0.148 |                   36 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3842 | Steps: 4 | Val loss: 0.2846 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=163121)[0m rmse: 0.14753328263759613
[2m[36m(func pid=163121)[0m mae:  0.1014082208275795
[2m[36m(func pid=163121)[0m rmse_per_class: [0.093, 0.204, 0.031, 0.295, 0.063, 0.163, 0.25, 0.091, 0.162, 0.124]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1719151735305786
[2m[36m(func pid=157825)[0m mae:  0.12545588612556458
[2m[36m(func pid=157825)[0m rmse_per_class: [0.121, 0.258, 0.092, 0.337, 0.063, 0.184, 0.273, 0.134, 0.151, 0.105]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.17909573018550873
[2m[36m(func pid=157242)[0m mae:  0.1318875551223755
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.265, 0.098, 0.332, 0.096, 0.19, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15732257068157196
[2m[36m(func pid=162590)[0m mae:  0.11179864406585693
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.242, 0.047, 0.325, 0.064, 0.174, 0.246, 0.121, 0.139, 0.116]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3326 | Steps: 4 | Val loss: 0.2828 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4068 | Steps: 4 | Val loss: 0.3147 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5051 | Steps: 4 | Val loss: 0.4044 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3774 | Steps: 4 | Val loss: 0.2882 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 15:41:49 (running for 00:13:03.50)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.516 |  0.179 |                   56 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.411 |  0.172 |                   56 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.384 |  0.157 |                   38 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.333 |  0.155 |                   37 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.15543895959854126
[2m[36m(func pid=163121)[0m mae:  0.10453671216964722
[2m[36m(func pid=163121)[0m rmse_per_class: [0.119, 0.201, 0.037, 0.323, 0.087, 0.166, 0.246, 0.146, 0.132, 0.097]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1716098040342331
[2m[36m(func pid=157825)[0m mae:  0.12524861097335815
[2m[36m(func pid=157825)[0m rmse_per_class: [0.12, 0.259, 0.091, 0.336, 0.063, 0.184, 0.273, 0.134, 0.15, 0.106]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.17904379963874817
[2m[36m(func pid=157242)[0m mae:  0.13183777034282684
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.265, 0.098, 0.331, 0.097, 0.19, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15954259037971497
[2m[36m(func pid=162590)[0m mae:  0.11429530382156372
[2m[36m(func pid=162590)[0m rmse_per_class: [0.103, 0.239, 0.052, 0.328, 0.069, 0.174, 0.252, 0.12, 0.141, 0.115]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3612 | Steps: 4 | Val loss: 0.2761 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4081 | Steps: 4 | Val loss: 0.3137 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5033 | Steps: 4 | Val loss: 0.4014 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3705 | Steps: 4 | Val loss: 0.2945 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=163121)[0m rmse: 0.15549638867378235
[2m[36m(func pid=163121)[0m mae:  0.1039227619767189
[2m[36m(func pid=163121)[0m rmse_per_class: [0.091, 0.222, 0.054, 0.294, 0.119, 0.177, 0.239, 0.105, 0.132, 0.121]
== Status ==
Current time: 2024-01-07 15:41:54 (running for 00:13:08.62)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.505 |  0.179 |                   57 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.407 |  0.172 |                   57 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.377 |  0.16  |                   39 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.361 |  0.155 |                   38 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17123958468437195
[2m[36m(func pid=157825)[0m mae:  0.12500999867916107
[2m[36m(func pid=157825)[0m rmse_per_class: [0.119, 0.259, 0.089, 0.336, 0.063, 0.184, 0.272, 0.134, 0.15, 0.106]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.1789170801639557
[2m[36m(func pid=157242)[0m mae:  0.1317373663187027
[2m[36m(func pid=157242)[0m rmse_per_class: [0.117, 0.264, 0.098, 0.331, 0.096, 0.19, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.16350221633911133
[2m[36m(func pid=162590)[0m mae:  0.11754844337701797
[2m[36m(func pid=162590)[0m rmse_per_class: [0.111, 0.239, 0.064, 0.331, 0.071, 0.174, 0.265, 0.12, 0.146, 0.113]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3473 | Steps: 4 | Val loss: 0.2779 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4037 | Steps: 4 | Val loss: 0.3130 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5018 | Steps: 4 | Val loss: 0.3977 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 15:41:59 (running for 00:13:13.67)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.503 |  0.179 |                   58 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.408 |  0.171 |                   58 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.371 |  0.164 |                   40 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.347 |  0.156 |                   39 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3709 | Steps: 4 | Val loss: 0.2962 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=163121)[0m rmse: 0.15633073449134827
[2m[36m(func pid=163121)[0m mae:  0.10565425455570221
[2m[36m(func pid=163121)[0m rmse_per_class: [0.101, 0.214, 0.039, 0.281, 0.141, 0.166, 0.258, 0.11, 0.137, 0.115]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17061984539031982
[2m[36m(func pid=157825)[0m mae:  0.12475959211587906
[2m[36m(func pid=157825)[0m rmse_per_class: [0.115, 0.259, 0.086, 0.334, 0.064, 0.184, 0.273, 0.135, 0.15, 0.108]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.17889587581157684
[2m[36m(func pid=157242)[0m mae:  0.13172176480293274
[2m[36m(func pid=157242)[0m rmse_per_class: [0.117, 0.264, 0.098, 0.332, 0.095, 0.19, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.16442428529262543
[2m[36m(func pid=162590)[0m mae:  0.11841360479593277
[2m[36m(func pid=162590)[0m rmse_per_class: [0.116, 0.238, 0.061, 0.331, 0.074, 0.174, 0.273, 0.121, 0.144, 0.114]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3357 | Steps: 4 | Val loss: 0.2756 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4090 | Steps: 4 | Val loss: 0.3128 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5080 | Steps: 4 | Val loss: 0.3995 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 15:42:05 (running for 00:13:18.79)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.502 |  0.179 |                   59 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.404 |  0.171 |                   59 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.371 |  0.164 |                   41 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.336 |  0.151 |                   40 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.15108348429203033
[2m[36m(func pid=163121)[0m mae:  0.10238219797611237
[2m[36m(func pid=163121)[0m rmse_per_class: [0.128, 0.205, 0.039, 0.277, 0.083, 0.174, 0.258, 0.09, 0.139, 0.118]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3780 | Steps: 4 | Val loss: 0.2940 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=157825)[0m rmse: 0.17081628739833832
[2m[36m(func pid=157825)[0m mae:  0.12476281076669693
[2m[36m(func pid=157825)[0m rmse_per_class: [0.117, 0.259, 0.086, 0.334, 0.063, 0.184, 0.272, 0.134, 0.15, 0.109]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.17892536520957947
[2m[36m(func pid=157242)[0m mae:  0.13175976276397705
[2m[36m(func pid=157242)[0m rmse_per_class: [0.117, 0.264, 0.098, 0.332, 0.096, 0.19, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.16325363516807556
[2m[36m(func pid=162590)[0m mae:  0.11736738681793213
[2m[36m(func pid=162590)[0m rmse_per_class: [0.109, 0.237, 0.062, 0.325, 0.075, 0.174, 0.275, 0.12, 0.143, 0.113]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3459 | Steps: 4 | Val loss: 0.2745 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4048 | Steps: 4 | Val loss: 0.3121 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4963 | Steps: 4 | Val loss: 0.3970 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=163121)[0m rmse: 0.14900195598602295
[2m[36m(func pid=163121)[0m mae:  0.10134965181350708
[2m[36m(func pid=163121)[0m rmse_per_class: [0.101, 0.204, 0.034, 0.317, 0.069, 0.157, 0.242, 0.092, 0.173, 0.1]
== Status ==
Current time: 2024-01-07 15:42:10 (running for 00:13:24.02)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.508 |  0.179 |                   60 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.409 |  0.171 |                   60 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.378 |  0.163 |                   42 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.346 |  0.149 |                   41 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3698 | Steps: 4 | Val loss: 0.2849 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=157825)[0m rmse: 0.1704881489276886
[2m[36m(func pid=157825)[0m mae:  0.12462302297353745
[2m[36m(func pid=157825)[0m rmse_per_class: [0.115, 0.259, 0.084, 0.333, 0.062, 0.184, 0.272, 0.135, 0.151, 0.11]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.178892582654953
[2m[36m(func pid=157242)[0m mae:  0.13172312080860138
[2m[36m(func pid=157242)[0m rmse_per_class: [0.117, 0.264, 0.098, 0.332, 0.095, 0.19, 0.294, 0.14, 0.145, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3487 | Steps: 4 | Val loss: 0.2702 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=162590)[0m rmse: 0.15838424861431122
[2m[36m(func pid=162590)[0m mae:  0.1134529560804367
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.237, 0.052, 0.307, 0.078, 0.174, 0.27, 0.12, 0.139, 0.109]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4032 | Steps: 4 | Val loss: 0.3122 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4988 | Steps: 4 | Val loss: 0.3974 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 15:42:15 (running for 00:13:29.22)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.496 |  0.179 |                   61 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.405 |  0.17  |                   61 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.37  |  0.158 |                   43 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.349 |  0.148 |                   42 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.14834967255592346
[2m[36m(func pid=163121)[0m mae:  0.09939313679933548
[2m[36m(func pid=163121)[0m rmse_per_class: [0.12, 0.203, 0.036, 0.305, 0.058, 0.171, 0.25, 0.09, 0.131, 0.119]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17059633135795593
[2m[36m(func pid=157825)[0m mae:  0.12470133602619171
[2m[36m(func pid=157825)[0m rmse_per_class: [0.115, 0.258, 0.085, 0.333, 0.063, 0.184, 0.273, 0.134, 0.15, 0.11]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3667 | Steps: 4 | Val loss: 0.2804 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=157242)[0m rmse: 0.17891338467597961
[2m[36m(func pid=157242)[0m mae:  0.13173988461494446
[2m[36m(func pid=157242)[0m rmse_per_class: [0.117, 0.265, 0.099, 0.332, 0.096, 0.19, 0.294, 0.14, 0.145, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15616312623023987
[2m[36m(func pid=162590)[0m mae:  0.11092595756053925
[2m[36m(func pid=162590)[0m rmse_per_class: [0.096, 0.237, 0.054, 0.304, 0.08, 0.174, 0.258, 0.119, 0.137, 0.104]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3478 | Steps: 4 | Val loss: 0.2672 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4040 | Steps: 4 | Val loss: 0.3114 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4962 | Steps: 4 | Val loss: 0.3947 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 15:42:20 (running for 00:13:34.65)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.499 |  0.179 |                   62 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.403 |  0.171 |                   62 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.367 |  0.156 |                   44 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.348 |  0.148 |                   43 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.14839622378349304
[2m[36m(func pid=163121)[0m mae:  0.0975014716386795
[2m[36m(func pid=163121)[0m rmse_per_class: [0.12, 0.203, 0.056, 0.281, 0.067, 0.157, 0.253, 0.097, 0.132, 0.117]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3696 | Steps: 4 | Val loss: 0.2782 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=157825)[0m rmse: 0.17037639021873474
[2m[36m(func pid=157825)[0m mae:  0.12451568990945816
[2m[36m(func pid=157825)[0m rmse_per_class: [0.116, 0.258, 0.084, 0.333, 0.062, 0.184, 0.273, 0.134, 0.149, 0.111]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.178856760263443
[2m[36m(func pid=157242)[0m mae:  0.1317065805196762
[2m[36m(func pid=157242)[0m rmse_per_class: [0.117, 0.265, 0.098, 0.332, 0.095, 0.19, 0.294, 0.14, 0.145, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15444591641426086
[2m[36m(func pid=162590)[0m mae:  0.10935129970312119
[2m[36m(func pid=162590)[0m rmse_per_class: [0.096, 0.236, 0.05, 0.306, 0.078, 0.174, 0.251, 0.118, 0.136, 0.101]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3432 | Steps: 4 | Val loss: 0.2797 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4033 | Steps: 4 | Val loss: 0.3115 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4871 | Steps: 4 | Val loss: 0.3918 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 15:42:26 (running for 00:13:39.78)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.496 |  0.179 |                   63 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.404 |  0.17  |                   63 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.37  |  0.154 |                   45 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                   44 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.15263590216636658
[2m[36m(func pid=163121)[0m mae:  0.10428150743246078
[2m[36m(func pid=163121)[0m rmse_per_class: [0.104, 0.201, 0.042, 0.316, 0.083, 0.163, 0.239, 0.092, 0.174, 0.112]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3852 | Steps: 4 | Val loss: 0.2791 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=157825)[0m rmse: 0.17044857144355774
[2m[36m(func pid=157825)[0m mae:  0.12451489269733429
[2m[36m(func pid=157825)[0m rmse_per_class: [0.118, 0.258, 0.084, 0.332, 0.062, 0.184, 0.273, 0.134, 0.148, 0.11]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.17889264225959778
[2m[36m(func pid=157242)[0m mae:  0.1317393034696579
[2m[36m(func pid=157242)[0m rmse_per_class: [0.117, 0.265, 0.099, 0.332, 0.094, 0.19, 0.294, 0.14, 0.146, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15478655695915222
[2m[36m(func pid=162590)[0m mae:  0.10994616895914078
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.234, 0.046, 0.31, 0.079, 0.173, 0.252, 0.118, 0.137, 0.1]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3330 | Steps: 4 | Val loss: 0.2724 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4116 | Steps: 4 | Val loss: 0.3112 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4885 | Steps: 4 | Val loss: 0.3884 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 15:42:31 (running for 00:13:45.13)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.487 |  0.179 |                   64 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.403 |  0.17  |                   64 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.385 |  0.155 |                   46 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.333 |  0.15  |                   45 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.14950713515281677
[2m[36m(func pid=163121)[0m mae:  0.0990741103887558
[2m[36m(func pid=163121)[0m rmse_per_class: [0.132, 0.206, 0.033, 0.319, 0.086, 0.178, 0.23, 0.093, 0.131, 0.088]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3683 | Steps: 4 | Val loss: 0.2842 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=157825)[0m rmse: 0.17090395092964172
[2m[36m(func pid=157825)[0m mae:  0.1246379017829895
[2m[36m(func pid=157825)[0m rmse_per_class: [0.123, 0.257, 0.086, 0.333, 0.062, 0.184, 0.274, 0.134, 0.148, 0.11]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.17880204319953918
[2m[36m(func pid=157242)[0m mae:  0.13165894150733948
[2m[36m(func pid=157242)[0m rmse_per_class: [0.117, 0.264, 0.099, 0.332, 0.094, 0.19, 0.294, 0.14, 0.145, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15784475207328796
[2m[36m(func pid=162590)[0m mae:  0.11294405162334442
[2m[36m(func pid=162590)[0m rmse_per_class: [0.101, 0.234, 0.046, 0.317, 0.08, 0.173, 0.259, 0.118, 0.147, 0.103]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3453 | Steps: 4 | Val loss: 0.2742 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4010 | Steps: 4 | Val loss: 0.3100 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4899 | Steps: 4 | Val loss: 0.3856 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 15:42:36 (running for 00:13:50.39)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.489 |  0.179 |                   65 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.412 |  0.171 |                   65 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.368 |  0.158 |                   47 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.345 |  0.153 |                   46 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.15341782569885254
[2m[36m(func pid=163121)[0m mae:  0.10205616056919098
[2m[36m(func pid=163121)[0m rmse_per_class: [0.096, 0.208, 0.061, 0.274, 0.12, 0.16, 0.275, 0.097, 0.13, 0.112]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3696 | Steps: 4 | Val loss: 0.2873 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=157825)[0m rmse: 0.17027825117111206
[2m[36m(func pid=157825)[0m mae:  0.12424669414758682
[2m[36m(func pid=157825)[0m rmse_per_class: [0.121, 0.257, 0.084, 0.332, 0.062, 0.184, 0.273, 0.134, 0.147, 0.11]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.17867286503314972
[2m[36m(func pid=157242)[0m mae:  0.13155433535575867
[2m[36m(func pid=157242)[0m rmse_per_class: [0.117, 0.264, 0.099, 0.332, 0.093, 0.19, 0.294, 0.14, 0.145, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15969160199165344
[2m[36m(func pid=162590)[0m mae:  0.1147073283791542
[2m[36m(func pid=162590)[0m rmse_per_class: [0.106, 0.232, 0.046, 0.32, 0.083, 0.173, 0.268, 0.117, 0.148, 0.104]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3421 | Steps: 4 | Val loss: 0.2773 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4072 | Steps: 4 | Val loss: 0.3104 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4847 | Steps: 4 | Val loss: 0.3846 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 15:42:41 (running for 00:13:55.64)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.49  |  0.179 |                   66 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.401 |  0.17  |                   66 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.37  |  0.16  |                   48 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.342 |  0.154 |                   47 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.15389706194400787
[2m[36m(func pid=163121)[0m mae:  0.10386810451745987
[2m[36m(func pid=163121)[0m rmse_per_class: [0.093, 0.207, 0.052, 0.31, 0.123, 0.173, 0.224, 0.091, 0.168, 0.097]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17038211226463318
[2m[36m(func pid=157825)[0m mae:  0.12427375465631485
[2m[36m(func pid=157825)[0m rmse_per_class: [0.121, 0.257, 0.084, 0.332, 0.062, 0.183, 0.274, 0.134, 0.146, 0.11]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3773 | Steps: 4 | Val loss: 0.2879 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=157242)[0m rmse: 0.17863573133945465
[2m[36m(func pid=157242)[0m mae:  0.13151773810386658
[2m[36m(func pid=157242)[0m rmse_per_class: [0.117, 0.264, 0.099, 0.332, 0.092, 0.19, 0.294, 0.14, 0.146, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.16010993719100952
[2m[36m(func pid=162590)[0m mae:  0.11482437700033188
[2m[36m(func pid=162590)[0m rmse_per_class: [0.11, 0.232, 0.049, 0.321, 0.081, 0.173, 0.265, 0.116, 0.145, 0.109]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3409 | Steps: 4 | Val loss: 0.2696 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4066 | Steps: 4 | Val loss: 0.3105 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4839 | Steps: 4 | Val loss: 0.3824 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 15:42:47 (running for 00:14:00.91)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.485 |  0.179 |                   67 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.407 |  0.17  |                   67 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.377 |  0.16  |                   49 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.341 |  0.148 |                   48 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.14836665987968445
[2m[36m(func pid=163121)[0m mae:  0.09752950072288513
[2m[36m(func pid=163121)[0m rmse_per_class: [0.134, 0.199, 0.033, 0.304, 0.109, 0.159, 0.229, 0.092, 0.13, 0.095]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1705903857946396
[2m[36m(func pid=157825)[0m mae:  0.12434887886047363
[2m[36m(func pid=157825)[0m rmse_per_class: [0.122, 0.257, 0.085, 0.333, 0.063, 0.183, 0.274, 0.134, 0.146, 0.11]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3616 | Steps: 4 | Val loss: 0.2870 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=157242)[0m rmse: 0.17856113612651825
[2m[36m(func pid=157242)[0m mae:  0.13143864274024963
[2m[36m(func pid=157242)[0m rmse_per_class: [0.117, 0.264, 0.099, 0.332, 0.092, 0.19, 0.293, 0.14, 0.145, 0.113]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15966473519802094
[2m[36m(func pid=162590)[0m mae:  0.11435724794864655
[2m[36m(func pid=162590)[0m rmse_per_class: [0.106, 0.233, 0.051, 0.321, 0.077, 0.173, 0.259, 0.116, 0.145, 0.115]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3456 | Steps: 4 | Val loss: 0.2675 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3977 | Steps: 4 | Val loss: 0.3105 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4876 | Steps: 4 | Val loss: 0.3839 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 15:42:52 (running for 00:14:06.33)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.484 |  0.179 |                   68 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.407 |  0.171 |                   68 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.362 |  0.16  |                   50 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.346 |  0.148 |                   49 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3762 | Steps: 4 | Val loss: 0.2847 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=163121)[0m rmse: 0.14785638451576233
[2m[36m(func pid=163121)[0m mae:  0.09789761900901794
[2m[36m(func pid=163121)[0m rmse_per_class: [0.093, 0.207, 0.059, 0.285, 0.077, 0.16, 0.259, 0.116, 0.131, 0.093]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17068646848201752
[2m[36m(func pid=157825)[0m mae:  0.12444691359996796
[2m[36m(func pid=157825)[0m rmse_per_class: [0.121, 0.257, 0.086, 0.333, 0.063, 0.183, 0.274, 0.133, 0.146, 0.111]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.17849035561084747
[2m[36m(func pid=157242)[0m mae:  0.1313926875591278
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.093, 0.19, 0.293, 0.139, 0.146, 0.112]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.1581333577632904
[2m[36m(func pid=162590)[0m mae:  0.11299441009759903
[2m[36m(func pid=162590)[0m rmse_per_class: [0.099, 0.233, 0.053, 0.319, 0.073, 0.173, 0.254, 0.116, 0.148, 0.113]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4005 | Steps: 4 | Val loss: 0.3107 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3619 | Steps: 4 | Val loss: 0.2784 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4869 | Steps: 4 | Val loss: 0.3847 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 15:42:58 (running for 00:14:11.67)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.488 |  0.178 |                   69 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.398 |  0.171 |                   69 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.376 |  0.158 |                   51 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.362 |  0.153 |                   50 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.17095443606376648
[2m[36m(func pid=157825)[0m mae:  0.12462204694747925
[2m[36m(func pid=157825)[0m rmse_per_class: [0.121, 0.257, 0.086, 0.334, 0.062, 0.183, 0.274, 0.133, 0.147, 0.111]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15318048000335693
[2m[36m(func pid=163121)[0m mae:  0.10419061034917831
[2m[36m(func pid=163121)[0m rmse_per_class: [0.12, 0.205, 0.034, 0.313, 0.082, 0.179, 0.245, 0.102, 0.157, 0.094]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3591 | Steps: 4 | Val loss: 0.2827 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=157242)[0m rmse: 0.1784742772579193
[2m[36m(func pid=157242)[0m mae:  0.13137605786323547
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.093, 0.19, 0.293, 0.139, 0.146, 0.112]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15637224912643433
[2m[36m(func pid=162590)[0m mae:  0.111264668405056
[2m[36m(func pid=162590)[0m rmse_per_class: [0.095, 0.234, 0.05, 0.321, 0.071, 0.174, 0.249, 0.119, 0.141, 0.108]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3957 | Steps: 4 | Val loss: 0.3097 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3771 | Steps: 4 | Val loss: 0.2773 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4881 | Steps: 4 | Val loss: 0.3831 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 15:43:03 (running for 00:14:16.89)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.487 |  0.178 |                   70 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.4   |  0.171 |                   70 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.359 |  0.156 |                   52 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.377 |  0.153 |                   51 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m rmse: 0.15345881879329681
[2m[36m(func pid=163121)[0m mae:  0.09984298050403595
[2m[36m(func pid=163121)[0m rmse_per_class: [0.109, 0.228, 0.043, 0.283, 0.148, 0.163, 0.228, 0.095, 0.131, 0.107]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.17015579342842102
[2m[36m(func pid=157825)[0m mae:  0.12426934391260147
[2m[36m(func pid=157825)[0m rmse_per_class: [0.116, 0.257, 0.084, 0.333, 0.063, 0.183, 0.274, 0.134, 0.147, 0.111]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3710 | Steps: 4 | Val loss: 0.2844 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=157242)[0m rmse: 0.17836442589759827
[2m[36m(func pid=157242)[0m mae:  0.13128116726875305
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.264, 0.099, 0.332, 0.093, 0.19, 0.293, 0.139, 0.146, 0.112]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15766283869743347
[2m[36m(func pid=162590)[0m mae:  0.11152024567127228
[2m[36m(func pid=162590)[0m rmse_per_class: [0.095, 0.235, 0.06, 0.324, 0.068, 0.173, 0.251, 0.124, 0.137, 0.108]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3974 | Steps: 4 | Val loss: 0.3099 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3562 | Steps: 4 | Val loss: 0.2819 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4834 | Steps: 4 | Val loss: 0.3833 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 15:43:08 (running for 00:14:22.10)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.488 |  0.178 |                   71 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.396 |  0.17  |                   71 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.371 |  0.158 |                   53 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.356 |  0.151 |                   52 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3694 | Steps: 4 | Val loss: 0.2861 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=163121)[0m rmse: 0.15103551745414734
[2m[36m(func pid=163121)[0m mae:  0.10246662050485611
[2m[36m(func pid=163121)[0m rmse_per_class: [0.101, 0.208, 0.033, 0.265, 0.109, 0.201, 0.254, 0.092, 0.142, 0.105]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.1700870245695114
[2m[36m(func pid=157825)[0m mae:  0.12423662096261978
[2m[36m(func pid=157825)[0m rmse_per_class: [0.115, 0.257, 0.084, 0.334, 0.063, 0.183, 0.273, 0.134, 0.147, 0.111]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.17832668125629425
[2m[36m(func pid=157242)[0m mae:  0.1312391608953476
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.264, 0.099, 0.332, 0.093, 0.189, 0.293, 0.139, 0.145, 0.112]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.1588866263628006
[2m[36m(func pid=162590)[0m mae:  0.11219529062509537
[2m[36m(func pid=162590)[0m rmse_per_class: [0.097, 0.235, 0.065, 0.325, 0.067, 0.174, 0.256, 0.127, 0.135, 0.108]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3996 | Steps: 4 | Val loss: 0.3092 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3519 | Steps: 4 | Val loss: 0.2874 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4796 | Steps: 4 | Val loss: 0.3833 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 15:43:13 (running for 00:14:27.39)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.483 |  0.178 |                   72 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   72 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.369 |  0.159 |                   54 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.352 |  0.161 |                   53 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3655 | Steps: 4 | Val loss: 0.2856 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=163121)[0m rmse: 0.1605779081583023
[2m[36m(func pid=163121)[0m mae:  0.10850097984075546
[2m[36m(func pid=163121)[0m rmse_per_class: [0.09, 0.207, 0.038, 0.315, 0.091, 0.175, 0.241, 0.101, 0.213, 0.136]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157825)[0m rmse: 0.16957987844944
[2m[36m(func pid=157825)[0m mae:  0.123928502202034
[2m[36m(func pid=157825)[0m rmse_per_class: [0.113, 0.257, 0.083, 0.334, 0.063, 0.182, 0.273, 0.134, 0.147, 0.11]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.17829552292823792
[2m[36m(func pid=157242)[0m mae:  0.1312102973461151
[2m[36m(func pid=157242)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.093, 0.189, 0.293, 0.139, 0.145, 0.112]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.1583838015794754
[2m[36m(func pid=162590)[0m mae:  0.11274266242980957
[2m[36m(func pid=162590)[0m rmse_per_class: [0.096, 0.232, 0.051, 0.322, 0.072, 0.175, 0.263, 0.123, 0.134, 0.116]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3979 | Steps: 4 | Val loss: 0.3095 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3606 | Steps: 4 | Val loss: 0.2747 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4720 | Steps: 4 | Val loss: 0.3794 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3639 | Steps: 4 | Val loss: 0.2869 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 15:43:19 (running for 00:14:32.75)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.16299999505281448
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.48  |  0.178 |                   73 |
| train_10f5e_00005 | RUNNING    | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   74 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.366 |  0.158 |                   55 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.352 |  0.161 |                   53 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.16998404264450073
[2m[36m(func pid=157825)[0m mae:  0.12413185834884644
[2m[36m(func pid=157825)[0m rmse_per_class: [0.114, 0.257, 0.085, 0.334, 0.063, 0.182, 0.273, 0.134, 0.148, 0.11]
[2m[36m(func pid=157825)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15269599854946136
[2m[36m(func pid=163121)[0m mae:  0.10062527656555176
[2m[36m(func pid=163121)[0m rmse_per_class: [0.122, 0.229, 0.051, 0.272, 0.073, 0.19, 0.263, 0.091, 0.13, 0.107]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.17825517058372498
[2m[36m(func pid=157242)[0m mae:  0.13116492331027985
[2m[36m(func pid=157242)[0m rmse_per_class: [0.117, 0.265, 0.099, 0.332, 0.092, 0.189, 0.292, 0.139, 0.145, 0.112]
[2m[36m(func pid=157242)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15957671403884888
[2m[36m(func pid=162590)[0m mae:  0.11349789053201675
[2m[36m(func pid=162590)[0m rmse_per_class: [0.103, 0.232, 0.056, 0.322, 0.07, 0.173, 0.264, 0.12, 0.135, 0.122]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=157825)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3974 | Steps: 4 | Val loss: 0.3091 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3394 | Steps: 4 | Val loss: 0.2970 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=157242)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4743 | Steps: 4 | Val loss: 0.3786 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 15:43:24 (running for 00:14:38.04)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: -0.16599999368190765
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 3 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00004 | RUNNING    | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.472 |  0.178 |                   74 |
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.364 |  0.16  |                   56 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.361 |  0.153 |                   54 |
| train_10f5e_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=157825)[0m rmse: 0.16976609826087952
[2m[36m(func pid=157825)[0m mae:  0.12408968061208725
[2m[36m(func pid=157825)[0m rmse_per_class: [0.113, 0.257, 0.083, 0.334, 0.063, 0.182, 0.274, 0.134, 0.149, 0.11]
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3789 | Steps: 4 | Val loss: 0.2855 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=163121)[0m rmse: 0.16181354224681854
[2m[36m(func pid=163121)[0m mae:  0.10987450927495956
[2m[36m(func pid=163121)[0m rmse_per_class: [0.098, 0.214, 0.046, 0.328, 0.111, 0.16, 0.27, 0.091, 0.145, 0.157]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=157242)[0m rmse: 0.17816731333732605
[2m[36m(func pid=157242)[0m mae:  0.13111133873462677
[2m[36m(func pid=157242)[0m rmse_per_class: [0.117, 0.265, 0.099, 0.332, 0.092, 0.189, 0.292, 0.139, 0.145, 0.111]
[2m[36m(func pid=162590)[0m rmse: 0.1591029167175293
[2m[36m(func pid=162590)[0m mae:  0.11298388242721558
[2m[36m(func pid=162590)[0m rmse_per_class: [0.106, 0.232, 0.056, 0.318, 0.071, 0.173, 0.261, 0.117, 0.135, 0.122]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3312 | Steps: 4 | Val loss: 0.2720 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3596 | Steps: 4 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=163121)[0m rmse: 0.1494062840938568
[2m[36m(func pid=163121)[0m mae:  0.09720306098461151
[2m[36m(func pid=163121)[0m rmse_per_class: [0.133, 0.201, 0.032, 0.304, 0.119, 0.168, 0.218, 0.093, 0.132, 0.093]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15785518288612366
[2m[36m(func pid=162590)[0m mae:  0.11190606653690338
[2m[36m(func pid=162590)[0m rmse_per_class: [0.103, 0.233, 0.052, 0.317, 0.078, 0.173, 0.254, 0.116, 0.136, 0.117]
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3474 | Steps: 4 | Val loss: 0.2686 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=176262)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176262)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=176262)[0m Configuration completed!
[2m[36m(func pid=176262)[0m New optimizer parameters:
[2m[36m(func pid=176262)[0m SGD (
[2m[36m(func pid=176262)[0m Parameter Group 0
[2m[36m(func pid=176262)[0m     dampening: 0
[2m[36m(func pid=176262)[0m     differentiable: False
[2m[36m(func pid=176262)[0m     foreach: None
[2m[36m(func pid=176262)[0m     lr: 0.0001
[2m[36m(func pid=176262)[0m     maximize: False
[2m[36m(func pid=176262)[0m     momentum: 0.99
[2m[36m(func pid=176262)[0m     nesterov: False
[2m[36m(func pid=176262)[0m     weight_decay: 0.0001
[2m[36m(func pid=176262)[0m )
[2m[36m(func pid=176262)[0m 
== Status ==
Current time: 2024-01-07 15:43:29 (running for 00:14:43.28)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.379 |  0.159 |                   57 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.331 |  0.149 |                   56 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 15:43:34 (running for 00:14:48.48)
Memory usage on this node: 23.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.36  |  0.158 |                   58 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.331 |  0.149 |                   56 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=176351)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176351)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=176351)[0m Configuration completed!
[2m[36m(func pid=176351)[0m New optimizer parameters:
[2m[36m(func pid=176351)[0m SGD (
[2m[36m(func pid=176351)[0m Parameter Group 0
[2m[36m(func pid=176351)[0m     dampening: 0
[2m[36m(func pid=176351)[0m     differentiable: False
[2m[36m(func pid=176351)[0m     foreach: None
[2m[36m(func pid=176351)[0m     lr: 0.001
[2m[36m(func pid=176351)[0m     maximize: False
[2m[36m(func pid=176351)[0m     momentum: 0.99
[2m[36m(func pid=176351)[0m     nesterov: False
[2m[36m(func pid=176351)[0m     weight_decay: 0.0001
[2m[36m(func pid=176351)[0m )
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.1478007733821869
[2m[36m(func pid=163121)[0m mae:  0.09751744568347931
[2m[36m(func pid=163121)[0m rmse_per_class: [0.097, 0.217, 0.044, 0.268, 0.102, 0.163, 0.253, 0.095, 0.153, 0.087]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3739 | Steps: 4 | Val loss: 0.2834 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0366 | Steps: 4 | Val loss: 0.7184 | Batch size: 32 | lr: 0.0001 | Duration: 4.44s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3404 | Steps: 4 | Val loss: 0.2806 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0175 | Steps: 4 | Val loss: 0.7036 | Batch size: 32 | lr: 0.001 | Duration: 4.69s
== Status ==
Current time: 2024-01-07 15:43:39 (running for 00:14:53.67)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.374 |  0.158 |                   59 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.347 |  0.148 |                   57 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15797406435012817
[2m[36m(func pid=162590)[0m mae:  0.1121504157781601
[2m[36m(func pid=162590)[0m rmse_per_class: [0.103, 0.233, 0.047, 0.316, 0.081, 0.173, 0.253, 0.115, 0.138, 0.119]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.18041370809078217
[2m[36m(func pid=176262)[0m mae:  0.1328641027212143
[2m[36m(func pid=176262)[0m rmse_per_class: [0.114, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15376046299934387
[2m[36m(func pid=163121)[0m mae:  0.10265908390283585
[2m[36m(func pid=163121)[0m rmse_per_class: [0.095, 0.215, 0.052, 0.292, 0.095, 0.177, 0.289, 0.098, 0.13, 0.095]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.18040110170841217
[2m[36m(func pid=176351)[0m mae:  0.13284525275230408
[2m[36m(func pid=176351)[0m rmse_per_class: [0.114, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0334 | Steps: 4 | Val loss: 0.7267 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3602 | Steps: 4 | Val loss: 0.2837 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3334 | Steps: 4 | Val loss: 0.2755 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.9047 | Steps: 4 | Val loss: 0.6602 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 15:43:45 (running for 00:14:58.98)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.36  |  0.159 |                   60 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.34  |  0.154 |                   58 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  1.037 |  0.18  |                    1 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  1.017 |  0.18  |                    1 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15855887532234192
[2m[36m(func pid=162590)[0m mae:  0.11290419101715088
[2m[36m(func pid=162590)[0m rmse_per_class: [0.107, 0.234, 0.048, 0.308, 0.082, 0.174, 0.261, 0.113, 0.145, 0.115]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.1522635519504547
[2m[36m(func pid=163121)[0m mae:  0.10069668292999268
[2m[36m(func pid=163121)[0m rmse_per_class: [0.098, 0.228, 0.048, 0.279, 0.09, 0.19, 0.219, 0.092, 0.133, 0.145]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.18084314465522766
[2m[36m(func pid=176262)[0m mae:  0.13326501846313477
[2m[36m(func pid=176262)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.106, 0.191, 0.302, 0.142, 0.143, 0.115]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.18059121072292328
[2m[36m(func pid=176351)[0m mae:  0.13303005695343018
[2m[36m(func pid=176351)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.331, 0.105, 0.191, 0.3, 0.141, 0.143, 0.115]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3611 | Steps: 4 | Val loss: 0.2806 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3343 | Steps: 4 | Val loss: 0.2741 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0066 | Steps: 4 | Val loss: 0.7284 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7390 | Steps: 4 | Val loss: 0.5757 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 15:43:50 (running for 00:15:04.16)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.361 |  0.156 |                   61 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.333 |  0.152 |                   59 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  1.033 |  0.181 |                    2 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.905 |  0.181 |                    2 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.1564706563949585
[2m[36m(func pid=162590)[0m mae:  0.11147989332675934
[2m[36m(func pid=162590)[0m rmse_per_class: [0.1, 0.235, 0.048, 0.296, 0.08, 0.174, 0.264, 0.113, 0.146, 0.109]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.18100139498710632
[2m[36m(func pid=176262)[0m mae:  0.13340255618095398
[2m[36m(func pid=176262)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.108, 0.191, 0.304, 0.142, 0.142, 0.116]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.14941391348838806
[2m[36m(func pid=163121)[0m mae:  0.10146263986825943
[2m[36m(func pid=163121)[0m rmse_per_class: [0.093, 0.2, 0.039, 0.273, 0.086, 0.173, 0.254, 0.091, 0.17, 0.116]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.1803441345691681
[2m[36m(func pid=176351)[0m mae:  0.13282188773155212
[2m[36m(func pid=176351)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.331, 0.105, 0.191, 0.3, 0.14, 0.143, 0.114]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3797 | Steps: 4 | Val loss: 0.2801 | Batch size: 32 | lr: 0.01 | Duration: 2.61s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3456 | Steps: 4 | Val loss: 0.2886 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.9737 | Steps: 4 | Val loss: 0.7180 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=162590)[0m rmse: 0.1561620831489563
[2m[36m(func pid=162590)[0m mae:  0.11074598878622055
[2m[36m(func pid=162590)[0m rmse_per_class: [0.097, 0.232, 0.056, 0.299, 0.081, 0.173, 0.263, 0.113, 0.143, 0.104]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.5809 | Steps: 4 | Val loss: 0.4774 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 15:43:55 (running for 00:15:09.46)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.38  |  0.156 |                   62 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.334 |  0.149 |                   60 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.974 |  0.181 |                    4 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.739 |  0.18  |                    3 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176262)[0m rmse: 0.1810559332370758
[2m[36m(func pid=176262)[0m mae:  0.13343118131160736
[2m[36m(func pid=176262)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.109, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.1582692712545395
[2m[36m(func pid=163121)[0m mae:  0.10668419301509857
[2m[36m(func pid=163121)[0m rmse_per_class: [0.121, 0.201, 0.062, 0.328, 0.079, 0.175, 0.278, 0.09, 0.138, 0.111]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.18007014691829681
[2m[36m(func pid=176351)[0m mae:  0.13254372775554657
[2m[36m(func pid=176351)[0m rmse_per_class: [0.114, 0.265, 0.105, 0.331, 0.104, 0.19, 0.298, 0.138, 0.144, 0.112]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3596 | Steps: 4 | Val loss: 0.2803 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.9291 | Steps: 4 | Val loss: 0.6965 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3552 | Steps: 4 | Val loss: 0.2823 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=162590)[0m rmse: 0.1561783254146576
[2m[36m(func pid=162590)[0m mae:  0.11007126420736313
[2m[36m(func pid=162590)[0m rmse_per_class: [0.096, 0.231, 0.059, 0.309, 0.082, 0.172, 0.257, 0.115, 0.138, 0.102]
[2m[36m(func pid=162590)[0m 
== Status ==
Current time: 2024-01-07 15:44:01 (running for 00:15:14.73)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.36  |  0.156 |                   63 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.346 |  0.158 |                   61 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.929 |  0.181 |                    5 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.581 |  0.18  |                    4 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176262)[0m rmse: 0.18107110261917114
[2m[36m(func pid=176262)[0m mae:  0.13345767557621002
[2m[36m(func pid=176262)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.109, 0.191, 0.306, 0.143, 0.142, 0.116]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4875 | Steps: 4 | Val loss: 0.3942 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=163121)[0m rmse: 0.15533843636512756
[2m[36m(func pid=163121)[0m mae:  0.10319454967975616
[2m[36m(func pid=163121)[0m rmse_per_class: [0.091, 0.222, 0.036, 0.315, 0.11, 0.159, 0.243, 0.117, 0.131, 0.128]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3649 | Steps: 4 | Val loss: 0.2804 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=176351)[0m rmse: 0.179620161652565
[2m[36m(func pid=176351)[0m mae:  0.13208609819412231
[2m[36m(func pid=176351)[0m rmse_per_class: [0.115, 0.265, 0.11, 0.333, 0.099, 0.189, 0.293, 0.136, 0.147, 0.109]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3343 | Steps: 4 | Val loss: 0.2811 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8794 | Steps: 4 | Val loss: 0.6687 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=162590)[0m rmse: 0.15592141449451447
[2m[36m(func pid=162590)[0m mae:  0.10972162336111069
[2m[36m(func pid=162590)[0m rmse_per_class: [0.102, 0.232, 0.053, 0.313, 0.082, 0.172, 0.255, 0.117, 0.136, 0.098]
[2m[36m(func pid=162590)[0m 
== Status ==
Current time: 2024-01-07 15:44:06 (running for 00:15:20.14)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.365 |  0.156 |                   64 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.355 |  0.155 |                   62 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.879 |  0.181 |                    6 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.487 |  0.18  |                    5 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176262)[0m rmse: 0.18104687333106995
[2m[36m(func pid=176262)[0m mae:  0.13344727456569672
[2m[36m(func pid=176262)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.109, 0.19, 0.306, 0.143, 0.142, 0.116]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4517 | Steps: 4 | Val loss: 0.3463 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=163121)[0m rmse: 0.15658673644065857
[2m[36m(func pid=163121)[0m mae:  0.10572037845849991
[2m[36m(func pid=163121)[0m rmse_per_class: [0.138, 0.202, 0.039, 0.268, 0.112, 0.172, 0.262, 0.105, 0.171, 0.097]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3747 | Steps: 4 | Val loss: 0.2815 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=176351)[0m rmse: 0.17903995513916016
[2m[36m(func pid=176351)[0m mae:  0.13138599693775177
[2m[36m(func pid=176351)[0m rmse_per_class: [0.116, 0.265, 0.115, 0.336, 0.092, 0.187, 0.287, 0.136, 0.151, 0.105]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3326 | Steps: 4 | Val loss: 0.2823 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8218 | Steps: 4 | Val loss: 0.6353 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 15:44:11 (running for 00:15:25.17)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.375 |  0.156 |                   65 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.334 |  0.157 |                   63 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.879 |  0.181 |                    6 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.452 |  0.179 |                    6 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15598449110984802
[2m[36m(func pid=162590)[0m mae:  0.11020555347204208
[2m[36m(func pid=162590)[0m rmse_per_class: [0.102, 0.228, 0.047, 0.32, 0.085, 0.173, 0.255, 0.115, 0.137, 0.098]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1809585988521576
[2m[36m(func pid=176262)[0m mae:  0.13334055244922638
[2m[36m(func pid=176262)[0m rmse_per_class: [0.113, 0.264, 0.098, 0.329, 0.109, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15783903002738953
[2m[36m(func pid=163121)[0m mae:  0.10325096547603607
[2m[36m(func pid=163121)[0m rmse_per_class: [0.098, 0.212, 0.079, 0.315, 0.087, 0.199, 0.257, 0.112, 0.133, 0.086]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4561 | Steps: 4 | Val loss: 0.3276 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=176351)[0m rmse: 0.1786370724439621
[2m[36m(func pid=176351)[0m mae:  0.1307237297296524
[2m[36m(func pid=176351)[0m rmse_per_class: [0.118, 0.266, 0.118, 0.341, 0.085, 0.186, 0.28, 0.136, 0.156, 0.101]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3656 | Steps: 4 | Val loss: 0.2821 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3432 | Steps: 4 | Val loss: 0.2638 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7793 | Steps: 4 | Val loss: 0.6001 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 15:44:16 (running for 00:15:30.60)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.366 |  0.156 |                   66 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.333 |  0.158 |                   64 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.822 |  0.181 |                    7 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.456 |  0.179 |                    7 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.1563514620065689
[2m[36m(func pid=162590)[0m mae:  0.11063706874847412
[2m[36m(func pid=162590)[0m rmse_per_class: [0.1, 0.229, 0.053, 0.321, 0.081, 0.172, 0.254, 0.113, 0.141, 0.099]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.14293333888053894
[2m[36m(func pid=163121)[0m mae:  0.09458691626787186
[2m[36m(func pid=163121)[0m rmse_per_class: [0.103, 0.198, 0.035, 0.299, 0.077, 0.17, 0.232, 0.092, 0.13, 0.094]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.18077842891216278
[2m[36m(func pid=176262)[0m mae:  0.13317503035068512
[2m[36m(func pid=176262)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.108, 0.19, 0.304, 0.143, 0.142, 0.116]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4729 | Steps: 4 | Val loss: 0.3304 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3625 | Steps: 4 | Val loss: 0.2776 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=176351)[0m rmse: 0.17846430838108063
[2m[36m(func pid=176351)[0m mae:  0.13009895384311676
[2m[36m(func pid=176351)[0m rmse_per_class: [0.12, 0.267, 0.12, 0.346, 0.077, 0.185, 0.274, 0.138, 0.161, 0.098]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7283 | Steps: 4 | Val loss: 0.5647 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3399 | Steps: 4 | Val loss: 0.2720 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 15:44:22 (running for 00:15:35.92)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.363 |  0.154 |                   67 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.143 |                   65 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.779 |  0.181 |                    8 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.473 |  0.178 |                    8 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15446898341178894
[2m[36m(func pid=162590)[0m mae:  0.10921118408441544
[2m[36m(func pid=162590)[0m rmse_per_class: [0.097, 0.229, 0.054, 0.305, 0.078, 0.171, 0.255, 0.112, 0.144, 0.099]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.14795979857444763
[2m[36m(func pid=163121)[0m mae:  0.1002526730298996
[2m[36m(func pid=163121)[0m rmse_per_class: [0.089, 0.213, 0.033, 0.259, 0.078, 0.158, 0.274, 0.116, 0.152, 0.109]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.18067282438278198
[2m[36m(func pid=176262)[0m mae:  0.13307982683181763
[2m[36m(func pid=176262)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.329, 0.107, 0.19, 0.303, 0.143, 0.143, 0.116]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.5044 | Steps: 4 | Val loss: 0.3480 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3599 | Steps: 4 | Val loss: 0.2756 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=176351)[0m rmse: 0.17872747778892517
[2m[36m(func pid=176351)[0m mae:  0.12962698936462402
[2m[36m(func pid=176351)[0m rmse_per_class: [0.122, 0.267, 0.122, 0.351, 0.069, 0.185, 0.27, 0.14, 0.166, 0.095]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3206 | Steps: 4 | Val loss: 0.2896 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.6730 | Steps: 4 | Val loss: 0.5297 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 15:44:27 (running for 00:15:41.17)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.36  |  0.153 |                   68 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.34  |  0.148 |                   66 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.728 |  0.181 |                    9 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.504 |  0.179 |                    9 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.1532350778579712
[2m[36m(func pid=162590)[0m mae:  0.10827422142028809
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.23, 0.053, 0.297, 0.074, 0.171, 0.255, 0.111, 0.142, 0.101]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.18048259615898132
[2m[36m(func pid=176262)[0m mae:  0.13291147351264954
[2m[36m(func pid=176262)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.33, 0.106, 0.19, 0.302, 0.143, 0.143, 0.115]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5247 | Steps: 4 | Val loss: 0.3662 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=163121)[0m rmse: 0.15470321476459503
[2m[36m(func pid=163121)[0m mae:  0.1018572598695755
[2m[36m(func pid=163121)[0m rmse_per_class: [0.138, 0.205, 0.06, 0.345, 0.089, 0.158, 0.227, 0.092, 0.133, 0.101]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3570 | Steps: 4 | Val loss: 0.2739 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=176351)[0m rmse: 0.178522989153862
[2m[36m(func pid=176351)[0m mae:  0.1287403702735901
[2m[36m(func pid=176351)[0m rmse_per_class: [0.124, 0.268, 0.119, 0.355, 0.064, 0.185, 0.267, 0.142, 0.169, 0.094]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6360 | Steps: 4 | Val loss: 0.4975 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3727 | Steps: 4 | Val loss: 0.2849 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 15:44:32 (running for 00:15:46.47)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.357 |  0.152 |                   69 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.321 |  0.155 |                   67 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.673 |  0.18  |                   10 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.525 |  0.179 |                   10 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.1519932895898819
[2m[36m(func pid=162590)[0m mae:  0.10746431350708008
[2m[36m(func pid=162590)[0m rmse_per_class: [0.096, 0.229, 0.047, 0.294, 0.074, 0.171, 0.256, 0.111, 0.138, 0.104]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15616604685783386
[2m[36m(func pid=163121)[0m mae:  0.10136117041110992
[2m[36m(func pid=163121)[0m rmse_per_class: [0.09, 0.218, 0.06, 0.336, 0.102, 0.19, 0.232, 0.113, 0.129, 0.092]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1803349405527115
[2m[36m(func pid=176262)[0m mae:  0.13277511298656464
[2m[36m(func pid=176262)[0m rmse_per_class: [0.114, 0.265, 0.099, 0.331, 0.105, 0.19, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5550 | Steps: 4 | Val loss: 0.3893 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3643 | Steps: 4 | Val loss: 0.2739 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=176351)[0m rmse: 0.17907847464084625
[2m[36m(func pid=176351)[0m mae:  0.1281820386648178
[2m[36m(func pid=176351)[0m rmse_per_class: [0.125, 0.267, 0.118, 0.359, 0.06, 0.185, 0.27, 0.144, 0.17, 0.092]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3328 | Steps: 4 | Val loss: 0.2863 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5931 | Steps: 4 | Val loss: 0.4685 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 15:44:38 (running for 00:15:51.83)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.364 |  0.152 |                   70 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.373 |  0.156 |                   68 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.636 |  0.18  |                   11 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.555 |  0.179 |                   11 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15193232893943787
[2m[36m(func pid=162590)[0m mae:  0.10705779492855072
[2m[36m(func pid=162590)[0m rmse_per_class: [0.095, 0.229, 0.05, 0.299, 0.072, 0.171, 0.25, 0.111, 0.136, 0.106]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.1601041555404663
[2m[36m(func pid=163121)[0m mae:  0.10662635415792465
[2m[36m(func pid=163121)[0m rmse_per_class: [0.099, 0.218, 0.035, 0.28, 0.201, 0.164, 0.264, 0.092, 0.146, 0.103]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.18013884127140045
[2m[36m(func pid=176262)[0m mae:  0.13259942829608917
[2m[36m(func pid=176262)[0m rmse_per_class: [0.114, 0.265, 0.1, 0.332, 0.104, 0.19, 0.298, 0.141, 0.144, 0.115]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5980 | Steps: 4 | Val loss: 0.4116 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3598 | Steps: 4 | Val loss: 0.2779 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=176351)[0m rmse: 0.17993685603141785
[2m[36m(func pid=176351)[0m mae:  0.12783017754554749
[2m[36m(func pid=176351)[0m rmse_per_class: [0.128, 0.268, 0.115, 0.363, 0.057, 0.186, 0.276, 0.146, 0.17, 0.092]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3285 | Steps: 4 | Val loss: 0.2885 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5532 | Steps: 4 | Val loss: 0.4410 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 15:44:43 (running for 00:15:57.26)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.36  |  0.154 |                   71 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.333 |  0.16  |                   69 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.593 |  0.18  |                   12 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.598 |  0.18  |                   12 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15405310690402985
[2m[36m(func pid=162590)[0m mae:  0.10870516300201416
[2m[36m(func pid=162590)[0m rmse_per_class: [0.096, 0.226, 0.055, 0.313, 0.073, 0.171, 0.249, 0.111, 0.137, 0.109]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.16028104722499847
[2m[36m(func pid=163121)[0m mae:  0.10700152069330215
[2m[36m(func pid=163121)[0m rmse_per_class: [0.098, 0.202, 0.046, 0.308, 0.108, 0.195, 0.244, 0.115, 0.149, 0.137]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17987267673015594
[2m[36m(func pid=176262)[0m mae:  0.13237646222114563
[2m[36m(func pid=176262)[0m rmse_per_class: [0.115, 0.265, 0.1, 0.332, 0.102, 0.19, 0.296, 0.14, 0.144, 0.113]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.6372 | Steps: 4 | Val loss: 0.4363 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3585 | Steps: 4 | Val loss: 0.2816 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=176351)[0m rmse: 0.18107853829860687
[2m[36m(func pid=176351)[0m mae:  0.12761779129505157
[2m[36m(func pid=176351)[0m rmse_per_class: [0.132, 0.268, 0.11, 0.366, 0.055, 0.186, 0.284, 0.147, 0.169, 0.091]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5240 | Steps: 4 | Val loss: 0.4166 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3252 | Steps: 4 | Val loss: 0.2721 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 15:44:49 (running for 00:16:02.68)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.359 |  0.155 |                   72 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.329 |  0.16  |                   70 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.553 |  0.18  |                   13 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.637 |  0.181 |                   13 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15531054139137268
[2m[36m(func pid=162590)[0m mae:  0.1099715605378151
[2m[36m(func pid=162590)[0m rmse_per_class: [0.097, 0.224, 0.053, 0.324, 0.073, 0.171, 0.25, 0.111, 0.137, 0.112]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17958204448223114
[2m[36m(func pid=176262)[0m mae:  0.13211724162101746
[2m[36m(func pid=176262)[0m rmse_per_class: [0.115, 0.266, 0.102, 0.333, 0.1, 0.19, 0.294, 0.14, 0.145, 0.112]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.14519774913787842
[2m[36m(func pid=163121)[0m mae:  0.09543545544147491
[2m[36m(func pid=163121)[0m rmse_per_class: [0.108, 0.207, 0.046, 0.298, 0.06, 0.162, 0.226, 0.091, 0.136, 0.118]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.6419 | Steps: 4 | Val loss: 0.4386 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3661 | Steps: 4 | Val loss: 0.2816 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=176351)[0m rmse: 0.18086768686771393
[2m[36m(func pid=176351)[0m mae:  0.12683378159999847
[2m[36m(func pid=176351)[0m rmse_per_class: [0.134, 0.268, 0.104, 0.367, 0.055, 0.186, 0.287, 0.148, 0.168, 0.091]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5061 | Steps: 4 | Val loss: 0.3955 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3362 | Steps: 4 | Val loss: 0.2621 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 15:44:54 (running for 00:16:07.91)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.366 |  0.155 |                   73 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.325 |  0.145 |                   71 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.524 |  0.18  |                   14 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.642 |  0.181 |                   14 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15543226897716522
[2m[36m(func pid=162590)[0m mae:  0.11045513302087784
[2m[36m(func pid=162590)[0m rmse_per_class: [0.097, 0.223, 0.045, 0.324, 0.081, 0.171, 0.252, 0.111, 0.138, 0.113]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.14372672140598297
[2m[36m(func pid=163121)[0m mae:  0.09443463385105133
[2m[36m(func pid=163121)[0m rmse_per_class: [0.117, 0.205, 0.037, 0.267, 0.052, 0.183, 0.237, 0.107, 0.132, 0.101]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17936643958091736
[2m[36m(func pid=176262)[0m mae:  0.1318971961736679
[2m[36m(func pid=176262)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.334, 0.098, 0.189, 0.292, 0.139, 0.146, 0.111]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.6384 | Steps: 4 | Val loss: 0.4508 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3791 | Steps: 4 | Val loss: 0.2820 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=176351)[0m rmse: 0.18192198872566223
[2m[36m(func pid=176351)[0m mae:  0.126774862408638
[2m[36m(func pid=176351)[0m rmse_per_class: [0.139, 0.268, 0.1, 0.37, 0.054, 0.187, 0.296, 0.149, 0.165, 0.091]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3774 | Steps: 4 | Val loss: 0.2802 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4897 | Steps: 4 | Val loss: 0.3814 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 15:44:59 (running for 00:16:13.14)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.16699999570846558
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.379 |  0.156 |                   74 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.336 |  0.144 |                   72 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.506 |  0.179 |                   15 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.638 |  0.182 |                   15 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15614278614521027
[2m[36m(func pid=162590)[0m mae:  0.11061342060565948
[2m[36m(func pid=162590)[0m rmse_per_class: [0.099, 0.224, 0.053, 0.322, 0.083, 0.172, 0.251, 0.111, 0.139, 0.108]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17923274636268616
[2m[36m(func pid=176262)[0m mae:  0.13173584640026093
[2m[36m(func pid=176262)[0m rmse_per_class: [0.116, 0.266, 0.106, 0.335, 0.097, 0.189, 0.29, 0.138, 0.146, 0.11]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15084883570671082
[2m[36m(func pid=163121)[0m mae:  0.1017550379037857
[2m[36m(func pid=163121)[0m rmse_per_class: [0.096, 0.235, 0.031, 0.313, 0.055, 0.158, 0.265, 0.09, 0.166, 0.1]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6707 | Steps: 4 | Val loss: 0.4549 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3650 | Steps: 4 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=176351)[0m rmse: 0.18246051669120789
[2m[36m(func pid=176351)[0m mae:  0.12652313709259033
[2m[36m(func pid=176351)[0m rmse_per_class: [0.143, 0.268, 0.093, 0.372, 0.054, 0.187, 0.301, 0.15, 0.164, 0.091]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4755 | Steps: 4 | Val loss: 0.3685 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3355 | Steps: 4 | Val loss: 0.2767 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=162590)[0m rmse: 0.15707550942897797
[2m[36m(func pid=162590)[0m mae:  0.11087943613529205
[2m[36m(func pid=162590)[0m rmse_per_class: [0.097, 0.228, 0.053, 0.324, 0.089, 0.172, 0.251, 0.116, 0.138, 0.104]
[2m[36m(func pid=162590)[0m 
== Status ==
Current time: 2024-01-07 15:45:04 (running for 00:16:18.51)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.365 |  0.157 |                   75 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.377 |  0.151 |                   73 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.49  |  0.179 |                   16 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.671 |  0.182 |                   16 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176262)[0m rmse: 0.17907527089118958
[2m[36m(func pid=176262)[0m mae:  0.13154694437980652
[2m[36m(func pid=176262)[0m rmse_per_class: [0.116, 0.266, 0.107, 0.337, 0.095, 0.189, 0.288, 0.137, 0.147, 0.108]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15455791354179382
[2m[36m(func pid=163121)[0m mae:  0.09900559484958649
[2m[36m(func pid=163121)[0m rmse_per_class: [0.09, 0.199, 0.08, 0.309, 0.159, 0.156, 0.236, 0.099, 0.13, 0.088]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6936 | Steps: 4 | Val loss: 0.4719 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3573 | Steps: 4 | Val loss: 0.2846 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=176351)[0m rmse: 0.18433110415935516
[2m[36m(func pid=176351)[0m mae:  0.12711912393569946
[2m[36m(func pid=176351)[0m rmse_per_class: [0.152, 0.269, 0.089, 0.375, 0.055, 0.188, 0.31, 0.152, 0.163, 0.092]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3274 | Steps: 4 | Val loss: 0.2787 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4558 | Steps: 4 | Val loss: 0.3568 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 15:45:10 (running for 00:16:23.75)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1614999994635582
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.357 |  0.158 |                   76 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.336 |  0.155 |                   74 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.476 |  0.179 |                   17 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.694 |  0.184 |                   17 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.1583748757839203
[2m[36m(func pid=162590)[0m mae:  0.11151707172393799
[2m[36m(func pid=162590)[0m rmse_per_class: [0.096, 0.23, 0.061, 0.323, 0.088, 0.172, 0.254, 0.12, 0.139, 0.101]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17893639206886292
[2m[36m(func pid=176262)[0m mae:  0.13136976957321167
[2m[36m(func pid=176262)[0m rmse_per_class: [0.117, 0.266, 0.109, 0.339, 0.093, 0.189, 0.286, 0.136, 0.148, 0.107]
[2m[36m(func pid=163121)[0m rmse: 0.15329182147979736
[2m[36m(func pid=163121)[0m mae:  0.10124282538890839
[2m[36m(func pid=163121)[0m rmse_per_class: [0.11, 0.245, 0.05, 0.27, 0.136, 0.161, 0.244, 0.09, 0.136, 0.091]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6616 | Steps: 4 | Val loss: 0.4657 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3635 | Steps: 4 | Val loss: 0.2842 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4638 | Steps: 4 | Val loss: 0.3509 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=176351)[0m rmse: 0.1845277100801468
[2m[36m(func pid=176351)[0m mae:  0.12687817215919495
[2m[36m(func pid=176351)[0m rmse_per_class: [0.158, 0.268, 0.084, 0.376, 0.055, 0.189, 0.311, 0.152, 0.161, 0.092]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3358 | Steps: 4 | Val loss: 0.2668 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=162590)[0m rmse: 0.1580353081226349
[2m[36m(func pid=162590)[0m mae:  0.11137745529413223
[2m[36m(func pid=162590)[0m rmse_per_class: [0.095, 0.228, 0.058, 0.322, 0.09, 0.172, 0.257, 0.119, 0.138, 0.102]
[2m[36m(func pid=162590)[0m 
== Status ==
Current time: 2024-01-07 15:45:15 (running for 00:16:28.96)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.364 |  0.158 |                   77 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.327 |  0.153 |                   75 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.456 |  0.179 |                   18 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.662 |  0.185 |                   18 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176262)[0m rmse: 0.17882239818572998
[2m[36m(func pid=176262)[0m mae:  0.1312226802110672
[2m[36m(func pid=176262)[0m rmse_per_class: [0.117, 0.266, 0.109, 0.339, 0.092, 0.189, 0.284, 0.136, 0.149, 0.106]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.14450934529304504
[2m[36m(func pid=163121)[0m mae:  0.097593292593956
[2m[36m(func pid=163121)[0m rmse_per_class: [0.092, 0.202, 0.033, 0.281, 0.072, 0.159, 0.242, 0.09, 0.165, 0.108]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6575 | Steps: 4 | Val loss: 0.4540 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3542 | Steps: 4 | Val loss: 0.2836 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=176351)[0m rmse: 0.18458136916160583
[2m[36m(func pid=176351)[0m mae:  0.1267353743314743
[2m[36m(func pid=176351)[0m rmse_per_class: [0.165, 0.266, 0.08, 0.376, 0.055, 0.189, 0.31, 0.152, 0.159, 0.092]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4495 | Steps: 4 | Val loss: 0.3441 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3390 | Steps: 4 | Val loss: 0.2964 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 15:45:20 (running for 00:16:34.05)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.354 |  0.157 |                   78 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.336 |  0.145 |                   76 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.464 |  0.179 |                   19 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.657 |  0.185 |                   19 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15747888386249542
[2m[36m(func pid=162590)[0m mae:  0.11125636100769043
[2m[36m(func pid=162590)[0m rmse_per_class: [0.1, 0.226, 0.058, 0.321, 0.085, 0.171, 0.257, 0.114, 0.138, 0.105]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15981373190879822
[2m[36m(func pid=163121)[0m mae:  0.10361870378255844
[2m[36m(func pid=163121)[0m rmse_per_class: [0.095, 0.272, 0.073, 0.34, 0.052, 0.176, 0.269, 0.089, 0.13, 0.103]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17869791388511658
[2m[36m(func pid=176262)[0m mae:  0.13104631006717682
[2m[36m(func pid=176262)[0m rmse_per_class: [0.118, 0.266, 0.111, 0.34, 0.09, 0.188, 0.282, 0.136, 0.149, 0.105]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6551 | Steps: 4 | Val loss: 0.4552 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3711 | Steps: 4 | Val loss: 0.2791 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3449 | Steps: 4 | Val loss: 0.2625 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4472 | Steps: 4 | Val loss: 0.3395 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=176351)[0m rmse: 0.1862061321735382
[2m[36m(func pid=176351)[0m mae:  0.12739644944667816
[2m[36m(func pid=176351)[0m rmse_per_class: [0.183, 0.266, 0.079, 0.378, 0.056, 0.19, 0.309, 0.153, 0.157, 0.093]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15440678596496582
[2m[36m(func pid=162590)[0m mae:  0.10870270431041718
[2m[36m(func pid=162590)[0m rmse_per_class: [0.101, 0.226, 0.049, 0.319, 0.078, 0.171, 0.246, 0.114, 0.136, 0.104]
== Status ==
Current time: 2024-01-07 15:45:25 (running for 00:16:39.42)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.371 |  0.154 |                   79 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.339 |  0.16  |                   77 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.449 |  0.179 |                   20 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.655 |  0.186 |                   20 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.14096345007419586
[2m[36m(func pid=163121)[0m mae:  0.09398070722818375
[2m[36m(func pid=163121)[0m rmse_per_class: [0.1, 0.203, 0.033, 0.289, 0.064, 0.155, 0.223, 0.09, 0.151, 0.101]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17869892716407776
[2m[36m(func pid=176262)[0m mae:  0.13090939819812775
[2m[36m(func pid=176262)[0m rmse_per_class: [0.119, 0.266, 0.114, 0.342, 0.087, 0.188, 0.281, 0.136, 0.15, 0.104]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6622 | Steps: 4 | Val loss: 0.4547 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3655 | Steps: 4 | Val loss: 0.2745 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3327 | Steps: 4 | Val loss: 0.2824 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=176351)[0m rmse: 0.18799865245819092
[2m[36m(func pid=176351)[0m mae:  0.12817029654979706
[2m[36m(func pid=176351)[0m rmse_per_class: [0.207, 0.265, 0.077, 0.38, 0.056, 0.192, 0.303, 0.154, 0.155, 0.093]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4448 | Steps: 4 | Val loss: 0.3359 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 15:45:30 (running for 00:16:44.50)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.365 |  0.152 |                   80 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.345 |  0.141 |                   78 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.447 |  0.179 |                   21 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.662 |  0.188 |                   21 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15189386904239655
[2m[36m(func pid=162590)[0m mae:  0.10667737573385239
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.224, 0.05, 0.308, 0.074, 0.171, 0.245, 0.111, 0.135, 0.104]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15835309028625488
[2m[36m(func pid=163121)[0m mae:  0.10546202957630157
[2m[36m(func pid=163121)[0m rmse_per_class: [0.092, 0.23, 0.032, 0.274, 0.152, 0.159, 0.268, 0.091, 0.191, 0.094]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17844292521476746
[2m[36m(func pid=176262)[0m mae:  0.13060136139392853
[2m[36m(func pid=176262)[0m rmse_per_class: [0.119, 0.266, 0.114, 0.343, 0.086, 0.188, 0.279, 0.136, 0.151, 0.103]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6202 | Steps: 4 | Val loss: 0.4411 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3580 | Steps: 4 | Val loss: 0.2711 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3269 | Steps: 4 | Val loss: 0.3094 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=176351)[0m rmse: 0.1886102855205536
[2m[36m(func pid=176351)[0m mae:  0.1284995824098587
[2m[36m(func pid=176351)[0m rmse_per_class: [0.223, 0.263, 0.075, 0.38, 0.056, 0.192, 0.297, 0.154, 0.154, 0.093]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4448 | Steps: 4 | Val loss: 0.3344 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 15:45:35 (running for 00:16:49.62)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.358 |  0.15  |                   81 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.333 |  0.158 |                   79 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.445 |  0.178 |                   22 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.62  |  0.189 |                   22 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.14989160001277924
[2m[36m(func pid=162590)[0m mae:  0.10545580089092255
[2m[36m(func pid=162590)[0m rmse_per_class: [0.097, 0.221, 0.049, 0.291, 0.071, 0.173, 0.251, 0.107, 0.134, 0.106]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.16534364223480225
[2m[36m(func pid=163121)[0m mae:  0.10849249362945557
[2m[36m(func pid=163121)[0m rmse_per_class: [0.106, 0.262, 0.055, 0.331, 0.117, 0.16, 0.295, 0.105, 0.129, 0.092]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17836633324623108
[2m[36m(func pid=176262)[0m mae:  0.1304127275943756
[2m[36m(func pid=176262)[0m rmse_per_class: [0.12, 0.266, 0.115, 0.344, 0.084, 0.188, 0.277, 0.136, 0.151, 0.102]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5899 | Steps: 4 | Val loss: 0.4241 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3580 | Steps: 4 | Val loss: 0.2699 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3561 | Steps: 4 | Val loss: 0.2599 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=176351)[0m rmse: 0.18796132504940033
[2m[36m(func pid=176351)[0m mae:  0.12824347615242004
[2m[36m(func pid=176351)[0m rmse_per_class: [0.226, 0.261, 0.073, 0.38, 0.056, 0.191, 0.292, 0.154, 0.153, 0.094]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4443 | Steps: 4 | Val loss: 0.3329 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 15:45:41 (running for 00:16:54.73)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.358 |  0.149 |                   82 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.327 |  0.165 |                   80 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.445 |  0.178 |                   23 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.59  |  0.188 |                   23 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.14904524385929108
[2m[36m(func pid=162590)[0m mae:  0.10473761707544327
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.221, 0.048, 0.287, 0.068, 0.173, 0.25, 0.107, 0.134, 0.106]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.14078472554683685
[2m[36m(func pid=163121)[0m mae:  0.09128003567457199
[2m[36m(func pid=163121)[0m rmse_per_class: [0.09, 0.228, 0.035, 0.259, 0.083, 0.156, 0.226, 0.09, 0.129, 0.111]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17835113406181335
[2m[36m(func pid=176262)[0m mae:  0.13032527267932892
[2m[36m(func pid=176262)[0m rmse_per_class: [0.12, 0.266, 0.116, 0.345, 0.083, 0.188, 0.276, 0.136, 0.151, 0.101]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5834 | Steps: 4 | Val loss: 0.4183 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3712 | Steps: 4 | Val loss: 0.2713 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3321 | Steps: 4 | Val loss: 0.2915 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4487 | Steps: 4 | Val loss: 0.3328 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=176351)[0m rmse: 0.18918325006961823
[2m[36m(func pid=176351)[0m mae:  0.12920764088630676
[2m[36m(func pid=176351)[0m rmse_per_class: [0.248, 0.26, 0.073, 0.38, 0.056, 0.192, 0.281, 0.154, 0.153, 0.094]
[2m[36m(func pid=176351)[0m 
== Status ==
Current time: 2024-01-07 15:45:46 (running for 00:16:59.98)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.371 |  0.15  |                   83 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.356 |  0.141 |                   81 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.444 |  0.178 |                   24 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.583 |  0.189 |                   24 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.14967499673366547
[2m[36m(func pid=162590)[0m mae:  0.10536307096481323
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.221, 0.047, 0.298, 0.068, 0.171, 0.247, 0.107, 0.135, 0.105]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15862052142620087
[2m[36m(func pid=163121)[0m mae:  0.1061466783285141
[2m[36m(func pid=163121)[0m rmse_per_class: [0.111, 0.263, 0.031, 0.26, 0.104, 0.158, 0.257, 0.095, 0.19, 0.117]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1783027946949005
[2m[36m(func pid=176262)[0m mae:  0.13014858961105347
[2m[36m(func pid=176262)[0m rmse_per_class: [0.121, 0.266, 0.118, 0.346, 0.081, 0.187, 0.275, 0.136, 0.152, 0.1]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5667 | Steps: 4 | Val loss: 0.4117 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3587 | Steps: 4 | Val loss: 0.2782 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3272 | Steps: 4 | Val loss: 0.2929 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=176351)[0m rmse: 0.1895160973072052
[2m[36m(func pid=176351)[0m mae:  0.12993444502353668
[2m[36m(func pid=176351)[0m rmse_per_class: [0.259, 0.258, 0.074, 0.381, 0.056, 0.191, 0.272, 0.154, 0.156, 0.094]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4510 | Steps: 4 | Val loss: 0.3353 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 15:45:51 (running for 00:17:05.11)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.359 |  0.153 |                   84 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.332 |  0.159 |                   82 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.449 |  0.178 |                   25 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.567 |  0.19  |                   25 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15318408608436584
[2m[36m(func pid=162590)[0m mae:  0.10877613723278046
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.221, 0.045, 0.315, 0.069, 0.17, 0.253, 0.107, 0.144, 0.109]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.16129139065742493
[2m[36m(func pid=163121)[0m mae:  0.1067720502614975
[2m[36m(func pid=163121)[0m rmse_per_class: [0.122, 0.205, 0.071, 0.329, 0.095, 0.159, 0.246, 0.104, 0.15, 0.131]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1784450113773346
[2m[36m(func pid=176262)[0m mae:  0.1300453245639801
[2m[36m(func pid=176262)[0m rmse_per_class: [0.122, 0.267, 0.12, 0.348, 0.079, 0.187, 0.274, 0.136, 0.152, 0.099]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5354 | Steps: 4 | Val loss: 0.4077 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3582 | Steps: 4 | Val loss: 0.2843 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=176351)[0m rmse: 0.18896923959255219
[2m[36m(func pid=176351)[0m mae:  0.13026437163352966
[2m[36m(func pid=176351)[0m rmse_per_class: [0.257, 0.256, 0.076, 0.381, 0.056, 0.19, 0.265, 0.155, 0.16, 0.094]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3387 | Steps: 4 | Val loss: 0.2735 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4583 | Steps: 4 | Val loss: 0.3367 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 15:45:56 (running for 00:17:10.40)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.358 |  0.156 |                   85 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.327 |  0.161 |                   83 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.451 |  0.178 |                   26 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.535 |  0.189 |                   26 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.1561509519815445
[2m[36m(func pid=162590)[0m mae:  0.11142055690288544
[2m[36m(func pid=162590)[0m rmse_per_class: [0.098, 0.222, 0.045, 0.327, 0.072, 0.17, 0.257, 0.108, 0.147, 0.116]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.14728614687919617
[2m[36m(func pid=163121)[0m mae:  0.09715936332941055
[2m[36m(func pid=163121)[0m rmse_per_class: [0.093, 0.22, 0.041, 0.32, 0.076, 0.156, 0.233, 0.094, 0.132, 0.108]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1784997284412384
[2m[36m(func pid=176262)[0m mae:  0.1299620121717453
[2m[36m(func pid=176262)[0m rmse_per_class: [0.123, 0.267, 0.121, 0.349, 0.077, 0.187, 0.273, 0.136, 0.153, 0.099]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.6265 | Steps: 4 | Val loss: 0.4087 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3587 | Steps: 4 | Val loss: 0.2865 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=176351)[0m rmse: 0.18833111226558685
[2m[36m(func pid=176351)[0m mae:  0.13063102960586548
[2m[36m(func pid=176351)[0m rmse_per_class: [0.249, 0.254, 0.083, 0.381, 0.056, 0.189, 0.256, 0.155, 0.166, 0.094]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3456 | Steps: 4 | Val loss: 0.2606 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4581 | Steps: 4 | Val loss: 0.3389 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 15:46:01 (running for 00:17:15.50)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.359 |  0.157 |                   86 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.339 |  0.147 |                   84 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.458 |  0.178 |                   27 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.627 |  0.188 |                   27 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15743178129196167
[2m[36m(func pid=162590)[0m mae:  0.11205965280532837
[2m[36m(func pid=162590)[0m rmse_per_class: [0.097, 0.222, 0.048, 0.332, 0.079, 0.17, 0.257, 0.107, 0.143, 0.119]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.14287427067756653
[2m[36m(func pid=163121)[0m mae:  0.09303094446659088
[2m[36m(func pid=163121)[0m rmse_per_class: [0.091, 0.205, 0.041, 0.257, 0.079, 0.158, 0.247, 0.089, 0.13, 0.131]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1786092221736908
[2m[36m(func pid=176262)[0m mae:  0.1298941820859909
[2m[36m(func pid=176262)[0m rmse_per_class: [0.124, 0.267, 0.122, 0.35, 0.075, 0.187, 0.273, 0.137, 0.153, 0.098]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5248 | Steps: 4 | Val loss: 0.4022 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3545 | Steps: 4 | Val loss: 0.2834 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=176351)[0m rmse: 0.186109259724617
[2m[36m(func pid=176351)[0m mae:  0.12993013858795166
[2m[36m(func pid=176351)[0m rmse_per_class: [0.221, 0.251, 0.092, 0.381, 0.056, 0.186, 0.251, 0.155, 0.174, 0.094]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3296 | Steps: 4 | Val loss: 0.2915 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4624 | Steps: 4 | Val loss: 0.3424 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=162590)[0m rmse: 0.15652930736541748
[2m[36m(func pid=162590)[0m mae:  0.11074329912662506
[2m[36m(func pid=162590)[0m rmse_per_class: [0.096, 0.226, 0.05, 0.327, 0.083, 0.171, 0.248, 0.107, 0.145, 0.112]
== Status ==
Current time: 2024-01-07 15:46:07 (running for 00:17:20.85)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.355 |  0.157 |                   87 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.346 |  0.143 |                   85 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.458 |  0.179 |                   28 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.525 |  0.186 |                   28 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15494392812252045
[2m[36m(func pid=163121)[0m mae:  0.10518958419561386
[2m[36m(func pid=163121)[0m rmse_per_class: [0.098, 0.207, 0.035, 0.331, 0.077, 0.162, 0.245, 0.12, 0.169, 0.106]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17867380380630493
[2m[36m(func pid=176262)[0m mae:  0.12976782023906708
[2m[36m(func pid=176262)[0m rmse_per_class: [0.125, 0.267, 0.123, 0.351, 0.073, 0.187, 0.272, 0.137, 0.154, 0.097]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5358 | Steps: 4 | Val loss: 0.3937 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3647 | Steps: 4 | Val loss: 0.2776 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=176351)[0m rmse: 0.18320642411708832
[2m[36m(func pid=176351)[0m mae:  0.128679096698761
[2m[36m(func pid=176351)[0m rmse_per_class: [0.188, 0.248, 0.098, 0.379, 0.056, 0.182, 0.248, 0.155, 0.185, 0.093]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3470 | Steps: 4 | Val loss: 0.2752 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 15:46:12 (running for 00:17:25.89)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.365 |  0.154 |                   88 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.33  |  0.155 |                   86 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.462 |  0.179 |                   29 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.536 |  0.183 |                   29 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15378417074680328
[2m[36m(func pid=162590)[0m mae:  0.10779782384634018
[2m[36m(func pid=162590)[0m rmse_per_class: [0.096, 0.231, 0.05, 0.315, 0.08, 0.17, 0.241, 0.108, 0.141, 0.106]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4654 | Steps: 4 | Val loss: 0.3394 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=163121)[0m rmse: 0.151036337018013
[2m[36m(func pid=163121)[0m mae:  0.09946184605360031
[2m[36m(func pid=163121)[0m rmse_per_class: [0.136, 0.208, 0.037, 0.298, 0.076, 0.168, 0.259, 0.098, 0.137, 0.094]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5548 | Steps: 4 | Val loss: 0.3842 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=176262)[0m rmse: 0.17847730219364166
[2m[36m(func pid=176262)[0m mae:  0.1296631246805191
[2m[36m(func pid=176262)[0m rmse_per_class: [0.124, 0.267, 0.122, 0.351, 0.073, 0.187, 0.271, 0.137, 0.155, 0.097]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3621 | Steps: 4 | Val loss: 0.2726 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=176351)[0m rmse: 0.18053483963012695
[2m[36m(func pid=176351)[0m mae:  0.12752148509025574
[2m[36m(func pid=176351)[0m rmse_per_class: [0.157, 0.247, 0.101, 0.378, 0.056, 0.179, 0.246, 0.154, 0.195, 0.093]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15127839148044586
[2m[36m(func pid=162590)[0m mae:  0.10550229251384735
[2m[36m(func pid=162590)[0m rmse_per_class: [0.095, 0.235, 0.046, 0.298, 0.079, 0.17, 0.241, 0.107, 0.138, 0.103]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3628 | Steps: 4 | Val loss: 0.2679 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 15:46:17 (running for 00:17:31.11)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.362 |  0.151 |                   89 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.347 |  0.151 |                   87 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.465 |  0.178 |                   30 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.555 |  0.181 |                   30 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4778 | Steps: 4 | Val loss: 0.3424 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=163121)[0m rmse: 0.1474054753780365
[2m[36m(func pid=163121)[0m mae:  0.09556597471237183
[2m[36m(func pid=163121)[0m rmse_per_class: [0.09, 0.195, 0.04, 0.269, 0.073, 0.164, 0.227, 0.119, 0.131, 0.165]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17852936685085297
[2m[36m(func pid=176262)[0m mae:  0.1295604407787323
[2m[36m(func pid=176262)[0m rmse_per_class: [0.125, 0.267, 0.123, 0.352, 0.072, 0.186, 0.27, 0.137, 0.155, 0.097]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5343 | Steps: 4 | Val loss: 0.3790 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3562 | Steps: 4 | Val loss: 0.2722 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=176351)[0m rmse: 0.17861714959144592
[2m[36m(func pid=176351)[0m mae:  0.12642593681812286
[2m[36m(func pid=176351)[0m rmse_per_class: [0.131, 0.248, 0.104, 0.375, 0.056, 0.176, 0.247, 0.154, 0.201, 0.092]
[2m[36m(func pid=176351)[0m 
== Status ==
Current time: 2024-01-07 15:46:22 (running for 00:17:36.26)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                   90 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.363 |  0.147 |                   88 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.478 |  0.179 |                   31 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.534 |  0.179 |                   31 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15124128758907318
[2m[36m(func pid=162590)[0m mae:  0.10622086375951767
[2m[36m(func pid=162590)[0m rmse_per_class: [0.094, 0.232, 0.042, 0.291, 0.081, 0.17, 0.249, 0.108, 0.139, 0.106]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3604 | Steps: 4 | Val loss: 0.2808 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4758 | Steps: 4 | Val loss: 0.3455 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=163121)[0m rmse: 0.15348581969738007
[2m[36m(func pid=163121)[0m mae:  0.10214302688837051
[2m[36m(func pid=163121)[0m rmse_per_class: [0.091, 0.206, 0.055, 0.276, 0.092, 0.178, 0.271, 0.09, 0.187, 0.09]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17856721580028534
[2m[36m(func pid=176262)[0m mae:  0.12944909930229187
[2m[36m(func pid=176262)[0m rmse_per_class: [0.126, 0.267, 0.124, 0.353, 0.07, 0.186, 0.27, 0.137, 0.155, 0.096]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4982 | Steps: 4 | Val loss: 0.3728 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3617 | Steps: 4 | Val loss: 0.2746 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=176351)[0m rmse: 0.1771286576986313
[2m[36m(func pid=176351)[0m mae:  0.1253202110528946
[2m[36m(func pid=176351)[0m rmse_per_class: [0.112, 0.253, 0.105, 0.372, 0.056, 0.174, 0.251, 0.154, 0.201, 0.092]
[2m[36m(func pid=176351)[0m 
== Status ==
Current time: 2024-01-07 15:46:27 (running for 00:17:41.57)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                   90 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.36  |  0.153 |                   89 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.476 |  0.179 |                   32 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.498 |  0.177 |                   32 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3412 | Steps: 4 | Val loss: 0.3084 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=162590)[0m rmse: 0.1523427665233612
[2m[36m(func pid=162590)[0m mae:  0.10776916891336441
[2m[36m(func pid=162590)[0m rmse_per_class: [0.095, 0.228, 0.041, 0.295, 0.077, 0.17, 0.258, 0.107, 0.146, 0.107]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4745 | Steps: 4 | Val loss: 0.3441 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=163121)[0m rmse: 0.15938006341457367
[2m[36m(func pid=163121)[0m mae:  0.10963882505893707
[2m[36m(func pid=163121)[0m rmse_per_class: [0.124, 0.194, 0.039, 0.276, 0.082, 0.159, 0.328, 0.099, 0.206, 0.086]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17820550501346588
[2m[36m(func pid=176262)[0m mae:  0.12918958067893982
[2m[36m(func pid=176262)[0m rmse_per_class: [0.126, 0.267, 0.122, 0.353, 0.069, 0.186, 0.269, 0.138, 0.156, 0.096]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3534 | Steps: 4 | Val loss: 0.2775 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.5081 | Steps: 4 | Val loss: 0.3617 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3314 | Steps: 4 | Val loss: 0.2663 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 15:46:33 (running for 00:17:47.01)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.362 |  0.152 |                   91 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.341 |  0.159 |                   90 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.475 |  0.178 |                   33 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.508 |  0.176 |                   33 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176351)[0m rmse: 0.17571483552455902
[2m[36m(func pid=176351)[0m mae:  0.12414786964654922
[2m[36m(func pid=176351)[0m rmse_per_class: [0.101, 0.259, 0.102, 0.367, 0.056, 0.176, 0.256, 0.153, 0.194, 0.091]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=162590)[0m rmse: 0.15362118184566498
[2m[36m(func pid=162590)[0m mae:  0.10897435247898102
[2m[36m(func pid=162590)[0m rmse_per_class: [0.099, 0.224, 0.042, 0.306, 0.075, 0.17, 0.261, 0.107, 0.143, 0.11]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4659 | Steps: 4 | Val loss: 0.3428 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=163121)[0m rmse: 0.14584743976593018
[2m[36m(func pid=163121)[0m mae:  0.09511904418468475
[2m[36m(func pid=163121)[0m rmse_per_class: [0.098, 0.211, 0.035, 0.3, 0.092, 0.162, 0.221, 0.093, 0.131, 0.114]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17799516022205353
[2m[36m(func pid=176262)[0m mae:  0.12902206182479858
[2m[36m(func pid=176262)[0m rmse_per_class: [0.127, 0.268, 0.121, 0.352, 0.068, 0.186, 0.268, 0.138, 0.156, 0.096]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5041 | Steps: 4 | Val loss: 0.3542 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3539 | Steps: 4 | Val loss: 0.2800 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3327 | Steps: 4 | Val loss: 0.2724 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=176351)[0m rmse: 0.17537018656730652
[2m[36m(func pid=176351)[0m mae:  0.12348280847072601
[2m[36m(func pid=176351)[0m rmse_per_class: [0.097, 0.264, 0.102, 0.361, 0.056, 0.18, 0.263, 0.153, 0.187, 0.092]
[2m[36m(func pid=176351)[0m 
== Status ==
Current time: 2024-01-07 15:46:38 (running for 00:17:52.33)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.353 |  0.154 |                   92 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.331 |  0.146 |                   91 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.466 |  0.178 |                   34 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.504 |  0.175 |                   34 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15468434989452362
[2m[36m(func pid=162590)[0m mae:  0.10953047126531601
[2m[36m(func pid=162590)[0m rmse_per_class: [0.103, 0.222, 0.045, 0.317, 0.072, 0.17, 0.258, 0.107, 0.139, 0.115]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4806 | Steps: 4 | Val loss: 0.3445 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=163121)[0m rmse: 0.15054690837860107
[2m[36m(func pid=163121)[0m mae:  0.0987517386674881
[2m[36m(func pid=163121)[0m rmse_per_class: [0.139, 0.195, 0.043, 0.308, 0.097, 0.159, 0.221, 0.105, 0.144, 0.093]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17781749367713928
[2m[36m(func pid=176262)[0m mae:  0.12874069809913635
[2m[36m(func pid=176262)[0m rmse_per_class: [0.127, 0.268, 0.121, 0.353, 0.067, 0.186, 0.267, 0.138, 0.156, 0.096]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4855 | Steps: 4 | Val loss: 0.3390 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3577 | Steps: 4 | Val loss: 0.2806 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3042 | Steps: 4 | Val loss: 0.2973 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 15:46:43 (running for 00:17:57.65)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.354 |  0.155 |                   93 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.333 |  0.151 |                   92 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.481 |  0.178 |                   35 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.485 |  0.173 |                   35 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15500134229660034
[2m[36m(func pid=162590)[0m mae:  0.10927672684192657
[2m[36m(func pid=162590)[0m rmse_per_class: [0.107, 0.222, 0.047, 0.322, 0.071, 0.169, 0.253, 0.108, 0.136, 0.118]
[2m[36m(func pid=176351)[0m rmse: 0.1733817607164383
[2m[36m(func pid=176351)[0m mae:  0.1219855397939682
[2m[36m(func pid=176351)[0m rmse_per_class: [0.096, 0.265, 0.088, 0.351, 0.056, 0.19, 0.269, 0.152, 0.173, 0.093]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4823 | Steps: 4 | Val loss: 0.3506 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=163121)[0m rmse: 0.16306835412979126
[2m[36m(func pid=163121)[0m mae:  0.11062387377023697
[2m[36m(func pid=163121)[0m rmse_per_class: [0.091, 0.201, 0.051, 0.276, 0.101, 0.165, 0.304, 0.094, 0.22, 0.128]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17826759815216064
[2m[36m(func pid=176262)[0m mae:  0.12887121737003326
[2m[36m(func pid=176262)[0m rmse_per_class: [0.128, 0.267, 0.124, 0.355, 0.065, 0.186, 0.267, 0.138, 0.156, 0.096]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3589 | Steps: 4 | Val loss: 0.2799 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4634 | Steps: 4 | Val loss: 0.3309 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 15:46:49 (running for 00:18:02.94)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.358 |  0.155 |                   94 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.304 |  0.163 |                   93 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.482 |  0.178 |                   36 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.463 |  0.173 |                   36 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.1547326296567917
[2m[36m(func pid=162590)[0m mae:  0.10862846672534943
[2m[36m(func pid=162590)[0m rmse_per_class: [0.103, 0.221, 0.055, 0.321, 0.071, 0.169, 0.249, 0.107, 0.135, 0.116]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3412 | Steps: 4 | Val loss: 0.2677 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=176351)[0m rmse: 0.1726725548505783
[2m[36m(func pid=176351)[0m mae:  0.12105382978916168
[2m[36m(func pid=176351)[0m rmse_per_class: [0.097, 0.264, 0.084, 0.341, 0.056, 0.199, 0.275, 0.151, 0.162, 0.097]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4882 | Steps: 4 | Val loss: 0.3568 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=163121)[0m rmse: 0.14891469478607178
[2m[36m(func pid=163121)[0m mae:  0.0960870161652565
[2m[36m(func pid=163121)[0m rmse_per_class: [0.128, 0.201, 0.03, 0.271, 0.094, 0.156, 0.259, 0.113, 0.13, 0.109]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1786566823720932
[2m[36m(func pid=176262)[0m mae:  0.1289154440164566
[2m[36m(func pid=176262)[0m rmse_per_class: [0.13, 0.267, 0.126, 0.356, 0.064, 0.186, 0.268, 0.139, 0.156, 0.095]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3632 | Steps: 4 | Val loss: 0.2781 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4640 | Steps: 4 | Val loss: 0.3211 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 15:46:54 (running for 00:18:08.19)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.363 |  0.154 |                   96 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.341 |  0.149 |                   94 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.488 |  0.179 |                   37 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.463 |  0.173 |                   36 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.1539115011692047
[2m[36m(func pid=162590)[0m mae:  0.10783908516168594
[2m[36m(func pid=162590)[0m rmse_per_class: [0.099, 0.22, 0.064, 0.315, 0.067, 0.17, 0.249, 0.106, 0.138, 0.112]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3351 | Steps: 4 | Val loss: 0.2579 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4907 | Steps: 4 | Val loss: 0.3552 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=176351)[0m rmse: 0.17061015963554382
[2m[36m(func pid=176351)[0m mae:  0.11962902545928955
[2m[36m(func pid=176351)[0m rmse_per_class: [0.099, 0.26, 0.069, 0.327, 0.056, 0.21, 0.281, 0.149, 0.149, 0.106]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.13774940371513367
[2m[36m(func pid=163121)[0m mae:  0.08933894336223602
[2m[36m(func pid=163121)[0m rmse_per_class: [0.096, 0.196, 0.037, 0.293, 0.068, 0.158, 0.221, 0.09, 0.129, 0.09]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17828300595283508
[2m[36m(func pid=176262)[0m mae:  0.12855741381645203
[2m[36m(func pid=176262)[0m rmse_per_class: [0.129, 0.267, 0.125, 0.356, 0.063, 0.185, 0.267, 0.139, 0.156, 0.095]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3591 | Steps: 4 | Val loss: 0.2743 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4767 | Steps: 4 | Val loss: 0.3173 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=162590)[0m rmse: 0.15157552063465118
[2m[36m(func pid=162590)[0m mae:  0.1065361276268959
[2m[36m(func pid=162590)[0m rmse_per_class: [0.095, 0.22, 0.056, 0.304, 0.068, 0.169, 0.252, 0.105, 0.138, 0.107]
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3320 | Steps: 4 | Val loss: 0.2769 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=162590)[0m 
== Status ==
Current time: 2024-01-07 15:46:59 (running for 00:18:13.45)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.359 |  0.152 |                   97 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.335 |  0.138 |                   95 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.491 |  0.178 |                   38 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.464 |  0.171 |                   37 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4862 | Steps: 4 | Val loss: 0.3543 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=176351)[0m rmse: 0.17056392133235931
[2m[36m(func pid=176351)[0m mae:  0.11903350055217743
[2m[36m(func pid=176351)[0m rmse_per_class: [0.099, 0.254, 0.073, 0.315, 0.056, 0.212, 0.287, 0.147, 0.141, 0.12]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15302734076976776
[2m[36m(func pid=163121)[0m mae:  0.10118705034255981
[2m[36m(func pid=163121)[0m rmse_per_class: [0.096, 0.212, 0.053, 0.267, 0.07, 0.173, 0.261, 0.09, 0.205, 0.104]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17821209132671356
[2m[36m(func pid=176262)[0m mae:  0.1285019814968109
[2m[36m(func pid=176262)[0m rmse_per_class: [0.13, 0.268, 0.124, 0.356, 0.063, 0.185, 0.267, 0.139, 0.156, 0.095]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3581 | Steps: 4 | Val loss: 0.2745 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4581 | Steps: 4 | Val loss: 0.3148 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 15:47:04 (running for 00:18:18.54)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.358 |  0.152 |                   98 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.332 |  0.153 |                   96 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.486 |  0.178 |                   39 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.477 |  0.171 |                   38 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15184929966926575
[2m[36m(func pid=162590)[0m mae:  0.10708077996969223
[2m[36m(func pid=162590)[0m rmse_per_class: [0.094, 0.218, 0.055, 0.299, 0.068, 0.169, 0.257, 0.105, 0.143, 0.111]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3245 | Steps: 4 | Val loss: 0.2781 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=176351)[0m rmse: 0.16976988315582275
[2m[36m(func pid=176351)[0m mae:  0.11841180175542831
[2m[36m(func pid=176351)[0m rmse_per_class: [0.1, 0.249, 0.064, 0.304, 0.056, 0.211, 0.292, 0.144, 0.136, 0.142]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4943 | Steps: 4 | Val loss: 0.3565 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=163121)[0m rmse: 0.1554439514875412
[2m[36m(func pid=163121)[0m mae:  0.10147831588983536
[2m[36m(func pid=163121)[0m rmse_per_class: [0.139, 0.2, 0.037, 0.272, 0.093, 0.165, 0.272, 0.09, 0.13, 0.154]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17835652828216553
[2m[36m(func pid=176262)[0m mae:  0.1284550130367279
[2m[36m(func pid=176262)[0m rmse_per_class: [0.132, 0.268, 0.124, 0.356, 0.062, 0.185, 0.266, 0.139, 0.156, 0.095]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3542 | Steps: 4 | Val loss: 0.2754 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4493 | Steps: 4 | Val loss: 0.3153 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 15:47:10 (running for 00:18:23.92)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00006 | RUNNING    | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.354 |  0.153 |                   99 |
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.324 |  0.155 |                   97 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.494 |  0.178 |                   40 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.458 |  0.17  |                   39 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15268251299858093
[2m[36m(func pid=162590)[0m mae:  0.10765258967876434
[2m[36m(func pid=162590)[0m rmse_per_class: [0.095, 0.221, 0.055, 0.301, 0.067, 0.169, 0.257, 0.105, 0.147, 0.111]
[2m[36m(func pid=162590)[0m 
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3376 | Steps: 4 | Val loss: 0.2917 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4976 | Steps: 4 | Val loss: 0.3557 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=176351)[0m rmse: 0.17016279697418213
[2m[36m(func pid=176351)[0m mae:  0.11842815577983856
[2m[36m(func pid=176351)[0m rmse_per_class: [0.101, 0.247, 0.058, 0.295, 0.056, 0.205, 0.296, 0.14, 0.133, 0.171]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.15588881075382233
[2m[36m(func pid=163121)[0m mae:  0.09969468414783478
[2m[36m(func pid=163121)[0m rmse_per_class: [0.092, 0.232, 0.063, 0.35, 0.122, 0.158, 0.226, 0.09, 0.129, 0.096]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17825362086296082
[2m[36m(func pid=176262)[0m mae:  0.12834496796131134
[2m[36m(func pid=176262)[0m rmse_per_class: [0.133, 0.268, 0.124, 0.356, 0.061, 0.186, 0.266, 0.139, 0.156, 0.095]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=162590)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3563 | Steps: 4 | Val loss: 0.2735 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4644 | Steps: 4 | Val loss: 0.3176 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 15:47:15 (running for 00:18:29.22)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 3 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.338 |  0.156 |                   98 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.498 |  0.178 |                   41 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.449 |  0.17  |                   40 |
| train_10f5e_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162590)[0m rmse: 0.15143375098705292
[2m[36m(func pid=162590)[0m mae:  0.10666284710168839
[2m[36m(func pid=162590)[0m rmse_per_class: [0.101, 0.224, 0.046, 0.295, 0.066, 0.169, 0.257, 0.105, 0.144, 0.107]
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3461 | Steps: 4 | Val loss: 0.2840 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5054 | Steps: 4 | Val loss: 0.3569 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=176351)[0m rmse: 0.17128989100456238
[2m[36m(func pid=176351)[0m mae:  0.11902264505624771
[2m[36m(func pid=176351)[0m rmse_per_class: [0.101, 0.249, 0.051, 0.29, 0.056, 0.194, 0.3, 0.135, 0.132, 0.203]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.1566469669342041
[2m[36m(func pid=163121)[0m mae:  0.10480660200119019
[2m[36m(func pid=163121)[0m rmse_per_class: [0.089, 0.195, 0.033, 0.303, 0.114, 0.169, 0.278, 0.103, 0.173, 0.109]
[2m[36m(func pid=163121)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17829111218452454
[2m[36m(func pid=176262)[0m mae:  0.12825976312160492
[2m[36m(func pid=176262)[0m rmse_per_class: [0.134, 0.267, 0.124, 0.357, 0.061, 0.186, 0.265, 0.139, 0.156, 0.094]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4769 | Steps: 4 | Val loss: 0.3199 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=163121)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3425 | Steps: 4 | Val loss: 0.2751 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.5113 | Steps: 4 | Val loss: 0.3581 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=176351)[0m rmse: 0.17278507351875305
[2m[36m(func pid=176351)[0m mae:  0.11994460970163345
[2m[36m(func pid=176351)[0m rmse_per_class: [0.101, 0.254, 0.044, 0.288, 0.056, 0.186, 0.301, 0.131, 0.133, 0.233]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=163121)[0m rmse: 0.1528860628604889
[2m[36m(func pid=163121)[0m mae:  0.09842385351657867
[2m[36m(func pid=163121)[0m rmse_per_class: [0.134, 0.23, 0.052, 0.262, 0.099, 0.16, 0.239, 0.112, 0.128, 0.11]
[2m[36m(func pid=176262)[0m rmse: 0.1782979816198349
[2m[36m(func pid=176262)[0m mae:  0.12817935645580292
[2m[36m(func pid=176262)[0m rmse_per_class: [0.135, 0.268, 0.123, 0.357, 0.06, 0.185, 0.265, 0.14, 0.156, 0.094]
== Status ==
Current time: 2024-01-07 15:47:21 (running for 00:18:34.95)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00007 | RUNNING    | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.346 |  0.157 |                   99 |
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.505 |  0.178 |                   42 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.477 |  0.173 |                   42 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=186938)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=186938)[0m Configuration completed!
[2m[36m(func pid=186938)[0m New optimizer parameters:
[2m[36m(func pid=186938)[0m SGD (
[2m[36m(func pid=186938)[0m Parameter Group 0
[2m[36m(func pid=186938)[0m     dampening: 0
[2m[36m(func pid=186938)[0m     differentiable: False
[2m[36m(func pid=186938)[0m     foreach: None
[2m[36m(func pid=186938)[0m     lr: 0.01
[2m[36m(func pid=186938)[0m     maximize: False
[2m[36m(func pid=186938)[0m     momentum: 0.99
[2m[36m(func pid=186938)[0m     nesterov: False
[2m[36m(func pid=186938)[0m     weight_decay: 0.0001
[2m[36m(func pid=186938)[0m )
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4547 | Steps: 4 | Val loss: 0.3222 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=176351)[0m rmse: 0.17442333698272705
[2m[36m(func pid=176351)[0m mae:  0.12087778002023697
[2m[36m(func pid=176351)[0m rmse_per_class: [0.101, 0.259, 0.044, 0.287, 0.056, 0.18, 0.302, 0.128, 0.133, 0.253]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4963 | Steps: 4 | Val loss: 0.3599 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8826 | Steps: 4 | Val loss: 0.5882 | Batch size: 32 | lr: 0.01 | Duration: 4.64s
[2m[36m(func pid=176262)[0m rmse: 0.17837642133235931
[2m[36m(func pid=176262)[0m mae:  0.12809665501117706
[2m[36m(func pid=176262)[0m rmse_per_class: [0.136, 0.268, 0.123, 0.357, 0.059, 0.185, 0.265, 0.14, 0.156, 0.094]
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4552 | Steps: 4 | Val loss: 0.3232 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=186938)[0m rmse: 0.1800733357667923
[2m[36m(func pid=186938)[0m mae:  0.13253799080848694
[2m[36m(func pid=186938)[0m rmse_per_class: [0.114, 0.264, 0.105, 0.335, 0.101, 0.191, 0.294, 0.14, 0.144, 0.113]
[2m[36m(func pid=176351)[0m rmse: 0.1753925234079361
[2m[36m(func pid=176351)[0m mae:  0.12144529819488525
[2m[36m(func pid=176351)[0m rmse_per_class: [0.1, 0.264, 0.043, 0.286, 0.056, 0.177, 0.301, 0.13, 0.133, 0.263]
== Status ==
Current time: 2024-01-07 15:47:26 (running for 00:18:40.34)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.511 |  0.178 |                   43 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.455 |  0.174 |                   43 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 15:47:32 (running for 00:18:46.42)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.511 |  0.178 |                   43 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.455 |  0.175 |                   44 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=187535)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=187535)[0m Configuration completed!
[2m[36m(func pid=187535)[0m New optimizer parameters:
[2m[36m(func pid=187535)[0m SGD (
[2m[36m(func pid=187535)[0m Parameter Group 0
[2m[36m(func pid=187535)[0m     dampening: 0
[2m[36m(func pid=187535)[0m     differentiable: False
[2m[36m(func pid=187535)[0m     foreach: None
[2m[36m(func pid=187535)[0m     lr: 0.1
[2m[36m(func pid=187535)[0m     maximize: False
[2m[36m(func pid=187535)[0m     momentum: 0.99
[2m[36m(func pid=187535)[0m     nesterov: False
[2m[36m(func pid=187535)[0m     weight_decay: 0.0001
[2m[36m(func pid=187535)[0m )
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4704 | Steps: 4 | Val loss: 0.3232 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.4869 | Steps: 4 | Val loss: 0.3927 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5016 | Steps: 4 | Val loss: 0.3627 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.6464 | Steps: 4 | Val loss: 0.3585 | Batch size: 32 | lr: 0.1 | Duration: 4.84s
== Status ==
Current time: 2024-01-07 15:47:37 (running for 00:18:51.45)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.496 |  0.178 |                   44 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.455 |  0.175 |                   44 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.883 |  0.18  |                    1 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176351)[0m rmse: 0.1764039248228073
[2m[36m(func pid=176351)[0m mae:  0.12211284786462784
[2m[36m(func pid=176351)[0m rmse_per_class: [0.099, 0.267, 0.042, 0.285, 0.056, 0.176, 0.301, 0.139, 0.133, 0.266]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.17971062660217285
[2m[36m(func pid=186938)[0m mae:  0.1318514347076416
[2m[36m(func pid=186938)[0m rmse_per_class: [0.117, 0.265, 0.117, 0.338, 0.096, 0.19, 0.287, 0.137, 0.147, 0.103]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17868691682815552
[2m[36m(func pid=176262)[0m mae:  0.12823587656021118
[2m[36m(func pid=176262)[0m rmse_per_class: [0.139, 0.267, 0.123, 0.358, 0.059, 0.185, 0.266, 0.14, 0.156, 0.094]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.18044395744800568
[2m[36m(func pid=187535)[0m mae:  0.1314808428287506
[2m[36m(func pid=187535)[0m rmse_per_class: [0.12, 0.265, 0.13, 0.351, 0.074, 0.19, 0.275, 0.14, 0.163, 0.098]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4542 | Steps: 4 | Val loss: 0.3227 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4902 | Steps: 4 | Val loss: 0.3239 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5153 | Steps: 4 | Val loss: 0.3675 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=176351)[0m rmse: 0.1775265634059906
[2m[36m(func pid=176351)[0m mae:  0.1226147785782814
[2m[36m(func pid=176351)[0m rmse_per_class: [0.102, 0.269, 0.042, 0.287, 0.056, 0.177, 0.297, 0.158, 0.133, 0.254]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8308 | Steps: 4 | Val loss: 0.3831 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 15:47:43 (running for 00:18:56.85)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.502 |  0.179 |                   45 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.454 |  0.178 |                   46 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.487 |  0.18  |                    2 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.646 |  0.18  |                    1 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.17901065945625305
[2m[36m(func pid=186938)[0m mae:  0.12954387068748474
[2m[36m(func pid=186938)[0m rmse_per_class: [0.122, 0.266, 0.142, 0.346, 0.077, 0.189, 0.269, 0.138, 0.148, 0.094]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17903143167495728
[2m[36m(func pid=176262)[0m mae:  0.12832169234752655
[2m[36m(func pid=176262)[0m rmse_per_class: [0.141, 0.267, 0.123, 0.359, 0.058, 0.185, 0.265, 0.14, 0.156, 0.094]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.19605982303619385
[2m[36m(func pid=187535)[0m mae:  0.13876520097255707
[2m[36m(func pid=187535)[0m rmse_per_class: [0.111, 0.261, 0.129, 0.376, 0.055, 0.2, 0.257, 0.151, 0.329, 0.092]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4601 | Steps: 4 | Val loss: 0.3216 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6192 | Steps: 4 | Val loss: 0.3602 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4903 | Steps: 4 | Val loss: 0.3565 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 15:47:48 (running for 00:19:02.08)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.515 |  0.179 |                   46 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.46  |  0.179 |                   47 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.49  |  0.179 |                    3 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.831 |  0.196 |                    2 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176351)[0m rmse: 0.17895115911960602
[2m[36m(func pid=176351)[0m mae:  0.12302982807159424
[2m[36m(func pid=176351)[0m rmse_per_class: [0.106, 0.27, 0.043, 0.291, 0.056, 0.179, 0.292, 0.19, 0.133, 0.229]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.182095006108284
[2m[36m(func pid=186938)[0m mae:  0.12802205979824066
[2m[36m(func pid=186938)[0m rmse_per_class: [0.122, 0.27, 0.171, 0.359, 0.059, 0.188, 0.271, 0.144, 0.145, 0.091]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17782457172870636
[2m[36m(func pid=176262)[0m mae:  0.12768211960792542
[2m[36m(func pid=176262)[0m rmse_per_class: [0.139, 0.267, 0.118, 0.357, 0.058, 0.184, 0.263, 0.14, 0.157, 0.094]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8848 | Steps: 4 | Val loss: 0.4308 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4531 | Steps: 4 | Val loss: 0.3225 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=187535)[0m rmse: 0.18443647027015686
[2m[36m(func pid=187535)[0m mae:  0.13158604502677917
[2m[36m(func pid=187535)[0m rmse_per_class: [0.134, 0.27, 0.061, 0.386, 0.056, 0.213, 0.25, 0.155, 0.222, 0.096]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5098 | Steps: 4 | Val loss: 0.3629 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.7677 | Steps: 4 | Val loss: 0.4186 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 15:47:53 (running for 00:19:07.30)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.49  |  0.178 |                   47 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.453 |  0.181 |                   48 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.619 |  0.182 |                    4 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.885 |  0.184 |                    3 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176351)[0m rmse: 0.18128669261932373
[2m[36m(func pid=176351)[0m mae:  0.12383818626403809
[2m[36m(func pid=176351)[0m rmse_per_class: [0.119, 0.27, 0.043, 0.299, 0.056, 0.182, 0.288, 0.222, 0.133, 0.202]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17854973673820496
[2m[36m(func pid=176262)[0m mae:  0.1279476433992386
[2m[36m(func pid=176262)[0m rmse_per_class: [0.143, 0.267, 0.121, 0.359, 0.057, 0.185, 0.264, 0.14, 0.157, 0.094]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.18872103095054626
[2m[36m(func pid=186938)[0m mae:  0.12769004702568054
[2m[36m(func pid=186938)[0m rmse_per_class: [0.115, 0.275, 0.158, 0.372, 0.055, 0.192, 0.336, 0.15, 0.142, 0.093]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8643 | Steps: 4 | Val loss: 0.3980 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4236 | Steps: 4 | Val loss: 0.3231 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8381 | Steps: 4 | Val loss: 0.4632 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4895 | Steps: 4 | Val loss: 0.3637 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=187535)[0m rmse: 0.19227257370948792
[2m[36m(func pid=187535)[0m mae:  0.12790390849113464
[2m[36m(func pid=187535)[0m rmse_per_class: [0.104, 0.411, 0.045, 0.374, 0.056, 0.256, 0.297, 0.155, 0.134, 0.091]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:47:58 (running for 00:19:12.42)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.51  |  0.179 |                   48 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.424 |  0.183 |                   49 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.768 |  0.189 |                    5 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.864 |  0.192 |                    4 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176351)[0m rmse: 0.1827225685119629
[2m[36m(func pid=176351)[0m mae:  0.12459887564182281
[2m[36m(func pid=176351)[0m rmse_per_class: [0.126, 0.27, 0.043, 0.309, 0.056, 0.184, 0.282, 0.252, 0.133, 0.173]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.19599351286888123
[2m[36m(func pid=186938)[0m mae:  0.12897039949893951
[2m[36m(func pid=186938)[0m rmse_per_class: [0.108, 0.277, 0.124, 0.381, 0.056, 0.197, 0.432, 0.153, 0.137, 0.095]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17879852652549744
[2m[36m(func pid=176262)[0m mae:  0.12803658843040466
[2m[36m(func pid=176262)[0m rmse_per_class: [0.146, 0.267, 0.121, 0.359, 0.057, 0.185, 0.264, 0.14, 0.156, 0.094]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.7763 | Steps: 4 | Val loss: 0.5035 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4594 | Steps: 4 | Val loss: 0.3254 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8133 | Steps: 4 | Val loss: 0.4853 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4949 | Steps: 4 | Val loss: 0.3599 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=187535)[0m rmse: 0.1981315314769745
[2m[36m(func pid=187535)[0m mae:  0.13519659638404846
[2m[36m(func pid=187535)[0m rmse_per_class: [0.112, 0.288, 0.049, 0.293, 0.056, 0.215, 0.323, 0.147, 0.138, 0.362]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:48:04 (running for 00:19:17.78)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.49  |  0.179 |                   49 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.459 |  0.185 |                   50 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.838 |  0.196 |                    6 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.776 |  0.198 |                    5 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176351)[0m rmse: 0.18468722701072693
[2m[36m(func pid=176351)[0m mae:  0.125509575009346
[2m[36m(func pid=176351)[0m rmse_per_class: [0.136, 0.269, 0.043, 0.32, 0.056, 0.187, 0.276, 0.279, 0.133, 0.148]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.20092801749706268
[2m[36m(func pid=186938)[0m mae:  0.1307796835899353
[2m[36m(func pid=186938)[0m rmse_per_class: [0.104, 0.278, 0.098, 0.385, 0.056, 0.2, 0.501, 0.155, 0.136, 0.096]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17844454944133759
[2m[36m(func pid=176262)[0m mae:  0.12784603238105774
[2m[36m(func pid=176262)[0m rmse_per_class: [0.145, 0.266, 0.119, 0.359, 0.057, 0.185, 0.263, 0.14, 0.157, 0.094]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.7152 | Steps: 4 | Val loss: 0.4135 | Batch size: 32 | lr: 0.1 | Duration: 3.33s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4303 | Steps: 4 | Val loss: 0.3277 | Batch size: 32 | lr: 0.001 | Duration: 3.24s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7735 | Steps: 4 | Val loss: 0.4701 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4978 | Steps: 4 | Val loss: 0.3598 | Batch size: 32 | lr: 0.0001 | Duration: 3.34s
[2m[36m(func pid=187535)[0m rmse: 0.19535651803016663
[2m[36m(func pid=187535)[0m mae:  0.1311863362789154
[2m[36m(func pid=187535)[0m rmse_per_class: [0.111, 0.295, 0.049, 0.306, 0.056, 0.223, 0.254, 0.392, 0.177, 0.09]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:48:09 (running for 00:19:23.30)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.495 |  0.178 |                   50 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.459 |  0.185 |                   50 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.774 |  0.198 |                    8 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.715 |  0.195 |                    6 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176351)[0m rmse: 0.18564049899578094
[2m[36m(func pid=176351)[0m mae:  0.1258862018585205
[2m[36m(func pid=176351)[0m rmse_per_class: [0.148, 0.269, 0.043, 0.331, 0.056, 0.189, 0.269, 0.291, 0.134, 0.126]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.19812723994255066
[2m[36m(func pid=186938)[0m mae:  0.12891331315040588
[2m[36m(func pid=186938)[0m rmse_per_class: [0.103, 0.272, 0.087, 0.386, 0.056, 0.195, 0.493, 0.155, 0.136, 0.097]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17858216166496277
[2m[36m(func pid=176262)[0m mae:  0.127868190407753
[2m[36m(func pid=176262)[0m rmse_per_class: [0.147, 0.266, 0.12, 0.359, 0.057, 0.185, 0.262, 0.14, 0.156, 0.094]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7950 | Steps: 4 | Val loss: 0.4462 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.6763 | Steps: 4 | Val loss: 0.4397 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4354 | Steps: 4 | Val loss: 0.3283 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4952 | Steps: 4 | Val loss: 0.3601 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=187535)[0m rmse: 0.20348696410655975
[2m[36m(func pid=187535)[0m mae:  0.13193556666374207
[2m[36m(func pid=187535)[0m rmse_per_class: [0.229, 0.203, 0.044, 0.382, 0.056, 0.228, 0.25, 0.411, 0.133, 0.097]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:48:14 (running for 00:19:28.37)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.498 |  0.179 |                   51 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.43  |  0.186 |                   51 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.676 |  0.185 |                    9 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.795 |  0.203 |                    7 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.18545088171958923
[2m[36m(func pid=186938)[0m mae:  0.12356407940387726
[2m[36m(func pid=186938)[0m rmse_per_class: [0.13, 0.255, 0.078, 0.385, 0.056, 0.18, 0.372, 0.155, 0.144, 0.097]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.18503408133983612
[2m[36m(func pid=176351)[0m mae:  0.12550640106201172
[2m[36m(func pid=176351)[0m rmse_per_class: [0.154, 0.266, 0.043, 0.338, 0.055, 0.191, 0.264, 0.289, 0.136, 0.113]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17873501777648926
[2m[36m(func pid=176262)[0m mae:  0.1278604418039322
[2m[36m(func pid=176262)[0m rmse_per_class: [0.15, 0.266, 0.12, 0.36, 0.056, 0.185, 0.262, 0.14, 0.156, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8741 | Steps: 4 | Val loss: 0.5324 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7177 | Steps: 4 | Val loss: 0.4215 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4058 | Steps: 4 | Val loss: 0.3270 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4926 | Steps: 4 | Val loss: 0.3539 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=187535)[0m rmse: 0.21152964234352112
[2m[36m(func pid=187535)[0m mae:  0.13623501360416412
[2m[36m(func pid=187535)[0m rmse_per_class: [0.106, 0.679, 0.044, 0.389, 0.056, 0.207, 0.251, 0.149, 0.139, 0.097]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:48:19 (running for 00:19:33.57)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.495 |  0.179 |                   52 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.435 |  0.185 |                   52 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.718 |  0.176 |                   10 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.874 |  0.212 |                    8 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.1762242466211319
[2m[36m(func pid=186938)[0m mae:  0.1230347603559494
[2m[36m(func pid=186938)[0m rmse_per_class: [0.163, 0.24, 0.068, 0.384, 0.056, 0.174, 0.246, 0.155, 0.18, 0.097]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.1828220784664154
[2m[36m(func pid=176351)[0m mae:  0.12442763894796371
[2m[36m(func pid=176351)[0m rmse_per_class: [0.155, 0.261, 0.046, 0.344, 0.055, 0.191, 0.259, 0.273, 0.14, 0.105]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1780208945274353
[2m[36m(func pid=176262)[0m mae:  0.12752662599086761
[2m[36m(func pid=176262)[0m rmse_per_class: [0.148, 0.265, 0.116, 0.358, 0.056, 0.185, 0.261, 0.14, 0.157, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6945 | Steps: 4 | Val loss: 0.4061 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8036 | Steps: 4 | Val loss: 0.5288 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4152 | Steps: 4 | Val loss: 0.3258 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4834 | Steps: 4 | Val loss: 0.3547 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=186938)[0m rmse: 0.1803836226463318
[2m[36m(func pid=186938)[0m mae:  0.12633481621742249
[2m[36m(func pid=186938)[0m rmse_per_class: [0.116, 0.286, 0.049, 0.377, 0.056, 0.21, 0.276, 0.155, 0.183, 0.096]
[2m[36m(func pid=186938)[0m 
== Status ==
Current time: 2024-01-07 15:48:25 (running for 00:19:38.96)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.493 |  0.178 |                   53 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.406 |  0.183 |                   53 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.695 |  0.18  |                   11 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.874 |  0.212 |                    8 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187535)[0m rmse: 0.2107740193605423
[2m[36m(func pid=187535)[0m mae:  0.13470833003520966
[2m[36m(func pid=187535)[0m rmse_per_class: [0.112, 0.25, 0.049, 0.386, 0.055, 0.567, 0.3, 0.156, 0.139, 0.094]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.17995643615722656
[2m[36m(func pid=176351)[0m mae:  0.12301496416330338
[2m[36m(func pid=176351)[0m rmse_per_class: [0.148, 0.257, 0.049, 0.349, 0.055, 0.191, 0.255, 0.25, 0.146, 0.099]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1782766431570053
[2m[36m(func pid=176262)[0m mae:  0.1275797188282013
[2m[36m(func pid=176262)[0m rmse_per_class: [0.151, 0.265, 0.116, 0.359, 0.056, 0.185, 0.261, 0.141, 0.157, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.6261 | Steps: 4 | Val loss: 0.3910 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.9839 | Steps: 4 | Val loss: 0.5491 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4126 | Steps: 4 | Val loss: 0.3227 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4982 | Steps: 4 | Val loss: 0.3575 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 15:48:30 (running for 00:19:44.09)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.483 |  0.178 |                   54 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.415 |  0.18  |                   54 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.626 |  0.185 |                   12 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.804 |  0.211 |                    9 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.18460184335708618
[2m[36m(func pid=186938)[0m mae:  0.1268388032913208
[2m[36m(func pid=186938)[0m rmse_per_class: [0.097, 0.336, 0.041, 0.353, 0.056, 0.259, 0.308, 0.155, 0.146, 0.094]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.17665475606918335
[2m[36m(func pid=176351)[0m mae:  0.12137113511562347
[2m[36m(func pid=176351)[0m rmse_per_class: [0.13, 0.252, 0.05, 0.353, 0.055, 0.19, 0.252, 0.237, 0.153, 0.096]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.20153912901878357
[2m[36m(func pid=187535)[0m mae:  0.13663390278816223
[2m[36m(func pid=187535)[0m rmse_per_class: [0.111, 0.298, 0.049, 0.278, 0.067, 0.252, 0.309, 0.156, 0.135, 0.36]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17882482707500458
[2m[36m(func pid=176262)[0m mae:  0.12777909636497498
[2m[36m(func pid=176262)[0m rmse_per_class: [0.156, 0.265, 0.116, 0.36, 0.056, 0.185, 0.26, 0.141, 0.156, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5552 | Steps: 4 | Val loss: 0.3545 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4082 | Steps: 4 | Val loss: 0.3211 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8027 | Steps: 4 | Val loss: 0.5601 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4902 | Steps: 4 | Val loss: 0.3571 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 15:48:35 (running for 00:19:49.35)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.498 |  0.179 |                   55 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.413 |  0.177 |                   55 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.555 |  0.175 |                   13 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.984 |  0.202 |                   10 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.17486737668514252
[2m[36m(func pid=186938)[0m mae:  0.11958211660385132
[2m[36m(func pid=186938)[0m rmse_per_class: [0.103, 0.289, 0.042, 0.302, 0.056, 0.257, 0.321, 0.153, 0.134, 0.091]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.1737617552280426
[2m[36m(func pid=176351)[0m mae:  0.1197802796959877
[2m[36m(func pid=176351)[0m rmse_per_class: [0.12, 0.247, 0.053, 0.356, 0.054, 0.189, 0.249, 0.215, 0.161, 0.094]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.23591072857379913
[2m[36m(func pid=187535)[0m mae:  0.15756943821907043
[2m[36m(func pid=187535)[0m rmse_per_class: [0.223, 0.29, 0.049, 0.314, 0.37, 0.232, 0.31, 0.152, 0.327, 0.091]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1789732426404953
[2m[36m(func pid=176262)[0m mae:  0.12783189117908478
[2m[36m(func pid=176262)[0m rmse_per_class: [0.159, 0.265, 0.116, 0.36, 0.055, 0.185, 0.26, 0.141, 0.157, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5639 | Steps: 4 | Val loss: 0.3506 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4260 | Steps: 4 | Val loss: 0.3215 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8210 | Steps: 4 | Val loss: 0.5910 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5017 | Steps: 4 | Val loss: 0.3587 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 15:48:40 (running for 00:19:54.36)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.49  |  0.179 |                   56 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.408 |  0.174 |                   56 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.564 |  0.174 |                   14 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.803 |  0.236 |                   11 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.17412760853767395
[2m[36m(func pid=186938)[0m mae:  0.11752863973379135
[2m[36m(func pid=186938)[0m rmse_per_class: [0.108, 0.243, 0.046, 0.344, 0.056, 0.186, 0.323, 0.146, 0.133, 0.156]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.17128142714500427
[2m[36m(func pid=176351)[0m mae:  0.11842001974582672
[2m[36m(func pid=176351)[0m rmse_per_class: [0.114, 0.241, 0.06, 0.359, 0.054, 0.188, 0.247, 0.188, 0.17, 0.092]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1793912798166275
[2m[36m(func pid=176262)[0m mae:  0.12796297669410706
[2m[36m(func pid=176262)[0m rmse_per_class: [0.164, 0.264, 0.115, 0.361, 0.055, 0.185, 0.26, 0.141, 0.157, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.20534992218017578
[2m[36m(func pid=187535)[0m mae:  0.12739165127277374
[2m[36m(func pid=187535)[0m rmse_per_class: [0.098, 0.307, 0.049, 0.371, 0.356, 0.233, 0.283, 0.121, 0.139, 0.097]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5403 | Steps: 4 | Val loss: 0.3876 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4183 | Steps: 4 | Val loss: 0.3203 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4861 | Steps: 4 | Val loss: 0.3568 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.9354 | Steps: 4 | Val loss: 0.6422 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 15:48:45 (running for 00:19:59.66)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.502 |  0.179 |                   57 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.426 |  0.171 |                   57 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.54  |  0.193 |                   15 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.821 |  0.205 |                   12 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.19302663207054138
[2m[36m(func pid=186938)[0m mae:  0.13112986087799072
[2m[36m(func pid=186938)[0m rmse_per_class: [0.11, 0.274, 0.048, 0.302, 0.056, 0.181, 0.316, 0.125, 0.136, 0.382]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.16897602379322052
[2m[36m(func pid=176351)[0m mae:  0.11692792177200317
[2m[36m(func pid=176351)[0m rmse_per_class: [0.107, 0.236, 0.065, 0.36, 0.054, 0.186, 0.245, 0.17, 0.176, 0.091]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17939922213554382
[2m[36m(func pid=176262)[0m mae:  0.12793631851673126
[2m[36m(func pid=176262)[0m rmse_per_class: [0.166, 0.264, 0.114, 0.361, 0.055, 0.185, 0.26, 0.141, 0.156, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.20480695366859436
[2m[36m(func pid=187535)[0m mae:  0.12598693370819092
[2m[36m(func pid=187535)[0m rmse_per_class: [0.112, 0.358, 0.084, 0.311, 0.098, 0.233, 0.274, 0.341, 0.14, 0.097]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5991 | Steps: 4 | Val loss: 0.4128 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4135 | Steps: 4 | Val loss: 0.3196 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4837 | Steps: 4 | Val loss: 0.3504 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.1175 | Steps: 4 | Val loss: 0.6062 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 15:48:51 (running for 00:20:04.90)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.486 |  0.179 |                   58 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.418 |  0.169 |                   58 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.599 |  0.208 |                   16 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.935 |  0.205 |                   13 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.20754198729991913
[2m[36m(func pid=186938)[0m mae:  0.14057089388370514
[2m[36m(func pid=186938)[0m rmse_per_class: [0.11, 0.287, 0.048, 0.303, 0.056, 0.205, 0.288, 0.179, 0.137, 0.463]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.16652598977088928
[2m[36m(func pid=176351)[0m mae:  0.1154847964644432
[2m[36m(func pid=176351)[0m rmse_per_class: [0.102, 0.232, 0.071, 0.361, 0.053, 0.184, 0.243, 0.15, 0.179, 0.091]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17854151129722595
[2m[36m(func pid=176262)[0m mae:  0.12750133872032166
[2m[36m(func pid=176262)[0m rmse_per_class: [0.162, 0.263, 0.112, 0.359, 0.055, 0.184, 0.259, 0.141, 0.157, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.20333950221538544
[2m[36m(func pid=187535)[0m mae:  0.11828847974538803
[2m[36m(func pid=187535)[0m rmse_per_class: [0.112, 0.28, 0.163, 0.576, 0.053, 0.233, 0.247, 0.134, 0.14, 0.097]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5974 | Steps: 4 | Val loss: 0.4168 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3996 | Steps: 4 | Val loss: 0.3099 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4699 | Steps: 4 | Val loss: 0.3459 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.1155 | Steps: 4 | Val loss: 0.5518 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 15:48:56 (running for 00:20:10.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.484 |  0.179 |                   59 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.413 |  0.167 |                   59 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.597 |  0.209 |                   17 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  1.117 |  0.203 |                   14 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.20856395363807678
[2m[36m(func pid=186938)[0m mae:  0.14070430397987366
[2m[36m(func pid=186938)[0m rmse_per_class: [0.104, 0.289, 0.047, 0.35, 0.056, 0.215, 0.286, 0.367, 0.135, 0.236]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.16251584887504578
[2m[36m(func pid=176351)[0m mae:  0.11306872218847275
[2m[36m(func pid=176351)[0m rmse_per_class: [0.096, 0.229, 0.064, 0.359, 0.053, 0.18, 0.241, 0.14, 0.173, 0.091]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1780332326889038
[2m[36m(func pid=176262)[0m mae:  0.12728974223136902
[2m[36m(func pid=176262)[0m rmse_per_class: [0.161, 0.263, 0.11, 0.359, 0.055, 0.184, 0.258, 0.141, 0.157, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.18692180514335632
[2m[36m(func pid=187535)[0m mae:  0.11241382360458374
[2m[36m(func pid=187535)[0m rmse_per_class: [0.098, 0.299, 0.053, 0.395, 0.056, 0.232, 0.349, 0.152, 0.14, 0.095]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6185 | Steps: 4 | Val loss: 0.4268 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4054 | Steps: 4 | Val loss: 0.3076 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4811 | Steps: 4 | Val loss: 0.3480 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 15:49:01 (running for 00:20:15.47)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.47  |  0.178 |                   60 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.4   |  0.163 |                   60 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.619 |  0.206 |                   18 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  1.116 |  0.187 |                   15 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.20619401335716248
[2m[36m(func pid=186938)[0m mae:  0.1365123838186264
[2m[36m(func pid=186938)[0m rmse_per_class: [0.109, 0.286, 0.042, 0.374, 0.056, 0.217, 0.285, 0.462, 0.133, 0.098]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.9426 | Steps: 4 | Val loss: 0.6141 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
[2m[36m(func pid=176351)[0m rmse: 0.16103999316692352
[2m[36m(func pid=176351)[0m mae:  0.1121264100074768
[2m[36m(func pid=176351)[0m rmse_per_class: [0.095, 0.233, 0.067, 0.358, 0.054, 0.177, 0.24, 0.129, 0.168, 0.091]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1785231977701187
[2m[36m(func pid=176262)[0m mae:  0.12742644548416138
[2m[36m(func pid=176262)[0m rmse_per_class: [0.165, 0.263, 0.109, 0.359, 0.055, 0.184, 0.258, 0.141, 0.158, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.5503 | Steps: 4 | Val loss: 0.4109 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=187535)[0m rmse: 0.20756444334983826
[2m[36m(func pid=187535)[0m mae:  0.13678494095802307
[2m[36m(func pid=187535)[0m rmse_per_class: [0.258, 0.301, 0.049, 0.325, 0.056, 0.241, 0.297, 0.156, 0.174, 0.219]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4027 | Steps: 4 | Val loss: 0.3045 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4652 | Steps: 4 | Val loss: 0.3407 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 15:49:06 (running for 00:20:20.59)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.481 |  0.179 |                   61 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.405 |  0.161 |                   61 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.55  |  0.201 |                   19 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.943 |  0.208 |                   16 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.20078745484352112
[2m[36m(func pid=186938)[0m mae:  0.13215604424476624
[2m[36m(func pid=186938)[0m rmse_per_class: [0.15, 0.273, 0.042, 0.38, 0.056, 0.211, 0.255, 0.41, 0.139, 0.091]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.9549 | Steps: 4 | Val loss: 0.5300 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=176351)[0m rmse: 0.16013212502002716
[2m[36m(func pid=176351)[0m mae:  0.11122467368841171
[2m[36m(func pid=176351)[0m rmse_per_class: [0.094, 0.242, 0.067, 0.356, 0.054, 0.175, 0.238, 0.124, 0.16, 0.091]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1774357706308365
[2m[36m(func pid=176262)[0m mae:  0.12695759534835815
[2m[36m(func pid=176262)[0m rmse_per_class: [0.159, 0.263, 0.107, 0.358, 0.055, 0.184, 0.258, 0.14, 0.158, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5677 | Steps: 4 | Val loss: 0.4017 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=187535)[0m rmse: 0.2028883695602417
[2m[36m(func pid=187535)[0m mae:  0.12963734567165375
[2m[36m(func pid=187535)[0m rmse_per_class: [0.11, 0.269, 0.049, 0.285, 0.056, 0.551, 0.297, 0.152, 0.139, 0.121]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4164 | Steps: 4 | Val loss: 0.3035 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4720 | Steps: 4 | Val loss: 0.3437 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 15:49:11 (running for 00:20:25.66)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.465 |  0.177 |                   62 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.403 |  0.16  |                   62 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.568 |  0.186 |                   20 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.955 |  0.203 |                   17 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.18623046576976776
[2m[36m(func pid=186938)[0m mae:  0.12466186285018921
[2m[36m(func pid=186938)[0m rmse_per_class: [0.18, 0.237, 0.055, 0.382, 0.056, 0.192, 0.234, 0.215, 0.217, 0.094]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8137 | Steps: 4 | Val loss: 0.4483 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=176351)[0m rmse: 0.16056033968925476
[2m[36m(func pid=176351)[0m mae:  0.11109372228384018
[2m[36m(func pid=176351)[0m rmse_per_class: [0.095, 0.257, 0.069, 0.354, 0.056, 0.173, 0.239, 0.121, 0.151, 0.091]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17826279997825623
[2m[36m(func pid=176262)[0m mae:  0.12731757760047913
[2m[36m(func pid=176262)[0m rmse_per_class: [0.165, 0.262, 0.108, 0.359, 0.055, 0.184, 0.258, 0.141, 0.158, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5525 | Steps: 4 | Val loss: 0.3956 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=187535)[0m rmse: 0.1768842488527298
[2m[36m(func pid=187535)[0m mae:  0.10319235175848007
[2m[36m(func pid=187535)[0m rmse_per_class: [0.11, 0.387, 0.04, 0.281, 0.056, 0.224, 0.336, 0.103, 0.14, 0.093]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4102 | Steps: 4 | Val loss: 0.3021 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 15:49:17 (running for 00:20:30.85)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.472 |  0.178 |                   63 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.416 |  0.161 |                   63 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.552 |  0.176 |                   21 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.814 |  0.177 |                   18 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.17610374093055725
[2m[36m(func pid=186938)[0m mae:  0.11938311159610748
[2m[36m(func pid=186938)[0m rmse_per_class: [0.148, 0.21, 0.07, 0.382, 0.056, 0.166, 0.236, 0.118, 0.278, 0.096]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4628 | Steps: 4 | Val loss: 0.3442 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=176351)[0m rmse: 0.16113556921482086
[2m[36m(func pid=176351)[0m mae:  0.1109018325805664
[2m[36m(func pid=176351)[0m rmse_per_class: [0.095, 0.268, 0.071, 0.35, 0.058, 0.171, 0.241, 0.121, 0.145, 0.091]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8474 | Steps: 4 | Val loss: 0.6284 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=176262)[0m rmse: 0.17872914671897888
[2m[36m(func pid=176262)[0m mae:  0.1274777352809906
[2m[36m(func pid=176262)[0m rmse_per_class: [0.17, 0.262, 0.108, 0.359, 0.055, 0.184, 0.258, 0.141, 0.157, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6185 | Steps: 4 | Val loss: 0.3993 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3937 | Steps: 4 | Val loss: 0.2967 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=187535)[0m rmse: 0.2055455893278122
[2m[36m(func pid=187535)[0m mae:  0.12222988903522491
[2m[36m(func pid=187535)[0m rmse_per_class: [0.118, 0.451, 0.063, 0.32, 0.056, 0.233, 0.278, 0.303, 0.136, 0.097]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:49:22 (running for 00:20:36.16)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.463 |  0.179 |                   64 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.41  |  0.161 |                   64 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.619 |  0.183 |                   22 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.847 |  0.206 |                   19 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.18305562436580658
[2m[36m(func pid=186938)[0m mae:  0.12210956960916519
[2m[36m(func pid=186938)[0m rmse_per_class: [0.102, 0.284, 0.096, 0.382, 0.056, 0.181, 0.267, 0.14, 0.227, 0.097]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4569 | Steps: 4 | Val loss: 0.3402 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=176351)[0m rmse: 0.1605890691280365
[2m[36m(func pid=176351)[0m mae:  0.11048565059900284
[2m[36m(func pid=176351)[0m rmse_per_class: [0.095, 0.274, 0.064, 0.343, 0.062, 0.171, 0.244, 0.122, 0.14, 0.091]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.9583 | Steps: 4 | Val loss: 0.6776 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=176262)[0m rmse: 0.17816409468650818
[2m[36m(func pid=176262)[0m mae:  0.12723693251609802
[2m[36m(func pid=176262)[0m rmse_per_class: [0.168, 0.262, 0.106, 0.358, 0.055, 0.184, 0.257, 0.141, 0.158, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5631 | Steps: 4 | Val loss: 0.3929 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4000 | Steps: 4 | Val loss: 0.2922 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=187535)[0m rmse: 0.21722492575645447
[2m[36m(func pid=187535)[0m mae:  0.14251907169818878
[2m[36m(func pid=187535)[0m rmse_per_class: [0.11, 0.231, 0.106, 0.308, 0.056, 0.233, 0.311, 0.109, 0.611, 0.096]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:49:27 (running for 00:20:41.38)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.457 |  0.178 |                   65 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.394 |  0.161 |                   65 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.563 |  0.191 |                   23 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.958 |  0.217 |                   20 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.19073712825775146
[2m[36m(func pid=186938)[0m mae:  0.1241530030965805
[2m[36m(func pid=186938)[0m rmse_per_class: [0.097, 0.36, 0.102, 0.379, 0.056, 0.24, 0.28, 0.15, 0.146, 0.097]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4570 | Steps: 4 | Val loss: 0.3367 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=176351)[0m rmse: 0.16018013656139374
[2m[36m(func pid=176351)[0m mae:  0.11026646941900253
[2m[36m(func pid=176351)[0m rmse_per_class: [0.095, 0.272, 0.06, 0.335, 0.069, 0.172, 0.25, 0.123, 0.136, 0.091]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.9149 | Steps: 4 | Val loss: 0.6196 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=176262)[0m rmse: 0.17769132554531097
[2m[36m(func pid=176262)[0m mae:  0.1270332783460617
[2m[36m(func pid=176262)[0m rmse_per_class: [0.166, 0.261, 0.104, 0.358, 0.055, 0.184, 0.257, 0.14, 0.159, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5319 | Steps: 4 | Val loss: 0.3758 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3939 | Steps: 4 | Val loss: 0.2875 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=187535)[0m rmse: 0.184739351272583
[2m[36m(func pid=187535)[0m mae:  0.1091700941324234
[2m[36m(func pid=187535)[0m rmse_per_class: [0.11, 0.272, 0.065, 0.286, 0.058, 0.233, 0.448, 0.113, 0.136, 0.127]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:49:32 (running for 00:20:46.56)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.457 |  0.178 |                   66 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.4   |  0.16  |                   66 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.532 |  0.191 |                   24 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.915 |  0.185 |                   21 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.19085395336151123
[2m[36m(func pid=186938)[0m mae:  0.12254288047552109
[2m[36m(func pid=186938)[0m rmse_per_class: [0.103, 0.336, 0.092, 0.367, 0.056, 0.297, 0.273, 0.153, 0.133, 0.097]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4686 | Steps: 4 | Val loss: 0.3388 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=176351)[0m rmse: 0.15886281430721283
[2m[36m(func pid=176351)[0m mae:  0.10973381996154785
[2m[36m(func pid=176351)[0m rmse_per_class: [0.094, 0.263, 0.053, 0.325, 0.076, 0.174, 0.253, 0.123, 0.135, 0.092]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.9345 | Steps: 4 | Val loss: 0.6209 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=176262)[0m rmse: 0.17848309874534607
[2m[36m(func pid=176262)[0m mae:  0.12734642624855042
[2m[36m(func pid=176262)[0m rmse_per_class: [0.172, 0.261, 0.106, 0.358, 0.054, 0.184, 0.257, 0.14, 0.159, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5394 | Steps: 4 | Val loss: 0.3337 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3918 | Steps: 4 | Val loss: 0.2849 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 15:49:37 (running for 00:20:51.66)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.469 |  0.178 |                   67 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.394 |  0.159 |                   67 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.539 |  0.174 |                   25 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.915 |  0.185 |                   21 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.17370393872261047
[2m[36m(func pid=186938)[0m mae:  0.11158089339733124
[2m[36m(func pid=186938)[0m rmse_per_class: [0.103, 0.223, 0.08, 0.323, 0.054, 0.327, 0.243, 0.154, 0.133, 0.097]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.2084008753299713
[2m[36m(func pid=187535)[0m mae:  0.12273800373077393
[2m[36m(func pid=187535)[0m rmse_per_class: [0.106, 0.298, 0.049, 0.323, 0.142, 0.233, 0.252, 0.112, 0.138, 0.432]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4496 | Steps: 4 | Val loss: 0.3344 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=176351)[0m rmse: 0.15788687765598297
[2m[36m(func pid=176351)[0m mae:  0.10909056663513184
[2m[36m(func pid=176351)[0m rmse_per_class: [0.093, 0.253, 0.051, 0.316, 0.082, 0.176, 0.256, 0.124, 0.134, 0.093]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5079 | Steps: 4 | Val loss: 0.3043 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=176262)[0m rmse: 0.1776655614376068
[2m[36m(func pid=176262)[0m mae:  0.1269833743572235
[2m[36m(func pid=176262)[0m rmse_per_class: [0.168, 0.261, 0.103, 0.357, 0.055, 0.184, 0.257, 0.14, 0.159, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.7992 | Steps: 4 | Val loss: 0.6182 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4019 | Steps: 4 | Val loss: 0.2854 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 15:49:43 (running for 00:20:56.89)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.45  |  0.178 |                   68 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.392 |  0.158 |                   68 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.508 |  0.155 |                   26 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.935 |  0.208 |                   22 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.15486569702625275
[2m[36m(func pid=186938)[0m mae:  0.09974811971187592
[2m[36m(func pid=186938)[0m rmse_per_class: [0.103, 0.224, 0.042, 0.269, 0.052, 0.232, 0.243, 0.154, 0.133, 0.097]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.23767820000648499
[2m[36m(func pid=187535)[0m mae:  0.14702513813972473
[2m[36m(func pid=187535)[0m rmse_per_class: [0.258, 0.282, 0.049, 0.306, 0.217, 0.23, 0.325, 0.13, 0.132, 0.449]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4485 | Steps: 4 | Val loss: 0.3317 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=176351)[0m rmse: 0.15870466828346252
[2m[36m(func pid=176351)[0m mae:  0.10952669382095337
[2m[36m(func pid=176351)[0m rmse_per_class: [0.094, 0.244, 0.054, 0.306, 0.092, 0.179, 0.265, 0.126, 0.133, 0.095]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5306 | Steps: 4 | Val loss: 0.3006 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=176262)[0m rmse: 0.17726434767246246
[2m[36m(func pid=176262)[0m mae:  0.12683692574501038
[2m[36m(func pid=176262)[0m rmse_per_class: [0.166, 0.26, 0.101, 0.356, 0.054, 0.184, 0.257, 0.14, 0.16, 0.094]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7863 | Steps: 4 | Val loss: 0.5748 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3774 | Steps: 4 | Val loss: 0.2845 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 15:49:48 (running for 00:21:02.18)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.449 |  0.177 |                   69 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.402 |  0.159 |                   69 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.531 |  0.152 |                   27 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.799 |  0.238 |                   23 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.15219059586524963
[2m[36m(func pid=186938)[0m mae:  0.09771870821714401
[2m[36m(func pid=186938)[0m rmse_per_class: [0.096, 0.25, 0.047, 0.275, 0.063, 0.168, 0.236, 0.153, 0.137, 0.096]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4552 | Steps: 4 | Val loss: 0.3344 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=187535)[0m rmse: 0.17860332131385803
[2m[36m(func pid=187535)[0m mae:  0.11305824667215347
[2m[36m(func pid=187535)[0m rmse_per_class: [0.097, 0.237, 0.049, 0.323, 0.204, 0.222, 0.28, 0.103, 0.18, 0.09]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.15863831341266632
[2m[36m(func pid=176351)[0m mae:  0.1096428632736206
[2m[36m(func pid=176351)[0m rmse_per_class: [0.097, 0.236, 0.049, 0.296, 0.102, 0.181, 0.269, 0.127, 0.133, 0.097]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5100 | Steps: 4 | Val loss: 0.2998 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=176262)[0m rmse: 0.17827634513378143
[2m[36m(func pid=176262)[0m mae:  0.12722168862819672
[2m[36m(func pid=176262)[0m rmse_per_class: [0.174, 0.26, 0.103, 0.358, 0.054, 0.184, 0.256, 0.14, 0.16, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.8315 | Steps: 4 | Val loss: 0.4293 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3840 | Steps: 4 | Val loss: 0.2838 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 15:49:53 (running for 00:21:07.51)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.455 |  0.178 |                   70 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.377 |  0.159 |                   70 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.51  |  0.162 |                   28 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.786 |  0.179 |                   24 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.1622035652399063
[2m[36m(func pid=186938)[0m mae:  0.10553721338510513
[2m[36m(func pid=186938)[0m rmse_per_class: [0.119, 0.253, 0.048, 0.273, 0.136, 0.177, 0.224, 0.15, 0.151, 0.093]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4509 | Steps: 4 | Val loss: 0.3323 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=187535)[0m rmse: 0.1664617508649826
[2m[36m(func pid=187535)[0m mae:  0.09821002185344696
[2m[36m(func pid=187535)[0m rmse_per_class: [0.104, 0.277, 0.032, 0.28, 0.169, 0.235, 0.238, 0.102, 0.137, 0.09]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176351)[0m rmse: 0.15895819664001465
[2m[36m(func pid=176351)[0m mae:  0.10974416881799698
[2m[36m(func pid=176351)[0m rmse_per_class: [0.101, 0.235, 0.042, 0.29, 0.11, 0.183, 0.27, 0.126, 0.132, 0.101]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4363 | Steps: 4 | Val loss: 0.3309 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=176262)[0m rmse: 0.17818813025951385
[2m[36m(func pid=176262)[0m mae:  0.1272435486316681
[2m[36m(func pid=176262)[0m rmse_per_class: [0.175, 0.26, 0.103, 0.357, 0.054, 0.183, 0.257, 0.14, 0.159, 0.093]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.6493 | Steps: 4 | Val loss: 0.4982 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3857 | Steps: 4 | Val loss: 0.2852 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 15:49:59 (running for 00:21:13.00)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.451 |  0.178 |                   71 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.384 |  0.159 |                   71 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.436 |  0.193 |                   29 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.832 |  0.166 |                   25 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.19283738732337952
[2m[36m(func pid=186938)[0m mae:  0.12610319256782532
[2m[36m(func pid=186938)[0m rmse_per_class: [0.242, 0.256, 0.047, 0.291, 0.238, 0.194, 0.258, 0.137, 0.177, 0.088]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4531 | Steps: 4 | Val loss: 0.3325 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=176351)[0m rmse: 0.16070371866226196
[2m[36m(func pid=176351)[0m mae:  0.11071447283029556
[2m[36m(func pid=176351)[0m rmse_per_class: [0.105, 0.239, 0.04, 0.285, 0.118, 0.183, 0.273, 0.126, 0.132, 0.105]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.1994340866804123
[2m[36m(func pid=187535)[0m mae:  0.10979970544576645
[2m[36m(func pid=187535)[0m rmse_per_class: [0.102, 0.279, 0.155, 0.41, 0.231, 0.209, 0.254, 0.129, 0.137, 0.09]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17860110104084015
[2m[36m(func pid=176262)[0m mae:  0.12748849391937256
[2m[36m(func pid=176262)[0m rmse_per_class: [0.177, 0.26, 0.104, 0.357, 0.054, 0.183, 0.257, 0.14, 0.159, 0.094]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4520 | Steps: 4 | Val loss: 0.3821 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3825 | Steps: 4 | Val loss: 0.2879 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.8485 | Steps: 4 | Val loss: 0.7439 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 15:50:04 (running for 00:21:18.30)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.453 |  0.179 |                   72 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.386 |  0.161 |                   72 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.452 |  0.211 |                   30 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.649 |  0.199 |                   26 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.21129150688648224
[2m[36m(func pid=186938)[0m mae:  0.14370490610599518
[2m[36m(func pid=186938)[0m rmse_per_class: [0.233, 0.248, 0.045, 0.349, 0.287, 0.21, 0.304, 0.113, 0.171, 0.153]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4333 | Steps: 4 | Val loss: 0.3335 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=176351)[0m rmse: 0.1631140410900116
[2m[36m(func pid=176351)[0m mae:  0.11204306781291962
[2m[36m(func pid=176351)[0m rmse_per_class: [0.108, 0.246, 0.04, 0.284, 0.127, 0.184, 0.273, 0.125, 0.132, 0.111]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.22983166575431824
[2m[36m(func pid=187535)[0m mae:  0.14599239826202393
[2m[36m(func pid=187535)[0m rmse_per_class: [0.576, 0.301, 0.129, 0.326, 0.087, 0.233, 0.259, 0.131, 0.147, 0.11]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17921674251556396
[2m[36m(func pid=176262)[0m mae:  0.12775678932666779
[2m[36m(func pid=176262)[0m rmse_per_class: [0.183, 0.259, 0.105, 0.358, 0.054, 0.183, 0.257, 0.14, 0.159, 0.094]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4948 | Steps: 4 | Val loss: 0.4194 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3951 | Steps: 4 | Val loss: 0.2913 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.0435 | Steps: 4 | Val loss: 0.7516 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 15:50:09 (running for 00:21:23.64)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.433 |  0.179 |                   73 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.383 |  0.163 |                   73 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.495 |  0.212 |                   31 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.848 |  0.23  |                   27 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.21160784363746643
[2m[36m(func pid=186938)[0m mae:  0.1465904712677002
[2m[36m(func pid=186938)[0m rmse_per_class: [0.115, 0.24, 0.04, 0.371, 0.252, 0.214, 0.319, 0.098, 0.151, 0.316]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4368 | Steps: 4 | Val loss: 0.3319 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=176351)[0m rmse: 0.16547466814517975
[2m[36m(func pid=176351)[0m mae:  0.1136176735162735
[2m[36m(func pid=176351)[0m rmse_per_class: [0.111, 0.251, 0.041, 0.285, 0.134, 0.182, 0.275, 0.125, 0.132, 0.118]
[2m[36m(func pid=176351)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.21045970916748047
[2m[36m(func pid=187535)[0m mae:  0.13796201348304749
[2m[36m(func pid=187535)[0m rmse_per_class: [0.108, 0.3, 0.034, 0.353, 0.053, 0.233, 0.359, 0.147, 0.379, 0.139]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.1790928840637207
[2m[36m(func pid=176262)[0m mae:  0.12770196795463562
[2m[36m(func pid=176262)[0m rmse_per_class: [0.182, 0.259, 0.104, 0.357, 0.054, 0.183, 0.257, 0.14, 0.159, 0.094]
[2m[36m(func pid=176262)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4688 | Steps: 4 | Val loss: 0.4116 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=176351)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3814 | Steps: 4 | Val loss: 0.2948 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.9079 | Steps: 4 | Val loss: 0.5840 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 15:50:15 (running for 00:21:28.98)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00008 | RUNNING    | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.437 |  0.179 |                   74 |
| train_10f5e_00009 | RUNNING    | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.395 |  0.165 |                   74 |
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.469 |  0.207 |                   32 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  1.044 |  0.21  |                   28 |
| train_10f5e_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186938)[0m rmse: 0.20679736137390137
[2m[36m(func pid=186938)[0m mae:  0.14130252599716187
[2m[36m(func pid=186938)[0m rmse_per_class: [0.095, 0.224, 0.047, 0.374, 0.218, 0.213, 0.321, 0.159, 0.145, 0.272]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=176262)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4287 | Steps: 4 | Val loss: 0.3290 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=176351)[0m rmse: 0.16754963994026184
[2m[36m(func pid=176351)[0m mae:  0.11499323695898056
[2m[36m(func pid=176351)[0m rmse_per_class: [0.116, 0.256, 0.041, 0.285, 0.138, 0.18, 0.275, 0.124, 0.132, 0.127]
[2m[36m(func pid=187535)[0m rmse: 0.17912225425243378
[2m[36m(func pid=187535)[0m mae:  0.10548211634159088
[2m[36m(func pid=187535)[0m rmse_per_class: [0.11, 0.267, 0.049, 0.402, 0.052, 0.231, 0.295, 0.127, 0.141, 0.116]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=176262)[0m rmse: 0.17837780714035034
[2m[36m(func pid=176262)[0m mae:  0.12743090093135834
[2m[36m(func pid=176262)[0m rmse_per_class: [0.177, 0.259, 0.103, 0.356, 0.054, 0.183, 0.258, 0.14, 0.16, 0.094]
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4688 | Steps: 4 | Val loss: 0.3832 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.9232 | Steps: 4 | Val loss: 0.4207 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=186938)[0m rmse: 0.19917502999305725
[2m[36m(func pid=186938)[0m mae:  0.13203111290931702
[2m[36m(func pid=186938)[0m rmse_per_class: [0.097, 0.244, 0.08, 0.372, 0.177, 0.204, 0.308, 0.256, 0.144, 0.109]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.17052873969078064
[2m[36m(func pid=187535)[0m mae:  0.09863002598285675
[2m[36m(func pid=187535)[0m rmse_per_class: [0.1, 0.229, 0.049, 0.447, 0.05, 0.179, 0.27, 0.147, 0.138, 0.097]
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4770 | Steps: 4 | Val loss: 0.3513 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=7413)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7413)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=7413)[0m Configuration completed!
[2m[36m(func pid=7413)[0m New optimizer parameters:
[2m[36m(func pid=7413)[0m SGD (
[2m[36m(func pid=7413)[0m Parameter Group 0
[2m[36m(func pid=7413)[0m     dampening: 0
[2m[36m(func pid=7413)[0m     differentiable: False
[2m[36m(func pid=7413)[0m     foreach: None
[2m[36m(func pid=7413)[0m     lr: 0.0001
[2m[36m(func pid=7413)[0m     maximize: False
[2m[36m(func pid=7413)[0m     momentum: 0.9
[2m[36m(func pid=7413)[0m     nesterov: False
[2m[36m(func pid=7413)[0m     weight_decay: 0.0001
[2m[36m(func pid=7413)[0m )
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.1893542855978012
[2m[36m(func pid=186938)[0m mae:  0.12136363983154297
[2m[36m(func pid=186938)[0m rmse_per_class: [0.098, 0.26, 0.093, 0.365, 0.127, 0.187, 0.282, 0.255, 0.139, 0.088]
== Status ==
Current time: 2024-01-07 15:50:20 (running for 00:21:34.27)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.469 |  0.199 |                   33 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.908 |  0.179 |                   29 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


== Status ==
Current time: 2024-01-07 15:50:27 (running for 00:21:40.95)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.469 |  0.199 |                   33 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.923 |  0.171 |                   30 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7482)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=7482)[0m Configuration completed!
[2m[36m(func pid=7482)[0m New optimizer parameters:
[2m[36m(func pid=7482)[0m SGD (
[2m[36m(func pid=7482)[0m Parameter Group 0
[2m[36m(func pid=7482)[0m     dampening: 0
[2m[36m(func pid=7482)[0m     differentiable: False
[2m[36m(func pid=7482)[0m     foreach: None
[2m[36m(func pid=7482)[0m     lr: 0.001
[2m[36m(func pid=7482)[0m     maximize: False
[2m[36m(func pid=7482)[0m     momentum: 0.9
[2m[36m(func pid=7482)[0m     nesterov: False
[2m[36m(func pid=7482)[0m     weight_decay: 0.0001
[2m[36m(func pid=7482)[0m )
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0441 | Steps: 4 | Val loss: 0.7194 | Batch size: 32 | lr: 0.0001 | Duration: 4.55s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4278 | Steps: 4 | Val loss: 0.3100 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.6268 | Steps: 4 | Val loss: 0.6158 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=7413)[0m rmse: 0.1803976595401764
[2m[36m(func pid=7413)[0m mae:  0.1328689157962799
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.264, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0233 | Steps: 4 | Val loss: 0.7051 | Batch size: 32 | lr: 0.001 | Duration: 4.76s
[2m[36m(func pid=7413)[0m 
== Status ==
Current time: 2024-01-07 15:50:32 (running for 00:21:46.15)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.428 |  0.167 |                   35 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.923 |  0.171 |                   30 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  1.044 |  0.18  |                    1 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.16706901788711548
[2m[36m(func pid=186938)[0m mae:  0.1065957099199295
[2m[36m(func pid=186938)[0m rmse_per_class: [0.1, 0.224, 0.118, 0.347, 0.081, 0.167, 0.244, 0.156, 0.144, 0.091]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.17937202751636505
[2m[36m(func pid=187535)[0m mae:  0.11156518757343292
[2m[36m(func pid=187535)[0m rmse_per_class: [0.214, 0.246, 0.048, 0.359, 0.054, 0.259, 0.284, 0.106, 0.132, 0.092]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.18036460876464844
[2m[36m(func pid=7482)[0m mae:  0.13281890749931335
[2m[36m(func pid=7482)[0m rmse_per_class: [0.114, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0313 | Steps: 4 | Val loss: 0.7282 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4524 | Steps: 4 | Val loss: 0.2862 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7844 | Steps: 4 | Val loss: 0.5326 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=7413)[0m rmse: 0.1808735579252243
[2m[36m(func pid=7413)[0m mae:  0.1332804560661316
[2m[36m(func pid=7413)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.106, 0.192, 0.302, 0.142, 0.143, 0.115]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.9263 | Steps: 4 | Val loss: 0.6754 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 15:50:37 (running for 00:21:51.43)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.452 |  0.151 |                   36 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.627 |  0.179 |                   31 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  1.031 |  0.181 |                    2 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  1.023 |  0.18  |                    1 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.15103557705879211
[2m[36m(func pid=186938)[0m mae:  0.09845917671918869
[2m[36m(func pid=186938)[0m rmse_per_class: [0.122, 0.203, 0.082, 0.319, 0.056, 0.166, 0.227, 0.102, 0.141, 0.093]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.19339796900749207
[2m[36m(func pid=187535)[0m mae:  0.12070516496896744
[2m[36m(func pid=187535)[0m rmse_per_class: [0.092, 0.286, 0.034, 0.366, 0.1, 0.211, 0.307, 0.163, 0.265, 0.109]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.18061497807502747
[2m[36m(func pid=7482)[0m mae:  0.13305237889289856
[2m[36m(func pid=7482)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.106, 0.191, 0.3, 0.141, 0.143, 0.115]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0172 | Steps: 4 | Val loss: 0.7316 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4141 | Steps: 4 | Val loss: 0.2838 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.6980 | Steps: 4 | Val loss: 0.5953 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 15:50:42 (running for 00:21:56.45)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.452 |  0.151 |                   36 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.784 |  0.193 |                   32 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  1.017 |  0.181 |                    3 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.926 |  0.181 |                    2 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.1809299737215042
[2m[36m(func pid=7413)[0m mae:  0.13333472609519958
[2m[36m(func pid=7413)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.107, 0.191, 0.303, 0.142, 0.142, 0.116]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.1515359729528427
[2m[36m(func pid=186938)[0m mae:  0.09968896210193634
[2m[36m(func pid=186938)[0m rmse_per_class: [0.162, 0.223, 0.05, 0.29, 0.052, 0.177, 0.225, 0.106, 0.138, 0.094]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7933 | Steps: 4 | Val loss: 0.6180 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=187535)[0m rmse: 0.23857030272483826
[2m[36m(func pid=187535)[0m mae:  0.14058026671409607
[2m[36m(func pid=187535)[0m rmse_per_class: [0.111, 0.255, 0.235, 0.292, 0.174, 0.228, 0.304, 0.245, 0.134, 0.407]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.18055395781993866
[2m[36m(func pid=7482)[0m mae:  0.13300977647304535
[2m[36m(func pid=7482)[0m rmse_per_class: [0.114, 0.264, 0.101, 0.331, 0.107, 0.191, 0.301, 0.14, 0.143, 0.114]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.9952 | Steps: 4 | Val loss: 0.7304 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4692 | Steps: 4 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.7237 | Steps: 4 | Val loss: 0.5762 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 15:50:48 (running for 00:22:01.76)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.414 |  0.152 |                   37 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.698 |  0.239 |                   33 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.995 |  0.181 |                    4 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.793 |  0.181 |                    3 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.18105646967887878
[2m[36m(func pid=7413)[0m mae:  0.13343176245689392
[2m[36m(func pid=7413)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.108, 0.191, 0.304, 0.143, 0.142, 0.116]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.15431973338127136
[2m[36m(func pid=186938)[0m mae:  0.10139777511358261
[2m[36m(func pid=186938)[0m rmse_per_class: [0.179, 0.23, 0.039, 0.268, 0.052, 0.185, 0.24, 0.122, 0.136, 0.093]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6755 | Steps: 4 | Val loss: 0.5495 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=187535)[0m rmse: 0.18942639231681824
[2m[36m(func pid=187535)[0m mae:  0.110784150660038
[2m[36m(func pid=187535)[0m rmse_per_class: [0.103, 0.279, 0.03, 0.288, 0.247, 0.233, 0.331, 0.11, 0.133, 0.14]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.18024978041648865
[2m[36m(func pid=7482)[0m mae:  0.1327347755432129
[2m[36m(func pid=7482)[0m rmse_per_class: [0.114, 0.264, 0.102, 0.33, 0.107, 0.19, 0.3, 0.14, 0.143, 0.112]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4202 | Steps: 4 | Val loss: 0.2847 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.9667 | Steps: 4 | Val loss: 0.7221 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8035 | Steps: 4 | Val loss: 0.6524 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 15:50:53 (running for 00:22:06.99)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.42  |  0.155 |                   39 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.724 |  0.189 |                   34 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.995 |  0.181 |                    4 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.676 |  0.18  |                    4 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.15545226633548737
[2m[36m(func pid=186938)[0m mae:  0.10233800113201141
[2m[36m(func pid=186938)[0m rmse_per_class: [0.153, 0.223, 0.043, 0.271, 0.052, 0.181, 0.267, 0.129, 0.144, 0.09]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18102428317070007
[2m[36m(func pid=7413)[0m mae:  0.1334156095981598
[2m[36m(func pid=7413)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.109, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5922 | Steps: 4 | Val loss: 0.4889 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=187535)[0m rmse: 0.22155146300792694
[2m[36m(func pid=187535)[0m mae:  0.13199448585510254
[2m[36m(func pid=187535)[0m rmse_per_class: [0.29, 0.27, 0.049, 0.335, 0.295, 0.233, 0.249, 0.117, 0.287, 0.092]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.1799909770488739
[2m[36m(func pid=7482)[0m mae:  0.1324652135372162
[2m[36m(func pid=7482)[0m rmse_per_class: [0.115, 0.264, 0.103, 0.331, 0.107, 0.19, 0.299, 0.139, 0.144, 0.11]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4262 | Steps: 4 | Val loss: 0.2892 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.9379 | Steps: 4 | Val loss: 0.7109 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 15:50:58 (running for 00:22:12.22)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.426 |  0.154 |                   40 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.803 |  0.222 |                   35 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.967 |  0.181 |                    5 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.592 |  0.18  |                    5 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.15392231941223145
[2m[36m(func pid=186938)[0m mae:  0.10225115716457367
[2m[36m(func pid=186938)[0m rmse_per_class: [0.111, 0.217, 0.045, 0.268, 0.053, 0.168, 0.274, 0.124, 0.19, 0.09]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.181076318025589
[2m[36m(func pid=7413)[0m mae:  0.1334611475467682
[2m[36m(func pid=7413)[0m rmse_per_class: [0.113, 0.264, 0.098, 0.329, 0.109, 0.19, 0.306, 0.143, 0.142, 0.116]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.0953 | Steps: 4 | Val loss: 0.6090 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5313 | Steps: 4 | Val loss: 0.4416 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=187535)[0m rmse: 0.22283264994621277
[2m[36m(func pid=187535)[0m mae:  0.13379347324371338
[2m[36m(func pid=187535)[0m rmse_per_class: [0.185, 0.24, 0.049, 0.31, 0.194, 0.232, 0.293, 0.13, 0.498, 0.097]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4205 | Steps: 4 | Val loss: 0.3056 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.9168 | Steps: 4 | Val loss: 0.6983 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=7482)[0m rmse: 0.1796540766954422
[2m[36m(func pid=7482)[0m mae:  0.13214489817619324
[2m[36m(func pid=7482)[0m rmse_per_class: [0.115, 0.264, 0.104, 0.331, 0.105, 0.189, 0.297, 0.138, 0.145, 0.108]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.16038855910301208
[2m[36m(func pid=186938)[0m mae:  0.10776223987340927
[2m[36m(func pid=186938)[0m rmse_per_class: [0.104, 0.205, 0.043, 0.267, 0.053, 0.164, 0.282, 0.125, 0.228, 0.133]
[2m[36m(func pid=186938)[0m 
== Status ==
Current time: 2024-01-07 15:51:03 (running for 00:22:17.67)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.42  |  0.16  |                   41 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  1.095 |  0.223 |                   36 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.938 |  0.181 |                    6 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.531 |  0.18  |                    6 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.18100446462631226
[2m[36m(func pid=7413)[0m mae:  0.13338737189769745
[2m[36m(func pid=7413)[0m rmse_per_class: [0.113, 0.264, 0.097, 0.328, 0.109, 0.19, 0.306, 0.144, 0.142, 0.116]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7504 | Steps: 4 | Val loss: 0.4739 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4980 | Steps: 4 | Val loss: 0.4056 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4466 | Steps: 4 | Val loss: 0.3191 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=187535)[0m rmse: 0.17203959822654724
[2m[36m(func pid=187535)[0m mae:  0.10295126587152481
[2m[36m(func pid=187535)[0m rmse_per_class: [0.111, 0.246, 0.048, 0.475, 0.06, 0.174, 0.261, 0.112, 0.138, 0.095]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8962 | Steps: 4 | Val loss: 0.6845 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=7482)[0m rmse: 0.1790471374988556
[2m[36m(func pid=7482)[0m mae:  0.13163061439990997
[2m[36m(func pid=7482)[0m rmse_per_class: [0.115, 0.264, 0.105, 0.331, 0.102, 0.188, 0.295, 0.138, 0.145, 0.107]
[2m[36m(func pid=7482)[0m 
== Status ==
Current time: 2024-01-07 15:51:09 (running for 00:22:22.90)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.447 |  0.173 |                   42 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.75  |  0.172 |                   37 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.917 |  0.181 |                    7 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.498 |  0.179 |                    7 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.17252354323863983
[2m[36m(func pid=186938)[0m mae:  0.11649830639362335
[2m[36m(func pid=186938)[0m rmse_per_class: [0.108, 0.209, 0.041, 0.301, 0.053, 0.173, 0.275, 0.118, 0.187, 0.262]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18101951479911804
[2m[36m(func pid=7413)[0m mae:  0.13341301679611206
[2m[36m(func pid=7413)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.328, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.8302 | Steps: 4 | Val loss: 0.5664 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4754 | Steps: 4 | Val loss: 0.3814 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3943 | Steps: 4 | Val loss: 0.3409 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8709 | Steps: 4 | Val loss: 0.6717 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=187535)[0m rmse: 0.16074910759925842
[2m[36m(func pid=187535)[0m mae:  0.09794411063194275
[2m[36m(func pid=187535)[0m rmse_per_class: [0.112, 0.267, 0.032, 0.321, 0.05, 0.239, 0.257, 0.099, 0.14, 0.091]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.1786166876554489
[2m[36m(func pid=7482)[0m mae:  0.13126301765441895
[2m[36m(func pid=7482)[0m rmse_per_class: [0.116, 0.264, 0.105, 0.332, 0.1, 0.188, 0.293, 0.137, 0.145, 0.106]
[2m[36m(func pid=7482)[0m 
== Status ==
Current time: 2024-01-07 15:51:14 (running for 00:22:28.29)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.394 |  0.186 |                   43 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.83  |  0.161 |                   38 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.896 |  0.181 |                    8 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.475 |  0.179 |                    8 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.18574407696723938
[2m[36m(func pid=186938)[0m mae:  0.12610165774822235
[2m[36m(func pid=186938)[0m rmse_per_class: [0.112, 0.215, 0.038, 0.335, 0.054, 0.191, 0.276, 0.123, 0.143, 0.371]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18100091814994812
[2m[36m(func pid=7413)[0m mae:  0.13339032232761383
[2m[36m(func pid=7413)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4616 | Steps: 4 | Val loss: 0.3652 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.7294 | Steps: 4 | Val loss: 0.5038 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4328 | Steps: 4 | Val loss: 0.3375 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8540 | Steps: 4 | Val loss: 0.6578 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=187535)[0m rmse: 0.22263653576374054
[2m[36m(func pid=187535)[0m mae:  0.12983207404613495
[2m[36m(func pid=187535)[0m rmse_per_class: [0.108, 0.256, 0.273, 0.367, 0.089, 0.248, 0.325, 0.223, 0.136, 0.202]
[2m[36m(func pid=7482)[0m rmse: 0.17817220091819763
[2m[36m(func pid=7482)[0m mae:  0.13086792826652527
[2m[36m(func pid=7482)[0m rmse_per_class: [0.117, 0.264, 0.105, 0.333, 0.097, 0.188, 0.29, 0.137, 0.146, 0.105]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:51:19 (running for 00:22:33.53)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.394 |  0.186 |                   43 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.729 |  0.223 |                   39 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.854 |  0.181 |                   10 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.462 |  0.178 |                    9 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.18102185428142548
[2m[36m(func pid=7413)[0m mae:  0.13340803980827332
[2m[36m(func pid=7413)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.329, 0.108, 0.19, 0.305, 0.144, 0.142, 0.118]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.18229462206363678
[2m[36m(func pid=186938)[0m mae:  0.12174315750598907
[2m[36m(func pid=186938)[0m rmse_per_class: [0.135, 0.222, 0.035, 0.342, 0.053, 0.196, 0.253, 0.12, 0.134, 0.333]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4470 | Steps: 4 | Val loss: 0.3545 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.7587 | Steps: 4 | Val loss: 0.6595 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8246 | Steps: 4 | Val loss: 0.6426 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4539 | Steps: 4 | Val loss: 0.3088 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=7482)[0m rmse: 0.1778096854686737
[2m[36m(func pid=7482)[0m mae:  0.13054123520851135
[2m[36m(func pid=7482)[0m rmse_per_class: [0.117, 0.265, 0.104, 0.334, 0.095, 0.188, 0.288, 0.137, 0.146, 0.105]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.25041690468788147
[2m[36m(func pid=187535)[0m mae:  0.1516828089952469
[2m[36m(func pid=187535)[0m rmse_per_class: [0.264, 0.267, 0.116, 0.343, 0.249, 0.229, 0.336, 0.172, 0.298, 0.23]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:51:24 (running for 00:22:38.61)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.433 |  0.182 |                   44 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.759 |  0.25  |                   40 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.825 |  0.181 |                   11 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.447 |  0.178 |                   10 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.18100109696388245
[2m[36m(func pid=7413)[0m mae:  0.13338527083396912
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.264, 0.095, 0.328, 0.108, 0.19, 0.305, 0.145, 0.142, 0.118]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.1677798628807068
[2m[36m(func pid=186938)[0m mae:  0.10888270288705826
[2m[36m(func pid=186938)[0m rmse_per_class: [0.148, 0.224, 0.041, 0.33, 0.052, 0.191, 0.225, 0.119, 0.134, 0.214]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4422 | Steps: 4 | Val loss: 0.3462 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.7630 | Steps: 4 | Val loss: 0.5455 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8070 | Steps: 4 | Val loss: 0.6289 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3798 | Steps: 4 | Val loss: 0.2764 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=7482)[0m rmse: 0.17731575667858124
[2m[36m(func pid=7482)[0m mae:  0.13011428713798523
[2m[36m(func pid=7482)[0m rmse_per_class: [0.117, 0.264, 0.105, 0.335, 0.091, 0.188, 0.286, 0.136, 0.146, 0.105]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.18525037169456482
[2m[36m(func pid=187535)[0m mae:  0.10843949019908905
[2m[36m(func pid=187535)[0m rmse_per_class: [0.121, 0.265, 0.038, 0.331, 0.244, 0.233, 0.248, 0.115, 0.162, 0.097]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:51:30 (running for 00:22:43.82)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.454 |  0.168 |                   45 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.763 |  0.185 |                   41 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.807 |  0.181 |                   12 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.442 |  0.177 |                   11 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.18099597096443176
[2m[36m(func pid=7413)[0m mae:  0.1333538144826889
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.264, 0.095, 0.328, 0.107, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.14834240078926086
[2m[36m(func pid=186938)[0m mae:  0.09464617818593979
[2m[36m(func pid=186938)[0m rmse_per_class: [0.107, 0.202, 0.058, 0.303, 0.051, 0.174, 0.225, 0.1, 0.137, 0.126]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4413 | Steps: 4 | Val loss: 0.3414 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.9596 | Steps: 4 | Val loss: 0.5746 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7874 | Steps: 4 | Val loss: 0.6169 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3942 | Steps: 4 | Val loss: 0.2645 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=7482)[0m rmse: 0.1767454296350479
[2m[36m(func pid=7482)[0m mae:  0.1296621412038803
[2m[36m(func pid=7482)[0m rmse_per_class: [0.117, 0.264, 0.104, 0.335, 0.088, 0.188, 0.284, 0.136, 0.146, 0.105]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.18507330119609833
[2m[36m(func pid=187535)[0m mae:  0.10534745454788208
[2m[36m(func pid=187535)[0m rmse_per_class: [0.112, 0.222, 0.049, 0.326, 0.227, 0.233, 0.314, 0.133, 0.139, 0.095]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:51:35 (running for 00:22:49.12)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.38  |  0.148 |                   46 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.96  |  0.185 |                   42 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.787 |  0.181 |                   13 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.441 |  0.177 |                   12 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.18100306391716003
[2m[36m(func pid=7413)[0m mae:  0.1333683878183365
[2m[36m(func pid=7413)[0m rmse_per_class: [0.113, 0.264, 0.095, 0.329, 0.107, 0.191, 0.305, 0.145, 0.142, 0.119]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.14024363458156586
[2m[36m(func pid=186938)[0m mae:  0.08901835232973099
[2m[36m(func pid=186938)[0m rmse_per_class: [0.09, 0.202, 0.083, 0.268, 0.052, 0.159, 0.221, 0.096, 0.139, 0.092]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4401 | Steps: 4 | Val loss: 0.3383 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8532 | Steps: 4 | Val loss: 0.7068 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7702 | Steps: 4 | Val loss: 0.6029 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3867 | Steps: 4 | Val loss: 0.2757 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=7482)[0m rmse: 0.17650972306728363
[2m[36m(func pid=7482)[0m mae:  0.12948726117610931
[2m[36m(func pid=7482)[0m rmse_per_class: [0.118, 0.264, 0.104, 0.335, 0.086, 0.188, 0.283, 0.136, 0.147, 0.105]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.23697519302368164
[2m[36m(func pid=187535)[0m mae:  0.14199186861515045
[2m[36m(func pid=187535)[0m rmse_per_class: [0.112, 0.506, 0.049, 0.362, 0.169, 0.232, 0.293, 0.124, 0.429, 0.094]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:51:40 (running for 00:22:54.25)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.387 |  0.149 |                   48 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.853 |  0.237 |                   43 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.787 |  0.181 |                   13 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.44  |  0.177 |                   13 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.14875930547714233
[2m[36m(func pid=186938)[0m mae:  0.09574869275093079
[2m[36m(func pid=186938)[0m rmse_per_class: [0.095, 0.214, 0.07, 0.258, 0.061, 0.179, 0.258, 0.123, 0.142, 0.088]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18102163076400757
[2m[36m(func pid=7413)[0m mae:  0.13335232436656952
[2m[36m(func pid=7413)[0m rmse_per_class: [0.113, 0.264, 0.095, 0.329, 0.106, 0.191, 0.305, 0.146, 0.142, 0.119]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4389 | Steps: 4 | Val loss: 0.3362 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8123 | Steps: 4 | Val loss: 0.4478 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3908 | Steps: 4 | Val loss: 0.2988 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7540 | Steps: 4 | Val loss: 0.5895 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=7482)[0m rmse: 0.17635443806648254
[2m[36m(func pid=7482)[0m mae:  0.12936829030513763
[2m[36m(func pid=7482)[0m rmse_per_class: [0.119, 0.264, 0.104, 0.336, 0.083, 0.188, 0.282, 0.136, 0.147, 0.105]
[2m[36m(func pid=7482)[0m 
== Status ==
Current time: 2024-01-07 15:51:45 (running for 00:22:59.41)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.391 |  0.16  |                   49 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.853 |  0.237 |                   43 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.77  |  0.181 |                   14 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.439 |  0.176 |                   14 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.15961216390132904
[2m[36m(func pid=186938)[0m mae:  0.10471848398447037
[2m[36m(func pid=186938)[0m rmse_per_class: [0.093, 0.22, 0.06, 0.259, 0.077, 0.197, 0.304, 0.144, 0.153, 0.089]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.2014588564634323
[2m[36m(func pid=187535)[0m mae:  0.12420696020126343
[2m[36m(func pid=187535)[0m rmse_per_class: [0.107, 0.274, 0.032, 0.298, 0.125, 0.21, 0.275, 0.12, 0.469, 0.103]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.1810508668422699
[2m[36m(func pid=7413)[0m mae:  0.13335584104061127
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.264, 0.095, 0.329, 0.106, 0.191, 0.305, 0.146, 0.142, 0.119]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4399 | Steps: 4 | Val loss: 0.3346 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4141 | Steps: 4 | Val loss: 0.3074 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4849 | Steps: 4 | Val loss: 0.5067 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7383 | Steps: 4 | Val loss: 0.5782 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=7482)[0m rmse: 0.1762266606092453
[2m[36m(func pid=7482)[0m mae:  0.12932029366493225
[2m[36m(func pid=7482)[0m rmse_per_class: [0.119, 0.263, 0.103, 0.337, 0.081, 0.189, 0.281, 0.136, 0.148, 0.105]
[2m[36m(func pid=7482)[0m 
== Status ==
Current time: 2024-01-07 15:51:51 (running for 00:23:04.81)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.414 |  0.164 |                   50 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.812 |  0.201 |                   44 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.754 |  0.181 |                   15 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.44  |  0.176 |                   15 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.16362406313419342
[2m[36m(func pid=186938)[0m mae:  0.1083572655916214
[2m[36m(func pid=186938)[0m rmse_per_class: [0.091, 0.218, 0.039, 0.266, 0.11, 0.197, 0.317, 0.141, 0.169, 0.089]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.1681911200284958
[2m[36m(func pid=187535)[0m mae:  0.10361003875732422
[2m[36m(func pid=187535)[0m rmse_per_class: [0.163, 0.242, 0.114, 0.274, 0.068, 0.211, 0.244, 0.099, 0.132, 0.135]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18100537359714508
[2m[36m(func pid=7413)[0m mae:  0.1333206444978714
[2m[36m(func pid=7413)[0m rmse_per_class: [0.113, 0.265, 0.095, 0.329, 0.106, 0.191, 0.304, 0.146, 0.143, 0.119]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4360 | Steps: 4 | Val loss: 0.3325 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3932 | Steps: 4 | Val loss: 0.3096 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.7710 | Steps: 4 | Val loss: 0.5350 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7251 | Steps: 4 | Val loss: 0.5687 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=7482)[0m rmse: 0.17586573958396912
[2m[36m(func pid=7482)[0m mae:  0.12908855080604553
[2m[36m(func pid=7482)[0m rmse_per_class: [0.119, 0.264, 0.101, 0.336, 0.08, 0.188, 0.281, 0.136, 0.148, 0.105]
[2m[36m(func pid=7482)[0m 
== Status ==
Current time: 2024-01-07 15:51:56 (running for 00:23:10.17)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.393 |  0.168 |                   51 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.485 |  0.168 |                   45 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.738 |  0.181 |                   16 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.436 |  0.176 |                   16 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.1684703528881073
[2m[36m(func pid=186938)[0m mae:  0.1096472293138504
[2m[36m(func pid=186938)[0m rmse_per_class: [0.139, 0.207, 0.032, 0.277, 0.169, 0.184, 0.317, 0.104, 0.169, 0.086]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18098804354667664
[2m[36m(func pid=7413)[0m mae:  0.13330841064453125
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.265, 0.095, 0.329, 0.105, 0.191, 0.304, 0.146, 0.143, 0.119]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.2217884510755539
[2m[36m(func pid=187535)[0m mae:  0.1330185830593109
[2m[36m(func pid=187535)[0m rmse_per_class: [0.152, 0.246, 0.118, 0.363, 0.132, 0.264, 0.318, 0.31, 0.135, 0.178]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4324 | Steps: 4 | Val loss: 0.3311 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4036 | Steps: 4 | Val loss: 0.3139 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.7118 | Steps: 4 | Val loss: 0.5606 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.7091 | Steps: 4 | Val loss: 0.5629 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=7482)[0m rmse: 0.17567244172096252
[2m[36m(func pid=7482)[0m mae:  0.1289307326078415
[2m[36m(func pid=7482)[0m rmse_per_class: [0.12, 0.263, 0.102, 0.337, 0.077, 0.188, 0.28, 0.136, 0.149, 0.105]
[2m[36m(func pid=7482)[0m 
== Status ==
Current time: 2024-01-07 15:52:01 (running for 00:23:15.32)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.404 |  0.175 |                   52 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.771 |  0.222 |                   46 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.725 |  0.181 |                   17 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.432 |  0.176 |                   17 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.17502591013908386
[2m[36m(func pid=186938)[0m mae:  0.11114022880792618
[2m[36m(func pid=186938)[0m rmse_per_class: [0.185, 0.207, 0.037, 0.298, 0.213, 0.169, 0.298, 0.098, 0.159, 0.085]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18088561296463013
[2m[36m(func pid=7413)[0m mae:  0.13323839008808136
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.265, 0.095, 0.329, 0.105, 0.191, 0.304, 0.146, 0.143, 0.119]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.21684226393699646
[2m[36m(func pid=187535)[0m mae:  0.1279720962047577
[2m[36m(func pid=187535)[0m rmse_per_class: [0.111, 0.239, 0.039, 0.376, 0.355, 0.222, 0.456, 0.104, 0.142, 0.124]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4350 | Steps: 4 | Val loss: 0.3306 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3624 | Steps: 4 | Val loss: 0.3152 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.7030 | Steps: 4 | Val loss: 0.5552 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7127 | Steps: 4 | Val loss: 0.6207 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=7482)[0m rmse: 0.17556485533714294
[2m[36m(func pid=7482)[0m mae:  0.12883850932121277
[2m[36m(func pid=7482)[0m rmse_per_class: [0.121, 0.263, 0.101, 0.337, 0.076, 0.188, 0.28, 0.136, 0.149, 0.105]
[2m[36m(func pid=7482)[0m 
== Status ==
Current time: 2024-01-07 15:52:06 (running for 00:23:20.41)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.175 |                   53 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.709 |  0.217 |                   47 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.712 |  0.181 |                   18 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.435 |  0.176 |                   18 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.17474326491355896
[2m[36m(func pid=186938)[0m mae:  0.11122077703475952
[2m[36m(func pid=186938)[0m rmse_per_class: [0.154, 0.229, 0.039, 0.321, 0.225, 0.171, 0.266, 0.093, 0.144, 0.107]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18087390065193176
[2m[36m(func pid=7413)[0m mae:  0.1332463175058365
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.265, 0.096, 0.329, 0.106, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.22454623878002167
[2m[36m(func pid=187535)[0m mae:  0.13599328696727753
[2m[36m(func pid=187535)[0m rmse_per_class: [0.11, 0.267, 0.049, 0.374, 0.263, 0.233, 0.255, 0.147, 0.457, 0.091]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4378 | Steps: 4 | Val loss: 0.3303 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3661 | Steps: 4 | Val loss: 0.3227 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6820 | Steps: 4 | Val loss: 0.5446 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=7482)[0m rmse: 0.17548415064811707
[2m[36m(func pid=7482)[0m mae:  0.1287519931793213
[2m[36m(func pid=7482)[0m rmse_per_class: [0.122, 0.263, 0.1, 0.337, 0.075, 0.188, 0.28, 0.136, 0.148, 0.106]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.0316 | Steps: 4 | Val loss: 0.6381 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 15:52:11 (running for 00:23:25.64)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.366 |  0.177 |                   54 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.713 |  0.225 |                   48 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.703 |  0.181 |                   19 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.438 |  0.175 |                   19 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.17706353962421417
[2m[36m(func pid=186938)[0m mae:  0.11397741734981537
[2m[36m(func pid=186938)[0m rmse_per_class: [0.105, 0.241, 0.041, 0.34, 0.203, 0.181, 0.235, 0.105, 0.134, 0.186]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.1808071732521057
[2m[36m(func pid=7413)[0m mae:  0.13319048285484314
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.19601383805274963
[2m[36m(func pid=187535)[0m mae:  0.12164069712162018
[2m[36m(func pid=187535)[0m rmse_per_class: [0.271, 0.284, 0.049, 0.33, 0.126, 0.233, 0.283, 0.151, 0.139, 0.094]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4386 | Steps: 4 | Val loss: 0.3302 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4205 | Steps: 4 | Val loss: 0.3404 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6776 | Steps: 4 | Val loss: 0.5346 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=7482)[0m rmse: 0.1754864752292633
[2m[36m(func pid=7482)[0m mae:  0.1288922280073166
[2m[36m(func pid=7482)[0m rmse_per_class: [0.122, 0.263, 0.098, 0.337, 0.075, 0.188, 0.281, 0.136, 0.149, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.8396 | Steps: 4 | Val loss: 0.4470 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 15:52:17 (running for 00:23:30.79)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.421 |  0.181 |                   55 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  1.032 |  0.196 |                   49 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.682 |  0.181 |                   20 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.439 |  0.175 |                   20 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.18149521946907043
[2m[36m(func pid=186938)[0m mae:  0.1187037006020546
[2m[36m(func pid=186938)[0m rmse_per_class: [0.091, 0.23, 0.039, 0.345, 0.167, 0.195, 0.228, 0.128, 0.133, 0.258]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18074671924114227
[2m[36m(func pid=7413)[0m mae:  0.13315698504447937
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.104, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.1866595298051834
[2m[36m(func pid=187535)[0m mae:  0.11072107404470444
[2m[36m(func pid=187535)[0m rmse_per_class: [0.24, 0.223, 0.049, 0.337, 0.124, 0.225, 0.274, 0.136, 0.139, 0.12]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4304 | Steps: 4 | Val loss: 0.3295 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3850 | Steps: 4 | Val loss: 0.3196 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6597 | Steps: 4 | Val loss: 0.5278 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=7482)[0m rmse: 0.17541763186454773
[2m[36m(func pid=7482)[0m mae:  0.12889592349529266
[2m[36m(func pid=7482)[0m rmse_per_class: [0.122, 0.263, 0.097, 0.337, 0.074, 0.188, 0.281, 0.136, 0.149, 0.108]
[2m[36m(func pid=7482)[0m 
== Status ==
Current time: 2024-01-07 15:52:22 (running for 00:23:36.02)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.385 |  0.171 |                   56 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.84  |  0.187 |                   50 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.678 |  0.181 |                   21 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.43  |  0.175 |                   21 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.17121323943138123
[2m[36m(func pid=186938)[0m mae:  0.11231982707977295
[2m[36m(func pid=186938)[0m rmse_per_class: [0.092, 0.211, 0.032, 0.336, 0.147, 0.194, 0.226, 0.131, 0.149, 0.194]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5681 | Steps: 4 | Val loss: 0.4455 | Batch size: 32 | lr: 0.1 | Duration: 3.25s
[2m[36m(func pid=7413)[0m rmse: 0.18072661757469177
[2m[36m(func pid=7413)[0m mae:  0.13315227627754211
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.265, 0.096, 0.33, 0.104, 0.191, 0.303, 0.144, 0.143, 0.118]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4265 | Steps: 4 | Val loss: 0.3287 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=187535)[0m rmse: 0.19073417782783508
[2m[36m(func pid=187535)[0m mae:  0.11480935662984848
[2m[36m(func pid=187535)[0m rmse_per_class: [0.109, 0.224, 0.028, 0.339, 0.09, 0.314, 0.295, 0.167, 0.14, 0.202]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3802 | Steps: 4 | Val loss: 0.2960 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.6557 | Steps: 4 | Val loss: 0.5183 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 15:52:27 (running for 00:23:41.03)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.385 |  0.171 |                   56 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.568 |  0.191 |                   51 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.66  |  0.181 |                   22 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.426 |  0.176 |                   22 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=7482)[0m rmse: 0.17560455203056335
[2m[36m(func pid=7482)[0m mae:  0.12900501489639282
[2m[36m(func pid=7482)[0m rmse_per_class: [0.124, 0.263, 0.097, 0.338, 0.073, 0.188, 0.281, 0.135, 0.15, 0.109]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.16268937289714813
[2m[36m(func pid=186938)[0m mae:  0.1053658276796341
[2m[36m(func pid=186938)[0m rmse_per_class: [0.091, 0.211, 0.039, 0.315, 0.138, 0.181, 0.229, 0.119, 0.193, 0.111]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7935 | Steps: 4 | Val loss: 0.5494 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=7413)[0m rmse: 0.1806989312171936
[2m[36m(func pid=7413)[0m mae:  0.13311223685741425
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.33, 0.104, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4272 | Steps: 4 | Val loss: 0.3279 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3600 | Steps: 4 | Val loss: 0.2822 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=187535)[0m rmse: 0.2176283895969391
[2m[36m(func pid=187535)[0m mae:  0.13173511624336243
[2m[36m(func pid=187535)[0m rmse_per_class: [0.111, 0.236, 0.273, 0.374, 0.068, 0.269, 0.278, 0.294, 0.139, 0.133]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.6458 | Steps: 4 | Val loss: 0.5114 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 15:52:32 (running for 00:23:46.54)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.36  |  0.156 |                   58 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.794 |  0.218 |                   52 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.656 |  0.181 |                   23 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.426 |  0.176 |                   22 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.1564171016216278
[2m[36m(func pid=186938)[0m mae:  0.10022829473018646
[2m[36m(func pid=186938)[0m rmse_per_class: [0.09, 0.215, 0.065, 0.286, 0.139, 0.16, 0.215, 0.108, 0.198, 0.088]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17564670741558075
[2m[36m(func pid=7482)[0m mae:  0.12894009053707123
[2m[36m(func pid=7482)[0m rmse_per_class: [0.126, 0.262, 0.097, 0.338, 0.071, 0.188, 0.28, 0.135, 0.149, 0.109]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18059992790222168
[2m[36m(func pid=7413)[0m mae:  0.1330251395702362
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.33, 0.103, 0.191, 0.302, 0.145, 0.143, 0.118]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.6003 | Steps: 4 | Val loss: 0.4780 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3873 | Steps: 4 | Val loss: 0.2843 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4224 | Steps: 4 | Val loss: 0.3264 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=187535)[0m rmse: 0.2019905149936676
[2m[36m(func pid=187535)[0m mae:  0.12762536108493805
[2m[36m(func pid=187535)[0m rmse_per_class: [0.131, 0.23, 0.062, 0.309, 0.123, 0.219, 0.265, 0.107, 0.461, 0.113]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.6374 | Steps: 4 | Val loss: 0.5077 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=186938)[0m rmse: 0.15595731139183044
[2m[36m(func pid=186938)[0m mae:  0.10015685856342316
[2m[36m(func pid=186938)[0m rmse_per_class: [0.092, 0.222, 0.094, 0.269, 0.119, 0.166, 0.241, 0.095, 0.176, 0.086]
[2m[36m(func pid=186938)[0m 
== Status ==
Current time: 2024-01-07 15:52:38 (running for 00:23:51.88)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.387 |  0.156 |                   59 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.6   |  0.202 |                   53 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.646 |  0.181 |                   24 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.427 |  0.176 |                   23 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=7482)[0m rmse: 0.17507421970367432
[2m[36m(func pid=7482)[0m mae:  0.12858696281909943
[2m[36m(func pid=7482)[0m rmse_per_class: [0.124, 0.262, 0.095, 0.337, 0.071, 0.187, 0.28, 0.135, 0.149, 0.109]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18048593401908875
[2m[36m(func pid=7413)[0m mae:  0.1329503059387207
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.33, 0.104, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.8604 | Steps: 4 | Val loss: 0.5038 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4094 | Steps: 4 | Val loss: 0.2956 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4362 | Steps: 4 | Val loss: 0.3272 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.6305 | Steps: 4 | Val loss: 0.5008 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=187535)[0m rmse: 0.18326035141944885
[2m[36m(func pid=187535)[0m mae:  0.10914821922779083
[2m[36m(func pid=187535)[0m rmse_per_class: [0.275, 0.204, 0.033, 0.295, 0.15, 0.233, 0.264, 0.153, 0.131, 0.096]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:52:43 (running for 00:23:57.17)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.409 |  0.161 |                   60 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.86  |  0.183 |                   54 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.637 |  0.18  |                   25 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.422 |  0.175 |                   24 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.16053315997123718
[2m[36m(func pid=186938)[0m mae:  0.10284165292978287
[2m[36m(func pid=186938)[0m rmse_per_class: [0.127, 0.22, 0.085, 0.268, 0.083, 0.202, 0.281, 0.106, 0.145, 0.088]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17494697868824005
[2m[36m(func pid=7482)[0m mae:  0.12851205468177795
[2m[36m(func pid=7482)[0m rmse_per_class: [0.124, 0.262, 0.094, 0.337, 0.071, 0.187, 0.28, 0.136, 0.149, 0.109]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.1804671585559845
[2m[36m(func pid=7413)[0m mae:  0.13292744755744934
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.33, 0.103, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.7803 | Steps: 4 | Val loss: 0.6068 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3845 | Steps: 4 | Val loss: 0.3020 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4297 | Steps: 4 | Val loss: 0.3266 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.6176 | Steps: 4 | Val loss: 0.4935 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=187535)[0m rmse: 0.20805446803569794
[2m[36m(func pid=187535)[0m mae:  0.12257237732410431
[2m[36m(func pid=187535)[0m rmse_per_class: [0.109, 0.336, 0.049, 0.328, 0.308, 0.233, 0.328, 0.154, 0.14, 0.096]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:52:48 (running for 00:24:02.64)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.384 |  0.163 |                   61 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.78  |  0.208 |                   55 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.631 |  0.18  |                   26 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.436 |  0.175 |                   25 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.16293618083000183
[2m[36m(func pid=186938)[0m mae:  0.10414157062768936
[2m[36m(func pid=186938)[0m rmse_per_class: [0.166, 0.203, 0.047, 0.274, 0.064, 0.234, 0.307, 0.111, 0.134, 0.089]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17510761320590973
[2m[36m(func pid=7482)[0m mae:  0.12849944829940796
[2m[36m(func pid=7482)[0m rmse_per_class: [0.127, 0.262, 0.094, 0.337, 0.07, 0.187, 0.279, 0.135, 0.15, 0.109]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.1803484857082367
[2m[36m(func pid=7413)[0m mae:  0.13283894956111908
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.33, 0.103, 0.19, 0.301, 0.144, 0.144, 0.117]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.9641 | Steps: 4 | Val loss: 0.5738 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4000 | Steps: 4 | Val loss: 0.3044 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6176 | Steps: 4 | Val loss: 0.4922 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4261 | Steps: 4 | Val loss: 0.3260 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=187535)[0m rmse: 0.2323978692293167
[2m[36m(func pid=187535)[0m mae:  0.13748393952846527
[2m[36m(func pid=187535)[0m rmse_per_class: [0.111, 0.307, 0.049, 0.352, 0.33, 0.218, 0.269, 0.117, 0.14, 0.431]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:52:54 (running for 00:24:08.06)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.4   |  0.163 |                   62 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.964 |  0.232 |                   56 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.618 |  0.18  |                   27 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.43  |  0.175 |                   26 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.16291527450084686
[2m[36m(func pid=186938)[0m mae:  0.10333479940891266
[2m[36m(func pid=186938)[0m rmse_per_class: [0.168, 0.201, 0.029, 0.286, 0.056, 0.233, 0.316, 0.12, 0.132, 0.088]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18037109076976776
[2m[36m(func pid=7413)[0m mae:  0.13287244737148285
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.265, 0.097, 0.33, 0.103, 0.19, 0.301, 0.143, 0.144, 0.116]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17509928345680237
[2m[36m(func pid=7482)[0m mae:  0.12839415669441223
[2m[36m(func pid=7482)[0m rmse_per_class: [0.128, 0.262, 0.094, 0.337, 0.069, 0.187, 0.279, 0.135, 0.15, 0.109]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.9145 | Steps: 4 | Val loss: 0.4318 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3681 | Steps: 4 | Val loss: 0.2958 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.6064 | Steps: 4 | Val loss: 0.4882 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4218 | Steps: 4 | Val loss: 0.3250 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=187535)[0m rmse: 0.18700017035007477
[2m[36m(func pid=187535)[0m mae:  0.11023279279470444
[2m[36m(func pid=187535)[0m rmse_per_class: [0.102, 0.222, 0.049, 0.298, 0.056, 0.299, 0.294, 0.193, 0.14, 0.216]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:52:59 (running for 00:24:13.33)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.368 |  0.16  |                   63 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.915 |  0.187 |                   57 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.618 |  0.18  |                   28 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.426 |  0.175 |                   27 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.15964177250862122
[2m[36m(func pid=186938)[0m mae:  0.10078161954879761
[2m[36m(func pid=186938)[0m rmse_per_class: [0.121, 0.227, 0.038, 0.292, 0.053, 0.202, 0.305, 0.139, 0.131, 0.088]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.180311918258667
[2m[36m(func pid=7413)[0m mae:  0.1328364759683609
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.265, 0.097, 0.33, 0.103, 0.19, 0.301, 0.143, 0.144, 0.116]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17482969164848328
[2m[36m(func pid=7482)[0m mae:  0.1282050907611847
[2m[36m(func pid=7482)[0m rmse_per_class: [0.127, 0.262, 0.093, 0.336, 0.069, 0.187, 0.279, 0.135, 0.15, 0.109]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7137 | Steps: 4 | Val loss: 0.4925 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4135 | Steps: 4 | Val loss: 0.2889 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5915 | Steps: 4 | Val loss: 0.4781 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4203 | Steps: 4 | Val loss: 0.3240 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=187535)[0m rmse: 0.16838720440864563
[2m[36m(func pid=187535)[0m mae:  0.10502227395772934
[2m[36m(func pid=187535)[0m rmse_per_class: [0.157, 0.222, 0.034, 0.284, 0.056, 0.268, 0.25, 0.193, 0.132, 0.088]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:53:04 (running for 00:24:18.50)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.413 |  0.157 |                   64 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.714 |  0.168 |                   58 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.606 |  0.18  |                   29 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.422 |  0.175 |                   28 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.1573212444782257
[2m[36m(func pid=186938)[0m mae:  0.09852783381938934
[2m[36m(func pid=186938)[0m rmse_per_class: [0.094, 0.257, 0.045, 0.302, 0.052, 0.17, 0.273, 0.155, 0.132, 0.092]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18024687469005585
[2m[36m(func pid=7413)[0m mae:  0.13277818262577057
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.265, 0.097, 0.33, 0.102, 0.19, 0.301, 0.143, 0.144, 0.116]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17445720732212067
[2m[36m(func pid=7482)[0m mae:  0.1280072033405304
[2m[36m(func pid=7482)[0m rmse_per_class: [0.126, 0.262, 0.092, 0.335, 0.069, 0.187, 0.279, 0.135, 0.151, 0.108]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6965 | Steps: 4 | Val loss: 0.4929 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3737 | Steps: 4 | Val loss: 0.2765 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5912 | Steps: 4 | Val loss: 0.4713 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4257 | Steps: 4 | Val loss: 0.3238 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=187535)[0m rmse: 0.2018921822309494
[2m[36m(func pid=187535)[0m mae:  0.13025957345962524
[2m[36m(func pid=187535)[0m rmse_per_class: [0.113, 0.22, 0.114, 0.336, 0.056, 0.173, 0.281, 0.111, 0.526, 0.09]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:53:10 (running for 00:24:23.79)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.413 |  0.157 |                   64 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.697 |  0.202 |                   59 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.591 |  0.18  |                   31 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.42  |  0.174 |                   29 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.18014812469482422
[2m[36m(func pid=7413)[0m mae:  0.13269668817520142
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.33, 0.101, 0.191, 0.301, 0.143, 0.144, 0.116]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.14906296133995056
[2m[36m(func pid=186938)[0m mae:  0.09449929744005203
[2m[36m(func pid=186938)[0m rmse_per_class: [0.093, 0.238, 0.046, 0.301, 0.051, 0.159, 0.236, 0.12, 0.137, 0.11]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17449942231178284
[2m[36m(func pid=7482)[0m mae:  0.12792716920375824
[2m[36m(func pid=7482)[0m rmse_per_class: [0.129, 0.262, 0.091, 0.334, 0.068, 0.187, 0.279, 0.135, 0.152, 0.108]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6490 | Steps: 4 | Val loss: 0.5071 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3567 | Steps: 4 | Val loss: 0.2799 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5966 | Steps: 4 | Val loss: 0.4700 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4225 | Steps: 4 | Val loss: 0.3227 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=187535)[0m rmse: 0.1999071091413498
[2m[36m(func pid=187535)[0m mae:  0.11391846835613251
[2m[36m(func pid=187535)[0m rmse_per_class: [0.109, 0.308, 0.266, 0.343, 0.059, 0.2, 0.339, 0.132, 0.134, 0.108]
== Status ==
Current time: 2024-01-07 15:53:15 (running for 00:24:29.06)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.374 |  0.149 |                   65 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.649 |  0.2   |                   60 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.591 |  0.18  |                   31 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.426 |  0.174 |                   30 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.14918218553066254
[2m[36m(func pid=186938)[0m mae:  0.0971006453037262
[2m[36m(func pid=186938)[0m rmse_per_class: [0.095, 0.202, 0.047, 0.303, 0.051, 0.17, 0.226, 0.097, 0.151, 0.15]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.1800784468650818
[2m[36m(func pid=7413)[0m mae:  0.13265566527843475
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.265, 0.097, 0.33, 0.101, 0.19, 0.3, 0.143, 0.144, 0.116]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.1739659607410431
[2m[36m(func pid=7482)[0m mae:  0.12757833302021027
[2m[36m(func pid=7482)[0m rmse_per_class: [0.126, 0.262, 0.091, 0.333, 0.068, 0.187, 0.279, 0.135, 0.152, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3740 | Steps: 4 | Val loss: 0.2915 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.5746 | Steps: 4 | Val loss: 0.4628 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6336 | Steps: 4 | Val loss: 0.4346 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4215 | Steps: 4 | Val loss: 0.3217 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 15:53:20 (running for 00:24:34.32)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.374 |  0.156 |                   67 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.649 |  0.2   |                   60 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.597 |  0.18  |                   32 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.422 |  0.174 |                   31 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.15623155236244202
[2m[36m(func pid=186938)[0m mae:  0.10334237664937973
[2m[36m(func pid=186938)[0m rmse_per_class: [0.108, 0.211, 0.045, 0.296, 0.051, 0.178, 0.225, 0.092, 0.159, 0.198]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18014641106128693
[2m[36m(func pid=7413)[0m mae:  0.13270413875579834
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.265, 0.098, 0.33, 0.101, 0.19, 0.3, 0.143, 0.144, 0.116]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.17984507977962494
[2m[36m(func pid=187535)[0m mae:  0.1044585257768631
[2m[36m(func pid=187535)[0m rmse_per_class: [0.111, 0.204, 0.033, 0.285, 0.163, 0.21, 0.333, 0.142, 0.14, 0.177]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.1736864149570465
[2m[36m(func pid=7482)[0m mae:  0.12731479108333588
[2m[36m(func pid=7482)[0m rmse_per_class: [0.126, 0.262, 0.09, 0.333, 0.067, 0.187, 0.278, 0.135, 0.152, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3679 | Steps: 4 | Val loss: 0.2924 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5811 | Steps: 4 | Val loss: 0.4600 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.6242 | Steps: 4 | Val loss: 0.4402 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4213 | Steps: 4 | Val loss: 0.3212 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 15:53:25 (running for 00:24:39.53)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.368 |  0.16  |                   68 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.634 |  0.18  |                   61 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.575 |  0.18  |                   33 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.422 |  0.174 |                   32 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.15983644127845764
[2m[36m(func pid=186938)[0m mae:  0.10696861892938614
[2m[36m(func pid=186938)[0m rmse_per_class: [0.127, 0.222, 0.039, 0.286, 0.053, 0.174, 0.233, 0.094, 0.157, 0.214]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.18007466197013855
[2m[36m(func pid=7413)[0m mae:  0.13262397050857544
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.331, 0.101, 0.19, 0.3, 0.143, 0.144, 0.116]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.19180512428283691
[2m[36m(func pid=187535)[0m mae:  0.1195148453116417
[2m[36m(func pid=187535)[0m rmse_per_class: [0.103, 0.226, 0.046, 0.272, 0.331, 0.177, 0.254, 0.111, 0.14, 0.26]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17350047826766968
[2m[36m(func pid=7482)[0m mae:  0.12711556255817413
[2m[36m(func pid=7482)[0m rmse_per_class: [0.126, 0.261, 0.091, 0.333, 0.067, 0.187, 0.277, 0.135, 0.151, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3517 | Steps: 4 | Val loss: 0.2963 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.5743 | Steps: 4 | Val loss: 0.4553 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5696 | Steps: 4 | Val loss: 0.4497 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4153 | Steps: 4 | Val loss: 0.3203 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 15:53:31 (running for 00:24:44.81)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.352 |  0.167 |                   69 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.624 |  0.192 |                   62 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.581 |  0.18  |                   34 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.421 |  0.174 |                   33 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.16661503911018372
[2m[36m(func pid=186938)[0m mae:  0.1104135736823082
[2m[36m(func pid=186938)[0m rmse_per_class: [0.121, 0.226, 0.094, 0.28, 0.066, 0.165, 0.264, 0.094, 0.147, 0.21]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.1799640953540802
[2m[36m(func pid=7413)[0m mae:  0.13252589106559753
[2m[36m(func pid=7413)[0m rmse_per_class: [0.114, 0.264, 0.098, 0.331, 0.1, 0.19, 0.299, 0.143, 0.144, 0.116]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17329636216163635
[2m[36m(func pid=7482)[0m mae:  0.12690375745296478
[2m[36m(func pid=7482)[0m rmse_per_class: [0.126, 0.261, 0.09, 0.333, 0.066, 0.187, 0.276, 0.135, 0.151, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.18425199389457703
[2m[36m(func pid=187535)[0m mae:  0.1121368408203125
[2m[36m(func pid=187535)[0m rmse_per_class: [0.199, 0.198, 0.049, 0.341, 0.116, 0.261, 0.272, 0.16, 0.14, 0.107]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3413 | Steps: 4 | Val loss: 0.3000 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.5699 | Steps: 4 | Val loss: 0.4514 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4244 | Steps: 4 | Val loss: 0.3201 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6667 | Steps: 4 | Val loss: 0.3911 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 15:53:36 (running for 00:24:50.01)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.341 |  0.168 |                   70 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.57  |  0.184 |                   63 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.574 |  0.18  |                   35 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.415 |  0.173 |                   34 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.16837535798549652
[2m[36m(func pid=186938)[0m mae:  0.11008604615926743
[2m[36m(func pid=186938)[0m rmse_per_class: [0.103, 0.221, 0.149, 0.278, 0.095, 0.164, 0.286, 0.095, 0.141, 0.15]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.17998023331165314
[2m[36m(func pid=7413)[0m mae:  0.13254062831401825
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.264, 0.098, 0.331, 0.1, 0.19, 0.299, 0.143, 0.144, 0.116]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17308086156845093
[2m[36m(func pid=7482)[0m mae:  0.1268511861562729
[2m[36m(func pid=7482)[0m rmse_per_class: [0.125, 0.261, 0.089, 0.333, 0.066, 0.187, 0.277, 0.135, 0.151, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.17853859066963196
[2m[36m(func pid=187535)[0m mae:  0.10905355215072632
[2m[36m(func pid=187535)[0m rmse_per_class: [0.106, 0.27, 0.049, 0.342, 0.055, 0.164, 0.254, 0.254, 0.204, 0.087]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3705 | Steps: 4 | Val loss: 0.3098 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5613 | Steps: 4 | Val loss: 0.4493 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4213 | Steps: 4 | Val loss: 0.3199 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.8211 | Steps: 4 | Val loss: 0.4147 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 15:53:41 (running for 00:24:55.19)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.371 |  0.172 |                   71 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.667 |  0.179 |                   64 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.57  |  0.18  |                   36 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   35 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.1724235862493515
[2m[36m(func pid=186938)[0m mae:  0.11176817119121552
[2m[36m(func pid=186938)[0m rmse_per_class: [0.1, 0.21, 0.184, 0.279, 0.12, 0.185, 0.304, 0.097, 0.136, 0.11]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.1799367368221283
[2m[36m(func pid=7413)[0m mae:  0.13251790404319763
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.331, 0.1, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17302927374839783
[2m[36m(func pid=7482)[0m mae:  0.1268737018108368
[2m[36m(func pid=7482)[0m rmse_per_class: [0.123, 0.26, 0.091, 0.333, 0.066, 0.187, 0.277, 0.135, 0.15, 0.108]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.17496617138385773
[2m[36m(func pid=187535)[0m mae:  0.10610377788543701
[2m[36m(func pid=187535)[0m rmse_per_class: [0.111, 0.263, 0.049, 0.3, 0.056, 0.19, 0.247, 0.111, 0.335, 0.088]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3799 | Steps: 4 | Val loss: 0.3048 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.5586 | Steps: 4 | Val loss: 0.4460 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4154 | Steps: 4 | Val loss: 0.3195 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7634 | Steps: 4 | Val loss: 0.4109 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 15:53:46 (running for 00:25:00.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.38  |  0.167 |                   72 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.821 |  0.175 |                   65 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.561 |  0.18  |                   37 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.421 |  0.173 |                   36 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.16666516661643982
[2m[36m(func pid=186938)[0m mae:  0.1080164685845375
[2m[36m(func pid=186938)[0m rmse_per_class: [0.1, 0.202, 0.125, 0.272, 0.136, 0.203, 0.306, 0.098, 0.134, 0.09]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.17992036044597626
[2m[36m(func pid=7413)[0m mae:  0.1325022280216217
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.331, 0.1, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17288896441459656
[2m[36m(func pid=7482)[0m mae:  0.12687775492668152
[2m[36m(func pid=7482)[0m rmse_per_class: [0.119, 0.261, 0.092, 0.332, 0.067, 0.187, 0.278, 0.135, 0.149, 0.109]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.1712774783372879
[2m[36m(func pid=187535)[0m mae:  0.10023226588964462
[2m[36m(func pid=187535)[0m rmse_per_class: [0.117, 0.246, 0.049, 0.414, 0.056, 0.182, 0.289, 0.129, 0.139, 0.091]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3433 | Steps: 4 | Val loss: 0.2939 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.5597 | Steps: 4 | Val loss: 0.4412 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4154 | Steps: 4 | Val loss: 0.3194 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 15:53:51 (running for 00:25:05.62)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.343 |  0.158 |                   73 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.763 |  0.171 |                   66 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.559 |  0.18  |                   38 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.415 |  0.173 |                   37 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.15831217169761658
[2m[36m(func pid=186938)[0m mae:  0.10285891592502594
[2m[36m(func pid=186938)[0m rmse_per_class: [0.101, 0.209, 0.047, 0.265, 0.152, 0.198, 0.292, 0.101, 0.133, 0.086]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6105 | Steps: 4 | Val loss: 0.5033 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=7413)[0m rmse: 0.1798519641160965
[2m[36m(func pid=7413)[0m mae:  0.1324489712715149
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.264, 0.098, 0.331, 0.1, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17314191162586212
[2m[36m(func pid=7482)[0m mae:  0.12699763476848602
[2m[36m(func pid=7482)[0m rmse_per_class: [0.119, 0.26, 0.095, 0.333, 0.066, 0.187, 0.278, 0.135, 0.15, 0.108]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.19781240820884705
[2m[36m(func pid=187535)[0m mae:  0.12038968503475189
[2m[36m(func pid=187535)[0m rmse_per_class: [0.399, 0.274, 0.032, 0.327, 0.056, 0.219, 0.273, 0.138, 0.14, 0.121]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3408 | Steps: 4 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.5519 | Steps: 4 | Val loss: 0.4399 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4162 | Steps: 4 | Val loss: 0.3188 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 15:53:57 (running for 00:25:10.80)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.341 |  0.154 |                   74 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.611 |  0.198 |                   67 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.56  |  0.18  |                   39 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.415 |  0.173 |                   38 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.1544499546289444
[2m[36m(func pid=186938)[0m mae:  0.09831897169351578
[2m[36m(func pid=186938)[0m rmse_per_class: [0.099, 0.225, 0.042, 0.27, 0.162, 0.176, 0.256, 0.096, 0.132, 0.086]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7261 | Steps: 4 | Val loss: 0.4824 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=7413)[0m rmse: 0.17982570827007294
[2m[36m(func pid=7413)[0m mae:  0.13243873417377472
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.332, 0.1, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.1730465143918991
[2m[36m(func pid=7482)[0m mae:  0.12686634063720703
[2m[36m(func pid=7482)[0m rmse_per_class: [0.118, 0.26, 0.097, 0.333, 0.065, 0.186, 0.277, 0.135, 0.149, 0.108]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3542 | Steps: 4 | Val loss: 0.2808 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=187535)[0m rmse: 0.20675091445446014
[2m[36m(func pid=187535)[0m mae:  0.1255568265914917
[2m[36m(func pid=187535)[0m rmse_per_class: [0.151, 0.228, 0.099, 0.349, 0.056, 0.467, 0.272, 0.118, 0.14, 0.187]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5463 | Steps: 4 | Val loss: 0.4359 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 15:54:02 (running for 00:25:16.02)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15550000220537186
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.354 |  0.15  |                   75 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.726 |  0.207 |                   68 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.552 |  0.18  |                   40 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.416 |  0.173 |                   39 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.15048888325691223
[2m[36m(func pid=186938)[0m mae:  0.09474986046552658
[2m[36m(func pid=186938)[0m rmse_per_class: [0.104, 0.22, 0.048, 0.274, 0.155, 0.162, 0.226, 0.097, 0.133, 0.086]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4147 | Steps: 4 | Val loss: 0.3187 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=7413)[0m rmse: 0.17985057830810547
[2m[36m(func pid=7413)[0m mae:  0.1324589103460312
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.332, 0.099, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.6093 | Steps: 4 | Val loss: 0.4030 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3616 | Steps: 4 | Val loss: 0.2808 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=7482)[0m rmse: 0.17321453988552094
[2m[36m(func pid=7482)[0m mae:  0.12690909206867218
[2m[36m(func pid=7482)[0m rmse_per_class: [0.118, 0.26, 0.1, 0.333, 0.065, 0.186, 0.277, 0.135, 0.149, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5457 | Steps: 4 | Val loss: 0.4341 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=187535)[0m rmse: 0.1803339272737503
[2m[36m(func pid=187535)[0m mae:  0.10314446687698364
[2m[36m(func pid=187535)[0m rmse_per_class: [0.109, 0.326, 0.175, 0.327, 0.057, 0.179, 0.241, 0.164, 0.14, 0.087]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:54:07 (running for 00:25:21.17)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15550000220537186
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.149 |                   76 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.609 |  0.18  |                   69 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.546 |  0.18  |                   41 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.415 |  0.173 |                   40 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.1491658240556717
[2m[36m(func pid=186938)[0m mae:  0.0940081849694252
[2m[36m(func pid=186938)[0m rmse_per_class: [0.106, 0.208, 0.048, 0.273, 0.155, 0.164, 0.223, 0.094, 0.135, 0.086]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4119 | Steps: 4 | Val loss: 0.3186 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=7413)[0m rmse: 0.17979994416236877
[2m[36m(func pid=7413)[0m mae:  0.1324121654033661
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.099, 0.331, 0.099, 0.19, 0.298, 0.142, 0.144, 0.115]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6977 | Steps: 4 | Val loss: 0.4233 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3654 | Steps: 4 | Val loss: 0.2774 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=7482)[0m rmse: 0.17334674298763275
[2m[36m(func pid=7482)[0m mae:  0.1269475370645523
[2m[36m(func pid=7482)[0m rmse_per_class: [0.117, 0.26, 0.103, 0.334, 0.065, 0.186, 0.278, 0.135, 0.149, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.5424 | Steps: 4 | Val loss: 0.4353 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=187535)[0m rmse: 0.17727753520011902
[2m[36m(func pid=187535)[0m mae:  0.11073771864175797
[2m[36m(func pid=187535)[0m rmse_per_class: [0.109, 0.204, 0.062, 0.288, 0.174, 0.206, 0.247, 0.165, 0.23, 0.087]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.15147610008716583
[2m[36m(func pid=186938)[0m mae:  0.095476895570755
[2m[36m(func pid=186938)[0m rmse_per_class: [0.123, 0.207, 0.048, 0.274, 0.154, 0.168, 0.224, 0.092, 0.138, 0.087]
[2m[36m(func pid=186938)[0m 
== Status ==
Current time: 2024-01-07 15:54:12 (running for 00:25:26.25)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15550000220537186
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.365 |  0.151 |                   77 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.698 |  0.177 |                   70 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.546 |  0.18  |                   42 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.412 |  0.173 |                   41 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4217 | Steps: 4 | Val loss: 0.3191 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=7413)[0m rmse: 0.17972728610038757
[2m[36m(func pid=7413)[0m mae:  0.13236215710639954
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.331, 0.1, 0.19, 0.298, 0.142, 0.144, 0.114]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.5638 | Steps: 4 | Val loss: 0.3829 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3392 | Steps: 4 | Val loss: 0.2793 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=7482)[0m rmse: 0.1740204393863678
[2m[36m(func pid=7482)[0m mae:  0.1271921843290329
[2m[36m(func pid=7482)[0m rmse_per_class: [0.119, 0.259, 0.109, 0.335, 0.064, 0.186, 0.278, 0.135, 0.148, 0.106]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5432 | Steps: 4 | Val loss: 0.4319 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=187535)[0m rmse: 0.17960354685783386
[2m[36m(func pid=187535)[0m mae:  0.11462293565273285
[2m[36m(func pid=187535)[0m rmse_per_class: [0.34, 0.226, 0.031, 0.274, 0.113, 0.2, 0.269, 0.123, 0.134, 0.087]
[2m[36m(func pid=187535)[0m 
== Status ==
Current time: 2024-01-07 15:54:17 (running for 00:25:31.27)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15550000220537186
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.339 |  0.156 |                   78 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.564 |  0.18  |                   71 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.542 |  0.18  |                   43 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.422 |  0.174 |                   42 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.15612144768238068
[2m[36m(func pid=186938)[0m mae:  0.0988910049200058
[2m[36m(func pid=186938)[0m rmse_per_class: [0.122, 0.216, 0.045, 0.283, 0.144, 0.167, 0.227, 0.102, 0.146, 0.11]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4107 | Steps: 4 | Val loss: 0.3187 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=7413)[0m rmse: 0.17965784668922424
[2m[36m(func pid=7413)[0m mae:  0.13230273127555847
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.332, 0.1, 0.19, 0.297, 0.141, 0.144, 0.115]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.5053 | Steps: 4 | Val loss: 0.3790 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3430 | Steps: 4 | Val loss: 0.2936 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=7482)[0m rmse: 0.17391785979270935
[2m[36m(func pid=7482)[0m mae:  0.12709513306617737
[2m[36m(func pid=7482)[0m rmse_per_class: [0.119, 0.259, 0.11, 0.335, 0.064, 0.186, 0.278, 0.134, 0.148, 0.106]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5327 | Steps: 4 | Val loss: 0.4305 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 15:54:22 (running for 00:25:36.50)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15550000220537186
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.343 |  0.165 |                   79 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.564 |  0.18  |                   71 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.543 |  0.18  |                   44 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.411 |  0.174 |                   43 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.16500291228294373
[2m[36m(func pid=186938)[0m mae:  0.10600407421588898
[2m[36m(func pid=186938)[0m rmse_per_class: [0.11, 0.219, 0.044, 0.293, 0.129, 0.165, 0.251, 0.119, 0.15, 0.17]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.17994222044944763
[2m[36m(func pid=187535)[0m mae:  0.10400845855474472
[2m[36m(func pid=187535)[0m rmse_per_class: [0.265, 0.213, 0.049, 0.295, 0.058, 0.231, 0.262, 0.112, 0.139, 0.175]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4186 | Steps: 4 | Val loss: 0.3186 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=7413)[0m rmse: 0.17968708276748657
[2m[36m(func pid=7413)[0m mae:  0.13232246041297913
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.332, 0.1, 0.19, 0.297, 0.141, 0.145, 0.114]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3732 | Steps: 4 | Val loss: 0.3103 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.6357 | Steps: 4 | Val loss: 0.4742 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=7482)[0m rmse: 0.1735265552997589
[2m[36m(func pid=7482)[0m mae:  0.12698587775230408
[2m[36m(func pid=7482)[0m rmse_per_class: [0.117, 0.259, 0.107, 0.335, 0.065, 0.185, 0.279, 0.134, 0.147, 0.106]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5358 | Steps: 4 | Val loss: 0.4268 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 15:54:27 (running for 00:25:41.57)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15550000220537186
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.373 |  0.173 |                   80 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.505 |  0.18  |                   72 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.533 |  0.18  |                   45 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.419 |  0.174 |                   44 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.1734224557876587
[2m[36m(func pid=186938)[0m mae:  0.11262641847133636
[2m[36m(func pid=186938)[0m rmse_per_class: [0.094, 0.223, 0.065, 0.308, 0.111, 0.162, 0.273, 0.143, 0.143, 0.213]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.19044682383537292
[2m[36m(func pid=187535)[0m mae:  0.11609592288732529
[2m[36m(func pid=187535)[0m rmse_per_class: [0.111, 0.196, 0.049, 0.367, 0.055, 0.359, 0.279, 0.138, 0.14, 0.211]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.17955397069454193
[2m[36m(func pid=7413)[0m mae:  0.13222327828407288
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.099, 0.332, 0.099, 0.19, 0.297, 0.141, 0.144, 0.114]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4103 | Steps: 4 | Val loss: 0.3180 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3390 | Steps: 4 | Val loss: 0.3095 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7366 | Steps: 4 | Val loss: 0.4082 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=7482)[0m rmse: 0.17320585250854492
[2m[36m(func pid=7482)[0m mae:  0.12686558067798615
[2m[36m(func pid=7482)[0m rmse_per_class: [0.117, 0.259, 0.103, 0.334, 0.065, 0.185, 0.279, 0.135, 0.148, 0.106]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5264 | Steps: 4 | Val loss: 0.4219 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 15:54:33 (running for 00:25:46.79)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15550000220537186
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.339 |  0.174 |                   81 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.636 |  0.19  |                   73 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.536 |  0.18  |                   46 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.41  |  0.173 |                   45 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.17352670431137085
[2m[36m(func pid=186938)[0m mae:  0.112282395362854
[2m[36m(func pid=186938)[0m rmse_per_class: [0.095, 0.21, 0.123, 0.316, 0.098, 0.163, 0.286, 0.133, 0.138, 0.173]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.18191267549991608
[2m[36m(func pid=187535)[0m mae:  0.11034488677978516
[2m[36m(func pid=187535)[0m rmse_per_class: [0.111, 0.367, 0.049, 0.297, 0.054, 0.303, 0.294, 0.126, 0.133, 0.085]
[2m[36m(func pid=187535)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.17946860194206238
[2m[36m(func pid=7413)[0m mae:  0.1321636289358139
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.332, 0.098, 0.19, 0.297, 0.141, 0.145, 0.114]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4103 | Steps: 4 | Val loss: 0.3180 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3761 | Steps: 4 | Val loss: 0.3046 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=187535)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.6912 | Steps: 4 | Val loss: 0.4349 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5294 | Steps: 4 | Val loss: 0.4224 | Batch size: 32 | lr: 0.0001 | Duration: 2.62s
[2m[36m(func pid=7482)[0m rmse: 0.1733945608139038
[2m[36m(func pid=7482)[0m mae:  0.12687300145626068
[2m[36m(func pid=7482)[0m rmse_per_class: [0.118, 0.259, 0.105, 0.335, 0.065, 0.185, 0.278, 0.134, 0.148, 0.107]
[2m[36m(func pid=7482)[0m 
== Status ==
Current time: 2024-01-07 15:54:38 (running for 00:25:52.15)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.15550000220537186
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.376 |  0.169 |                   82 |
| train_10f5e_00011 | RUNNING    | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.737 |  0.182 |                   74 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.526 |  0.179 |                   47 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.41  |  0.173 |                   46 |
| train_10f5e_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.16886006295681
[2m[36m(func pid=186938)[0m mae:  0.1089288741350174
[2m[36m(func pid=186938)[0m rmse_per_class: [0.097, 0.2, 0.158, 0.322, 0.079, 0.175, 0.286, 0.112, 0.138, 0.122]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=187535)[0m rmse: 0.18947294354438782
[2m[36m(func pid=187535)[0m mae:  0.122940793633461
[2m[36m(func pid=187535)[0m rmse_per_class: [0.12, 0.226, 0.049, 0.272, 0.066, 0.187, 0.307, 0.107, 0.473, 0.088]
[2m[36m(func pid=7413)[0m rmse: 0.17953428626060486
[2m[36m(func pid=7413)[0m mae:  0.13222888112068176
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.332, 0.099, 0.19, 0.297, 0.141, 0.145, 0.114]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4130 | Steps: 4 | Val loss: 0.3178 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3343 | Steps: 4 | Val loss: 0.2885 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5203 | Steps: 4 | Val loss: 0.4177 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=7482)[0m rmse: 0.1737673580646515
[2m[36m(func pid=7482)[0m mae:  0.1269116848707199
[2m[36m(func pid=7482)[0m rmse_per_class: [0.12, 0.259, 0.108, 0.336, 0.064, 0.185, 0.278, 0.134, 0.148, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.15745016932487488
[2m[36m(func pid=186938)[0m mae:  0.10247908532619476
[2m[36m(func pid=186938)[0m rmse_per_class: [0.103, 0.202, 0.106, 0.306, 0.069, 0.181, 0.274, 0.093, 0.145, 0.095]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.17950306832790375
[2m[36m(func pid=7413)[0m mae:  0.13219715654850006
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.332, 0.098, 0.19, 0.297, 0.141, 0.145, 0.114]
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4161 | Steps: 4 | Val loss: 0.3173 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3232 | Steps: 4 | Val loss: 0.2737 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=7482)[0m rmse: 0.17376841604709625
[2m[36m(func pid=7482)[0m mae:  0.12675881385803223
[2m[36m(func pid=7482)[0m rmse_per_class: [0.121, 0.259, 0.108, 0.336, 0.063, 0.185, 0.277, 0.134, 0.148, 0.107]
== Status ==
Current time: 2024-01-07 15:54:43 (running for 00:25:57.44)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.334 |  0.157 |                   83 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.529 |  0.18  |                   48 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.413 |  0.174 |                   47 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=19000)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=19000)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=19000)[0m Configuration completed!
[2m[36m(func pid=19000)[0m New optimizer parameters:
[2m[36m(func pid=19000)[0m SGD (
[2m[36m(func pid=19000)[0m Parameter Group 0
[2m[36m(func pid=19000)[0m     dampening: 0
[2m[36m(func pid=19000)[0m     differentiable: False
[2m[36m(func pid=19000)[0m     foreach: None
[2m[36m(func pid=19000)[0m     lr: 0.01
[2m[36m(func pid=19000)[0m     maximize: False
[2m[36m(func pid=19000)[0m     momentum: 0.9
[2m[36m(func pid=19000)[0m     nesterov: False
[2m[36m(func pid=19000)[0m     weight_decay: 0.0001
[2m[36m(func pid=19000)[0m )
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 15:54:49 (running for 00:26:02.76)
Memory usage on this node: 23.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.323 |  0.146 |                   84 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.52  |  0.18  |                   49 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.416 |  0.174 |                   48 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.14591538906097412
[2m[36m(func pid=186938)[0m mae:  0.09553767740726471
[2m[36m(func pid=186938)[0m rmse_per_class: [0.113, 0.2, 0.044, 0.284, 0.061, 0.173, 0.248, 0.097, 0.151, 0.087]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5220 | Steps: 4 | Val loss: 0.4122 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4102 | Steps: 4 | Val loss: 0.3163 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3512 | Steps: 4 | Val loss: 0.2678 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8857 | Steps: 4 | Val loss: 0.5974 | Batch size: 32 | lr: 0.01 | Duration: 4.58s
[2m[36m(func pid=7413)[0m rmse: 0.17942357063293457
[2m[36m(func pid=7413)[0m mae:  0.13215462863445282
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.332, 0.097, 0.19, 0.297, 0.141, 0.145, 0.114]
[2m[36m(func pid=7413)[0m 
== Status ==
Current time: 2024-01-07 15:54:54 (running for 00:26:07.88)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.323 |  0.146 |                   84 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.522 |  0.179 |                   50 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.41  |  0.173 |                   49 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=7482)[0m rmse: 0.17318245768547058
[2m[36m(func pid=7482)[0m mae:  0.12628594040870667
[2m[36m(func pid=7482)[0m rmse_per_class: [0.12, 0.259, 0.106, 0.334, 0.063, 0.185, 0.277, 0.134, 0.147, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.13853858411312103
[2m[36m(func pid=186938)[0m mae:  0.0905371829867363
[2m[36m(func pid=186938)[0m rmse_per_class: [0.11, 0.196, 0.03, 0.268, 0.056, 0.159, 0.223, 0.107, 0.149, 0.088]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.18006159365177155
[2m[36m(func pid=19000)[0m mae:  0.13252265751361847
[2m[36m(func pid=19000)[0m rmse_per_class: [0.114, 0.264, 0.103, 0.335, 0.101, 0.191, 0.295, 0.141, 0.143, 0.114]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5149 | Steps: 4 | Val loss: 0.4122 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3491 | Steps: 4 | Val loss: 0.2622 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4187 | Steps: 4 | Val loss: 0.3157 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5090 | Steps: 4 | Val loss: 0.4300 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 15:54:59 (running for 00:26:12.94)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.351 |  0.139 |                   85 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.515 |  0.179 |                   51 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.41  |  0.173 |                   49 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.886 |  0.18  |                    1 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.17943421006202698
[2m[36m(func pid=7413)[0m mae:  0.13215503096580505
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.099, 0.332, 0.098, 0.19, 0.297, 0.141, 0.145, 0.114]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.13522179424762726
[2m[36m(func pid=186938)[0m mae:  0.08716615289449692
[2m[36m(func pid=186938)[0m rmse_per_class: [0.097, 0.194, 0.033, 0.258, 0.059, 0.156, 0.217, 0.115, 0.135, 0.088]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17306570708751678
[2m[36m(func pid=7482)[0m mae:  0.1260424554347992
[2m[36m(func pid=7482)[0m rmse_per_class: [0.122, 0.259, 0.105, 0.333, 0.063, 0.185, 0.277, 0.134, 0.146, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.17948439717292786
[2m[36m(func pid=19000)[0m mae:  0.1319798231124878
[2m[36m(func pid=19000)[0m rmse_per_class: [0.116, 0.264, 0.107, 0.337, 0.098, 0.19, 0.29, 0.137, 0.146, 0.109]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5226 | Steps: 4 | Val loss: 0.4107 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3995 | Steps: 4 | Val loss: 0.2586 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4143 | Steps: 4 | Val loss: 0.3148 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 15:55:04 (running for 00:26:18.23)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.349 |  0.135 |                   86 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.523 |  0.179 |                   52 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.419 |  0.173 |                   50 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.509 |  0.179 |                    2 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.17931440472602844
[2m[36m(func pid=7413)[0m mae:  0.13206371665000916
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.264, 0.098, 0.332, 0.097, 0.19, 0.296, 0.141, 0.145, 0.114]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4517 | Steps: 4 | Val loss: 0.3427 | Batch size: 32 | lr: 0.01 | Duration: 3.26s
[2m[36m(func pid=186938)[0m rmse: 0.13609102368354797
[2m[36m(func pid=186938)[0m mae:  0.08636684715747833
[2m[36m(func pid=186938)[0m rmse_per_class: [0.099, 0.197, 0.035, 0.257, 0.06, 0.157, 0.219, 0.112, 0.131, 0.095]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17278967797756195
[2m[36m(func pid=7482)[0m mae:  0.12569797039031982
[2m[36m(func pid=7482)[0m rmse_per_class: [0.124, 0.259, 0.104, 0.332, 0.062, 0.184, 0.276, 0.134, 0.146, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.17840585112571716
[2m[36m(func pid=19000)[0m mae:  0.13091841340065002
[2m[36m(func pid=19000)[0m rmse_per_class: [0.126, 0.264, 0.107, 0.342, 0.087, 0.188, 0.282, 0.135, 0.148, 0.105]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5062 | Steps: 4 | Val loss: 0.4094 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3832 | Steps: 4 | Val loss: 0.2660 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4080 | Steps: 4 | Val loss: 0.3140 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 15:55:09 (running for 00:26:23.58)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.399 |  0.136 |                   87 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.506 |  0.179 |                   53 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.414 |  0.173 |                   51 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.452 |  0.178 |                    3 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.14048652350902557
[2m[36m(func pid=186938)[0m mae:  0.08922518789768219
[2m[36m(func pid=186938)[0m rmse_per_class: [0.1, 0.202, 0.034, 0.266, 0.078, 0.163, 0.219, 0.099, 0.134, 0.111]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.1793036162853241
[2m[36m(func pid=7413)[0m mae:  0.13205179572105408
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.332, 0.097, 0.19, 0.296, 0.141, 0.145, 0.114]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4960 | Steps: 4 | Val loss: 0.3220 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=7482)[0m rmse: 0.17194190621376038
[2m[36m(func pid=7482)[0m mae:  0.12524312734603882
[2m[36m(func pid=7482)[0m rmse_per_class: [0.12, 0.26, 0.098, 0.332, 0.063, 0.184, 0.275, 0.134, 0.146, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3343 | Steps: 4 | Val loss: 0.2888 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5090 | Steps: 4 | Val loss: 0.4075 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=19000)[0m rmse: 0.17681342363357544
[2m[36m(func pid=19000)[0m mae:  0.12908446788787842
[2m[36m(func pid=19000)[0m rmse_per_class: [0.142, 0.263, 0.099, 0.346, 0.073, 0.186, 0.273, 0.136, 0.148, 0.101]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4187 | Steps: 4 | Val loss: 0.3141 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 15:55:15 (running for 00:26:28.89)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.334 |  0.154 |                   89 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.506 |  0.179 |                   53 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.408 |  0.172 |                   52 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.496 |  0.177 |                    4 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.1541460007429123
[2m[36m(func pid=186938)[0m mae:  0.09932549297809601
[2m[36m(func pid=186938)[0m rmse_per_class: [0.117, 0.204, 0.028, 0.291, 0.097, 0.171, 0.238, 0.096, 0.144, 0.156]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.17921139299869537
[2m[36m(func pid=7413)[0m mae:  0.1319705694913864
[2m[36m(func pid=7413)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.097, 0.19, 0.296, 0.14, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17212386429309845
[2m[36m(func pid=7482)[0m mae:  0.12528583407402039
[2m[36m(func pid=7482)[0m rmse_per_class: [0.123, 0.259, 0.098, 0.332, 0.063, 0.184, 0.275, 0.134, 0.146, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5019 | Steps: 4 | Val loss: 0.3204 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3261 | Steps: 4 | Val loss: 0.3032 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5097 | Steps: 4 | Val loss: 0.4082 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=19000)[0m rmse: 0.17519353330135345
[2m[36m(func pid=19000)[0m mae:  0.12714585661888123
[2m[36m(func pid=19000)[0m rmse_per_class: [0.148, 0.263, 0.09, 0.349, 0.065, 0.184, 0.268, 0.138, 0.147, 0.101]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4100 | Steps: 4 | Val loss: 0.3143 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 15:55:20 (running for 00:26:34.00)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.326 |  0.165 |                   90 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.509 |  0.179 |                   54 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.419 |  0.172 |                   53 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.502 |  0.175 |                    5 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.16468004882335663
[2m[36m(func pid=186938)[0m mae:  0.10763194411993027
[2m[36m(func pid=186938)[0m rmse_per_class: [0.118, 0.211, 0.031, 0.319, 0.118, 0.169, 0.25, 0.093, 0.148, 0.19]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.17924697697162628
[2m[36m(func pid=7413)[0m mae:  0.1320033222436905
[2m[36m(func pid=7413)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.332, 0.098, 0.19, 0.296, 0.14, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17218470573425293
[2m[36m(func pid=7482)[0m mae:  0.12528447806835175
[2m[36m(func pid=7482)[0m rmse_per_class: [0.124, 0.259, 0.097, 0.333, 0.063, 0.184, 0.275, 0.134, 0.146, 0.106]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4889 | Steps: 4 | Val loss: 0.3193 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3558 | Steps: 4 | Val loss: 0.3164 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5100 | Steps: 4 | Val loss: 0.4067 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4103 | Steps: 4 | Val loss: 0.3135 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=19000)[0m rmse: 0.17421875894069672
[2m[36m(func pid=19000)[0m mae:  0.1263304054737091
[2m[36m(func pid=19000)[0m rmse_per_class: [0.142, 0.262, 0.088, 0.351, 0.061, 0.182, 0.265, 0.139, 0.15, 0.103]
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 15:55:25 (running for 00:26:39.20)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.356 |  0.173 |                   91 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.51  |  0.179 |                   55 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.41  |  0.172 |                   54 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.489 |  0.174 |                    6 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.17310446500778198
[2m[36m(func pid=186938)[0m mae:  0.11363468319177628
[2m[36m(func pid=186938)[0m rmse_per_class: [0.127, 0.219, 0.041, 0.34, 0.124, 0.162, 0.258, 0.097, 0.153, 0.209]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.17923448979854584
[2m[36m(func pid=7413)[0m mae:  0.1319848746061325
[2m[36m(func pid=7413)[0m rmse_per_class: [0.116, 0.265, 0.098, 0.332, 0.098, 0.19, 0.296, 0.14, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17127318680286407
[2m[36m(func pid=7482)[0m mae:  0.12494061142206192
[2m[36m(func pid=7482)[0m rmse_per_class: [0.12, 0.259, 0.091, 0.333, 0.064, 0.184, 0.275, 0.135, 0.146, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4509 | Steps: 4 | Val loss: 0.3197 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3634 | Steps: 4 | Val loss: 0.3203 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5082 | Steps: 4 | Val loss: 0.4038 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4118 | Steps: 4 | Val loss: 0.3135 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=19000)[0m rmse: 0.17407330870628357
[2m[36m(func pid=19000)[0m mae:  0.12637139856815338
[2m[36m(func pid=19000)[0m rmse_per_class: [0.134, 0.262, 0.086, 0.352, 0.058, 0.181, 0.265, 0.139, 0.16, 0.104]
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 15:55:30 (running for 00:26:44.40)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.363 |  0.175 |                   92 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.51  |  0.179 |                   56 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.41  |  0.171 |                   55 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.451 |  0.174 |                    7 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.17491655051708221
[2m[36m(func pid=186938)[0m mae:  0.1151718869805336
[2m[36m(func pid=186938)[0m rmse_per_class: [0.124, 0.214, 0.064, 0.347, 0.129, 0.163, 0.261, 0.092, 0.168, 0.187]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.1791912168264389
[2m[36m(func pid=7413)[0m mae:  0.1319626271724701
[2m[36m(func pid=7413)[0m rmse_per_class: [0.116, 0.264, 0.099, 0.332, 0.097, 0.19, 0.296, 0.14, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17153850197792053
[2m[36m(func pid=7482)[0m mae:  0.1250341385602951
[2m[36m(func pid=7482)[0m rmse_per_class: [0.123, 0.259, 0.091, 0.334, 0.063, 0.184, 0.275, 0.135, 0.145, 0.108]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4329 | Steps: 4 | Val loss: 0.3192 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3271 | Steps: 4 | Val loss: 0.2977 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4987 | Steps: 4 | Val loss: 0.3995 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4046 | Steps: 4 | Val loss: 0.3135 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=19000)[0m rmse: 0.17407549917697906
[2m[36m(func pid=19000)[0m mae:  0.12663128972053528
[2m[36m(func pid=19000)[0m rmse_per_class: [0.126, 0.262, 0.084, 0.348, 0.057, 0.18, 0.267, 0.138, 0.175, 0.105]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=186938)[0m rmse: 0.16461023688316345
[2m[36m(func pid=186938)[0m mae:  0.10700587183237076
[2m[36m(func pid=186938)[0m rmse_per_class: [0.107, 0.201, 0.08, 0.327, 0.14, 0.164, 0.248, 0.091, 0.162, 0.127]
[2m[36m(func pid=186938)[0m 
== Status ==
Current time: 2024-01-07 15:55:36 (running for 00:26:49.75)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.327 |  0.165 |                   93 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.508 |  0.179 |                   57 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.412 |  0.172 |                   56 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.433 |  0.174 |                    8 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.17913052439689636
[2m[36m(func pid=7413)[0m mae:  0.13192972540855408
[2m[36m(func pid=7413)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.096, 0.19, 0.296, 0.14, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.171408548951149
[2m[36m(func pid=7482)[0m mae:  0.12502971291542053
[2m[36m(func pid=7482)[0m rmse_per_class: [0.122, 0.258, 0.089, 0.334, 0.064, 0.184, 0.275, 0.135, 0.145, 0.109]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4291 | Steps: 4 | Val loss: 0.3186 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3299 | Steps: 4 | Val loss: 0.2818 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5034 | Steps: 4 | Val loss: 0.3988 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4081 | Steps: 4 | Val loss: 0.3127 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=19000)[0m rmse: 0.17396560311317444
[2m[36m(func pid=19000)[0m mae:  0.12652094662189484
[2m[36m(func pid=19000)[0m rmse_per_class: [0.113, 0.26, 0.099, 0.339, 0.056, 0.179, 0.273, 0.137, 0.173, 0.112]
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 15:55:41 (running for 00:26:54.81)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.33  |  0.154 |                   94 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.499 |  0.179 |                   58 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.405 |  0.171 |                   57 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.429 |  0.174 |                    9 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.15429671108722687
[2m[36m(func pid=186938)[0m mae:  0.09918303787708282
[2m[36m(func pid=186938)[0m rmse_per_class: [0.092, 0.2, 0.068, 0.306, 0.14, 0.16, 0.233, 0.104, 0.149, 0.092]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.17903117835521698
[2m[36m(func pid=7413)[0m mae:  0.13185127079486847
[2m[36m(func pid=7413)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.096, 0.189, 0.296, 0.14, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17099764943122864
[2m[36m(func pid=7482)[0m mae:  0.12490548193454742
[2m[36m(func pid=7482)[0m rmse_per_class: [0.12, 0.258, 0.086, 0.334, 0.065, 0.183, 0.275, 0.135, 0.145, 0.109]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3154 | Steps: 4 | Val loss: 0.2736 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4124 | Steps: 4 | Val loss: 0.3126 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5088 | Steps: 4 | Val loss: 0.3985 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4040 | Steps: 4 | Val loss: 0.3126 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 15:55:46 (running for 00:27:00.18)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.315 |  0.146 |                   95 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.503 |  0.179 |                   59 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.408 |  0.171 |                   58 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.429 |  0.174 |                    9 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.14645496010780334
[2m[36m(func pid=186938)[0m mae:  0.09417261183261871
[2m[36m(func pid=186938)[0m rmse_per_class: [0.09, 0.201, 0.039, 0.277, 0.138, 0.158, 0.232, 0.107, 0.136, 0.086]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1711224615573883
[2m[36m(func pid=19000)[0m mae:  0.12415947765111923
[2m[36m(func pid=19000)[0m rmse_per_class: [0.104, 0.257, 0.099, 0.325, 0.056, 0.179, 0.278, 0.134, 0.156, 0.123]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.17900192737579346
[2m[36m(func pid=7413)[0m mae:  0.1318129599094391
[2m[36m(func pid=7413)[0m rmse_per_class: [0.116, 0.264, 0.099, 0.333, 0.096, 0.189, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.1709737479686737
[2m[36m(func pid=7482)[0m mae:  0.12486350536346436
[2m[36m(func pid=7482)[0m rmse_per_class: [0.12, 0.258, 0.086, 0.333, 0.066, 0.183, 0.275, 0.135, 0.144, 0.109]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3471 | Steps: 4 | Val loss: 0.2711 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4064 | Steps: 4 | Val loss: 0.3059 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4981 | Steps: 4 | Val loss: 0.3975 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4031 | Steps: 4 | Val loss: 0.3125 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=186938)[0m rmse: 0.14381784200668335
[2m[36m(func pid=186938)[0m mae:  0.09215384721755981
[2m[36m(func pid=186938)[0m rmse_per_class: [0.091, 0.202, 0.029, 0.26, 0.131, 0.156, 0.24, 0.111, 0.131, 0.086]
== Status ==
Current time: 2024-01-07 15:55:51 (running for 00:27:05.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.347 |  0.144 |                   96 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.509 |  0.179 |                   60 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.404 |  0.171 |                   59 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.412 |  0.171 |                   10 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1671995222568512
[2m[36m(func pid=19000)[0m mae:  0.12109023332595825
[2m[36m(func pid=19000)[0m rmse_per_class: [0.1, 0.256, 0.081, 0.311, 0.056, 0.18, 0.279, 0.132, 0.145, 0.132]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.1789342761039734
[2m[36m(func pid=7413)[0m mae:  0.1317846029996872
[2m[36m(func pid=7413)[0m rmse_per_class: [0.116, 0.264, 0.098, 0.332, 0.096, 0.19, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17088958621025085
[2m[36m(func pid=7482)[0m mae:  0.12485750764608383
[2m[36m(func pid=7482)[0m rmse_per_class: [0.12, 0.258, 0.086, 0.333, 0.066, 0.184, 0.276, 0.135, 0.144, 0.109]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3412 | Steps: 4 | Val loss: 0.2700 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5005 | Steps: 4 | Val loss: 0.3963 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4110 | Steps: 4 | Val loss: 0.3004 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4015 | Steps: 4 | Val loss: 0.3124 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 15:55:57 (running for 00:27:10.99)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.341 |  0.144 |                   97 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.498 |  0.179 |                   61 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.403 |  0.171 |                   60 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.406 |  0.167 |                   11 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.1442345678806305
[2m[36m(func pid=186938)[0m mae:  0.09225685894489288
[2m[36m(func pid=186938)[0m rmse_per_class: [0.092, 0.2, 0.026, 0.262, 0.128, 0.157, 0.258, 0.103, 0.13, 0.086]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.17892569303512573
[2m[36m(func pid=7413)[0m mae:  0.13177579641342163
[2m[36m(func pid=7413)[0m rmse_per_class: [0.116, 0.264, 0.098, 0.333, 0.095, 0.19, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.16406887769699097
[2m[36m(func pid=19000)[0m mae:  0.11873092502355576
[2m[36m(func pid=19000)[0m rmse_per_class: [0.1, 0.255, 0.062, 0.305, 0.058, 0.18, 0.276, 0.132, 0.139, 0.133]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17098838090896606
[2m[36m(func pid=7482)[0m mae:  0.12492541968822479
[2m[36m(func pid=7482)[0m rmse_per_class: [0.119, 0.257, 0.087, 0.334, 0.066, 0.184, 0.276, 0.135, 0.144, 0.109]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3405 | Steps: 4 | Val loss: 0.2676 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4981 | Steps: 4 | Val loss: 0.3957 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3999 | Steps: 4 | Val loss: 0.2984 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4027 | Steps: 4 | Val loss: 0.3119 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 15:56:02 (running for 00:27:16.08)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.341 |  0.146 |                   98 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.5   |  0.179 |                   62 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.401 |  0.171 |                   61 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.411 |  0.164 |                   12 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.1460852175951004
[2m[36m(func pid=186938)[0m mae:  0.09204961359500885
[2m[36m(func pid=186938)[0m rmse_per_class: [0.108, 0.195, 0.026, 0.261, 0.122, 0.157, 0.259, 0.116, 0.13, 0.087]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.17890790104866028
[2m[36m(func pid=7413)[0m mae:  0.13176804780960083
[2m[36m(func pid=7413)[0m rmse_per_class: [0.116, 0.264, 0.098, 0.333, 0.096, 0.19, 0.295, 0.14, 0.145, 0.112]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1638954132795334
[2m[36m(func pid=19000)[0m mae:  0.11880682408809662
[2m[36m(func pid=19000)[0m rmse_per_class: [0.102, 0.254, 0.056, 0.314, 0.06, 0.18, 0.271, 0.134, 0.143, 0.124]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17078280448913574
[2m[36m(func pid=7482)[0m mae:  0.12482650578022003
[2m[36m(func pid=7482)[0m rmse_per_class: [0.118, 0.257, 0.087, 0.334, 0.066, 0.184, 0.276, 0.135, 0.144, 0.108]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3746 | Steps: 4 | Val loss: 0.2758 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4904 | Steps: 4 | Val loss: 0.3928 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3984 | Steps: 4 | Val loss: 0.3017 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4030 | Steps: 4 | Val loss: 0.3118 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 15:56:07 (running for 00:27:21.19)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00010 | RUNNING    | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.375 |  0.153 |                   99 |
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.498 |  0.179 |                   63 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.403 |  0.171 |                   62 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.4   |  0.164 |                   13 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.15251001715660095
[2m[36m(func pid=186938)[0m mae:  0.0946064293384552
[2m[36m(func pid=186938)[0m rmse_per_class: [0.141, 0.199, 0.029, 0.276, 0.121, 0.169, 0.258, 0.117, 0.13, 0.086]
[2m[36m(func pid=186938)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.17883017659187317
[2m[36m(func pid=7413)[0m mae:  0.1316937506198883
[2m[36m(func pid=7413)[0m rmse_per_class: [0.116, 0.264, 0.099, 0.332, 0.095, 0.19, 0.295, 0.14, 0.145, 0.112]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1665305495262146
[2m[36m(func pid=19000)[0m mae:  0.12053785473108292
[2m[36m(func pid=19000)[0m rmse_per_class: [0.109, 0.255, 0.059, 0.328, 0.064, 0.179, 0.269, 0.135, 0.145, 0.123]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17085464298725128
[2m[36m(func pid=7482)[0m mae:  0.12479964643716812
[2m[36m(func pid=7482)[0m rmse_per_class: [0.118, 0.257, 0.088, 0.334, 0.066, 0.183, 0.276, 0.135, 0.144, 0.108]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=186938)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3619 | Steps: 4 | Val loss: 0.2826 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4917 | Steps: 4 | Val loss: 0.3900 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3985 | Steps: 4 | Val loss: 0.3085 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 15:56:12 (running for 00:27:26.48)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 3 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.49  |  0.179 |                   64 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.403 |  0.171 |                   63 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.398 |  0.167 |                   14 |
| train_10f5e_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 TERMINATED)


[2m[36m(func pid=186938)[0m rmse: 0.15473759174346924
[2m[36m(func pid=186938)[0m mae:  0.09699946641921997
[2m[36m(func pid=186938)[0m rmse_per_class: [0.182, 0.198, 0.028, 0.292, 0.101, 0.174, 0.252, 0.098, 0.134, 0.088]
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3974 | Steps: 4 | Val loss: 0.3116 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=7413)[0m rmse: 0.17877452075481415
[2m[36m(func pid=7413)[0m mae:  0.13163888454437256
[2m[36m(func pid=7413)[0m rmse_per_class: [0.116, 0.264, 0.099, 0.333, 0.094, 0.189, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1698240041732788
[2m[36m(func pid=19000)[0m mae:  0.12274299561977386
[2m[36m(func pid=19000)[0m rmse_per_class: [0.112, 0.255, 0.063, 0.342, 0.068, 0.179, 0.268, 0.135, 0.143, 0.133]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17097164690494537
[2m[36m(func pid=7482)[0m mae:  0.12478910386562347
[2m[36m(func pid=7482)[0m rmse_per_class: [0.12, 0.257, 0.089, 0.334, 0.066, 0.183, 0.276, 0.134, 0.144, 0.108]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4900 | Steps: 4 | Val loss: 0.3902 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3918 | Steps: 4 | Val loss: 0.3091 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4030 | Steps: 4 | Val loss: 0.3113 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=7413)[0m rmse: 0.1787858009338379
[2m[36m(func pid=7413)[0m mae:  0.13164469599723816
[2m[36m(func pid=7413)[0m rmse_per_class: [0.116, 0.264, 0.099, 0.333, 0.094, 0.189, 0.294, 0.14, 0.145, 0.112]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m rmse: 0.17079225182533264
[2m[36m(func pid=7482)[0m mae:  0.12467477470636368
[2m[36m(func pid=7482)[0m rmse_per_class: [0.12, 0.257, 0.087, 0.334, 0.065, 0.183, 0.275, 0.134, 0.144, 0.108]
[2m[36m(func pid=19000)[0m rmse: 0.1696990728378296
[2m[36m(func pid=19000)[0m mae:  0.12239518016576767
[2m[36m(func pid=19000)[0m rmse_per_class: [0.109, 0.254, 0.065, 0.345, 0.07, 0.179, 0.265, 0.139, 0.141, 0.129]
== Status ==
Current time: 2024-01-07 15:56:19 (running for 00:27:32.69)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.49  |  0.179 |                   66 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.397 |  0.171 |                   64 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.398 |  0.17  |                   15 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=23101)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=23101)[0m Configuration completed!
[2m[36m(func pid=23101)[0m New optimizer parameters:
[2m[36m(func pid=23101)[0m SGD (
[2m[36m(func pid=23101)[0m Parameter Group 0
[2m[36m(func pid=23101)[0m     dampening: 0
[2m[36m(func pid=23101)[0m     differentiable: False
[2m[36m(func pid=23101)[0m     foreach: None
[2m[36m(func pid=23101)[0m     lr: 0.1
[2m[36m(func pid=23101)[0m     maximize: False
[2m[36m(func pid=23101)[0m     momentum: 0.9
[2m[36m(func pid=23101)[0m     nesterov: False
[2m[36m(func pid=23101)[0m     weight_decay: 0.0001
[2m[36m(func pid=23101)[0m )
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4874 | Steps: 4 | Val loss: 0.3870 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 15:56:24 (running for 00:27:37.94)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.487 |  0.179 |                   67 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.403 |  0.171 |                   65 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.392 |  0.17  |                   16 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.1787334680557251
[2m[36m(func pid=7413)[0m mae:  0.13158941268920898
[2m[36m(func pid=7413)[0m rmse_per_class: [0.117, 0.264, 0.099, 0.333, 0.094, 0.189, 0.294, 0.14, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3992 | Steps: 4 | Val loss: 0.3116 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3947 | Steps: 4 | Val loss: 0.3045 | Batch size: 32 | lr: 0.01 | Duration: 3.25s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.6529 | Steps: 4 | Val loss: 0.3622 | Batch size: 32 | lr: 0.1 | Duration: 4.76s
[2m[36m(func pid=7482)[0m rmse: 0.17132392525672913
[2m[36m(func pid=7482)[0m mae:  0.12488651275634766
[2m[36m(func pid=7482)[0m rmse_per_class: [0.124, 0.257, 0.089, 0.334, 0.065, 0.183, 0.275, 0.134, 0.145, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1682935506105423
[2m[36m(func pid=19000)[0m mae:  0.12163136899471283
[2m[36m(func pid=19000)[0m rmse_per_class: [0.104, 0.254, 0.061, 0.338, 0.073, 0.179, 0.267, 0.139, 0.141, 0.127]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4916 | Steps: 4 | Val loss: 0.3860 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=23101)[0m rmse: 0.1775895059108734
[2m[36m(func pid=23101)[0m mae:  0.1292552798986435
[2m[36m(func pid=23101)[0m rmse_per_class: [0.117, 0.265, 0.119, 0.347, 0.077, 0.188, 0.279, 0.138, 0.148, 0.098]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 15:56:29 (running for 00:27:43.37)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.492 |  0.179 |                   68 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.171 |                   66 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.395 |  0.168 |                   17 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.653 |  0.178 |                    1 |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.1786307990550995
[2m[36m(func pid=7413)[0m mae:  0.1315121352672577
[2m[36m(func pid=7413)[0m rmse_per_class: [0.117, 0.264, 0.098, 0.333, 0.093, 0.189, 0.294, 0.14, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4064 | Steps: 4 | Val loss: 0.3121 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3797 | Steps: 4 | Val loss: 0.3023 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.6971 | Steps: 4 | Val loss: 0.3219 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=7482)[0m rmse: 0.17217054963111877
[2m[36m(func pid=7482)[0m mae:  0.1251956820487976
[2m[36m(func pid=7482)[0m rmse_per_class: [0.13, 0.257, 0.092, 0.335, 0.064, 0.183, 0.274, 0.133, 0.145, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4815 | Steps: 4 | Val loss: 0.3840 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=19000)[0m rmse: 0.1680213063955307
[2m[36m(func pid=19000)[0m mae:  0.1211424246430397
[2m[36m(func pid=19000)[0m rmse_per_class: [0.107, 0.254, 0.074, 0.333, 0.072, 0.178, 0.268, 0.134, 0.142, 0.118]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.17308712005615234
[2m[36m(func pid=23101)[0m mae:  0.12123791873455048
[2m[36m(func pid=23101)[0m rmse_per_class: [0.108, 0.268, 0.105, 0.358, 0.055, 0.19, 0.279, 0.134, 0.144, 0.091]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 15:56:34 (running for 00:27:48.56)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.481 |  0.179 |                   69 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.406 |  0.172 |                   67 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.38  |  0.168 |                   18 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.697 |  0.173 |                    2 |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.1786501556634903
[2m[36m(func pid=7413)[0m mae:  0.1315428912639618
[2m[36m(func pid=7413)[0m rmse_per_class: [0.117, 0.264, 0.098, 0.333, 0.093, 0.189, 0.294, 0.14, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4056 | Steps: 4 | Val loss: 0.3125 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3894 | Steps: 4 | Val loss: 0.2976 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5931 | Steps: 4 | Val loss: 0.3687 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4820 | Steps: 4 | Val loss: 0.3825 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=7482)[0m rmse: 0.17222490906715393
[2m[36m(func pid=7482)[0m mae:  0.12518270313739777
[2m[36m(func pid=7482)[0m rmse_per_class: [0.13, 0.257, 0.093, 0.336, 0.064, 0.183, 0.274, 0.133, 0.145, 0.106]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.165687695145607
[2m[36m(func pid=19000)[0m mae:  0.1190873235464096
[2m[36m(func pid=19000)[0m rmse_per_class: [0.109, 0.253, 0.074, 0.328, 0.07, 0.177, 0.266, 0.131, 0.141, 0.109]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.175417959690094
[2m[36m(func pid=23101)[0m mae:  0.12611468136310577
[2m[36m(func pid=23101)[0m rmse_per_class: [0.117, 0.259, 0.069, 0.361, 0.056, 0.191, 0.274, 0.153, 0.182, 0.093]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=7413)[0m rmse: 0.1785764843225479
[2m[36m(func pid=7413)[0m mae:  0.13147418200969696
[2m[36m(func pid=7413)[0m rmse_per_class: [0.117, 0.264, 0.099, 0.333, 0.093, 0.189, 0.294, 0.14, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
== Status ==
Current time: 2024-01-07 15:56:40 (running for 00:27:53.70)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.482 |  0.179 |                   70 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.406 |  0.172 |                   68 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.389 |  0.166 |                   19 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.593 |  0.175 |                    3 |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4093 | Steps: 4 | Val loss: 0.3123 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3851 | Steps: 4 | Val loss: 0.2922 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.5239 | Steps: 4 | Val loss: 0.3321 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4785 | Steps: 4 | Val loss: 0.3825 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=7482)[0m rmse: 0.17255814373493195
[2m[36m(func pid=7482)[0m mae:  0.12512728571891785
[2m[36m(func pid=7482)[0m rmse_per_class: [0.132, 0.257, 0.096, 0.336, 0.064, 0.183, 0.273, 0.133, 0.145, 0.106]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1627560704946518
[2m[36m(func pid=19000)[0m mae:  0.11693372577428818
[2m[36m(func pid=19000)[0m rmse_per_class: [0.112, 0.251, 0.063, 0.318, 0.069, 0.177, 0.265, 0.13, 0.14, 0.104]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.16460344195365906
[2m[36m(func pid=23101)[0m mae:  0.11644778400659561
[2m[36m(func pid=23101)[0m rmse_per_class: [0.101, 0.26, 0.045, 0.3, 0.056, 0.185, 0.295, 0.137, 0.133, 0.134]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 15:56:45 (running for 00:27:59.08)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.478 |  0.179 |                   71 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.409 |  0.173 |                   69 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.385 |  0.163 |                   20 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.524 |  0.165 |                    4 |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.17851167917251587
[2m[36m(func pid=7413)[0m mae:  0.1314133256673813
[2m[36m(func pid=7413)[0m rmse_per_class: [0.117, 0.264, 0.098, 0.333, 0.093, 0.189, 0.293, 0.14, 0.145, 0.112]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4011 | Steps: 4 | Val loss: 0.3111 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3887 | Steps: 4 | Val loss: 0.2885 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5003 | Steps: 4 | Val loss: 0.2975 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4893 | Steps: 4 | Val loss: 0.3840 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=7482)[0m rmse: 0.17158663272857666
[2m[36m(func pid=7482)[0m mae:  0.12462717294692993
[2m[36m(func pid=7482)[0m rmse_per_class: [0.126, 0.257, 0.093, 0.336, 0.064, 0.183, 0.273, 0.133, 0.145, 0.106]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.16051968932151794
[2m[36m(func pid=19000)[0m mae:  0.11546985059976578
[2m[36m(func pid=19000)[0m rmse_per_class: [0.11, 0.248, 0.053, 0.311, 0.069, 0.177, 0.267, 0.128, 0.137, 0.104]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.16944348812103271
[2m[36m(func pid=23101)[0m mae:  0.11750875413417816
[2m[36m(func pid=23101)[0m rmse_per_class: [0.101, 0.24, 0.047, 0.31, 0.056, 0.183, 0.257, 0.132, 0.133, 0.236]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 15:56:50 (running for 00:28:04.37)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.489 |  0.179 |                   72 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.401 |  0.172 |                   70 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.389 |  0.161 |                   21 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.5   |  0.169 |                    5 |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.17852289974689484
[2m[36m(func pid=7413)[0m mae:  0.13145367801189423
[2m[36m(func pid=7413)[0m rmse_per_class: [0.117, 0.264, 0.098, 0.333, 0.094, 0.189, 0.293, 0.139, 0.145, 0.112]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4160 | Steps: 4 | Val loss: 0.3106 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3823 | Steps: 4 | Val loss: 0.2874 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4524 | Steps: 4 | Val loss: 0.3084 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4811 | Steps: 4 | Val loss: 0.3843 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=7482)[0m rmse: 0.17152521014213562
[2m[36m(func pid=7482)[0m mae:  0.12444417178630829
[2m[36m(func pid=7482)[0m rmse_per_class: [0.127, 0.257, 0.093, 0.335, 0.064, 0.182, 0.272, 0.133, 0.145, 0.106]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.16733430325984955
[2m[36m(func pid=23101)[0m mae:  0.11790908873081207
[2m[36m(func pid=23101)[0m rmse_per_class: [0.131, 0.232, 0.043, 0.357, 0.055, 0.194, 0.265, 0.119, 0.186, 0.09]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1597367227077484
[2m[36m(func pid=19000)[0m mae:  0.11488822847604752
[2m[36m(func pid=19000)[0m rmse_per_class: [0.103, 0.248, 0.057, 0.311, 0.067, 0.176, 0.266, 0.128, 0.138, 0.104]
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 15:56:56 (running for 00:28:09.67)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.481 |  0.179 |                   73 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.416 |  0.172 |                   71 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.382 |  0.16  |                   22 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.452 |  0.167 |                    6 |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.1785004585981369
[2m[36m(func pid=7413)[0m mae:  0.13144955039024353
[2m[36m(func pid=7413)[0m rmse_per_class: [0.117, 0.264, 0.098, 0.333, 0.094, 0.189, 0.294, 0.139, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3968 | Steps: 4 | Val loss: 0.3096 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4420 | Steps: 4 | Val loss: 0.2968 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3843 | Steps: 4 | Val loss: 0.2878 | Batch size: 32 | lr: 0.01 | Duration: 3.30s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4776 | Steps: 4 | Val loss: 0.3816 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=7482)[0m rmse: 0.1707213670015335
[2m[36m(func pid=7482)[0m mae:  0.12401741743087769
[2m[36m(func pid=7482)[0m rmse_per_class: [0.123, 0.257, 0.089, 0.335, 0.065, 0.182, 0.272, 0.133, 0.145, 0.106]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.16351081430912018
[2m[36m(func pid=23101)[0m mae:  0.11285020411014557
[2m[36m(func pid=23101)[0m rmse_per_class: [0.101, 0.242, 0.042, 0.33, 0.061, 0.205, 0.278, 0.152, 0.133, 0.09]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 15:57:01 (running for 00:28:14.96)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00012 | RUNNING    | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.481 |  0.179 |                   73 |
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.397 |  0.171 |                   72 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.384 |  0.16  |                   23 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.442 |  0.164 |                    7 |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.17841020226478577
[2m[36m(func pid=7413)[0m mae:  0.13138267397880554
[2m[36m(func pid=7413)[0m rmse_per_class: [0.117, 0.264, 0.098, 0.333, 0.093, 0.189, 0.294, 0.139, 0.145, 0.113]
[2m[36m(func pid=7413)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15990474820137024
[2m[36m(func pid=19000)[0m mae:  0.11527960002422333
[2m[36m(func pid=19000)[0m rmse_per_class: [0.099, 0.246, 0.059, 0.312, 0.065, 0.177, 0.266, 0.128, 0.141, 0.105]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4060 | Steps: 4 | Val loss: 0.3095 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4325 | Steps: 4 | Val loss: 0.2999 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=7413)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4739 | Steps: 4 | Val loss: 0.3797 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3784 | Steps: 4 | Val loss: 0.2906 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=7482)[0m rmse: 0.17025794088840485
[2m[36m(func pid=7482)[0m mae:  0.123714879155159
[2m[36m(func pid=7482)[0m rmse_per_class: [0.12, 0.257, 0.087, 0.335, 0.066, 0.182, 0.271, 0.133, 0.145, 0.106]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.1704184114933014
[2m[36m(func pid=23101)[0m mae:  0.11655382812023163
[2m[36m(func pid=23101)[0m rmse_per_class: [0.098, 0.244, 0.041, 0.295, 0.2, 0.182, 0.26, 0.111, 0.135, 0.139]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 15:57:06 (running for 00:28:20.27)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15700000524520874
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 3 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.406 |  0.17  |                   73 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.384 |  0.16  |                   23 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.433 |  0.17  |                    8 |
| train_10f5e_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=7413)[0m rmse: 0.17836229503154755
[2m[36m(func pid=7413)[0m mae:  0.13134905695915222
[2m[36m(func pid=7413)[0m rmse_per_class: [0.117, 0.264, 0.098, 0.333, 0.092, 0.189, 0.293, 0.139, 0.145, 0.112]
[2m[36m(func pid=19000)[0m rmse: 0.16094088554382324
[2m[36m(func pid=19000)[0m mae:  0.1165483146905899
[2m[36m(func pid=19000)[0m rmse_per_class: [0.098, 0.245, 0.057, 0.32, 0.065, 0.177, 0.269, 0.127, 0.144, 0.107]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3989 | Steps: 4 | Val loss: 0.3095 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3852 | Steps: 4 | Val loss: 0.3385 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=7482)[0m rmse: 0.1704416424036026
[2m[36m(func pid=7482)[0m mae:  0.1237066239118576
[2m[36m(func pid=7482)[0m rmse_per_class: [0.121, 0.257, 0.089, 0.336, 0.066, 0.182, 0.271, 0.133, 0.145, 0.107]
[2m[36m(func pid=7482)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3819 | Steps: 4 | Val loss: 0.2929 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=23101)[0m rmse: 0.18371422588825226
[2m[36m(func pid=23101)[0m mae:  0.12841932475566864
[2m[36m(func pid=23101)[0m rmse_per_class: [0.098, 0.233, 0.041, 0.361, 0.162, 0.18, 0.281, 0.122, 0.253, 0.107]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1620105803012848
[2m[36m(func pid=19000)[0m mae:  0.11731666326522827
[2m[36m(func pid=19000)[0m rmse_per_class: [0.098, 0.244, 0.057, 0.327, 0.066, 0.176, 0.268, 0.127, 0.142, 0.115]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=7482)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3991 | Steps: 4 | Val loss: 0.3091 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4237 | Steps: 4 | Val loss: 0.2792 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 15:57:12 (running for 00:28:26.27)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.15700000524520874
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00013 | RUNNING    | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   74 |
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.382 |  0.162 |                   25 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.385 |  0.184 |                    9 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=7482)[0m rmse: 0.17004241049289703
[2m[36m(func pid=7482)[0m mae:  0.12351701408624649
[2m[36m(func pid=7482)[0m rmse_per_class: [0.118, 0.257, 0.087, 0.336, 0.066, 0.182, 0.27, 0.133, 0.145, 0.107]
[2m[36m(func pid=25864)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=25864)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=25864)[0m Configuration completed!
[2m[36m(func pid=25864)[0m New optimizer parameters:
[2m[36m(func pid=25864)[0m SGD (
[2m[36m(func pid=25864)[0m Parameter Group 0
[2m[36m(func pid=25864)[0m     dampening: 0
[2m[36m(func pid=25864)[0m     differentiable: False
[2m[36m(func pid=25864)[0m     foreach: None
[2m[36m(func pid=25864)[0m     lr: 0.0001
[2m[36m(func pid=25864)[0m     maximize: False
[2m[36m(func pid=25864)[0m     momentum: 0.99
[2m[36m(func pid=25864)[0m     nesterov: False
[2m[36m(func pid=25864)[0m     weight_decay: 1e-05
[2m[36m(func pid=25864)[0m )
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3804 | Steps: 4 | Val loss: 0.2943 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=23101)[0m rmse: 0.15428854525089264
[2m[36m(func pid=23101)[0m mae:  0.10581620037555695
[2m[36m(func pid=23101)[0m rmse_per_class: [0.1, 0.221, 0.043, 0.314, 0.086, 0.191, 0.254, 0.111, 0.132, 0.091]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.16289065778255463
[2m[36m(func pid=19000)[0m mae:  0.11771740764379501
[2m[36m(func pid=19000)[0m rmse_per_class: [0.105, 0.245, 0.054, 0.331, 0.067, 0.176, 0.264, 0.127, 0.139, 0.121]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3754 | Steps: 4 | Val loss: 0.2779 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0459 | Steps: 4 | Val loss: 0.7192 | Batch size: 32 | lr: 0.0001 | Duration: 4.38s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3730 | Steps: 4 | Val loss: 0.2956 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
[2m[36m(func pid=23101)[0m rmse: 0.1527436226606369
[2m[36m(func pid=23101)[0m mae:  0.10714870691299438
[2m[36m(func pid=23101)[0m rmse_per_class: [0.094, 0.218, 0.04, 0.287, 0.054, 0.203, 0.242, 0.126, 0.141, 0.123]
[2m[36m(func pid=25864)[0m rmse: 0.18036812543869019
[2m[36m(func pid=25864)[0m mae:  0.13284051418304443
[2m[36m(func pid=25864)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=19000)[0m rmse: 0.16464634239673615
[2m[36m(func pid=19000)[0m mae:  0.11844968795776367
[2m[36m(func pid=19000)[0m rmse_per_class: [0.124, 0.247, 0.052, 0.329, 0.067, 0.176, 0.265, 0.127, 0.138, 0.123]
== Status ==
Current time: 2024-01-07 15:57:18 (running for 00:28:31.94)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.38  |  0.163 |                   26 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.424 |  0.154 |                   10 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 15:57:24 (running for 00:28:38.43)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.373 |  0.165 |                   27 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.424 |  0.154 |                   10 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=26476)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=26476)[0m Configuration completed!
[2m[36m(func pid=26476)[0m New optimizer parameters:
[2m[36m(func pid=26476)[0m SGD (
[2m[36m(func pid=26476)[0m Parameter Group 0
[2m[36m(func pid=26476)[0m     dampening: 0
[2m[36m(func pid=26476)[0m     differentiable: False
[2m[36m(func pid=26476)[0m     foreach: None
[2m[36m(func pid=26476)[0m     lr: 0.001
[2m[36m(func pid=26476)[0m     maximize: False
[2m[36m(func pid=26476)[0m     momentum: 0.99
[2m[36m(func pid=26476)[0m     nesterov: False
[2m[36m(func pid=26476)[0m     weight_decay: 1e-05
[2m[36m(func pid=26476)[0m )
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0313 | Steps: 4 | Val loss: 0.7266 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3844 | Steps: 4 | Val loss: 0.2944 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3861 | Steps: 4 | Val loss: 0.2780 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0231 | Steps: 4 | Val loss: 0.7040 | Batch size: 32 | lr: 0.001 | Duration: 4.37s
== Status ==
Current time: 2024-01-07 15:57:29 (running for 00:28:43.45)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.373 |  0.165 |                   27 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.375 |  0.153 |                   11 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  1.046 |  0.18  |                    1 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.15305432677268982
[2m[36m(func pid=23101)[0m mae:  0.10751928389072418
[2m[36m(func pid=23101)[0m rmse_per_class: [0.098, 0.213, 0.039, 0.317, 0.053, 0.177, 0.235, 0.106, 0.174, 0.118]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.18076148629188538
[2m[36m(func pid=25864)[0m mae:  0.13319703936576843
[2m[36m(func pid=25864)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.106, 0.192, 0.302, 0.142, 0.143, 0.115]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1643022745847702
[2m[36m(func pid=19000)[0m mae:  0.11767999082803726
[2m[36m(func pid=19000)[0m rmse_per_class: [0.13, 0.248, 0.051, 0.325, 0.067, 0.176, 0.265, 0.127, 0.138, 0.116]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.18040607869625092
[2m[36m(func pid=26476)[0m mae:  0.13285622000694275
[2m[36m(func pid=26476)[0m rmse_per_class: [0.114, 0.263, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4029 | Steps: 4 | Val loss: 0.2767 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0083 | Steps: 4 | Val loss: 0.7279 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3817 | Steps: 4 | Val loss: 0.2909 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.9083 | Steps: 4 | Val loss: 0.6608 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 15:57:35 (running for 00:28:49.42)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.384 |  0.164 |                   28 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.403 |  0.153 |                   13 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  1.031 |  0.181 |                    2 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  1.023 |  0.18  |                    1 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.1527317762374878
[2m[36m(func pid=23101)[0m mae:  0.10367542505264282
[2m[36m(func pid=23101)[0m rmse_per_class: [0.111, 0.212, 0.04, 0.319, 0.056, 0.216, 0.243, 0.108, 0.132, 0.091]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.18098175525665283
[2m[36m(func pid=25864)[0m mae:  0.1333877146244049
[2m[36m(func pid=25864)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.108, 0.191, 0.304, 0.142, 0.142, 0.116]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.16165971755981445
[2m[36m(func pid=19000)[0m mae:  0.11605712026357651
[2m[36m(func pid=19000)[0m rmse_per_class: [0.116, 0.243, 0.051, 0.321, 0.069, 0.175, 0.267, 0.127, 0.138, 0.109]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.18054834008216858
[2m[36m(func pid=26476)[0m mae:  0.1329742968082428
[2m[36m(func pid=26476)[0m rmse_per_class: [0.113, 0.264, 0.101, 0.332, 0.106, 0.191, 0.3, 0.142, 0.143, 0.115]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.9701 | Steps: 4 | Val loss: 0.7162 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4168 | Steps: 4 | Val loss: 0.2793 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3864 | Steps: 4 | Val loss: 0.2882 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7355 | Steps: 4 | Val loss: 0.5741 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 15:57:41 (running for 00:28:54.81)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.382 |  0.162 |                   29 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.403 |  0.153 |                   13 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.97  |  0.181 |                    4 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.908 |  0.181 |                    2 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.1563473641872406
[2m[36m(func pid=23101)[0m mae:  0.10600171238183975
[2m[36m(func pid=23101)[0m rmse_per_class: [0.096, 0.213, 0.081, 0.299, 0.071, 0.169, 0.266, 0.126, 0.134, 0.109]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.18113742768764496
[2m[36m(func pid=25864)[0m mae:  0.1335168480873108
[2m[36m(func pid=25864)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.33, 0.109, 0.191, 0.305, 0.142, 0.142, 0.116]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1595948189496994
[2m[36m(func pid=19000)[0m mae:  0.1145903617143631
[2m[36m(func pid=19000)[0m rmse_per_class: [0.098, 0.241, 0.054, 0.32, 0.068, 0.175, 0.265, 0.131, 0.137, 0.106]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.18021751940250397
[2m[36m(func pid=26476)[0m mae:  0.13265974819660187
[2m[36m(func pid=26476)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.331, 0.106, 0.191, 0.299, 0.141, 0.143, 0.113]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3950 | Steps: 4 | Val loss: 0.3107 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.9326 | Steps: 4 | Val loss: 0.6958 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.5815 | Steps: 4 | Val loss: 0.4727 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3822 | Steps: 4 | Val loss: 0.2890 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 15:57:46 (running for 00:29:00.30)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.386 |  0.16  |                   30 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.395 |  0.173 |                   15 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.97  |  0.181 |                    4 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.736 |  0.18  |                    3 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.1730227768421173
[2m[36m(func pid=23101)[0m mae:  0.12293603271245956
[2m[36m(func pid=23101)[0m rmse_per_class: [0.105, 0.213, 0.043, 0.316, 0.088, 0.204, 0.283, 0.103, 0.242, 0.132]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.18113231658935547
[2m[36m(func pid=25864)[0m mae:  0.13350839912891388
[2m[36m(func pid=25864)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.109, 0.191, 0.306, 0.142, 0.142, 0.116]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.17946869134902954
[2m[36m(func pid=26476)[0m mae:  0.13196632266044617
[2m[36m(func pid=26476)[0m rmse_per_class: [0.114, 0.264, 0.104, 0.332, 0.104, 0.19, 0.295, 0.139, 0.143, 0.109]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.16099882125854492
[2m[36m(func pid=19000)[0m mae:  0.11459998786449432
[2m[36m(func pid=19000)[0m rmse_per_class: [0.096, 0.24, 0.077, 0.321, 0.067, 0.175, 0.266, 0.129, 0.137, 0.103]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3681 | Steps: 4 | Val loss: 0.2727 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8850 | Steps: 4 | Val loss: 0.6699 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4821 | Steps: 4 | Val loss: 0.3904 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3819 | Steps: 4 | Val loss: 0.2874 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 15:57:51 (running for 00:29:05.63)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.382 |  0.161 |                   31 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.395 |  0.173 |                   15 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.885 |  0.181 |                    6 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.581 |  0.179 |                    4 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.15221664309501648
[2m[36m(func pid=23101)[0m mae:  0.10098756849765778
[2m[36m(func pid=23101)[0m rmse_per_class: [0.11, 0.21, 0.051, 0.305, 0.116, 0.171, 0.225, 0.111, 0.133, 0.09]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.1811036616563797
[2m[36m(func pid=25864)[0m mae:  0.13346409797668457
[2m[36m(func pid=25864)[0m rmse_per_class: [0.113, 0.264, 0.099, 0.329, 0.109, 0.19, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.17841888964176178
[2m[36m(func pid=26476)[0m mae:  0.13094212114810944
[2m[36m(func pid=26476)[0m rmse_per_class: [0.114, 0.264, 0.106, 0.334, 0.102, 0.188, 0.29, 0.137, 0.145, 0.104]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.16059602797031403
[2m[36m(func pid=19000)[0m mae:  0.11394671350717545
[2m[36m(func pid=19000)[0m rmse_per_class: [0.096, 0.24, 0.082, 0.317, 0.067, 0.175, 0.263, 0.126, 0.138, 0.102]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3928 | Steps: 4 | Val loss: 0.2832 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8328 | Steps: 4 | Val loss: 0.6373 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4567 | Steps: 4 | Val loss: 0.3430 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3836 | Steps: 4 | Val loss: 0.2866 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 15:57:57 (running for 00:29:11.09)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.382 |  0.161 |                   32 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.368 |  0.152 |                   16 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.833 |  0.181 |                    7 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.482 |  0.178 |                    5 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.1809738576412201
[2m[36m(func pid=25864)[0m mae:  0.13334570825099945
[2m[36m(func pid=25864)[0m rmse_per_class: [0.113, 0.265, 0.099, 0.329, 0.109, 0.19, 0.305, 0.143, 0.142, 0.115]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.15828335285186768
[2m[36m(func pid=23101)[0m mae:  0.10638735443353653
[2m[36m(func pid=23101)[0m rmse_per_class: [0.1, 0.212, 0.049, 0.313, 0.102, 0.221, 0.263, 0.1, 0.133, 0.09]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.17739656567573547
[2m[36m(func pid=26476)[0m mae:  0.12983694672584534
[2m[36m(func pid=26476)[0m rmse_per_class: [0.115, 0.264, 0.109, 0.338, 0.096, 0.187, 0.283, 0.136, 0.146, 0.101]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15898020565509796
[2m[36m(func pid=19000)[0m mae:  0.11368916183710098
[2m[36m(func pid=19000)[0m rmse_per_class: [0.096, 0.242, 0.056, 0.32, 0.072, 0.175, 0.259, 0.125, 0.144, 0.101]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7697 | Steps: 4 | Val loss: 0.6007 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4509 | Steps: 4 | Val loss: 0.3246 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3682 | Steps: 4 | Val loss: 0.3145 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3722 | Steps: 4 | Val loss: 0.2906 | Batch size: 32 | lr: 0.01 | Duration: 3.27s
== Status ==
Current time: 2024-01-07 15:58:02 (running for 00:29:16.45)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.384 |  0.159 |                   33 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.393 |  0.158 |                   17 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.833 |  0.181 |                    7 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.451 |  0.176 |                    7 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.1763942390680313
[2m[36m(func pid=26476)[0m mae:  0.12855884432792664
[2m[36m(func pid=26476)[0m rmse_per_class: [0.115, 0.263, 0.112, 0.343, 0.088, 0.185, 0.276, 0.136, 0.148, 0.097]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.18076875805854797
[2m[36m(func pid=25864)[0m mae:  0.13316971063613892
[2m[36m(func pid=25864)[0m rmse_per_class: [0.113, 0.265, 0.098, 0.329, 0.108, 0.19, 0.304, 0.143, 0.143, 0.115]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.1736544668674469
[2m[36m(func pid=23101)[0m mae:  0.12085660547018051
[2m[36m(func pid=23101)[0m rmse_per_class: [0.095, 0.22, 0.039, 0.303, 0.091, 0.177, 0.291, 0.097, 0.315, 0.108]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.16134348511695862
[2m[36m(func pid=19000)[0m mae:  0.11594120413064957
[2m[36m(func pid=19000)[0m rmse_per_class: [0.104, 0.245, 0.048, 0.325, 0.077, 0.175, 0.262, 0.123, 0.149, 0.105]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4658 | Steps: 4 | Val loss: 0.3256 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7278 | Steps: 4 | Val loss: 0.5667 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3769 | Steps: 4 | Val loss: 0.2907 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3755 | Steps: 4 | Val loss: 0.2921 | Batch size: 32 | lr: 0.01 | Duration: 3.41s
[2m[36m(func pid=26476)[0m rmse: 0.17569871246814728
[2m[36m(func pid=26476)[0m mae:  0.1272595077753067
[2m[36m(func pid=26476)[0m rmse_per_class: [0.115, 0.264, 0.113, 0.348, 0.08, 0.184, 0.272, 0.137, 0.149, 0.095]
[2m[36m(func pid=26476)[0m 
== Status ==
Current time: 2024-01-07 15:58:08 (running for 00:29:22.04)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.372 |  0.161 |                   34 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.368 |  0.174 |                   18 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.77  |  0.181 |                    8 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.466 |  0.176 |                    8 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.18068556487560272
[2m[36m(func pid=25864)[0m mae:  0.13308432698249817
[2m[36m(func pid=25864)[0m rmse_per_class: [0.113, 0.265, 0.098, 0.329, 0.108, 0.19, 0.303, 0.143, 0.143, 0.115]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.16264767944812775
[2m[36m(func pid=23101)[0m mae:  0.10888858139514923
[2m[36m(func pid=23101)[0m rmse_per_class: [0.179, 0.208, 0.047, 0.31, 0.081, 0.176, 0.24, 0.113, 0.134, 0.138]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.16263385117053986
[2m[36m(func pid=19000)[0m mae:  0.11656894534826279
[2m[36m(func pid=19000)[0m rmse_per_class: [0.117, 0.247, 0.048, 0.325, 0.078, 0.175, 0.263, 0.123, 0.144, 0.107]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4931 | Steps: 4 | Val loss: 0.3391 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3879 | Steps: 4 | Val loss: 0.3102 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.6788 | Steps: 4 | Val loss: 0.5312 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=26476)[0m rmse: 0.1756117045879364
[2m[36m(func pid=26476)[0m mae:  0.1260136216878891
[2m[36m(func pid=26476)[0m rmse_per_class: [0.115, 0.265, 0.113, 0.354, 0.072, 0.184, 0.273, 0.138, 0.149, 0.093]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3715 | Steps: 4 | Val loss: 0.2929 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 15:58:13 (running for 00:29:27.65)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.375 |  0.163 |                   35 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.388 |  0.163 |                   20 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.728 |  0.181 |                    9 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.493 |  0.176 |                    9 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.1627439260482788
[2m[36m(func pid=23101)[0m mae:  0.10915776342153549
[2m[36m(func pid=23101)[0m rmse_per_class: [0.091, 0.205, 0.085, 0.364, 0.068, 0.214, 0.24, 0.107, 0.133, 0.12]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.18050530552864075
[2m[36m(func pid=25864)[0m mae:  0.13291998207569122
[2m[36m(func pid=25864)[0m rmse_per_class: [0.114, 0.265, 0.099, 0.33, 0.106, 0.19, 0.301, 0.143, 0.143, 0.115]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.16313262283802032
[2m[36m(func pid=19000)[0m mae:  0.11689617484807968
[2m[36m(func pid=19000)[0m rmse_per_class: [0.116, 0.245, 0.051, 0.325, 0.08, 0.175, 0.265, 0.124, 0.142, 0.108]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5308 | Steps: 4 | Val loss: 0.3574 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4081 | Steps: 4 | Val loss: 0.2994 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6256 | Steps: 4 | Val loss: 0.4961 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=26476)[0m rmse: 0.17630718648433685
[2m[36m(func pid=26476)[0m mae:  0.1250978410243988
[2m[36m(func pid=26476)[0m rmse_per_class: [0.115, 0.266, 0.113, 0.359, 0.065, 0.183, 0.281, 0.14, 0.148, 0.092]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3740 | Steps: 4 | Val loss: 0.2930 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 15:58:19 (running for 00:29:33.03)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.371 |  0.163 |                   36 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.408 |  0.162 |                   21 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.679 |  0.181 |                   10 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.531 |  0.176 |                   10 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.16176417469978333
[2m[36m(func pid=23101)[0m mae:  0.11276097595691681
[2m[36m(func pid=23101)[0m rmse_per_class: [0.092, 0.222, 0.05, 0.344, 0.067, 0.166, 0.255, 0.14, 0.194, 0.088]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.18030254542827606
[2m[36m(func pid=25864)[0m mae:  0.13274136185646057
[2m[36m(func pid=25864)[0m rmse_per_class: [0.114, 0.265, 0.1, 0.33, 0.105, 0.19, 0.3, 0.142, 0.144, 0.114]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5715 | Steps: 4 | Val loss: 0.3823 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=19000)[0m rmse: 0.16280868649482727
[2m[36m(func pid=19000)[0m mae:  0.11686961352825165
[2m[36m(func pid=19000)[0m rmse_per_class: [0.104, 0.244, 0.052, 0.326, 0.08, 0.175, 0.265, 0.127, 0.141, 0.114]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5873 | Steps: 4 | Val loss: 0.4674 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3614 | Steps: 4 | Val loss: 0.2667 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=26476)[0m rmse: 0.17813904583454132
[2m[36m(func pid=26476)[0m mae:  0.12466099113225937
[2m[36m(func pid=26476)[0m rmse_per_class: [0.116, 0.267, 0.112, 0.364, 0.06, 0.183, 0.299, 0.141, 0.147, 0.091]
[2m[36m(func pid=26476)[0m 
== Status ==
Current time: 2024-01-07 15:58:24 (running for 00:29:38.58)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.374 |  0.163 |                   37 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.408 |  0.162 |                   21 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.587 |  0.18  |                   12 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.572 |  0.178 |                   11 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3732 | Steps: 4 | Val loss: 0.2915 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=23101)[0m rmse: 0.1471162736415863
[2m[36m(func pid=23101)[0m mae:  0.09848877042531967
[2m[36m(func pid=23101)[0m rmse_per_class: [0.112, 0.21, 0.038, 0.275, 0.104, 0.171, 0.243, 0.098, 0.132, 0.089]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.1800544708967209
[2m[36m(func pid=25864)[0m mae:  0.1325339376926422
[2m[36m(func pid=25864)[0m rmse_per_class: [0.114, 0.266, 0.1, 0.331, 0.104, 0.19, 0.298, 0.141, 0.144, 0.113]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.6080 | Steps: 4 | Val loss: 0.4093 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=19000)[0m rmse: 0.16194745898246765
[2m[36m(func pid=19000)[0m mae:  0.11596038192510605
[2m[36m(func pid=19000)[0m rmse_per_class: [0.099, 0.24, 0.056, 0.327, 0.082, 0.175, 0.262, 0.126, 0.138, 0.114]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5610 | Steps: 4 | Val loss: 0.4393 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3669 | Steps: 4 | Val loss: 0.2799 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=26476)[0m rmse: 0.1806480884552002
[2m[36m(func pid=26476)[0m mae:  0.12453822046518326
[2m[36m(func pid=26476)[0m rmse_per_class: [0.116, 0.269, 0.107, 0.369, 0.057, 0.184, 0.325, 0.143, 0.146, 0.091]
[2m[36m(func pid=26476)[0m 
== Status ==
Current time: 2024-01-07 15:58:30 (running for 00:29:44.02)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.373 |  0.162 |                   38 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.361 |  0.147 |                   22 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.561 |  0.18  |                   13 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.608 |  0.181 |                   12 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.15588605403900146
[2m[36m(func pid=23101)[0m mae:  0.10434148460626602
[2m[36m(func pid=23101)[0m rmse_per_class: [0.092, 0.226, 0.066, 0.303, 0.115, 0.168, 0.253, 0.102, 0.132, 0.102]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17976152896881104
[2m[36m(func pid=25864)[0m mae:  0.13227897882461548
[2m[36m(func pid=25864)[0m rmse_per_class: [0.115, 0.265, 0.101, 0.332, 0.102, 0.19, 0.296, 0.14, 0.145, 0.112]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3701 | Steps: 4 | Val loss: 0.2893 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.6218 | Steps: 4 | Val loss: 0.4291 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=19000)[0m rmse: 0.16084198653697968
[2m[36m(func pid=19000)[0m mae:  0.11510644853115082
[2m[36m(func pid=19000)[0m rmse_per_class: [0.099, 0.238, 0.055, 0.324, 0.082, 0.175, 0.26, 0.123, 0.137, 0.116]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3496 | Steps: 4 | Val loss: 0.2920 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5294 | Steps: 4 | Val loss: 0.4173 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=26476)[0m rmse: 0.18298719823360443
[2m[36m(func pid=26476)[0m mae:  0.12470326572656631
[2m[36m(func pid=26476)[0m rmse_per_class: [0.117, 0.271, 0.103, 0.372, 0.055, 0.184, 0.348, 0.145, 0.145, 0.091]
[2m[36m(func pid=26476)[0m 
== Status ==
Current time: 2024-01-07 15:58:35 (running for 00:29:49.59)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.37  |  0.161 |                   39 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.35  |  0.159 |                   24 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.561 |  0.18  |                   13 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.622 |  0.183 |                   13 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.15907225012779236
[2m[36m(func pid=23101)[0m mae:  0.11173371970653534
[2m[36m(func pid=23101)[0m rmse_per_class: [0.1, 0.206, 0.036, 0.332, 0.075, 0.167, 0.256, 0.098, 0.199, 0.12]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17965325713157654
[2m[36m(func pid=25864)[0m mae:  0.1321544200181961
[2m[36m(func pid=25864)[0m rmse_per_class: [0.115, 0.266, 0.103, 0.333, 0.101, 0.19, 0.294, 0.139, 0.145, 0.111]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3794 | Steps: 4 | Val loss: 0.2885 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.6312 | Steps: 4 | Val loss: 0.4465 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3484 | Steps: 4 | Val loss: 0.2653 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5057 | Steps: 4 | Val loss: 0.3991 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=19000)[0m rmse: 0.16044950485229492
[2m[36m(func pid=19000)[0m mae:  0.11458683013916016
[2m[36m(func pid=19000)[0m rmse_per_class: [0.101, 0.237, 0.056, 0.324, 0.078, 0.175, 0.258, 0.12, 0.137, 0.118]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.18521669507026672
[2m[36m(func pid=26476)[0m mae:  0.1251356303691864
[2m[36m(func pid=26476)[0m rmse_per_class: [0.117, 0.272, 0.098, 0.375, 0.055, 0.185, 0.368, 0.147, 0.144, 0.092]
[2m[36m(func pid=26476)[0m 
== Status ==
Current time: 2024-01-07 15:58:41 (running for 00:29:55.13)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.379 |  0.16  |                   40 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.348 |  0.145 |                   25 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.529 |  0.18  |                   14 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.631 |  0.185 |                   14 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=23101)[0m rmse: 0.14480678737163544

[2m[36m(func pid=23101)[0m mae:  0.09755715727806091
[2m[36m(func pid=23101)[0m rmse_per_class: [0.118, 0.209, 0.035, 0.292, 0.057, 0.164, 0.239, 0.101, 0.132, 0.102]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17949654161930084
[2m[36m(func pid=25864)[0m mae:  0.13196583092212677
[2m[36m(func pid=25864)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.333, 0.1, 0.19, 0.292, 0.138, 0.146, 0.11]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.6460 | Steps: 4 | Val loss: 0.4471 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3681 | Steps: 4 | Val loss: 0.2877 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=26476)[0m rmse: 0.18588341772556305
[2m[36m(func pid=26476)[0m mae:  0.1251307874917984
[2m[36m(func pid=26476)[0m rmse_per_class: [0.118, 0.273, 0.092, 0.376, 0.054, 0.186, 0.376, 0.147, 0.145, 0.092]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3782 | Steps: 4 | Val loss: 0.2604 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4899 | Steps: 4 | Val loss: 0.3821 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=19000)[0m rmse: 0.1597650796175003
[2m[36m(func pid=19000)[0m mae:  0.11382601410150528
[2m[36m(func pid=19000)[0m rmse_per_class: [0.101, 0.239, 0.053, 0.325, 0.077, 0.174, 0.255, 0.12, 0.136, 0.118]
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 15:58:47 (running for 00:30:00.70)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.368 |  0.16  |                   41 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.378 |  0.143 |                   26 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.506 |  0.179 |                   15 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.646 |  0.186 |                   15 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.14262837171554565
[2m[36m(func pid=23101)[0m mae:  0.09530048072338104
[2m[36m(func pid=23101)[0m rmse_per_class: [0.099, 0.203, 0.056, 0.268, 0.06, 0.164, 0.24, 0.11, 0.137, 0.09]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.1793401837348938
[2m[36m(func pid=25864)[0m mae:  0.13176828622817993
[2m[36m(func pid=25864)[0m rmse_per_class: [0.116, 0.266, 0.107, 0.335, 0.098, 0.19, 0.29, 0.138, 0.146, 0.108]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6661 | Steps: 4 | Val loss: 0.4538 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3809 | Steps: 4 | Val loss: 0.2864 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=26476)[0m rmse: 0.18748454749584198
[2m[36m(func pid=26476)[0m mae:  0.12545612454414368
[2m[36m(func pid=26476)[0m rmse_per_class: [0.117, 0.275, 0.086, 0.377, 0.055, 0.186, 0.393, 0.149, 0.145, 0.092]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3870 | Steps: 4 | Val loss: 0.2782 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4712 | Steps: 4 | Val loss: 0.3676 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=19000)[0m rmse: 0.15886573493480682
[2m[36m(func pid=19000)[0m mae:  0.11311972141265869
[2m[36m(func pid=19000)[0m rmse_per_class: [0.103, 0.24, 0.048, 0.323, 0.076, 0.174, 0.255, 0.123, 0.136, 0.111]
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 15:58:52 (running for 00:30:06.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.381 |  0.159 |                   42 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.378 |  0.143 |                   26 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.471 |  0.179 |                   17 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.666 |  0.187 |                   16 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.1563337743282318
[2m[36m(func pid=23101)[0m mae:  0.10699103772640228
[2m[36m(func pid=23101)[0m rmse_per_class: [0.093, 0.222, 0.046, 0.294, 0.066, 0.17, 0.235, 0.099, 0.152, 0.188]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.179163858294487
[2m[36m(func pid=25864)[0m mae:  0.13155873119831085
[2m[36m(func pid=25864)[0m rmse_per_class: [0.117, 0.266, 0.109, 0.336, 0.096, 0.189, 0.288, 0.137, 0.147, 0.107]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6757 | Steps: 4 | Val loss: 0.4582 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3737 | Steps: 4 | Val loss: 0.2874 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=26476)[0m rmse: 0.18893809616565704
[2m[36m(func pid=26476)[0m mae:  0.12599238753318787
[2m[36m(func pid=26476)[0m rmse_per_class: [0.119, 0.276, 0.081, 0.379, 0.055, 0.188, 0.405, 0.15, 0.145, 0.092]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3511 | Steps: 4 | Val loss: 0.2724 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4559 | Steps: 4 | Val loss: 0.3562 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=19000)[0m rmse: 0.15963201224803925
[2m[36m(func pid=19000)[0m mae:  0.11389930546283722
[2m[36m(func pid=19000)[0m rmse_per_class: [0.1, 0.239, 0.052, 0.322, 0.078, 0.174, 0.261, 0.124, 0.137, 0.108]
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 15:58:57 (running for 00:30:11.62)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.374 |  0.16  |                   43 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.351 |  0.15  |                   28 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.471 |  0.179 |                   17 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.676 |  0.189 |                   17 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.14973178505897522
[2m[36m(func pid=23101)[0m mae:  0.09995286911725998
[2m[36m(func pid=23101)[0m rmse_per_class: [0.115, 0.217, 0.04, 0.282, 0.077, 0.165, 0.26, 0.097, 0.132, 0.113]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17895689606666565
[2m[36m(func pid=25864)[0m mae:  0.131306454539299
[2m[36m(func pid=25864)[0m rmse_per_class: [0.117, 0.266, 0.11, 0.337, 0.094, 0.189, 0.285, 0.137, 0.148, 0.106]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6766 | Steps: 4 | Val loss: 0.4669 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3717 | Steps: 4 | Val loss: 0.2877 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=26476)[0m rmse: 0.1906152069568634
[2m[36m(func pid=26476)[0m mae:  0.12677350640296936
[2m[36m(func pid=26476)[0m rmse_per_class: [0.121, 0.278, 0.077, 0.381, 0.055, 0.188, 0.417, 0.151, 0.146, 0.093]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3555 | Steps: 4 | Val loss: 0.2776 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4553 | Steps: 4 | Val loss: 0.3481 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=19000)[0m rmse: 0.1597626954317093
[2m[36m(func pid=19000)[0m mae:  0.11455684900283813
[2m[36m(func pid=19000)[0m rmse_per_class: [0.099, 0.237, 0.053, 0.318, 0.075, 0.174, 0.268, 0.121, 0.146, 0.106]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.15263134241104126
[2m[36m(func pid=23101)[0m mae:  0.103998102247715
[2m[36m(func pid=23101)[0m rmse_per_class: [0.097, 0.208, 0.057, 0.32, 0.087, 0.163, 0.239, 0.099, 0.144, 0.112]
== Status ==
Current time: 2024-01-07 15:59:03 (running for 00:30:17.08)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.372 |  0.16  |                   44 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.355 |  0.153 |                   29 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.456 |  0.179 |                   18 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.677 |  0.191 |                   18 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.1788705736398697
[2m[36m(func pid=25864)[0m mae:  0.13113842904567719
[2m[36m(func pid=25864)[0m rmse_per_class: [0.118, 0.266, 0.113, 0.339, 0.092, 0.189, 0.284, 0.136, 0.149, 0.104]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6934 | Steps: 4 | Val loss: 0.4787 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3630 | Steps: 4 | Val loss: 0.2884 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=26476)[0m rmse: 0.19231942296028137
[2m[36m(func pid=26476)[0m mae:  0.12769654393196106
[2m[36m(func pid=26476)[0m rmse_per_class: [0.124, 0.28, 0.074, 0.382, 0.055, 0.19, 0.428, 0.152, 0.146, 0.093]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3620 | Steps: 4 | Val loss: 0.2936 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4533 | Steps: 4 | Val loss: 0.3426 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 15:59:08 (running for 00:30:22.17)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.363 |  0.16  |                   45 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.355 |  0.153 |                   29 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.455 |  0.179 |                   19 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.693 |  0.192 |                   19 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19000)[0m rmse: 0.15995773673057556
[2m[36m(func pid=19000)[0m mae:  0.11518174409866333
[2m[36m(func pid=19000)[0m rmse_per_class: [0.105, 0.234, 0.047, 0.316, 0.074, 0.173, 0.274, 0.118, 0.152, 0.105]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6856 | Steps: 4 | Val loss: 0.4816 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=23101)[0m rmse: 0.16155917942523956
[2m[36m(func pid=23101)[0m mae:  0.11082343012094498
[2m[36m(func pid=23101)[0m rmse_per_class: [0.091, 0.206, 0.045, 0.336, 0.083, 0.165, 0.25, 0.107, 0.225, 0.107]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.1788192242383957
[2m[36m(func pid=25864)[0m mae:  0.13101962208747864
[2m[36m(func pid=25864)[0m rmse_per_class: [0.118, 0.266, 0.114, 0.339, 0.091, 0.189, 0.282, 0.136, 0.15, 0.103]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.1929025501012802
[2m[36m(func pid=26476)[0m mae:  0.12822315096855164
[2m[36m(func pid=26476)[0m rmse_per_class: [0.13, 0.281, 0.071, 0.383, 0.056, 0.191, 0.426, 0.152, 0.146, 0.093]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3699 | Steps: 4 | Val loss: 0.2880 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3637 | Steps: 4 | Val loss: 0.2669 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4402 | Steps: 4 | Val loss: 0.3385 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 15:59:14 (running for 00:30:27.84)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.363 |  0.16  |                   45 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.364 |  0.149 |                   31 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.453 |  0.179 |                   20 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.686 |  0.193 |                   20 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.14875774085521698
[2m[36m(func pid=23101)[0m mae:  0.09794920682907104
[2m[36m(func pid=23101)[0m rmse_per_class: [0.134, 0.21, 0.039, 0.264, 0.098, 0.161, 0.262, 0.093, 0.131, 0.096]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6618 | Steps: 4 | Val loss: 0.4814 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=25864)[0m rmse: 0.17879092693328857
[2m[36m(func pid=25864)[0m mae:  0.13085024058818817
[2m[36m(func pid=25864)[0m rmse_per_class: [0.119, 0.266, 0.117, 0.341, 0.088, 0.189, 0.28, 0.136, 0.15, 0.102]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1600426435470581
[2m[36m(func pid=19000)[0m mae:  0.11477340757846832
[2m[36m(func pid=19000)[0m rmse_per_class: [0.121, 0.234, 0.045, 0.316, 0.072, 0.173, 0.274, 0.118, 0.144, 0.104]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.19301439821720123
[2m[36m(func pid=26476)[0m mae:  0.12855832278728485
[2m[36m(func pid=26476)[0m rmse_per_class: [0.141, 0.281, 0.069, 0.384, 0.056, 0.191, 0.416, 0.153, 0.147, 0.094]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3682 | Steps: 4 | Val loss: 0.2827 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4417 | Steps: 4 | Val loss: 0.3363 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3845 | Steps: 4 | Val loss: 0.2866 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6592 | Steps: 4 | Val loss: 0.4740 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 15:59:19 (running for 00:30:33.32)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.37  |  0.16  |                   46 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.368 |  0.155 |                   32 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.44  |  0.179 |                   21 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.662 |  0.193 |                   21 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.15549223124980927
[2m[36m(func pid=23101)[0m mae:  0.10182163864374161
[2m[36m(func pid=23101)[0m rmse_per_class: [0.106, 0.214, 0.059, 0.33, 0.118, 0.167, 0.229, 0.106, 0.133, 0.092]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17868800461292267
[2m[36m(func pid=25864)[0m mae:  0.13066670298576355
[2m[36m(func pid=25864)[0m rmse_per_class: [0.12, 0.266, 0.118, 0.343, 0.086, 0.189, 0.279, 0.136, 0.15, 0.101]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15939202904701233
[2m[36m(func pid=19000)[0m mae:  0.11342908442020416
[2m[36m(func pid=19000)[0m rmse_per_class: [0.128, 0.233, 0.046, 0.319, 0.07, 0.173, 0.266, 0.118, 0.138, 0.103]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.1925179660320282
[2m[36m(func pid=26476)[0m mae:  0.12865881621837616
[2m[36m(func pid=26476)[0m rmse_per_class: [0.158, 0.281, 0.067, 0.384, 0.056, 0.19, 0.393, 0.153, 0.15, 0.094]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3573 | Steps: 4 | Val loss: 0.2793 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4430 | Steps: 4 | Val loss: 0.3357 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3631 | Steps: 4 | Val loss: 0.2835 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.6084 | Steps: 4 | Val loss: 0.4490 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 15:59:25 (running for 00:30:38.79)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.385 |  0.159 |                   47 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.357 |  0.153 |                   33 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.442 |  0.179 |                   22 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.659 |  0.193 |                   22 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.15329554677009583
[2m[36m(func pid=23101)[0m mae:  0.10485400259494781
[2m[36m(func pid=23101)[0m rmse_per_class: [0.099, 0.209, 0.057, 0.297, 0.082, 0.158, 0.275, 0.096, 0.172, 0.087]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17870840430259705
[2m[36m(func pid=25864)[0m mae:  0.13050690293312073
[2m[36m(func pid=25864)[0m rmse_per_class: [0.121, 0.266, 0.119, 0.344, 0.084, 0.189, 0.277, 0.136, 0.151, 0.1]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15671300888061523
[2m[36m(func pid=19000)[0m mae:  0.11095535755157471
[2m[36m(func pid=19000)[0m rmse_per_class: [0.11, 0.235, 0.047, 0.323, 0.069, 0.173, 0.252, 0.121, 0.136, 0.101]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.19073089957237244
[2m[36m(func pid=26476)[0m mae:  0.12811116874217987
[2m[36m(func pid=26476)[0m rmse_per_class: [0.167, 0.279, 0.064, 0.384, 0.056, 0.189, 0.366, 0.153, 0.156, 0.093]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3423 | Steps: 4 | Val loss: 0.2561 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4423 | Steps: 4 | Val loss: 0.3328 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3658 | Steps: 4 | Val loss: 0.2818 | Batch size: 32 | lr: 0.01 | Duration: 3.24s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5966 | Steps: 4 | Val loss: 0.4413 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 15:59:30 (running for 00:30:44.26)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.363 |  0.157 |                   48 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.342 |  0.139 |                   34 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.443 |  0.179 |                   23 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.608 |  0.191 |                   23 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.13893087208271027
[2m[36m(func pid=23101)[0m mae:  0.0929928719997406
[2m[36m(func pid=23101)[0m rmse_per_class: [0.092, 0.217, 0.036, 0.265, 0.067, 0.16, 0.227, 0.095, 0.136, 0.096]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17837359011173248
[2m[36m(func pid=25864)[0m mae:  0.1302071362733841
[2m[36m(func pid=25864)[0m rmse_per_class: [0.12, 0.266, 0.118, 0.345, 0.084, 0.188, 0.275, 0.136, 0.152, 0.1]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15549394488334656
[2m[36m(func pid=19000)[0m mae:  0.10960026830434799
[2m[36m(func pid=19000)[0m rmse_per_class: [0.103, 0.235, 0.049, 0.322, 0.069, 0.173, 0.248, 0.121, 0.135, 0.1]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.19073989987373352
[2m[36m(func pid=26476)[0m mae:  0.12871262431144714
[2m[36m(func pid=26476)[0m rmse_per_class: [0.192, 0.277, 0.065, 0.384, 0.056, 0.188, 0.335, 0.153, 0.163, 0.093]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3564 | Steps: 4 | Val loss: 0.2890 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4504 | Steps: 4 | Val loss: 0.3336 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3698 | Steps: 4 | Val loss: 0.2784 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5969 | Steps: 4 | Val loss: 0.4374 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 15:59:36 (running for 00:30:49.69)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.366 |  0.155 |                   49 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.356 |  0.16  |                   35 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.442 |  0.178 |                   24 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.597 |  0.191 |                   24 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.1600370854139328
[2m[36m(func pid=23101)[0m mae:  0.10780016332864761
[2m[36m(func pid=23101)[0m rmse_per_class: [0.159, 0.217, 0.045, 0.323, 0.085, 0.172, 0.256, 0.091, 0.135, 0.118]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17839811742305756
[2m[36m(func pid=25864)[0m mae:  0.13005462288856506
[2m[36m(func pid=25864)[0m rmse_per_class: [0.121, 0.266, 0.12, 0.346, 0.081, 0.188, 0.274, 0.136, 0.153, 0.099]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15413103997707367
[2m[36m(func pid=19000)[0m mae:  0.10854190587997437
[2m[36m(func pid=19000)[0m rmse_per_class: [0.098, 0.236, 0.05, 0.311, 0.07, 0.173, 0.248, 0.122, 0.134, 0.1]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.19116570055484772
[2m[36m(func pid=26476)[0m mae:  0.12964802980422974
[2m[36m(func pid=26476)[0m rmse_per_class: [0.215, 0.276, 0.065, 0.384, 0.056, 0.187, 0.308, 0.154, 0.173, 0.093]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3465 | Steps: 4 | Val loss: 0.2772 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4537 | Steps: 4 | Val loss: 0.3349 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.6352 | Steps: 4 | Val loss: 0.4341 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3711 | Steps: 4 | Val loss: 0.2784 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 15:59:41 (running for 00:30:55.22)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.37  |  0.154 |                   50 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.347 |  0.148 |                   36 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.45  |  0.178 |                   25 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.597 |  0.191 |                   25 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.1481567919254303
[2m[36m(func pid=23101)[0m mae:  0.10173572599887848
[2m[36m(func pid=23101)[0m rmse_per_class: [0.092, 0.214, 0.047, 0.275, 0.073, 0.159, 0.288, 0.094, 0.14, 0.098]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17833682894706726
[2m[36m(func pid=25864)[0m mae:  0.1298220008611679
[2m[36m(func pid=25864)[0m rmse_per_class: [0.122, 0.266, 0.12, 0.348, 0.079, 0.188, 0.273, 0.136, 0.153, 0.098]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.19140392541885376
[2m[36m(func pid=26476)[0m mae:  0.13087430596351624
[2m[36m(func pid=26476)[0m rmse_per_class: [0.231, 0.274, 0.066, 0.385, 0.056, 0.186, 0.284, 0.154, 0.185, 0.093]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15448850393295288
[2m[36m(func pid=19000)[0m mae:  0.10936535894870758
[2m[36m(func pid=19000)[0m rmse_per_class: [0.096, 0.236, 0.051, 0.306, 0.069, 0.173, 0.251, 0.121, 0.137, 0.105]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3699 | Steps: 4 | Val loss: 0.2651 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4552 | Steps: 4 | Val loss: 0.3382 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5497 | Steps: 4 | Val loss: 0.4206 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3631 | Steps: 4 | Val loss: 0.2826 | Batch size: 32 | lr: 0.01 | Duration: 3.28s
== Status ==
Current time: 2024-01-07 15:59:46 (running for 00:31:00.53)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.371 |  0.154 |                   51 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.37  |  0.145 |                   37 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.178 |                   26 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.635 |  0.191 |                   26 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.14488007128238678
[2m[36m(func pid=23101)[0m mae:  0.09567290544509888
[2m[36m(func pid=23101)[0m rmse_per_class: [0.101, 0.214, 0.031, 0.289, 0.085, 0.161, 0.227, 0.093, 0.131, 0.116]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17845715582370758
[2m[36m(func pid=25864)[0m mae:  0.1297117918729782
[2m[36m(func pid=25864)[0m rmse_per_class: [0.123, 0.266, 0.121, 0.349, 0.077, 0.188, 0.273, 0.136, 0.154, 0.097]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.18987710773944855
[2m[36m(func pid=26476)[0m mae:  0.13115867972373962
[2m[36m(func pid=26476)[0m rmse_per_class: [0.225, 0.27, 0.067, 0.384, 0.056, 0.183, 0.266, 0.154, 0.2, 0.093]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1573125422000885
[2m[36m(func pid=19000)[0m mae:  0.11211249977350235
[2m[36m(func pid=19000)[0m rmse_per_class: [0.096, 0.236, 0.055, 0.31, 0.07, 0.173, 0.257, 0.118, 0.145, 0.113]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3533 | Steps: 4 | Val loss: 0.2980 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4555 | Steps: 4 | Val loss: 0.3399 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5923 | Steps: 4 | Val loss: 0.4212 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 15:59:52 (running for 00:31:05.98)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.363 |  0.157 |                   52 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.353 |  0.165 |                   38 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.455 |  0.178 |                   27 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.55  |  0.19  |                   27 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.16528591513633728
[2m[36m(func pid=23101)[0m mae:  0.11155994236469269
[2m[36m(func pid=23101)[0m rmse_per_class: [0.09, 0.228, 0.041, 0.324, 0.125, 0.19, 0.267, 0.1, 0.202, 0.087]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3670 | Steps: 4 | Val loss: 0.2840 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=25864)[0m rmse: 0.17840781807899475
[2m[36m(func pid=25864)[0m mae:  0.129511758685112
[2m[36m(func pid=25864)[0m rmse_per_class: [0.124, 0.266, 0.121, 0.35, 0.075, 0.188, 0.273, 0.137, 0.154, 0.097]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.189883291721344
[2m[36m(func pid=26476)[0m mae:  0.13213802874088287
[2m[36m(func pid=26476)[0m rmse_per_class: [0.222, 0.266, 0.074, 0.384, 0.056, 0.181, 0.254, 0.154, 0.215, 0.093]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15741345286369324
[2m[36m(func pid=19000)[0m mae:  0.11226669698953629
[2m[36m(func pid=19000)[0m rmse_per_class: [0.096, 0.233, 0.054, 0.32, 0.068, 0.173, 0.254, 0.117, 0.145, 0.113]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3489 | Steps: 4 | Val loss: 0.2867 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4614 | Steps: 4 | Val loss: 0.3412 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5256 | Steps: 4 | Val loss: 0.4137 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 15:59:57 (running for 00:31:11.48)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.367 |  0.157 |                   53 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.349 |  0.157 |                   39 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.456 |  0.178 |                   28 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.592 |  0.19  |                   28 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.15698176622390747
[2m[36m(func pid=23101)[0m mae:  0.10548464208841324
[2m[36m(func pid=23101)[0m rmse_per_class: [0.111, 0.233, 0.051, 0.322, 0.092, 0.17, 0.254, 0.092, 0.14, 0.106]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.1782820075750351
[2m[36m(func pid=25864)[0m mae:  0.1292700320482254
[2m[36m(func pid=25864)[0m rmse_per_class: [0.124, 0.266, 0.121, 0.351, 0.074, 0.187, 0.273, 0.137, 0.154, 0.096]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3611 | Steps: 4 | Val loss: 0.2852 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
[2m[36m(func pid=26476)[0m rmse: 0.1887173354625702
[2m[36m(func pid=26476)[0m mae:  0.13216903805732727
[2m[36m(func pid=26476)[0m rmse_per_class: [0.205, 0.261, 0.083, 0.383, 0.056, 0.178, 0.248, 0.154, 0.227, 0.092]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3538 | Steps: 4 | Val loss: 0.2559 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4648 | Steps: 4 | Val loss: 0.3440 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=19000)[0m rmse: 0.15689128637313843
[2m[36m(func pid=19000)[0m mae:  0.1119975596666336
[2m[36m(func pid=19000)[0m rmse_per_class: [0.097, 0.231, 0.052, 0.327, 0.068, 0.173, 0.254, 0.116, 0.141, 0.11]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5375 | Steps: 4 | Val loss: 0.4111 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 16:00:03 (running for 00:31:16.87)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.361 |  0.157 |                   54 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.349 |  0.157 |                   39 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.465 |  0.178 |                   30 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.526 |  0.189 |                   29 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.17835769057273865
[2m[36m(func pid=25864)[0m mae:  0.1291777491569519
[2m[36m(func pid=25864)[0m rmse_per_class: [0.124, 0.266, 0.12, 0.352, 0.072, 0.187, 0.273, 0.137, 0.155, 0.096]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.13764338195323944
[2m[36m(func pid=23101)[0m mae:  0.09168298542499542
[2m[36m(func pid=23101)[0m rmse_per_class: [0.09, 0.201, 0.031, 0.262, 0.065, 0.165, 0.227, 0.096, 0.133, 0.107]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3720 | Steps: 4 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=26476)[0m rmse: 0.1875288337469101
[2m[36m(func pid=26476)[0m mae:  0.1318075954914093
[2m[36m(func pid=26476)[0m rmse_per_class: [0.185, 0.255, 0.094, 0.382, 0.056, 0.175, 0.247, 0.153, 0.236, 0.092]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4642 | Steps: 4 | Val loss: 0.3428 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3686 | Steps: 4 | Val loss: 0.2867 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=19000)[0m rmse: 0.15571369230747223
[2m[36m(func pid=19000)[0m mae:  0.11067178100347519
[2m[36m(func pid=19000)[0m rmse_per_class: [0.097, 0.231, 0.052, 0.324, 0.067, 0.173, 0.256, 0.115, 0.137, 0.104]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5408 | Steps: 4 | Val loss: 0.3987 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 16:00:08 (running for 00:31:22.29)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.372 |  0.156 |                   55 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.354 |  0.138 |                   40 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.464 |  0.178 |                   31 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.538 |  0.188 |                   30 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.1779564619064331
[2m[36m(func pid=25864)[0m mae:  0.1288233995437622
[2m[36m(func pid=25864)[0m rmse_per_class: [0.124, 0.266, 0.119, 0.352, 0.072, 0.187, 0.271, 0.137, 0.155, 0.095]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.1588502824306488
[2m[36m(func pid=23101)[0m mae:  0.10767189413309097
[2m[36m(func pid=23101)[0m rmse_per_class: [0.106, 0.22, 0.058, 0.304, 0.071, 0.215, 0.275, 0.092, 0.161, 0.086]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.1847768872976303
[2m[36m(func pid=26476)[0m mae:  0.13040050864219666
[2m[36m(func pid=26476)[0m rmse_per_class: [0.157, 0.249, 0.098, 0.381, 0.056, 0.174, 0.25, 0.153, 0.238, 0.092]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3715 | Steps: 4 | Val loss: 0.2800 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3582 | Steps: 4 | Val loss: 0.3096 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4825 | Steps: 4 | Val loss: 0.3446 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=19000)[0m rmse: 0.15410402417182922
[2m[36m(func pid=19000)[0m mae:  0.1090884804725647
[2m[36m(func pid=19000)[0m rmse_per_class: [0.097, 0.228, 0.048, 0.321, 0.069, 0.172, 0.252, 0.118, 0.135, 0.101]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4982 | Steps: 4 | Val loss: 0.3763 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 16:00:13 (running for 00:31:27.64)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.372 |  0.154 |                   56 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.358 |  0.164 |                   42 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.464 |  0.178 |                   31 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.541 |  0.185 |                   31 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.1640234738588333
[2m[36m(func pid=23101)[0m mae:  0.1118447333574295
[2m[36m(func pid=23101)[0m rmse_per_class: [0.117, 0.232, 0.073, 0.355, 0.071, 0.162, 0.268, 0.096, 0.154, 0.112]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.1778736114501953
[2m[36m(func pid=25864)[0m mae:  0.1286233514547348
[2m[36m(func pid=25864)[0m rmse_per_class: [0.125, 0.267, 0.118, 0.352, 0.07, 0.187, 0.271, 0.138, 0.155, 0.095]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.1803797036409378
[2m[36m(func pid=26476)[0m mae:  0.12797971069812775
[2m[36m(func pid=26476)[0m rmse_per_class: [0.128, 0.246, 0.094, 0.377, 0.056, 0.176, 0.255, 0.152, 0.227, 0.092]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3669 | Steps: 4 | Val loss: 0.2814 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3433 | Steps: 4 | Val loss: 0.2633 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4763 | Steps: 4 | Val loss: 0.3441 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4794 | Steps: 4 | Val loss: 0.3680 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=19000)[0m rmse: 0.15560060739517212
[2m[36m(func pid=19000)[0m mae:  0.10990021377801895
[2m[36m(func pid=19000)[0m rmse_per_class: [0.1, 0.234, 0.051, 0.321, 0.07, 0.173, 0.25, 0.12, 0.136, 0.101]
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 16:00:19 (running for 00:31:33.05)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.367 |  0.156 |                   57 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.343 |  0.143 |                   43 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.483 |  0.178 |                   32 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.498 |  0.18  |                   32 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.14288510382175446
[2m[36m(func pid=23101)[0m mae:  0.09565582871437073
[2m[36m(func pid=23101)[0m rmse_per_class: [0.09, 0.21, 0.034, 0.265, 0.082, 0.183, 0.224, 0.093, 0.135, 0.113]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17760567367076874
[2m[36m(func pid=25864)[0m mae:  0.1283540427684784
[2m[36m(func pid=25864)[0m rmse_per_class: [0.125, 0.266, 0.118, 0.353, 0.07, 0.187, 0.27, 0.138, 0.155, 0.095]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.17879799008369446
[2m[36m(func pid=26476)[0m mae:  0.12707120180130005
[2m[36m(func pid=26476)[0m rmse_per_class: [0.113, 0.247, 0.099, 0.374, 0.056, 0.178, 0.262, 0.151, 0.211, 0.095]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3688 | Steps: 4 | Val loss: 0.2825 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4781 | Steps: 4 | Val loss: 0.3449 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3539 | Steps: 4 | Val loss: 0.2777 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4903 | Steps: 4 | Val loss: 0.3482 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=19000)[0m rmse: 0.15734753012657166
[2m[36m(func pid=19000)[0m mae:  0.11083103716373444
[2m[36m(func pid=19000)[0m rmse_per_class: [0.101, 0.239, 0.062, 0.314, 0.073, 0.173, 0.253, 0.118, 0.138, 0.103]
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 16:00:24 (running for 00:31:38.42)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.369 |  0.157 |                   58 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.343 |  0.143 |                   43 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.478 |  0.177 |                   34 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.479 |  0.179 |                   33 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.1773649901151657
[2m[36m(func pid=25864)[0m mae:  0.1280200332403183
[2m[36m(func pid=25864)[0m rmse_per_class: [0.125, 0.267, 0.116, 0.353, 0.068, 0.187, 0.27, 0.138, 0.155, 0.095]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.1543748527765274
[2m[36m(func pid=23101)[0m mae:  0.10349701344966888
[2m[36m(func pid=23101)[0m rmse_per_class: [0.111, 0.204, 0.037, 0.303, 0.107, 0.164, 0.271, 0.102, 0.158, 0.087]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.17546303570270538
[2m[36m(func pid=26476)[0m mae:  0.12508340179920197
[2m[36m(func pid=26476)[0m rmse_per_class: [0.099, 0.253, 0.082, 0.367, 0.056, 0.189, 0.27, 0.15, 0.187, 0.102]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3641 | Steps: 4 | Val loss: 0.2823 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4824 | Steps: 4 | Val loss: 0.3417 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3604 | Steps: 4 | Val loss: 0.2844 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4772 | Steps: 4 | Val loss: 0.3434 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 16:00:30 (running for 00:31:43.81)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.369 |  0.157 |                   58 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.354 |  0.154 |                   44 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.177 |                   35 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.49  |  0.175 |                   34 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.17694422602653503
[2m[36m(func pid=25864)[0m mae:  0.1277875006198883
[2m[36m(func pid=25864)[0m rmse_per_class: [0.125, 0.267, 0.114, 0.352, 0.068, 0.187, 0.268, 0.138, 0.156, 0.095]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15777018666267395
[2m[36m(func pid=19000)[0m mae:  0.1114901676774025
[2m[36m(func pid=19000)[0m rmse_per_class: [0.101, 0.24, 0.061, 0.305, 0.073, 0.172, 0.259, 0.114, 0.139, 0.115]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.1595069020986557
[2m[36m(func pid=23101)[0m mae:  0.10450045019388199
[2m[36m(func pid=23101)[0m rmse_per_class: [0.111, 0.208, 0.076, 0.324, 0.123, 0.188, 0.253, 0.092, 0.13, 0.09]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.17557664215564728
[2m[36m(func pid=26476)[0m mae:  0.12486670911312103
[2m[36m(func pid=26476)[0m rmse_per_class: [0.096, 0.26, 0.08, 0.361, 0.056, 0.195, 0.279, 0.149, 0.167, 0.113]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4837 | Steps: 4 | Val loss: 0.3451 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3390 | Steps: 4 | Val loss: 0.2779 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3630 | Steps: 4 | Val loss: 0.2811 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4722 | Steps: 4 | Val loss: 0.3388 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 16:00:35 (running for 00:31:49.20)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.364 |  0.158 |                   59 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.339 |  0.155 |                   46 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.177 |                   35 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.477 |  0.176 |                   35 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.1548723727464676
[2m[36m(func pid=23101)[0m mae:  0.1032896414399147
[2m[36m(func pid=23101)[0m rmse_per_class: [0.089, 0.237, 0.03, 0.259, 0.118, 0.182, 0.241, 0.091, 0.131, 0.17]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17713135480880737
[2m[36m(func pid=25864)[0m mae:  0.12776030600070953
[2m[36m(func pid=25864)[0m rmse_per_class: [0.125, 0.267, 0.115, 0.353, 0.067, 0.187, 0.269, 0.138, 0.156, 0.094]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.156960129737854
[2m[36m(func pid=19000)[0m mae:  0.11139005422592163
[2m[36m(func pid=19000)[0m rmse_per_class: [0.101, 0.238, 0.053, 0.3, 0.071, 0.172, 0.261, 0.113, 0.14, 0.12]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.17559468746185303
[2m[36m(func pid=26476)[0m mae:  0.12450041621923447
[2m[36m(func pid=26476)[0m rmse_per_class: [0.096, 0.266, 0.073, 0.352, 0.056, 0.2, 0.286, 0.147, 0.151, 0.129]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3620 | Steps: 4 | Val loss: 0.2912 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4871 | Steps: 4 | Val loss: 0.3427 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3642 | Steps: 4 | Val loss: 0.2803 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4633 | Steps: 4 | Val loss: 0.3356 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=25864)[0m rmse: 0.1767951399087906
[2m[36m(func pid=25864)[0m mae:  0.12755262851715088
[2m[36m(func pid=25864)[0m rmse_per_class: [0.125, 0.267, 0.113, 0.353, 0.067, 0.187, 0.268, 0.138, 0.156, 0.095]
== Status ==
Current time: 2024-01-07 16:00:40 (running for 00:31:54.55)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.363 |  0.157 |                   60 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.339 |  0.155 |                   46 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.487 |  0.177 |                   37 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.472 |  0.176 |                   36 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.16033998131752014
[2m[36m(func pid=23101)[0m mae:  0.10883168876171112
[2m[36m(func pid=23101)[0m rmse_per_class: [0.128, 0.203, 0.033, 0.312, 0.097, 0.165, 0.27, 0.101, 0.168, 0.125]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.17576152086257935
[2m[36m(func pid=26476)[0m mae:  0.12412629276514053
[2m[36m(func pid=26476)[0m rmse_per_class: [0.097, 0.267, 0.068, 0.342, 0.056, 0.201, 0.292, 0.145, 0.14, 0.149]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15619318187236786
[2m[36m(func pid=19000)[0m mae:  0.11101126670837402
[2m[36m(func pid=19000)[0m rmse_per_class: [0.106, 0.236, 0.047, 0.301, 0.069, 0.172, 0.261, 0.113, 0.141, 0.117]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4899 | Steps: 4 | Val loss: 0.3460 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3596 | Steps: 4 | Val loss: 0.2900 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4730 | Steps: 4 | Val loss: 0.3335 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3772 | Steps: 4 | Val loss: 0.2805 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=25864)[0m rmse: 0.1768724024295807
[2m[36m(func pid=25864)[0m mae:  0.12741081416606903
[2m[36m(func pid=25864)[0m rmse_per_class: [0.126, 0.267, 0.113, 0.354, 0.065, 0.187, 0.268, 0.139, 0.156, 0.094]
[2m[36m(func pid=25864)[0m 
== Status ==
Current time: 2024-01-07 16:00:46 (running for 00:31:59.87)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.364 |  0.156 |                   61 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.362 |  0.16  |                   47 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.49  |  0.177 |                   38 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.463 |  0.176 |                   37 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m rmse: 0.15802296996116638
[2m[36m(func pid=23101)[0m mae:  0.10306432098150253
[2m[36m(func pid=23101)[0m rmse_per_class: [0.09, 0.207, 0.097, 0.345, 0.075, 0.219, 0.225, 0.105, 0.13, 0.088]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.17599572241306305
[2m[36m(func pid=26476)[0m mae:  0.1237652525305748
[2m[36m(func pid=26476)[0m rmse_per_class: [0.099, 0.263, 0.066, 0.33, 0.056, 0.198, 0.298, 0.142, 0.135, 0.174]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15613558888435364
[2m[36m(func pid=19000)[0m mae:  0.11109600216150284
[2m[36m(func pid=19000)[0m rmse_per_class: [0.101, 0.233, 0.049, 0.304, 0.068, 0.172, 0.261, 0.113, 0.144, 0.117]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4900 | Steps: 4 | Val loss: 0.3510 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3389 | Steps: 4 | Val loss: 0.2618 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4559 | Steps: 4 | Val loss: 0.3313 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3592 | Steps: 4 | Val loss: 0.2810 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 16:00:51 (running for 00:32:05.30)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.377 |  0.156 |                   62 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.36  |  0.158 |                   48 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.49  |  0.177 |                   39 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.473 |  0.176 |                   38 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.17715123295783997
[2m[36m(func pid=25864)[0m mae:  0.1273673176765442
[2m[36m(func pid=25864)[0m rmse_per_class: [0.127, 0.267, 0.114, 0.355, 0.064, 0.186, 0.269, 0.139, 0.157, 0.094]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.14466388523578644
[2m[36m(func pid=23101)[0m mae:  0.0951029509305954
[2m[36m(func pid=23101)[0m rmse_per_class: [0.102, 0.212, 0.033, 0.273, 0.072, 0.161, 0.221, 0.096, 0.13, 0.146]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.1757500171661377
[2m[36m(func pid=26476)[0m mae:  0.12304427474737167
[2m[36m(func pid=26476)[0m rmse_per_class: [0.099, 0.256, 0.066, 0.316, 0.056, 0.191, 0.302, 0.14, 0.133, 0.197]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15598192811012268
[2m[36m(func pid=19000)[0m mae:  0.11083564907312393
[2m[36m(func pid=19000)[0m rmse_per_class: [0.098, 0.232, 0.051, 0.312, 0.07, 0.172, 0.258, 0.114, 0.141, 0.112]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4893 | Steps: 4 | Val loss: 0.3551 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3480 | Steps: 4 | Val loss: 0.2686 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4412 | Steps: 4 | Val loss: 0.3287 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3582 | Steps: 4 | Val loss: 0.2808 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 16:00:57 (running for 00:32:10.71)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.359 |  0.156 |                   63 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.339 |  0.145 |                   49 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.489 |  0.177 |                   40 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.456 |  0.176 |                   39 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.17735113203525543
[2m[36m(func pid=25864)[0m mae:  0.1272963583469391
[2m[36m(func pid=25864)[0m rmse_per_class: [0.128, 0.266, 0.114, 0.356, 0.062, 0.186, 0.271, 0.139, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.1474485844373703
[2m[36m(func pid=23101)[0m mae:  0.09805826842784882
[2m[36m(func pid=23101)[0m rmse_per_class: [0.092, 0.199, 0.049, 0.265, 0.095, 0.16, 0.265, 0.114, 0.145, 0.089]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.1740686595439911
[2m[36m(func pid=26476)[0m mae:  0.12160613387823105
[2m[36m(func pid=26476)[0m rmse_per_class: [0.1, 0.249, 0.058, 0.302, 0.056, 0.186, 0.304, 0.136, 0.133, 0.216]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15572170913219452
[2m[36m(func pid=19000)[0m mae:  0.11054164171218872
[2m[36m(func pid=19000)[0m rmse_per_class: [0.097, 0.23, 0.048, 0.315, 0.074, 0.172, 0.255, 0.115, 0.137, 0.113]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3462 | Steps: 4 | Val loss: 0.3065 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4953 | Steps: 4 | Val loss: 0.3546 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4640 | Steps: 4 | Val loss: 0.3282 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3637 | Steps: 4 | Val loss: 0.2821 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 16:01:02 (running for 00:32:16.11)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.358 |  0.156 |                   64 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.348 |  0.147 |                   50 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.495 |  0.177 |                   41 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.441 |  0.174 |                   40 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.17731516063213348
[2m[36m(func pid=25864)[0m mae:  0.12723210453987122
[2m[36m(func pid=25864)[0m rmse_per_class: [0.129, 0.266, 0.113, 0.356, 0.062, 0.186, 0.271, 0.139, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.16809071600437164
[2m[36m(func pid=23101)[0m mae:  0.11520659923553467
[2m[36m(func pid=23101)[0m rmse_per_class: [0.095, 0.206, 0.067, 0.318, 0.108, 0.165, 0.302, 0.089, 0.159, 0.171]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.17220529913902283
[2m[36m(func pid=26476)[0m mae:  0.12017150223255157
[2m[36m(func pid=26476)[0m rmse_per_class: [0.101, 0.248, 0.047, 0.292, 0.056, 0.182, 0.305, 0.132, 0.133, 0.224]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15625616908073425
[2m[36m(func pid=19000)[0m mae:  0.11051996052265167
[2m[36m(func pid=19000)[0m rmse_per_class: [0.1, 0.23, 0.05, 0.321, 0.073, 0.173, 0.25, 0.116, 0.135, 0.114]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5034 | Steps: 4 | Val loss: 0.3535 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3391 | Steps: 4 | Val loss: 0.2631 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4439 | Steps: 4 | Val loss: 0.3273 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 16:01:07 (running for 00:32:21.43)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.364 |  0.156 |                   65 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.346 |  0.168 |                   51 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.503 |  0.177 |                   42 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.464 |  0.172 |                   41 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.1770658791065216
[2m[36m(func pid=25864)[0m mae:  0.1270482838153839
[2m[36m(func pid=25864)[0m rmse_per_class: [0.129, 0.266, 0.112, 0.356, 0.061, 0.186, 0.27, 0.139, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3638 | Steps: 4 | Val loss: 0.2806 | Batch size: 32 | lr: 0.01 | Duration: 3.28s
[2m[36m(func pid=23101)[0m rmse: 0.1465885043144226
[2m[36m(func pid=23101)[0m mae:  0.09480465948581696
[2m[36m(func pid=23101)[0m rmse_per_class: [0.113, 0.207, 0.044, 0.27, 0.103, 0.162, 0.241, 0.098, 0.13, 0.097]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.1714695394039154
[2m[36m(func pid=26476)[0m mae:  0.11909278482198715
[2m[36m(func pid=26476)[0m rmse_per_class: [0.101, 0.251, 0.044, 0.291, 0.056, 0.177, 0.306, 0.129, 0.133, 0.225]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4913 | Steps: 4 | Val loss: 0.3506 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=19000)[0m rmse: 0.154983788728714
[2m[36m(func pid=19000)[0m mae:  0.109389528632164
[2m[36m(func pid=19000)[0m rmse_per_class: [0.101, 0.227, 0.05, 0.322, 0.073, 0.172, 0.248, 0.114, 0.135, 0.108]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3517 | Steps: 4 | Val loss: 0.2672 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4807 | Steps: 4 | Val loss: 0.3252 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 16:01:13 (running for 00:32:26.86)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.364 |  0.155 |                   66 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.339 |  0.147 |                   52 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.491 |  0.177 |                   43 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.444 |  0.171 |                   42 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.17662420868873596
[2m[36m(func pid=25864)[0m mae:  0.12673720717430115
[2m[36m(func pid=25864)[0m rmse_per_class: [0.13, 0.266, 0.109, 0.356, 0.061, 0.186, 0.268, 0.14, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.14260491728782654
[2m[36m(func pid=23101)[0m mae:  0.09360755980014801
[2m[36m(func pid=23101)[0m rmse_per_class: [0.103, 0.209, 0.027, 0.262, 0.088, 0.158, 0.269, 0.092, 0.131, 0.087]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.17077834904193878
[2m[36m(func pid=26476)[0m mae:  0.11796188354492188
[2m[36m(func pid=26476)[0m rmse_per_class: [0.1, 0.257, 0.043, 0.295, 0.056, 0.176, 0.304, 0.129, 0.134, 0.213]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3657 | Steps: 4 | Val loss: 0.2790 | Batch size: 32 | lr: 0.01 | Duration: 3.25s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5085 | Steps: 4 | Val loss: 0.3538 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3295 | Steps: 4 | Val loss: 0.2764 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=19000)[0m rmse: 0.15393102169036865
[2m[36m(func pid=19000)[0m mae:  0.10873763263225555
[2m[36m(func pid=19000)[0m rmse_per_class: [0.097, 0.228, 0.046, 0.32, 0.075, 0.172, 0.25, 0.113, 0.135, 0.105]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4876 | Steps: 4 | Val loss: 0.3219 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 16:01:18 (running for 00:32:32.33)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.366 |  0.154 |                   67 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.352 |  0.143 |                   53 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.508 |  0.177 |                   44 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.481 |  0.171 |                   43 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.17681221663951874
[2m[36m(func pid=25864)[0m mae:  0.1266813427209854
[2m[36m(func pid=25864)[0m rmse_per_class: [0.131, 0.266, 0.11, 0.356, 0.06, 0.186, 0.269, 0.14, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.1561465561389923
[2m[36m(func pid=23101)[0m mae:  0.10349990427494049
[2m[36m(func pid=23101)[0m rmse_per_class: [0.09, 0.214, 0.109, 0.283, 0.081, 0.166, 0.253, 0.094, 0.182, 0.09]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.16989561915397644
[2m[36m(func pid=26476)[0m mae:  0.1168830543756485
[2m[36m(func pid=26476)[0m rmse_per_class: [0.099, 0.261, 0.042, 0.298, 0.056, 0.175, 0.302, 0.133, 0.134, 0.198]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3556 | Steps: 4 | Val loss: 0.2775 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4975 | Steps: 4 | Val loss: 0.3548 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3613 | Steps: 4 | Val loss: 0.2820 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4619 | Steps: 4 | Val loss: 0.3181 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=19000)[0m rmse: 0.15379053354263306
[2m[36m(func pid=19000)[0m mae:  0.10859241336584091
[2m[36m(func pid=19000)[0m rmse_per_class: [0.096, 0.228, 0.05, 0.311, 0.076, 0.172, 0.254, 0.112, 0.136, 0.104]
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 16:01:23 (running for 00:32:37.62)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.356 |  0.154 |                   68 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.156 |                   54 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.498 |  0.177 |                   45 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.488 |  0.17  |                   44 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.17696377635002136
[2m[36m(func pid=25864)[0m mae:  0.12673607468605042
[2m[36m(func pid=25864)[0m rmse_per_class: [0.132, 0.266, 0.11, 0.357, 0.06, 0.186, 0.269, 0.14, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.15633606910705566
[2m[36m(func pid=23101)[0m mae:  0.10458836704492569
[2m[36m(func pid=23101)[0m rmse_per_class: [0.14, 0.21, 0.035, 0.318, 0.068, 0.164, 0.232, 0.096, 0.134, 0.166]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.1694193035364151
[2m[36m(func pid=26476)[0m mae:  0.11638566106557846
[2m[36m(func pid=26476)[0m rmse_per_class: [0.099, 0.264, 0.043, 0.296, 0.056, 0.176, 0.298, 0.149, 0.134, 0.179]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3573 | Steps: 4 | Val loss: 0.2766 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4867 | Steps: 4 | Val loss: 0.3531 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3492 | Steps: 4 | Val loss: 0.2729 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4653 | Steps: 4 | Val loss: 0.3142 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=19000)[0m rmse: 0.15414679050445557
[2m[36m(func pid=19000)[0m mae:  0.10859449207782745
[2m[36m(func pid=19000)[0m rmse_per_class: [0.097, 0.23, 0.058, 0.299, 0.075, 0.171, 0.258, 0.111, 0.137, 0.104]
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 16:01:29 (running for 00:32:42.95)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.357 |  0.154 |                   69 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.361 |  0.156 |                   55 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.487 |  0.177 |                   46 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.462 |  0.169 |                   45 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.1767309010028839
[2m[36m(func pid=25864)[0m mae:  0.1265890896320343
[2m[36m(func pid=25864)[0m rmse_per_class: [0.133, 0.266, 0.109, 0.357, 0.059, 0.186, 0.268, 0.14, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.1493280827999115
[2m[36m(func pid=23101)[0m mae:  0.09740898013114929
[2m[36m(func pid=23101)[0m rmse_per_class: [0.098, 0.256, 0.039, 0.301, 0.069, 0.16, 0.243, 0.091, 0.131, 0.104]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.16939067840576172
[2m[36m(func pid=26476)[0m mae:  0.1161314994096756
[2m[36m(func pid=26476)[0m rmse_per_class: [0.099, 0.267, 0.043, 0.289, 0.056, 0.178, 0.291, 0.176, 0.134, 0.161]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3618 | Steps: 4 | Val loss: 0.2775 | Batch size: 32 | lr: 0.01 | Duration: 3.32s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4940 | Steps: 4 | Val loss: 0.3557 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3469 | Steps: 4 | Val loss: 0.2645 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4454 | Steps: 4 | Val loss: 0.3119 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=19000)[0m rmse: 0.15454606711864471
[2m[36m(func pid=19000)[0m mae:  0.1089644804596901
[2m[36m(func pid=19000)[0m rmse_per_class: [0.103, 0.234, 0.052, 0.301, 0.073, 0.171, 0.255, 0.111, 0.137, 0.108]
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 16:01:34 (running for 00:32:48.29)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.362 |  0.155 |                   70 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.349 |  0.149 |                   56 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.494 |  0.177 |                   47 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.465 |  0.169 |                   46 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.1769755482673645
[2m[36m(func pid=25864)[0m mae:  0.12661011517047882
[2m[36m(func pid=25864)[0m rmse_per_class: [0.136, 0.266, 0.108, 0.358, 0.059, 0.186, 0.269, 0.14, 0.156, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.14439764618873596
[2m[36m(func pid=23101)[0m mae:  0.09583784639835358
[2m[36m(func pid=23101)[0m rmse_per_class: [0.089, 0.219, 0.029, 0.267, 0.112, 0.169, 0.23, 0.094, 0.148, 0.086]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.17052367329597473
[2m[36m(func pid=26476)[0m mae:  0.1164393424987793
[2m[36m(func pid=26476)[0m rmse_per_class: [0.102, 0.27, 0.044, 0.284, 0.056, 0.18, 0.283, 0.211, 0.133, 0.143]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3616 | Steps: 4 | Val loss: 0.2796 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4968 | Steps: 4 | Val loss: 0.3541 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3528 | Steps: 4 | Val loss: 0.2893 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4800 | Steps: 4 | Val loss: 0.3126 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 16:01:39 (running for 00:32:53.62)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.362 |  0.156 |                   71 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.347 |  0.144 |                   57 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.494 |  0.177 |                   47 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.445 |  0.171 |                   47 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19000)[0m rmse: 0.1557726114988327
[2m[36m(func pid=19000)[0m mae:  0.10979475826025009
[2m[36m(func pid=19000)[0m rmse_per_class: [0.109, 0.231, 0.055, 0.308, 0.073, 0.171, 0.256, 0.111, 0.138, 0.108]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17682698369026184
[2m[36m(func pid=25864)[0m mae:  0.12647652626037598
[2m[36m(func pid=25864)[0m rmse_per_class: [0.136, 0.265, 0.107, 0.358, 0.058, 0.186, 0.268, 0.14, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.17331728339195251
[2m[36m(func pid=26476)[0m mae:  0.11763248592615128
[2m[36m(func pid=26476)[0m rmse_per_class: [0.106, 0.272, 0.045, 0.287, 0.056, 0.181, 0.275, 0.252, 0.133, 0.126]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.16525782644748688
[2m[36m(func pid=23101)[0m mae:  0.10832260549068451
[2m[36m(func pid=23101)[0m rmse_per_class: [0.113, 0.213, 0.096, 0.294, 0.127, 0.162, 0.268, 0.095, 0.131, 0.153]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4969 | Steps: 4 | Val loss: 0.3551 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3648 | Steps: 4 | Val loss: 0.2790 | Batch size: 32 | lr: 0.01 | Duration: 3.31s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4331 | Steps: 4 | Val loss: 0.3143 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3576 | Steps: 4 | Val loss: 0.2964 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 16:01:45 (running for 00:32:59.13)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.362 |  0.156 |                   71 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.353 |  0.165 |                   58 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.497 |  0.177 |                   49 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.48  |  0.173 |                   48 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.177040696144104
[2m[36m(func pid=25864)[0m mae:  0.12655460834503174
[2m[36m(func pid=25864)[0m rmse_per_class: [0.138, 0.265, 0.108, 0.358, 0.058, 0.186, 0.268, 0.14, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15425845980644226
[2m[36m(func pid=19000)[0m mae:  0.10874916613101959
[2m[36m(func pid=19000)[0m rmse_per_class: [0.1, 0.228, 0.051, 0.317, 0.073, 0.171, 0.252, 0.11, 0.136, 0.104]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.1764260232448578
[2m[36m(func pid=26476)[0m mae:  0.1189327985048294
[2m[36m(func pid=26476)[0m rmse_per_class: [0.119, 0.271, 0.044, 0.297, 0.056, 0.184, 0.271, 0.276, 0.133, 0.114]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.15804454684257507
[2m[36m(func pid=23101)[0m mae:  0.10430659353733063
[2m[36m(func pid=23101)[0m rmse_per_class: [0.089, 0.286, 0.036, 0.347, 0.078, 0.166, 0.237, 0.094, 0.134, 0.111]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5062 | Steps: 4 | Val loss: 0.3548 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4311 | Steps: 4 | Val loss: 0.3164 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3542 | Steps: 4 | Val loss: 0.2813 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3481 | Steps: 4 | Val loss: 0.2720 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 16:01:51 (running for 00:33:04.69)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.365 |  0.154 |                   72 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.358 |  0.158 |                   59 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.506 |  0.177 |                   50 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.433 |  0.176 |                   49 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.17698873579502106
[2m[36m(func pid=25864)[0m mae:  0.12644721567630768
[2m[36m(func pid=25864)[0m rmse_per_class: [0.139, 0.265, 0.107, 0.358, 0.057, 0.186, 0.267, 0.141, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.17885635793209076
[2m[36m(func pid=26476)[0m mae:  0.1201774850487709
[2m[36m(func pid=26476)[0m rmse_per_class: [0.127, 0.27, 0.044, 0.311, 0.056, 0.185, 0.266, 0.291, 0.133, 0.105]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15469594299793243
[2m[36m(func pid=19000)[0m mae:  0.10908051580190659
[2m[36m(func pid=19000)[0m rmse_per_class: [0.095, 0.23, 0.052, 0.324, 0.073, 0.171, 0.254, 0.11, 0.136, 0.102]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.14901691675186157
[2m[36m(func pid=23101)[0m mae:  0.09946658462285995
[2m[36m(func pid=23101)[0m rmse_per_class: [0.102, 0.202, 0.039, 0.298, 0.065, 0.166, 0.239, 0.092, 0.201, 0.088]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4880 | Steps: 4 | Val loss: 0.3573 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4258 | Steps: 4 | Val loss: 0.3207 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3505 | Steps: 4 | Val loss: 0.2862 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3626 | Steps: 4 | Val loss: 0.2821 | Batch size: 32 | lr: 0.01 | Duration: 3.27s
== Status ==
Current time: 2024-01-07 16:01:56 (running for 00:33:10.09)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.155 |                   73 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.348 |  0.149 |                   60 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.488 |  0.177 |                   51 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.431 |  0.179 |                   50 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.17743471264839172
[2m[36m(func pid=25864)[0m mae:  0.12664291262626648
[2m[36m(func pid=25864)[0m rmse_per_class: [0.143, 0.265, 0.107, 0.359, 0.057, 0.186, 0.268, 0.141, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.1814301759004593
[2m[36m(func pid=26476)[0m mae:  0.12147215753793716
[2m[36m(func pid=26476)[0m rmse_per_class: [0.144, 0.269, 0.044, 0.325, 0.056, 0.187, 0.263, 0.295, 0.133, 0.099]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.15754157304763794
[2m[36m(func pid=23101)[0m mae:  0.10541129112243652
[2m[36m(func pid=23101)[0m rmse_per_class: [0.122, 0.248, 0.039, 0.267, 0.066, 0.161, 0.273, 0.099, 0.132, 0.167]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15516380965709686
[2m[36m(func pid=19000)[0m mae:  0.10960259288549423
[2m[36m(func pid=19000)[0m rmse_per_class: [0.093, 0.23, 0.048, 0.326, 0.077, 0.171, 0.254, 0.11, 0.136, 0.106]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4910 | Steps: 4 | Val loss: 0.3481 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4509 | Steps: 4 | Val loss: 0.3245 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3406 | Steps: 4 | Val loss: 0.2575 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3637 | Steps: 4 | Val loss: 0.2843 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 16:02:01 (running for 00:33:15.60)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15925000235438347
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.363 |  0.155 |                   74 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.35  |  0.158 |                   61 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.491 |  0.176 |                   52 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.426 |  0.181 |                   51 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.17644019424915314
[2m[36m(func pid=25864)[0m mae:  0.12617850303649902
[2m[36m(func pid=25864)[0m rmse_per_class: [0.14, 0.265, 0.104, 0.357, 0.057, 0.186, 0.266, 0.14, 0.156, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.1823960691690445
[2m[36m(func pid=26476)[0m mae:  0.12220605462789536
[2m[36m(func pid=26476)[0m rmse_per_class: [0.151, 0.268, 0.044, 0.338, 0.056, 0.187, 0.259, 0.292, 0.135, 0.096]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.1385524570941925
[2m[36m(func pid=23101)[0m mae:  0.0918041542172432
[2m[36m(func pid=23101)[0m rmse_per_class: [0.089, 0.196, 0.048, 0.264, 0.068, 0.165, 0.227, 0.091, 0.145, 0.091]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15761110186576843
[2m[36m(func pid=19000)[0m mae:  0.11078783124685287
[2m[36m(func pid=19000)[0m rmse_per_class: [0.094, 0.225, 0.064, 0.327, 0.08, 0.172, 0.252, 0.11, 0.136, 0.116]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4432 | Steps: 4 | Val loss: 0.3263 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4886 | Steps: 4 | Val loss: 0.3499 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3629 | Steps: 4 | Val loss: 0.2669 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3686 | Steps: 4 | Val loss: 0.2864 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 16:02:07 (running for 00:33:20.96)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.364 |  0.158 |                   75 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.341 |  0.139 |                   62 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.491 |  0.176 |                   52 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.443 |  0.182 |                   53 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.17680624127388
[2m[36m(func pid=25864)[0m mae:  0.12635961174964905
[2m[36m(func pid=25864)[0m rmse_per_class: [0.144, 0.265, 0.103, 0.358, 0.057, 0.186, 0.266, 0.14, 0.156, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.1815277636051178
[2m[36m(func pid=26476)[0m mae:  0.12227050960063934
[2m[36m(func pid=26476)[0m rmse_per_class: [0.149, 0.265, 0.044, 0.347, 0.055, 0.188, 0.255, 0.279, 0.139, 0.094]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.14391657710075378
[2m[36m(func pid=23101)[0m mae:  0.09482911974191666
[2m[36m(func pid=23101)[0m rmse_per_class: [0.09, 0.209, 0.038, 0.284, 0.091, 0.163, 0.257, 0.092, 0.132, 0.084]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15949687361717224
[2m[36m(func pid=19000)[0m mae:  0.11224158108234406
[2m[36m(func pid=19000)[0m rmse_per_class: [0.095, 0.233, 0.06, 0.326, 0.081, 0.172, 0.251, 0.11, 0.139, 0.127]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4338 | Steps: 4 | Val loss: 0.3318 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4968 | Steps: 4 | Val loss: 0.3483 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3396 | Steps: 4 | Val loss: 0.2896 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3641 | Steps: 4 | Val loss: 0.2861 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=26476)[0m rmse: 0.18060672283172607
[2m[36m(func pid=26476)[0m mae:  0.12231556326150894
[2m[36m(func pid=26476)[0m rmse_per_class: [0.155, 0.262, 0.042, 0.355, 0.055, 0.188, 0.253, 0.254, 0.148, 0.093]
[2m[36m(func pid=26476)[0m 
== Status ==
Current time: 2024-01-07 16:02:12 (running for 00:33:26.25)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.369 |  0.159 |                   76 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.363 |  0.144 |                   63 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.489 |  0.177 |                   53 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.434 |  0.181 |                   54 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.17676283419132233
[2m[36m(func pid=25864)[0m mae:  0.12638182938098907
[2m[36m(func pid=25864)[0m rmse_per_class: [0.146, 0.264, 0.102, 0.358, 0.056, 0.186, 0.265, 0.14, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.16190996766090393
[2m[36m(func pid=23101)[0m mae:  0.1083863377571106
[2m[36m(func pid=23101)[0m rmse_per_class: [0.137, 0.219, 0.064, 0.291, 0.105, 0.161, 0.291, 0.107, 0.139, 0.106]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15946030616760254
[2m[36m(func pid=19000)[0m mae:  0.11271063983440399
[2m[36m(func pid=19000)[0m rmse_per_class: [0.098, 0.239, 0.051, 0.321, 0.083, 0.172, 0.251, 0.111, 0.145, 0.123]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4119 | Steps: 4 | Val loss: 0.3271 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4848 | Steps: 4 | Val loss: 0.3500 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3375 | Steps: 4 | Val loss: 0.2647 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 16:02:17 (running for 00:33:31.53)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.364 |  0.159 |                   77 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.34  |  0.162 |                   64 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.497 |  0.177 |                   54 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.412 |  0.177 |                   55 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3743 | Steps: 4 | Val loss: 0.2883 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=26476)[0m rmse: 0.1766778975725174
[2m[36m(func pid=26476)[0m mae:  0.12110638618469238
[2m[36m(func pid=26476)[0m rmse_per_class: [0.137, 0.255, 0.043, 0.357, 0.055, 0.187, 0.252, 0.23, 0.159, 0.093]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.1772439181804657
[2m[36m(func pid=25864)[0m mae:  0.1265898048877716
[2m[36m(func pid=25864)[0m rmse_per_class: [0.15, 0.264, 0.102, 0.358, 0.056, 0.186, 0.264, 0.141, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.1437012255191803
[2m[36m(func pid=23101)[0m mae:  0.09342480450868607
[2m[36m(func pid=23101)[0m rmse_per_class: [0.096, 0.209, 0.037, 0.286, 0.082, 0.162, 0.248, 0.094, 0.13, 0.094]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.16111773252487183
[2m[36m(func pid=19000)[0m mae:  0.11391135305166245
[2m[36m(func pid=19000)[0m rmse_per_class: [0.102, 0.24, 0.06, 0.319, 0.082, 0.172, 0.256, 0.11, 0.149, 0.121]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4310 | Steps: 4 | Val loss: 0.3255 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4812 | Steps: 4 | Val loss: 0.3535 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3671 | Steps: 4 | Val loss: 0.2795 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=26476)[0m rmse: 0.17406007647514343
[2m[36m(func pid=26476)[0m mae:  0.12014289945363998
[2m[36m(func pid=26476)[0m rmse_per_class: [0.123, 0.249, 0.043, 0.36, 0.055, 0.185, 0.249, 0.213, 0.172, 0.092]
[2m[36m(func pid=26476)[0m 
== Status ==
Current time: 2024-01-07 16:02:23 (running for 00:33:36.87)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.374 |  0.161 |                   78 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.338 |  0.144 |                   65 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.485 |  0.177 |                   55 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.431 |  0.174 |                   56 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=25864)[0m rmse: 0.1778912991285324
[2m[36m(func pid=25864)[0m mae:  0.12687095999717712
[2m[36m(func pid=25864)[0m rmse_per_class: [0.155, 0.264, 0.103, 0.359, 0.056, 0.186, 0.265, 0.141, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3567 | Steps: 4 | Val loss: 0.2851 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=23101)[0m rmse: 0.15217724442481995
[2m[36m(func pid=23101)[0m mae:  0.10051538795232773
[2m[36m(func pid=23101)[0m rmse_per_class: [0.113, 0.221, 0.032, 0.308, 0.089, 0.175, 0.275, 0.093, 0.129, 0.087]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4220 | Steps: 4 | Val loss: 0.3253 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=19000)[0m rmse: 0.1590508371591568
[2m[36m(func pid=19000)[0m mae:  0.11224231868982315
[2m[36m(func pid=19000)[0m rmse_per_class: [0.106, 0.238, 0.054, 0.316, 0.082, 0.172, 0.256, 0.114, 0.142, 0.111]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4842 | Steps: 4 | Val loss: 0.3541 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3593 | Steps: 4 | Val loss: 0.2790 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 16:02:28 (running for 00:33:42.08)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.357 |  0.159 |                   79 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.367 |  0.152 |                   66 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.481 |  0.178 |                   56 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.422 |  0.172 |                   57 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.17179881036281586
[2m[36m(func pid=26476)[0m mae:  0.11936633288860321
[2m[36m(func pid=26476)[0m rmse_per_class: [0.116, 0.244, 0.044, 0.363, 0.054, 0.184, 0.246, 0.189, 0.187, 0.092]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17818140983581543
[2m[36m(func pid=25864)[0m mae:  0.1269567608833313
[2m[36m(func pid=25864)[0m rmse_per_class: [0.158, 0.264, 0.103, 0.36, 0.055, 0.186, 0.264, 0.141, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3536 | Steps: 4 | Val loss: 0.2822 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=23101)[0m rmse: 0.15551064908504486
[2m[36m(func pid=23101)[0m mae:  0.10372457653284073
[2m[36m(func pid=23101)[0m rmse_per_class: [0.091, 0.213, 0.086, 0.264, 0.1, 0.17, 0.257, 0.091, 0.141, 0.142]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4326 | Steps: 4 | Val loss: 0.3278 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5029 | Steps: 4 | Val loss: 0.3573 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=19000)[0m rmse: 0.15710827708244324
[2m[36m(func pid=19000)[0m mae:  0.11059828102588654
[2m[36m(func pid=19000)[0m rmse_per_class: [0.113, 0.232, 0.046, 0.316, 0.08, 0.172, 0.256, 0.115, 0.136, 0.104]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3314 | Steps: 4 | Val loss: 0.2725 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 16:02:33 (running for 00:33:47.47)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.157 |                   80 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.359 |  0.156 |                   67 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.484 |  0.178 |                   57 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.433 |  0.171 |                   58 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.17065224051475525
[2m[36m(func pid=26476)[0m mae:  0.1192270889878273
[2m[36m(func pid=26476)[0m rmse_per_class: [0.113, 0.239, 0.047, 0.366, 0.054, 0.182, 0.243, 0.165, 0.205, 0.091]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17885597050189972
[2m[36m(func pid=25864)[0m mae:  0.12720707058906555
[2m[36m(func pid=25864)[0m rmse_per_class: [0.164, 0.264, 0.102, 0.361, 0.055, 0.186, 0.264, 0.141, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3648 | Steps: 4 | Val loss: 0.2788 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=23101)[0m rmse: 0.1486310064792633
[2m[36m(func pid=23101)[0m mae:  0.09831451624631882
[2m[36m(func pid=23101)[0m rmse_per_class: [0.091, 0.21, 0.033, 0.311, 0.098, 0.166, 0.222, 0.1, 0.133, 0.122]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4100 | Steps: 4 | Val loss: 0.3313 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4833 | Steps: 4 | Val loss: 0.3538 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=19000)[0m rmse: 0.15456834435462952
[2m[36m(func pid=19000)[0m mae:  0.10846252739429474
[2m[36m(func pid=19000)[0m rmse_per_class: [0.107, 0.227, 0.048, 0.315, 0.074, 0.171, 0.252, 0.113, 0.134, 0.104]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3398 | Steps: 4 | Val loss: 0.2714 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 16:02:39 (running for 00:33:52.72)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.365 |  0.155 |                   81 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.331 |  0.149 |                   68 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.503 |  0.179 |                   58 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.41  |  0.17  |                   59 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.1698150336742401
[2m[36m(func pid=26476)[0m mae:  0.11908368766307831
[2m[36m(func pid=26476)[0m rmse_per_class: [0.111, 0.233, 0.055, 0.368, 0.054, 0.181, 0.242, 0.145, 0.218, 0.091]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17855572700500488
[2m[36m(func pid=25864)[0m mae:  0.1270071417093277
[2m[36m(func pid=25864)[0m rmse_per_class: [0.164, 0.264, 0.101, 0.36, 0.055, 0.186, 0.263, 0.141, 0.158, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.15038137137889862
[2m[36m(func pid=23101)[0m mae:  0.09864956140518188
[2m[36m(func pid=23101)[0m rmse_per_class: [0.135, 0.205, 0.047, 0.29, 0.085, 0.165, 0.258, 0.09, 0.13, 0.1]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3580 | Steps: 4 | Val loss: 0.2770 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4050 | Steps: 4 | Val loss: 0.3315 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4821 | Steps: 4 | Val loss: 0.3521 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=19000)[0m rmse: 0.1529727578163147
[2m[36m(func pid=19000)[0m mae:  0.10773495584726334
[2m[36m(func pid=19000)[0m rmse_per_class: [0.099, 0.222, 0.048, 0.313, 0.072, 0.17, 0.255, 0.109, 0.134, 0.108]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3461 | Steps: 4 | Val loss: 0.2631 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 16:02:44 (running for 00:33:58.06)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.358 |  0.153 |                   82 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.34  |  0.15  |                   69 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.483 |  0.179 |                   59 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.405 |  0.168 |                   60 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.1681273877620697
[2m[36m(func pid=26476)[0m mae:  0.1181909590959549
[2m[36m(func pid=26476)[0m rmse_per_class: [0.106, 0.228, 0.062, 0.369, 0.053, 0.178, 0.242, 0.132, 0.22, 0.092]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17859089374542236
[2m[36m(func pid=25864)[0m mae:  0.127059668302536
[2m[36m(func pid=25864)[0m rmse_per_class: [0.167, 0.264, 0.1, 0.361, 0.055, 0.186, 0.262, 0.141, 0.158, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.14359940588474274
[2m[36m(func pid=23101)[0m mae:  0.09575624763965607
[2m[36m(func pid=23101)[0m rmse_per_class: [0.091, 0.202, 0.035, 0.273, 0.102, 0.161, 0.248, 0.091, 0.138, 0.097]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3576 | Steps: 4 | Val loss: 0.2745 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4526 | Steps: 4 | Val loss: 0.3370 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4804 | Steps: 4 | Val loss: 0.3501 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3427 | Steps: 4 | Val loss: 0.3044 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 16:02:49 (running for 00:34:03.23)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.358 |  0.152 |                   83 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.346 |  0.144 |                   70 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.482 |  0.179 |                   60 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.405 |  0.168 |                   60 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19000)[0m rmse: 0.15176327526569366
[2m[36m(func pid=19000)[0m mae:  0.10687434673309326
[2m[36m(func pid=19000)[0m rmse_per_class: [0.099, 0.223, 0.047, 0.306, 0.068, 0.17, 0.253, 0.108, 0.135, 0.109]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=26476)[0m rmse: 0.16827698051929474
[2m[36m(func pid=26476)[0m mae:  0.1180061474442482
[2m[36m(func pid=26476)[0m rmse_per_class: [0.104, 0.228, 0.075, 0.371, 0.053, 0.176, 0.242, 0.125, 0.217, 0.091]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.1786128431558609
[2m[36m(func pid=25864)[0m mae:  0.12705454230308533
[2m[36m(func pid=25864)[0m rmse_per_class: [0.169, 0.264, 0.1, 0.36, 0.055, 0.186, 0.261, 0.141, 0.158, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.15889987349510193
[2m[36m(func pid=23101)[0m mae:  0.10662166029214859
[2m[36m(func pid=23101)[0m rmse_per_class: [0.1, 0.219, 0.035, 0.36, 0.128, 0.159, 0.232, 0.091, 0.152, 0.11]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4085 | Steps: 4 | Val loss: 0.3321 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3643 | Steps: 4 | Val loss: 0.2728 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4832 | Steps: 4 | Val loss: 0.3514 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3545 | Steps: 4 | Val loss: 0.2734 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=26476)[0m rmse: 0.16652461886405945
[2m[36m(func pid=26476)[0m mae:  0.11690930277109146
[2m[36m(func pid=26476)[0m rmse_per_class: [0.1, 0.232, 0.078, 0.371, 0.053, 0.174, 0.243, 0.122, 0.199, 0.092]
[2m[36m(func pid=26476)[0m 
== Status ==
Current time: 2024-01-07 16:02:54 (running for 00:34:08.54)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.358 |  0.152 |                   83 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.343 |  0.159 |                   71 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.48  |  0.179 |                   61 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.409 |  0.167 |                   62 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19000)[0m rmse: 0.15099243819713593
[2m[36m(func pid=19000)[0m mae:  0.10639232397079468
[2m[36m(func pid=19000)[0m rmse_per_class: [0.099, 0.224, 0.045, 0.299, 0.07, 0.17, 0.251, 0.108, 0.138, 0.108]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.17914870381355286
[2m[36m(func pid=25864)[0m mae:  0.12729334831237793
[2m[36m(func pid=25864)[0m rmse_per_class: [0.174, 0.263, 0.1, 0.361, 0.055, 0.186, 0.261, 0.141, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.15259119868278503
[2m[36m(func pid=23101)[0m mae:  0.10081468522548676
[2m[36m(func pid=23101)[0m rmse_per_class: [0.105, 0.2, 0.067, 0.282, 0.1, 0.16, 0.263, 0.095, 0.133, 0.12]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4203 | Steps: 4 | Val loss: 0.3292 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3647 | Steps: 4 | Val loss: 0.2712 | Batch size: 32 | lr: 0.01 | Duration: 3.27s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4634 | Steps: 4 | Val loss: 0.3439 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3138 | Steps: 4 | Val loss: 0.2651 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 16:03:00 (running for 00:34:13.94)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.364 |  0.151 |                   84 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.355 |  0.153 |                   72 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.483 |  0.179 |                   62 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.42  |  0.166 |                   63 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.16561606526374817
[2m[36m(func pid=26476)[0m mae:  0.11585010588169098
[2m[36m(func pid=26476)[0m rmse_per_class: [0.097, 0.241, 0.082, 0.371, 0.054, 0.172, 0.244, 0.122, 0.181, 0.092]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15018466114997864
[2m[36m(func pid=19000)[0m mae:  0.10552289336919785
[2m[36m(func pid=19000)[0m rmse_per_class: [0.097, 0.227, 0.043, 0.295, 0.072, 0.169, 0.247, 0.107, 0.14, 0.104]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m rmse: 0.1780851185321808
[2m[36m(func pid=25864)[0m mae:  0.1268136352300644
[2m[36m(func pid=25864)[0m rmse_per_class: [0.17, 0.263, 0.097, 0.359, 0.055, 0.185, 0.26, 0.141, 0.158, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.14545473456382751
[2m[36m(func pid=23101)[0m mae:  0.09553052484989166
[2m[36m(func pid=23101)[0m rmse_per_class: [0.1, 0.201, 0.029, 0.275, 0.085, 0.156, 0.255, 0.125, 0.129, 0.099]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4187 | Steps: 4 | Val loss: 0.3256 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3612 | Steps: 4 | Val loss: 0.2735 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4678 | Steps: 4 | Val loss: 0.3455 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 16:03:05 (running for 00:34:19.12)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.365 |  0.15  |                   85 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.314 |  0.145 |                   73 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.463 |  0.178 |                   63 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.419 |  0.165 |                   64 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.16520194709300995
[2m[36m(func pid=26476)[0m mae:  0.11507955938577652
[2m[36m(func pid=26476)[0m rmse_per_class: [0.095, 0.254, 0.083, 0.37, 0.055, 0.17, 0.245, 0.123, 0.164, 0.093]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3287 | Steps: 4 | Val loss: 0.2713 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=25864)[0m rmse: 0.1786840707063675
[2m[36m(func pid=25864)[0m mae:  0.12700185179710388
[2m[36m(func pid=25864)[0m rmse_per_class: [0.176, 0.263, 0.097, 0.36, 0.055, 0.185, 0.259, 0.141, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1519148051738739
[2m[36m(func pid=19000)[0m mae:  0.10697895288467407
[2m[36m(func pid=19000)[0m rmse_per_class: [0.098, 0.227, 0.044, 0.298, 0.075, 0.17, 0.251, 0.108, 0.14, 0.11]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.1496409922838211
[2m[36m(func pid=23101)[0m mae:  0.10056155920028687
[2m[36m(func pid=23101)[0m rmse_per_class: [0.1, 0.203, 0.031, 0.275, 0.099, 0.158, 0.249, 0.099, 0.172, 0.11]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4250 | Steps: 4 | Val loss: 0.3252 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4714 | Steps: 4 | Val loss: 0.3420 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3650 | Steps: 4 | Val loss: 0.2771 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 16:03:10 (running for 00:34:24.35)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.361 |  0.152 |                   86 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.15  |                   74 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.468 |  0.179 |                   64 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.425 |  0.166 |                   65 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.16606900095939636
[2m[36m(func pid=26476)[0m mae:  0.114823117852211
[2m[36m(func pid=26476)[0m rmse_per_class: [0.095, 0.268, 0.087, 0.37, 0.056, 0.17, 0.247, 0.125, 0.15, 0.093]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3379 | Steps: 4 | Val loss: 0.2678 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=25864)[0m rmse: 0.178260937333107
[2m[36m(func pid=25864)[0m mae:  0.12684842944145203
[2m[36m(func pid=25864)[0m rmse_per_class: [0.174, 0.263, 0.096, 0.359, 0.055, 0.185, 0.259, 0.141, 0.158, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15433445572853088
[2m[36m(func pid=19000)[0m mae:  0.1088636964559555
[2m[36m(func pid=19000)[0m rmse_per_class: [0.095, 0.225, 0.042, 0.306, 0.085, 0.171, 0.256, 0.108, 0.136, 0.12]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.14612457156181335
[2m[36m(func pid=23101)[0m mae:  0.09602460265159607
[2m[36m(func pid=23101)[0m rmse_per_class: [0.095, 0.197, 0.062, 0.278, 0.095, 0.161, 0.235, 0.104, 0.136, 0.098]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4083 | Steps: 4 | Val loss: 0.3237 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4625 | Steps: 4 | Val loss: 0.3420 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 16:03:16 (running for 00:34:29.69)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.365 |  0.154 |                   87 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.338 |  0.146 |                   75 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.471 |  0.178 |                   65 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.408 |  0.167 |                   66 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3555 | Steps: 4 | Val loss: 0.2806 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=26476)[0m rmse: 0.16745054721832275
[2m[36m(func pid=26476)[0m mae:  0.11510489881038666
[2m[36m(func pid=26476)[0m rmse_per_class: [0.095, 0.281, 0.091, 0.368, 0.057, 0.17, 0.25, 0.127, 0.141, 0.094]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3285 | Steps: 4 | Val loss: 0.2803 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=25864)[0m rmse: 0.1786845177412033
[2m[36m(func pid=25864)[0m mae:  0.12703005969524384
[2m[36m(func pid=25864)[0m rmse_per_class: [0.18, 0.263, 0.096, 0.359, 0.055, 0.185, 0.258, 0.141, 0.158, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15667444467544556
[2m[36m(func pid=19000)[0m mae:  0.11086602509021759
[2m[36m(func pid=19000)[0m rmse_per_class: [0.096, 0.224, 0.046, 0.31, 0.086, 0.171, 0.26, 0.108, 0.139, 0.126]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.15322890877723694
[2m[36m(func pid=23101)[0m mae:  0.09948340058326721
[2m[36m(func pid=23101)[0m rmse_per_class: [0.102, 0.199, 0.087, 0.332, 0.096, 0.159, 0.23, 0.092, 0.131, 0.104]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3840 | Steps: 4 | Val loss: 0.3163 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4605 | Steps: 4 | Val loss: 0.3423 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 16:03:21 (running for 00:34:35.01)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.355 |  0.157 |                   88 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.328 |  0.153 |                   76 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.462 |  0.179 |                   66 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.166 |                   67 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.1662549525499344
[2m[36m(func pid=26476)[0m mae:  0.1145024448633194
[2m[36m(func pid=26476)[0m rmse_per_class: [0.094, 0.281, 0.077, 0.365, 0.061, 0.173, 0.252, 0.128, 0.136, 0.096]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3514 | Steps: 4 | Val loss: 0.2813 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3342 | Steps: 4 | Val loss: 0.2829 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=25864)[0m rmse: 0.1789858341217041
[2m[36m(func pid=25864)[0m mae:  0.1270836889743805
[2m[36m(func pid=25864)[0m rmse_per_class: [0.184, 0.263, 0.095, 0.36, 0.054, 0.185, 0.257, 0.141, 0.158, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4209 | Steps: 4 | Val loss: 0.3147 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=23101)[0m rmse: 0.1560746431350708
[2m[36m(func pid=23101)[0m mae:  0.10586774349212646
[2m[36m(func pid=23101)[0m rmse_per_class: [0.098, 0.199, 0.033, 0.316, 0.132, 0.158, 0.266, 0.105, 0.158, 0.096]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1570587456226349
[2m[36m(func pid=19000)[0m mae:  0.11113343387842178
[2m[36m(func pid=19000)[0m rmse_per_class: [0.097, 0.225, 0.047, 0.312, 0.084, 0.171, 0.257, 0.107, 0.138, 0.132]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4476 | Steps: 4 | Val loss: 0.3377 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 16:03:26 (running for 00:34:40.27)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.351 |  0.157 |                   89 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.334 |  0.156 |                   77 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.461 |  0.179 |                   67 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.421 |  0.167 |                   68 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.16689595580101013
[2m[36m(func pid=26476)[0m mae:  0.1145092025399208
[2m[36m(func pid=26476)[0m rmse_per_class: [0.094, 0.277, 0.08, 0.363, 0.064, 0.177, 0.254, 0.129, 0.134, 0.099]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3130 | Steps: 4 | Val loss: 0.2606 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3602 | Steps: 4 | Val loss: 0.2804 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
[2m[36m(func pid=25864)[0m rmse: 0.17824536561965942
[2m[36m(func pid=25864)[0m mae:  0.1267104148864746
[2m[36m(func pid=25864)[0m rmse_per_class: [0.181, 0.262, 0.094, 0.359, 0.054, 0.185, 0.257, 0.141, 0.158, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3944 | Steps: 4 | Val loss: 0.3103 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=23101)[0m rmse: 0.1431877166032791
[2m[36m(func pid=23101)[0m mae:  0.09501419216394424
[2m[36m(func pid=23101)[0m rmse_per_class: [0.099, 0.2, 0.033, 0.266, 0.104, 0.158, 0.245, 0.09, 0.134, 0.103]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.1559528410434723
[2m[36m(func pid=19000)[0m mae:  0.11046036332845688
[2m[36m(func pid=19000)[0m rmse_per_class: [0.098, 0.223, 0.046, 0.313, 0.081, 0.172, 0.255, 0.108, 0.137, 0.125]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4631 | Steps: 4 | Val loss: 0.3400 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 16:03:31 (running for 00:34:45.59)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.36  |  0.156 |                   90 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.313 |  0.143 |                   78 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.448 |  0.178 |                   68 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.394 |  0.167 |                   69 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.16665488481521606
[2m[36m(func pid=26476)[0m mae:  0.11425572633743286
[2m[36m(func pid=26476)[0m rmse_per_class: [0.094, 0.268, 0.077, 0.358, 0.069, 0.18, 0.256, 0.13, 0.132, 0.102]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3121 | Steps: 4 | Val loss: 0.2672 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3507 | Steps: 4 | Val loss: 0.2814 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=25864)[0m rmse: 0.17919088900089264
[2m[36m(func pid=25864)[0m mae:  0.12708911299705505
[2m[36m(func pid=25864)[0m rmse_per_class: [0.19, 0.262, 0.094, 0.36, 0.054, 0.185, 0.257, 0.141, 0.157, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3977 | Steps: 4 | Val loss: 0.3043 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=23101)[0m rmse: 0.1465444266796112
[2m[36m(func pid=23101)[0m mae:  0.09683892875909805
[2m[36m(func pid=23101)[0m rmse_per_class: [0.121, 0.202, 0.041, 0.285, 0.087, 0.159, 0.249, 0.094, 0.132, 0.097]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15595604479312897
[2m[36m(func pid=19000)[0m mae:  0.11053997278213501
[2m[36m(func pid=19000)[0m rmse_per_class: [0.107, 0.221, 0.045, 0.318, 0.077, 0.173, 0.256, 0.108, 0.138, 0.118]
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 16:03:37 (running for 00:34:50.85)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.351 |  0.156 |                   91 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.312 |  0.147 |                   79 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.463 |  0.179 |                   69 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.398 |  0.165 |                   70 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.16498319804668427
[2m[36m(func pid=26476)[0m mae:  0.11333570629358292
[2m[36m(func pid=26476)[0m rmse_per_class: [0.094, 0.254, 0.069, 0.352, 0.073, 0.185, 0.254, 0.13, 0.132, 0.106]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4370 | Steps: 4 | Val loss: 0.3387 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3244 | Steps: 4 | Val loss: 0.2722 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3747 | Steps: 4 | Val loss: 0.2796 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=25864)[0m rmse: 0.1792553961277008
[2m[36m(func pid=25864)[0m mae:  0.127109557390213
[2m[36m(func pid=25864)[0m rmse_per_class: [0.193, 0.261, 0.094, 0.36, 0.054, 0.185, 0.257, 0.14, 0.156, 0.093]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3859 | Steps: 4 | Val loss: 0.2995 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=23101)[0m rmse: 0.14973051846027374
[2m[36m(func pid=23101)[0m mae:  0.09981707483530045
[2m[36m(func pid=23101)[0m rmse_per_class: [0.105, 0.197, 0.052, 0.305, 0.097, 0.166, 0.245, 0.091, 0.153, 0.087]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15455158054828644
[2m[36m(func pid=19000)[0m mae:  0.10892055928707123
[2m[36m(func pid=19000)[0m rmse_per_class: [0.107, 0.221, 0.041, 0.322, 0.076, 0.171, 0.248, 0.109, 0.137, 0.114]
[2m[36m(func pid=19000)[0m 
== Status ==
Current time: 2024-01-07 16:03:42 (running for 00:34:56.19)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.375 |  0.155 |                   92 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.324 |  0.15  |                   80 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.437 |  0.179 |                   70 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.386 |  0.164 |                   71 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.16422174870967865
[2m[36m(func pid=26476)[0m mae:  0.1131749302148819
[2m[36m(func pid=26476)[0m rmse_per_class: [0.096, 0.243, 0.059, 0.344, 0.08, 0.19, 0.256, 0.13, 0.132, 0.112]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4502 | Steps: 4 | Val loss: 0.3326 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3119 | Steps: 4 | Val loss: 0.2637 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3573 | Steps: 4 | Val loss: 0.2797 | Batch size: 32 | lr: 0.01 | Duration: 3.24s
[2m[36m(func pid=25864)[0m rmse: 0.17789161205291748
[2m[36m(func pid=25864)[0m mae:  0.12651950120925903
[2m[36m(func pid=25864)[0m rmse_per_class: [0.184, 0.26, 0.092, 0.358, 0.054, 0.184, 0.256, 0.14, 0.157, 0.094]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3966 | Steps: 4 | Val loss: 0.2940 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=23101)[0m rmse: 0.1449251025915146
[2m[36m(func pid=23101)[0m mae:  0.09605352580547333
[2m[36m(func pid=23101)[0m rmse_per_class: [0.11, 0.204, 0.031, 0.269, 0.102, 0.158, 0.249, 0.09, 0.134, 0.101]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 16:03:47 (running for 00:35:01.47)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.375 |  0.155 |                   92 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.312 |  0.145 |                   81 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.45  |  0.178 |                   71 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.397 |  0.163 |                   72 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.16319993138313293
[2m[36m(func pid=26476)[0m mae:  0.11280791461467743
[2m[36m(func pid=26476)[0m rmse_per_class: [0.096, 0.235, 0.045, 0.334, 0.09, 0.195, 0.257, 0.129, 0.132, 0.12]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15458577871322632
[2m[36m(func pid=19000)[0m mae:  0.10898236930370331
[2m[36m(func pid=19000)[0m rmse_per_class: [0.111, 0.221, 0.043, 0.322, 0.077, 0.17, 0.248, 0.107, 0.138, 0.109]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4369 | Steps: 4 | Val loss: 0.3337 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3164 | Steps: 4 | Val loss: 0.2807 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=25864)[0m rmse: 0.17870232462882996
[2m[36m(func pid=25864)[0m mae:  0.12692680954933167
[2m[36m(func pid=25864)[0m rmse_per_class: [0.191, 0.26, 0.093, 0.358, 0.054, 0.184, 0.256, 0.14, 0.156, 0.094]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3797 | Steps: 4 | Val loss: 0.2917 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3491 | Steps: 4 | Val loss: 0.2796 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=23101)[0m rmse: 0.1553838551044464
[2m[36m(func pid=23101)[0m mae:  0.10388787090778351
[2m[36m(func pid=23101)[0m rmse_per_class: [0.097, 0.217, 0.037, 0.297, 0.097, 0.168, 0.259, 0.095, 0.137, 0.151]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 16:03:52 (running for 00:35:06.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.357 |  0.155 |                   93 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.316 |  0.155 |                   82 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.437 |  0.179 |                   72 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.38  |  0.163 |                   73 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.16322004795074463
[2m[36m(func pid=26476)[0m mae:  0.11275241523981094
[2m[36m(func pid=26476)[0m rmse_per_class: [0.099, 0.233, 0.041, 0.323, 0.096, 0.192, 0.258, 0.128, 0.132, 0.128]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4353 | Steps: 4 | Val loss: 0.3298 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=19000)[0m rmse: 0.15476760268211365
[2m[36m(func pid=19000)[0m mae:  0.1092432513833046
[2m[36m(func pid=19000)[0m rmse_per_class: [0.107, 0.221, 0.051, 0.319, 0.076, 0.17, 0.25, 0.106, 0.139, 0.108]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3075 | Steps: 4 | Val loss: 0.2723 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3814 | Steps: 4 | Val loss: 0.2909 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=25864)[0m rmse: 0.17780622839927673
[2m[36m(func pid=25864)[0m mae:  0.12659852206707
[2m[36m(func pid=25864)[0m rmse_per_class: [0.185, 0.259, 0.092, 0.357, 0.054, 0.184, 0.257, 0.14, 0.156, 0.094]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3531 | Steps: 4 | Val loss: 0.2793 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=23101)[0m rmse: 0.1470966935157776
[2m[36m(func pid=23101)[0m mae:  0.0958390086889267
[2m[36m(func pid=23101)[0m rmse_per_class: [0.099, 0.208, 0.061, 0.315, 0.086, 0.157, 0.222, 0.092, 0.131, 0.1]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 16:03:58 (running for 00:35:11.94)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1562500037252903
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.349 |  0.155 |                   94 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.308 |  0.147 |                   83 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.178 |                   73 |
| train_10f5e_00017 | RUNNING    | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.381 |  0.164 |                   74 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.16387873888015747
[2m[36m(func pid=26476)[0m mae:  0.11284273862838745
[2m[36m(func pid=26476)[0m rmse_per_class: [0.103, 0.235, 0.04, 0.314, 0.101, 0.189, 0.259, 0.127, 0.133, 0.138]
[2m[36m(func pid=26476)[0m 
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4373 | Steps: 4 | Val loss: 0.3268 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=19000)[0m rmse: 0.15491890907287598
[2m[36m(func pid=19000)[0m mae:  0.10895590484142303
[2m[36m(func pid=19000)[0m rmse_per_class: [0.106, 0.221, 0.059, 0.317, 0.074, 0.17, 0.25, 0.106, 0.14, 0.107]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3083 | Steps: 4 | Val loss: 0.2819 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=26476)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3840 | Steps: 4 | Val loss: 0.2920 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=25864)[0m rmse: 0.1770627200603485
[2m[36m(func pid=25864)[0m mae:  0.12621530890464783
[2m[36m(func pid=25864)[0m rmse_per_class: [0.181, 0.259, 0.09, 0.356, 0.054, 0.184, 0.256, 0.14, 0.156, 0.094]
[2m[36m(func pid=25864)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3706 | Steps: 4 | Val loss: 0.2778 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=23101)[0m rmse: 0.15575125813484192
[2m[36m(func pid=23101)[0m mae:  0.10518336296081543
[2m[36m(func pid=23101)[0m rmse_per_class: [0.101, 0.218, 0.033, 0.307, 0.084, 0.16, 0.247, 0.091, 0.165, 0.152]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 16:04:03 (running for 00:35:17.23)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.15700000524520874
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 3 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.353 |  0.155 |                   95 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.308 |  0.156 |                   84 |
| train_10f5e_00016 | RUNNING    | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.437 |  0.177 |                   74 |
| train_10f5e_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=26476)[0m rmse: 0.16524361073970795
[2m[36m(func pid=26476)[0m mae:  0.11349073797464371
[2m[36m(func pid=26476)[0m rmse_per_class: [0.106, 0.239, 0.04, 0.304, 0.106, 0.184, 0.263, 0.127, 0.133, 0.151]
[2m[36m(func pid=25864)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4436 | Steps: 4 | Val loss: 0.3286 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=19000)[0m rmse: 0.15457238256931305
[2m[36m(func pid=19000)[0m mae:  0.10819564014673233
[2m[36m(func pid=19000)[0m rmse_per_class: [0.1, 0.223, 0.066, 0.311, 0.075, 0.169, 0.25, 0.105, 0.141, 0.105]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3267 | Steps: 4 | Val loss: 0.2757 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=25864)[0m rmse: 0.17796429991722107
[2m[36m(func pid=25864)[0m mae:  0.12662658095359802
[2m[36m(func pid=25864)[0m rmse_per_class: [0.189, 0.259, 0.092, 0.357, 0.054, 0.184, 0.256, 0.14, 0.156, 0.094]
[2m[36m(func pid=23101)[0m rmse: 0.1545945256948471
[2m[36m(func pid=23101)[0m mae:  0.10258545726537704
[2m[36m(func pid=23101)[0m rmse_per_class: [0.098, 0.21, 0.05, 0.279, 0.118, 0.16, 0.266, 0.093, 0.134, 0.138]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3568 | Steps: 4 | Val loss: 0.2742 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=19000)[0m rmse: 0.15234431624412537
[2m[36m(func pid=19000)[0m mae:  0.10686546564102173
[2m[36m(func pid=19000)[0m rmse_per_class: [0.097, 0.222, 0.055, 0.301, 0.077, 0.169, 0.253, 0.105, 0.14, 0.103]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3225 | Steps: 4 | Val loss: 0.2602 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=43619)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43619)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=43619)[0m Configuration completed!
[2m[36m(func pid=43619)[0m New optimizer parameters:
[2m[36m(func pid=43619)[0m SGD (
[2m[36m(func pid=43619)[0m Parameter Group 0
[2m[36m(func pid=43619)[0m     dampening: 0
[2m[36m(func pid=43619)[0m     differentiable: False
[2m[36m(func pid=43619)[0m     foreach: None
[2m[36m(func pid=43619)[0m     lr: 0.01
[2m[36m(func pid=43619)[0m     maximize: False
[2m[36m(func pid=43619)[0m     momentum: 0.99
[2m[36m(func pid=43619)[0m     nesterov: False
[2m[36m(func pid=43619)[0m     weight_decay: 1e-05
[2m[36m(func pid=43619)[0m )
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.14273236691951752
[2m[36m(func pid=23101)[0m mae:  0.0932512953877449
[2m[36m(func pid=23101)[0m rmse_per_class: [0.092, 0.2, 0.042, 0.28, 0.098, 0.159, 0.23, 0.091, 0.129, 0.105]
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3552 | Steps: 4 | Val loss: 0.2725 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 16:04:10 (running for 00:35:24.57)
Memory usage on this node: 20.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.357 |  0.152 |                   97 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.327 |  0.155 |                   85 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=43707)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43707)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=43707)[0m Configuration completed!
[2m[36m(func pid=43707)[0m New optimizer parameters:
[2m[36m(func pid=43707)[0m SGD (
[2m[36m(func pid=43707)[0m Parameter Group 0
[2m[36m(func pid=43707)[0m     dampening: 0
[2m[36m(func pid=43707)[0m     differentiable: False
[2m[36m(func pid=43707)[0m     foreach: None
[2m[36m(func pid=43707)[0m     lr: 0.1
[2m[36m(func pid=43707)[0m     maximize: False
[2m[36m(func pid=43707)[0m     momentum: 0.99
[2m[36m(func pid=43707)[0m     nesterov: False
[2m[36m(func pid=43707)[0m     weight_decay: 1e-05
[2m[36m(func pid=43707)[0m )
[2m[36m(func pid=43707)[0m 
== Status ==
Current time: 2024-01-07 16:04:16 (running for 00:35:30.23)
Memory usage on this node: 24.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.355 |  0.151 |                   98 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.322 |  0.143 |                   86 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19000)[0m rmse: 0.15135732293128967
[2m[36m(func pid=19000)[0m mae:  0.10641493648290634
[2m[36m(func pid=19000)[0m rmse_per_class: [0.098, 0.224, 0.049, 0.294, 0.078, 0.17, 0.253, 0.105, 0.14, 0.103]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8754 | Steps: 4 | Val loss: 0.5838 | Batch size: 32 | lr: 0.01 | Duration: 4.60s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3287 | Steps: 4 | Val loss: 0.2693 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3662 | Steps: 4 | Val loss: 0.2731 | Batch size: 32 | lr: 0.01 | Duration: 3.30s
[2m[36m(func pid=43619)[0m rmse: 0.17998768389225006
[2m[36m(func pid=43619)[0m mae:  0.13246387243270874
[2m[36m(func pid=43619)[0m rmse_per_class: [0.114, 0.264, 0.104, 0.334, 0.101, 0.191, 0.294, 0.141, 0.144, 0.114]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.6734 | Steps: 4 | Val loss: 0.3514 | Batch size: 32 | lr: 0.1 | Duration: 4.70s
[2m[36m(func pid=23101)[0m rmse: 0.14694413542747498
[2m[36m(func pid=23101)[0m mae:  0.09834398329257965
[2m[36m(func pid=23101)[0m rmse_per_class: [0.093, 0.2, 0.035, 0.295, 0.072, 0.164, 0.259, 0.091, 0.166, 0.093]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 16:04:22 (running for 00:35:36.20)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.366 |  0.152 |                   99 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.147 |                   87 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.875 |  0.18  |                    1 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19000)[0m rmse: 0.15153448283672333
[2m[36m(func pid=19000)[0m mae:  0.10672930628061295
[2m[36m(func pid=19000)[0m rmse_per_class: [0.103, 0.223, 0.044, 0.297, 0.078, 0.17, 0.251, 0.105, 0.142, 0.102]
[2m[36m(func pid=19000)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.1789124459028244
[2m[36m(func pid=43707)[0m mae:  0.12992672622203827
[2m[36m(func pid=43707)[0m rmse_per_class: [0.129, 0.266, 0.123, 0.344, 0.076, 0.189, 0.275, 0.14, 0.15, 0.097]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.4954 | Steps: 4 | Val loss: 0.3843 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3357 | Steps: 4 | Val loss: 0.2745 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8216 | Steps: 4 | Val loss: 0.3467 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=19000)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3537 | Steps: 4 | Val loss: 0.2761 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
[2m[36m(func pid=43619)[0m rmse: 0.17824877798557281
[2m[36m(func pid=43619)[0m mae:  0.1307925432920456
[2m[36m(func pid=43619)[0m rmse_per_class: [0.114, 0.265, 0.109, 0.336, 0.093, 0.19, 0.285, 0.137, 0.147, 0.108]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.15170378983020782
[2m[36m(func pid=23101)[0m mae:  0.09903328120708466
[2m[36m(func pid=23101)[0m rmse_per_class: [0.111, 0.199, 0.047, 0.302, 0.088, 0.177, 0.251, 0.096, 0.13, 0.116]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 16:04:28 (running for 00:35:41.76)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00014 | RUNNING    | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.366 |  0.152 |                   99 |
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.336 |  0.152 |                   88 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.495 |  0.178 |                    2 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.822 |  0.185 |                    2 |
| train_10f5e_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.18545736372470856
[2m[36m(func pid=43707)[0m mae:  0.12717388570308685
[2m[36m(func pid=43707)[0m rmse_per_class: [0.272, 0.28, 0.066, 0.336, 0.055, 0.196, 0.262, 0.152, 0.145, 0.092]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=19000)[0m rmse: 0.15299242734909058
[2m[36m(func pid=19000)[0m mae:  0.1081959456205368
[2m[36m(func pid=19000)[0m rmse_per_class: [0.104, 0.22, 0.041, 0.304, 0.083, 0.171, 0.257, 0.105, 0.143, 0.102]
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5199 | Steps: 4 | Val loss: 0.3203 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3142 | Steps: 4 | Val loss: 0.2834 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.9857 | Steps: 4 | Val loss: 0.3458 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=43619)[0m rmse: 0.1753402054309845
[2m[36m(func pid=43619)[0m mae:  0.12744757533073425
[2m[36m(func pid=43619)[0m rmse_per_class: [0.115, 0.267, 0.111, 0.343, 0.074, 0.187, 0.272, 0.136, 0.147, 0.102]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=23101)[0m rmse: 0.15617674589157104
[2m[36m(func pid=23101)[0m mae:  0.10255340486764908
[2m[36m(func pid=23101)[0m rmse_per_class: [0.101, 0.246, 0.053, 0.295, 0.14, 0.159, 0.224, 0.095, 0.158, 0.091]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.17536774277687073
[2m[36m(func pid=43707)[0m mae:  0.11873700469732285
[2m[36m(func pid=43707)[0m rmse_per_class: [0.113, 0.273, 0.048, 0.316, 0.056, 0.194, 0.294, 0.156, 0.208, 0.095]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3391 | Steps: 4 | Val loss: 0.2683 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6431 | Steps: 4 | Val loss: 0.3596 | Batch size: 32 | lr: 0.01 | Duration: 3.27s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.7671 | Steps: 4 | Val loss: 0.4709 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=23101)[0m rmse: 0.1473882496356964
[2m[36m(func pid=23101)[0m mae:  0.09566850960254669
[2m[36m(func pid=23101)[0m rmse_per_class: [0.106, 0.203, 0.029, 0.261, 0.139, 0.167, 0.243, 0.094, 0.131, 0.101]
[2m[36m(func pid=43619)[0m rmse: 0.1736578494310379
[2m[36m(func pid=43619)[0m mae:  0.12281397730112076
[2m[36m(func pid=43619)[0m rmse_per_class: [0.113, 0.266, 0.099, 0.352, 0.059, 0.184, 0.285, 0.14, 0.142, 0.095]
== Status ==
Current time: 2024-01-07 16:04:33 (running for 00:35:47.19)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.314 |  0.156 |                   89 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.52  |  0.175 |                    3 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.986 |  0.175 |                    3 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=45149)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=45149)[0m Configuration completed!
[2m[36m(func pid=45149)[0m New optimizer parameters:
[2m[36m(func pid=45149)[0m SGD (
[2m[36m(func pid=45149)[0m Parameter Group 0
[2m[36m(func pid=45149)[0m     dampening: 0
[2m[36m(func pid=45149)[0m     differentiable: False
[2m[36m(func pid=45149)[0m     foreach: None
[2m[36m(func pid=45149)[0m     lr: 0.0001
[2m[36m(func pid=45149)[0m     maximize: False
[2m[36m(func pid=45149)[0m     momentum: 0.9
[2m[36m(func pid=45149)[0m     nesterov: False
[2m[36m(func pid=45149)[0m     weight_decay: 1e-05
[2m[36m(func pid=45149)[0m )
[2m[36m(func pid=45149)[0m 
== Status ==
Current time: 2024-01-07 16:04:39 (running for 00:35:52.70)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.339 |  0.147 |                   90 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.643 |  0.174 |                    4 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.767 |  0.175 |                    4 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.17455774545669556
[2m[36m(func pid=43707)[0m mae:  0.11927478015422821
[2m[36m(func pid=43707)[0m rmse_per_class: [0.111, 0.326, 0.044, 0.293, 0.056, 0.229, 0.304, 0.156, 0.133, 0.093]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3293 | Steps: 4 | Val loss: 0.2659 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.7442 | Steps: 4 | Val loss: 0.4097 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0428 | Steps: 4 | Val loss: 0.7197 | Batch size: 32 | lr: 0.0001 | Duration: 4.45s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8291 | Steps: 4 | Val loss: 0.4097 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=23101)[0m rmse: 0.1457473188638687
[2m[36m(func pid=23101)[0m mae:  0.09503702819347382
[2m[36m(func pid=23101)[0m rmse_per_class: [0.103, 0.201, 0.05, 0.277, 0.094, 0.156, 0.254, 0.095, 0.129, 0.096]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.1771458089351654
[2m[36m(func pid=43619)[0m mae:  0.11971496045589447
[2m[36m(func pid=43619)[0m rmse_per_class: [0.109, 0.265, 0.075, 0.364, 0.055, 0.182, 0.348, 0.146, 0.137, 0.092]
[2m[36m(func pid=43619)[0m 
== Status ==
Current time: 2024-01-07 16:04:44 (running for 00:35:57.88)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.146 |                   91 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.744 |  0.177 |                    5 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.767 |  0.175 |                    4 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  1.043 |  0.18  |                    1 |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m rmse: 0.18042071163654327
[2m[36m(func pid=45149)[0m mae:  0.13288412988185883
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.264, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.21015706658363342
[2m[36m(func pid=43707)[0m mae:  0.14253024756908417
[2m[36m(func pid=43707)[0m rmse_per_class: [0.111, 0.268, 0.051, 0.349, 0.056, 0.224, 0.287, 0.155, 0.136, 0.464]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3155 | Steps: 4 | Val loss: 0.2743 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8339 | Steps: 4 | Val loss: 0.4477 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.7717 | Steps: 4 | Val loss: 0.4308 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 1.0334 | Steps: 4 | Val loss: 0.7278 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=23101)[0m rmse: 0.14564543962478638
[2m[36m(func pid=23101)[0m mae:  0.0977352112531662
[2m[36m(func pid=23101)[0m rmse_per_class: [0.093, 0.209, 0.034, 0.316, 0.068, 0.162, 0.23, 0.093, 0.156, 0.095]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.18553517758846283
[2m[36m(func pid=43619)[0m mae:  0.1205466017127037
[2m[36m(func pid=43619)[0m rmse_per_class: [0.108, 0.265, 0.06, 0.374, 0.055, 0.182, 0.436, 0.151, 0.134, 0.091]
[2m[36m(func pid=43619)[0m 
== Status ==
Current time: 2024-01-07 16:04:49 (running for 00:36:03.26)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.316 |  0.146 |                   92 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.834 |  0.186 |                    6 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.829 |  0.21  |                    5 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  1.033 |  0.181 |                    2 |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m rmse: 0.18084317445755005
[2m[36m(func pid=45149)[0m mae:  0.1332635134458542
[2m[36m(func pid=45149)[0m rmse_per_class: [0.113, 0.264, 0.1, 0.331, 0.106, 0.192, 0.302, 0.142, 0.143, 0.115]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.19565482437610626
[2m[36m(func pid=43707)[0m mae:  0.13014894723892212
[2m[36m(func pid=43707)[0m rmse_per_class: [0.134, 0.301, 0.049, 0.362, 0.055, 0.22, 0.299, 0.126, 0.149, 0.261]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3216 | Steps: 4 | Val loss: 0.2751 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8351 | Steps: 4 | Val loss: 0.4517 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8321 | Steps: 4 | Val loss: 0.5612 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 1.0160 | Steps: 4 | Val loss: 0.7327 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=23101)[0m rmse: 0.15082521736621857
[2m[36m(func pid=23101)[0m mae:  0.10050477087497711
[2m[36m(func pid=23101)[0m rmse_per_class: [0.133, 0.202, 0.035, 0.288, 0.083, 0.16, 0.261, 0.093, 0.154, 0.102]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.18967774510383606
[2m[36m(func pid=43619)[0m mae:  0.12139897048473358
[2m[36m(func pid=43619)[0m rmse_per_class: [0.106, 0.259, 0.052, 0.38, 0.056, 0.182, 0.482, 0.154, 0.135, 0.092]
[2m[36m(func pid=43619)[0m 
== Status ==
Current time: 2024-01-07 16:04:54 (running for 00:36:08.60)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.322 |  0.151 |                   93 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.835 |  0.19  |                    7 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.832 |  0.207 |                    7 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  1.033 |  0.181 |                    2 |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.20715086162090302
[2m[36m(func pid=43707)[0m mae:  0.14162223041057587
[2m[36m(func pid=43707)[0m rmse_per_class: [0.113, 0.302, 0.049, 0.271, 0.053, 0.231, 0.315, 0.498, 0.143, 0.096]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.18090631067752838
[2m[36m(func pid=45149)[0m mae:  0.13331690430641174
[2m[36m(func pid=45149)[0m rmse_per_class: [0.113, 0.263, 0.1, 0.33, 0.107, 0.192, 0.303, 0.143, 0.142, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3367 | Steps: 4 | Val loss: 0.2690 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7371 | Steps: 4 | Val loss: 0.4219 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8296 | Steps: 4 | Val loss: 0.4398 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.9922 | Steps: 4 | Val loss: 0.7295 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=23101)[0m rmse: 0.14743831753730774
[2m[36m(func pid=23101)[0m mae:  0.09691287577152252
[2m[36m(func pid=23101)[0m rmse_per_class: [0.114, 0.201, 0.049, 0.299, 0.089, 0.161, 0.244, 0.09, 0.132, 0.095]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 16:05:00 (running for 00:36:14.04)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.337 |  0.147 |                   94 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.835 |  0.19  |                    7 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.832 |  0.207 |                    7 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.992 |  0.181 |                    4 |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.2168988734483719
[2m[36m(func pid=43707)[0m mae:  0.1404690444469452
[2m[36m(func pid=43707)[0m rmse_per_class: [0.111, 0.277, 0.093, 0.309, 0.504, 0.221, 0.269, 0.106, 0.181, 0.097]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.18095503747463226
[2m[36m(func pid=45149)[0m mae:  0.13335590064525604
[2m[36m(func pid=45149)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.108, 0.191, 0.305, 0.143, 0.142, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.18595464527606964
[2m[36m(func pid=43619)[0m mae:  0.11961156129837036
[2m[36m(func pid=43619)[0m rmse_per_class: [0.111, 0.249, 0.049, 0.381, 0.056, 0.179, 0.451, 0.155, 0.136, 0.092]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3438 | Steps: 4 | Val loss: 0.2831 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.9686 | Steps: 4 | Val loss: 0.7218 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7138 | Steps: 4 | Val loss: 0.6476 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7244 | Steps: 4 | Val loss: 0.3973 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=23101)[0m rmse: 0.15952907502651215
[2m[36m(func pid=23101)[0m mae:  0.10188750922679901
[2m[36m(func pid=23101)[0m rmse_per_class: [0.09, 0.207, 0.084, 0.304, 0.183, 0.176, 0.229, 0.096, 0.131, 0.095]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 16:05:05 (running for 00:36:19.38)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.344 |  0.16  |                   95 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.737 |  0.186 |                    8 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.83  |  0.217 |                    8 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.969 |  0.181 |                    5 |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m rmse: 0.18091920018196106
[2m[36m(func pid=45149)[0m mae:  0.13332122564315796
[2m[36m(func pid=45149)[0m rmse_per_class: [0.113, 0.263, 0.099, 0.329, 0.108, 0.191, 0.305, 0.144, 0.142, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.19601751863956451
[2m[36m(func pid=43707)[0m mae:  0.13222792744636536
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.345, 0.053, 0.388, 0.212, 0.188, 0.266, 0.155, 0.146, 0.095]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.1796078383922577
[2m[36m(func pid=43619)[0m mae:  0.11731290817260742
[2m[36m(func pid=43619)[0m rmse_per_class: [0.176, 0.243, 0.047, 0.382, 0.056, 0.173, 0.335, 0.155, 0.136, 0.092]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3447 | Steps: 4 | Val loss: 0.2870 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.9450 | Steps: 4 | Val loss: 0.7108 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.9355 | Steps: 4 | Val loss: 0.4334 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.5846 | Steps: 4 | Val loss: 0.3745 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=23101)[0m rmse: 0.16346240043640137
[2m[36m(func pid=23101)[0m mae:  0.10694453865289688
[2m[36m(func pid=23101)[0m rmse_per_class: [0.136, 0.224, 0.033, 0.293, 0.22, 0.162, 0.247, 0.091, 0.138, 0.09]
[2m[36m(func pid=23101)[0m 
== Status ==
Current time: 2024-01-07 16:05:10 (running for 00:36:24.67)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.345 |  0.163 |                   96 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.724 |  0.18  |                    9 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.714 |  0.196 |                    9 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.945 |  0.181 |                    6 |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m rmse: 0.1810506284236908
[2m[36m(func pid=45149)[0m mae:  0.1334279477596283
[2m[36m(func pid=45149)[0m rmse_per_class: [0.113, 0.263, 0.098, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.19288215041160583
[2m[36m(func pid=43707)[0m mae:  0.1252066195011139
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.223, 0.041, 0.374, 0.067, 0.252, 0.256, 0.156, 0.133, 0.316]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.1736641228199005
[2m[36m(func pid=43619)[0m mae:  0.11686438322067261
[2m[36m(func pid=43619)[0m rmse_per_class: [0.192, 0.259, 0.044, 0.378, 0.056, 0.18, 0.246, 0.155, 0.135, 0.091]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3104 | Steps: 4 | Val loss: 0.2600 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.9155 | Steps: 4 | Val loss: 0.6978 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6420 | Steps: 4 | Val loss: 0.4824 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=23101)[0m rmse: 0.14342103898525238
[2m[36m(func pid=23101)[0m mae:  0.09390220791101456
[2m[36m(func pid=23101)[0m rmse_per_class: [0.089, 0.195, 0.091, 0.276, 0.08, 0.159, 0.224, 0.091, 0.138, 0.091]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.5819 | Steps: 4 | Val loss: 0.3766 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 16:05:16 (running for 00:36:30.04)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.31  |  0.143 |                   97 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.585 |  0.174 |                   10 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.936 |  0.193 |                   10 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.916 |  0.181 |                    7 |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m rmse: 0.18106232583522797
[2m[36m(func pid=45149)[0m mae:  0.133435919880867
[2m[36m(func pid=45149)[0m rmse_per_class: [0.113, 0.264, 0.098, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.19690394401550293
[2m[36m(func pid=43707)[0m mae:  0.12099047005176544
[2m[36m(func pid=43707)[0m rmse_per_class: [0.16, 0.298, 0.049, 0.55, 0.053, 0.221, 0.258, 0.154, 0.133, 0.093]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.17679579555988312
[2m[36m(func pid=43619)[0m mae:  0.12113648653030396
[2m[36m(func pid=43619)[0m rmse_per_class: [0.133, 0.291, 0.058, 0.364, 0.056, 0.211, 0.273, 0.155, 0.133, 0.094]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3313 | Steps: 4 | Val loss: 0.2661 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8961 | Steps: 4 | Val loss: 0.6843 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.9102 | Steps: 4 | Val loss: 0.5125 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=23101)[0m rmse: 0.1433705985546112
[2m[36m(func pid=23101)[0m mae:  0.09392019361257553
[2m[36m(func pid=23101)[0m rmse_per_class: [0.104, 0.199, 0.038, 0.308, 0.055, 0.174, 0.231, 0.091, 0.13, 0.104]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.5440 | Steps: 4 | Val loss: 0.3812 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 16:05:21 (running for 00:36:35.58)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.331 |  0.143 |                   98 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.582 |  0.177 |                   11 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.642 |  0.197 |                   11 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.896 |  0.181 |                    8 |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m rmse: 0.1810304820537567
[2m[36m(func pid=45149)[0m mae:  0.1334044188261032
[2m[36m(func pid=45149)[0m rmse_per_class: [0.113, 0.264, 0.097, 0.329, 0.109, 0.19, 0.306, 0.144, 0.142, 0.117]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.19332295656204224
[2m[36m(func pid=43707)[0m mae:  0.12528496980667114
[2m[36m(func pid=43707)[0m rmse_per_class: [0.258, 0.293, 0.049, 0.323, 0.054, 0.229, 0.22, 0.111, 0.299, 0.095]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3260 | Steps: 4 | Val loss: 0.2645 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=43619)[0m rmse: 0.17691223323345184
[2m[36m(func pid=43619)[0m mae:  0.12469057738780975
[2m[36m(func pid=43619)[0m rmse_per_class: [0.096, 0.254, 0.068, 0.325, 0.056, 0.214, 0.312, 0.153, 0.141, 0.151]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8710 | Steps: 4 | Val loss: 0.6696 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8412 | Steps: 4 | Val loss: 0.5779 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=23101)[0m rmse: 0.14369474351406097
[2m[36m(func pid=23101)[0m mae:  0.09712866693735123
[2m[36m(func pid=23101)[0m rmse_per_class: [0.107, 0.196, 0.044, 0.29, 0.062, 0.159, 0.237, 0.092, 0.148, 0.102]
[2m[36m(func pid=23101)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5933 | Steps: 4 | Val loss: 0.4036 | Batch size: 32 | lr: 0.01 | Duration: 3.24s
== Status ==
Current time: 2024-01-07 16:05:27 (running for 00:36:40.80)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00015 | RUNNING    | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.326 |  0.144 |                   99 |
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.544 |  0.177 |                   12 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.91  |  0.193 |                   12 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.871 |  0.181 |                    9 |
| train_10f5e_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m rmse: 0.1809445470571518
[2m[36m(func pid=45149)[0m mae:  0.13334323465824127
[2m[36m(func pid=45149)[0m rmse_per_class: [0.113, 0.264, 0.096, 0.328, 0.108, 0.19, 0.306, 0.145, 0.142, 0.117]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.2217026650905609
[2m[36m(func pid=43707)[0m mae:  0.14570732414722443
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.238, 0.075, 0.378, 0.056, 0.222, 0.289, 0.561, 0.193, 0.094]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=23101)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3289 | Steps: 4 | Val loss: 0.2630 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=43619)[0m rmse: 0.18028132617473602
[2m[36m(func pid=43619)[0m mae:  0.12747004628181458
[2m[36m(func pid=43619)[0m rmse_per_class: [0.107, 0.247, 0.052, 0.296, 0.056, 0.184, 0.328, 0.149, 0.159, 0.225]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8427 | Steps: 4 | Val loss: 0.6546 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7495 | Steps: 4 | Val loss: 0.5909 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=23101)[0m rmse: 0.14410614967346191
[2m[36m(func pid=23101)[0m mae:  0.0957646295428276
[2m[36m(func pid=23101)[0m rmse_per_class: [0.097, 0.193, 0.057, 0.272, 0.073, 0.158, 0.245, 0.113, 0.143, 0.09]
[2m[36m(func pid=45149)[0m rmse: 0.18096670508384705
[2m[36m(func pid=45149)[0m mae:  0.13334469497203827
[2m[36m(func pid=45149)[0m rmse_per_class: [0.113, 0.264, 0.095, 0.328, 0.108, 0.19, 0.306, 0.145, 0.142, 0.118]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.1985742449760437
[2m[36m(func pid=43707)[0m mae:  0.12479972839355469
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.357, 0.114, 0.387, 0.056, 0.288, 0.323, 0.117, 0.133, 0.098]
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.6019 | Steps: 4 | Val loss: 0.3604 | Batch size: 32 | lr: 0.01 | Duration: 3.25s
[2m[36m(func pid=43619)[0m rmse: 0.1789916753768921
[2m[36m(func pid=43619)[0m mae:  0.12491437047719955
[2m[36m(func pid=43619)[0m rmse_per_class: [0.11, 0.266, 0.048, 0.304, 0.056, 0.172, 0.325, 0.129, 0.16, 0.22]
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8245 | Steps: 4 | Val loss: 0.6415 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=45149)[0m rmse: 0.1810067743062973
[2m[36m(func pid=45149)[0m mae:  0.13336046040058136
[2m[36m(func pid=45149)[0m rmse_per_class: [0.113, 0.264, 0.095, 0.329, 0.108, 0.19, 0.305, 0.145, 0.142, 0.118]
== Status ==
Current time: 2024-01-07 16:05:32 (running for 00:36:46.15)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.593 |  0.18  |                   13 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.841 |  0.222 |                   13 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.843 |  0.181 |                   10 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=47987)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=47987)[0m Configuration completed!
[2m[36m(func pid=47987)[0m New optimizer parameters:
[2m[36m(func pid=47987)[0m SGD (
[2m[36m(func pid=47987)[0m Parameter Group 0
[2m[36m(func pid=47987)[0m     dampening: 0
[2m[36m(func pid=47987)[0m     differentiable: False
[2m[36m(func pid=47987)[0m     foreach: None
[2m[36m(func pid=47987)[0m     lr: 0.001
[2m[36m(func pid=47987)[0m     maximize: False
[2m[36m(func pid=47987)[0m     momentum: 0.9
[2m[36m(func pid=47987)[0m     nesterov: False
[2m[36m(func pid=47987)[0m     weight_decay: 1e-05
[2m[36m(func pid=47987)[0m )
[2m[36m(func pid=47987)[0m 
== Status ==
Current time: 2024-01-07 16:05:39 (running for 00:36:53.44)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.593 |  0.18  |                   13 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.841 |  0.222 |                   13 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.824 |  0.181 |                   11 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8036 | Steps: 4 | Val loss: 0.6274 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5090 | Steps: 4 | Val loss: 0.3384 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.9715 | Steps: 4 | Val loss: 0.5314 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 1.0291 | Steps: 4 | Val loss: 0.7066 | Batch size: 32 | lr: 0.001 | Duration: 4.61s
== Status ==
Current time: 2024-01-07 16:05:44 (running for 00:36:58.46)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.602 |  0.179 |                   14 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.75  |  0.199 |                   14 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.824 |  0.181 |                   11 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m rmse: 0.18101638555526733
[2m[36m(func pid=45149)[0m mae:  0.13335391879081726
[2m[36m(func pid=45149)[0m rmse_per_class: [0.113, 0.264, 0.095, 0.329, 0.107, 0.191, 0.305, 0.146, 0.142, 0.118]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.1996917426586151
[2m[36m(func pid=43707)[0m mae:  0.13020746409893036
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.245, 0.098, 0.342, 0.056, 0.176, 0.283, 0.145, 0.16, 0.381]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.17897666990756989
[2m[36m(func pid=43619)[0m mae:  0.12123562395572662
[2m[36m(func pid=43619)[0m rmse_per_class: [0.109, 0.274, 0.073, 0.279, 0.056, 0.177, 0.304, 0.215, 0.16, 0.143]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.18036606907844543
[2m[36m(func pid=47987)[0m mae:  0.1328543722629547
[2m[36m(func pid=47987)[0m rmse_per_class: [0.114, 0.264, 0.101, 0.333, 0.103, 0.191, 0.298, 0.142, 0.143, 0.116]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7857 | Steps: 4 | Val loss: 0.6142 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8360 | Steps: 4 | Val loss: 0.4816 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5148 | Steps: 4 | Val loss: 0.3918 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.9261 | Steps: 4 | Val loss: 0.6745 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=45149)[0m rmse: 0.1809910237789154
[2m[36m(func pid=45149)[0m mae:  0.1333238184452057
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.264, 0.095, 0.329, 0.107, 0.191, 0.305, 0.146, 0.142, 0.119]
[2m[36m(func pid=45149)[0m 
== Status ==
Current time: 2024-01-07 16:05:50 (running for 00:37:04.31)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.509 |  0.179 |                   15 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.971 |  0.2   |                   15 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.786 |  0.181 |                   13 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  1.029 |  0.18  |                    1 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.18692688643932343
[2m[36m(func pid=43707)[0m mae:  0.11818035691976547
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.271, 0.044, 0.401, 0.058, 0.228, 0.261, 0.155, 0.198, 0.142]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.2085699588060379
[2m[36m(func pid=43619)[0m mae:  0.1392526477575302
[2m[36m(func pid=43619)[0m rmse_per_class: [0.106, 0.282, 0.061, 0.335, 0.056, 0.197, 0.284, 0.536, 0.138, 0.091]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.180731400847435
[2m[36m(func pid=47987)[0m mae:  0.13315153121948242
[2m[36m(func pid=47987)[0m rmse_per_class: [0.113, 0.264, 0.102, 0.332, 0.105, 0.191, 0.301, 0.142, 0.143, 0.115]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7623 | Steps: 4 | Val loss: 0.6005 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6734 | Steps: 4 | Val loss: 0.7792 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5754 | Steps: 4 | Val loss: 0.4317 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7874 | Steps: 4 | Val loss: 0.6154 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 16:05:55 (running for 00:37:09.55)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.515 |  0.209 |                   16 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.836 |  0.187 |                   16 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.762 |  0.181 |                   14 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.926 |  0.181 |                    2 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m rmse: 0.18105566501617432
[2m[36m(func pid=45149)[0m mae:  0.13336525857448578
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.264, 0.094, 0.329, 0.106, 0.191, 0.305, 0.146, 0.142, 0.119]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.24996955692768097
[2m[36m(func pid=43707)[0m mae:  0.16454164683818817
[2m[36m(func pid=43707)[0m rmse_per_class: [0.614, 0.301, 0.049, 0.371, 0.264, 0.232, 0.278, 0.156, 0.141, 0.095]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.21680757403373718
[2m[36m(func pid=43619)[0m mae:  0.14351089298725128
[2m[36m(func pid=43619)[0m rmse_per_class: [0.104, 0.284, 0.044, 0.373, 0.056, 0.21, 0.295, 0.575, 0.135, 0.092]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.18064475059509277
[2m[36m(func pid=47987)[0m mae:  0.13307379186153412
[2m[36m(func pid=47987)[0m rmse_per_class: [0.114, 0.264, 0.103, 0.331, 0.106, 0.191, 0.301, 0.141, 0.143, 0.114]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7496 | Steps: 4 | Val loss: 0.5896 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.0736 | Steps: 4 | Val loss: 0.6904 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6138 | Steps: 4 | Val loss: 0.4226 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6687 | Steps: 4 | Val loss: 0.5467 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 16:06:01 (running for 00:37:14.90)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.575 |  0.217 |                   17 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.673 |  0.25  |                   17 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.75  |  0.181 |                   15 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.787 |  0.181 |                    3 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m rmse: 0.18101489543914795
[2m[36m(func pid=45149)[0m mae:  0.1333405077457428
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.264, 0.095, 0.329, 0.106, 0.191, 0.304, 0.146, 0.142, 0.119]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.24042706191539764
[2m[36m(func pid=43707)[0m mae:  0.15209032595157623
[2m[36m(func pid=43707)[0m rmse_per_class: [0.14, 0.3, 0.049, 0.374, 0.542, 0.229, 0.265, 0.153, 0.258, 0.095]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.20795300602912903
[2m[36m(func pid=43619)[0m mae:  0.13397422432899475
[2m[36m(func pid=43619)[0m rmse_per_class: [0.099, 0.276, 0.048, 0.381, 0.056, 0.213, 0.356, 0.421, 0.136, 0.093]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.18027323484420776
[2m[36m(func pid=47987)[0m mae:  0.13274075090885162
[2m[36m(func pid=47987)[0m rmse_per_class: [0.114, 0.264, 0.104, 0.331, 0.105, 0.19, 0.3, 0.14, 0.143, 0.112]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7345 | Steps: 4 | Val loss: 0.5771 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8516 | Steps: 4 | Val loss: 0.4826 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5832 | Steps: 4 | Val loss: 0.4877 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6069 | Steps: 4 | Val loss: 0.4063 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 16:06:06 (running for 00:37:20.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.614 |  0.208 |                   18 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  1.074 |  0.24  |                   18 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.734 |  0.181 |                   16 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.669 |  0.18  |                    4 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m rmse: 0.18099479377269745
[2m[36m(func pid=45149)[0m mae:  0.13331659138202667
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.264, 0.095, 0.329, 0.105, 0.191, 0.304, 0.146, 0.142, 0.119]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.20242071151733398
[2m[36m(func pid=43707)[0m mae:  0.12316789478063583
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.266, 0.043, 0.34, 0.299, 0.165, 0.234, 0.102, 0.373, 0.091]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.18012003600597382
[2m[36m(func pid=47987)[0m mae:  0.13256800174713135
[2m[36m(func pid=47987)[0m rmse_per_class: [0.115, 0.264, 0.106, 0.331, 0.105, 0.189, 0.299, 0.139, 0.143, 0.11]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.18979862332344055
[2m[36m(func pid=43619)[0m mae:  0.12054508924484253
[2m[36m(func pid=43619)[0m rmse_per_class: [0.135, 0.243, 0.048, 0.383, 0.056, 0.205, 0.361, 0.239, 0.134, 0.094]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7271 | Steps: 4 | Val loss: 0.5701 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6982 | Steps: 4 | Val loss: 0.5460 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5290 | Steps: 4 | Val loss: 0.4396 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5623 | Steps: 4 | Val loss: 0.3888 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 16:06:12 (running for 00:37:25.91)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.607 |  0.19  |                   19 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.852 |  0.202 |                   19 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.727 |  0.181 |                   17 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.583 |  0.18  |                    5 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.1757253110408783
[2m[36m(func pid=43707)[0m mae:  0.11031492054462433
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.24, 0.042, 0.287, 0.053, 0.289, 0.323, 0.18, 0.131, 0.1]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.1808372437953949
[2m[36m(func pid=45149)[0m mae:  0.13321039080619812
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.264, 0.095, 0.329, 0.106, 0.191, 0.304, 0.145, 0.143, 0.118]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17959293723106384
[2m[36m(func pid=47987)[0m mae:  0.13209691643714905
[2m[36m(func pid=47987)[0m rmse_per_class: [0.115, 0.263, 0.107, 0.333, 0.102, 0.188, 0.296, 0.139, 0.144, 0.108]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.17047062516212463
[2m[36m(func pid=43619)[0m mae:  0.10990859568119049
[2m[36m(func pid=43619)[0m rmse_per_class: [0.227, 0.221, 0.044, 0.383, 0.056, 0.185, 0.231, 0.129, 0.136, 0.093]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.9370 | Steps: 4 | Val loss: 0.4100 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.7128 | Steps: 4 | Val loss: 0.5645 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4956 | Steps: 4 | Val loss: 0.4053 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5412 | Steps: 4 | Val loss: 0.3918 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 16:06:17 (running for 00:37:31.05)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.562 |  0.17  |                   20 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.937 |  0.15  |                   21 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.727 |  0.181 |                   17 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.529 |  0.18  |                    6 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.15037274360656738
[2m[36m(func pid=43707)[0m mae:  0.08714790642261505
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.229, 0.101, 0.273, 0.056, 0.161, 0.226, 0.105, 0.133, 0.108]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.18083307147026062
[2m[36m(func pid=45149)[0m mae:  0.13322217762470245
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.265, 0.095, 0.329, 0.107, 0.191, 0.303, 0.144, 0.143, 0.117]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17908956110477448
[2m[36m(func pid=47987)[0m mae:  0.1316920667886734
[2m[36m(func pid=47987)[0m rmse_per_class: [0.115, 0.263, 0.107, 0.334, 0.099, 0.188, 0.294, 0.138, 0.145, 0.107]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.1786818504333496
[2m[36m(func pid=43619)[0m mae:  0.1181894987821579
[2m[36m(func pid=43619)[0m rmse_per_class: [0.152, 0.369, 0.041, 0.384, 0.056, 0.169, 0.252, 0.122, 0.15, 0.092]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.7130 | Steps: 4 | Val loss: 0.4920 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6911 | Steps: 4 | Val loss: 0.5539 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4726 | Steps: 4 | Val loss: 0.3803 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=43707)[0m rmse: 0.1853894740343094
[2m[36m(func pid=43707)[0m mae:  0.11533983796834946
[2m[36m(func pid=43707)[0m rmse_per_class: [0.104, 0.238, 0.035, 0.307, 0.056, 0.219, 0.383, 0.15, 0.26, 0.1]
[2m[36m(func pid=43707)[0m 
== Status ==
Current time: 2024-01-07 16:06:22 (running for 00:37:36.42)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.541 |  0.179 |                   21 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.713 |  0.185 |                   22 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.713 |  0.181 |                   18 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.496 |  0.179 |                    7 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5215 | Steps: 4 | Val loss: 0.4029 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=45149)[0m rmse: 0.180809885263443
[2m[36m(func pid=45149)[0m mae:  0.1331816166639328
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.265, 0.095, 0.329, 0.106, 0.191, 0.303, 0.144, 0.143, 0.118]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17852655053138733
[2m[36m(func pid=47987)[0m mae:  0.13126663863658905
[2m[36m(func pid=47987)[0m rmse_per_class: [0.116, 0.264, 0.106, 0.334, 0.096, 0.187, 0.292, 0.138, 0.146, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.18834617733955383
[2m[36m(func pid=43619)[0m mae:  0.12837707996368408
[2m[36m(func pid=43619)[0m rmse_per_class: [0.093, 0.403, 0.048, 0.384, 0.056, 0.167, 0.3, 0.141, 0.201, 0.09]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.7958 | Steps: 4 | Val loss: 0.7110 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6822 | Steps: 4 | Val loss: 0.5445 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4609 | Steps: 4 | Val loss: 0.3657 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 16:06:28 (running for 00:37:41.85)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.522 |  0.188 |                   22 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.796 |  0.223 |                   23 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.691 |  0.181 |                   19 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.473 |  0.179 |                    8 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.2232544869184494
[2m[36m(func pid=43707)[0m mae:  0.13818159699440002
[2m[36m(func pid=43707)[0m rmse_per_class: [0.53, 0.259, 0.049, 0.364, 0.056, 0.216, 0.299, 0.154, 0.172, 0.133]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.1807679384946823
[2m[36m(func pid=45149)[0m mae:  0.1331479698419571
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.265, 0.096, 0.329, 0.105, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5264 | Steps: 4 | Val loss: 0.3975 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
[2m[36m(func pid=47987)[0m rmse: 0.1781729757785797
[2m[36m(func pid=47987)[0m mae:  0.1309460997581482
[2m[36m(func pid=47987)[0m rmse_per_class: [0.116, 0.265, 0.107, 0.334, 0.093, 0.187, 0.29, 0.137, 0.146, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.0474 | Steps: 4 | Val loss: 0.5061 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=43619)[0m rmse: 0.19174018502235413
[2m[36m(func pid=43619)[0m mae:  0.13158726692199707
[2m[36m(func pid=43619)[0m rmse_per_class: [0.101, 0.271, 0.062, 0.379, 0.056, 0.198, 0.317, 0.149, 0.292, 0.093]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6735 | Steps: 4 | Val loss: 0.5331 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4530 | Steps: 4 | Val loss: 0.3552 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 16:06:33 (running for 00:37:47.14)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.526 |  0.192 |                   23 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  1.047 |  0.197 |                   24 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.682 |  0.181 |                   20 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.461 |  0.178 |                    9 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.19733735918998718
[2m[36m(func pid=43707)[0m mae:  0.12218089401721954
[2m[36m(func pid=43707)[0m rmse_per_class: [0.159, 0.251, 0.049, 0.311, 0.056, 0.174, 0.237, 0.155, 0.375, 0.207]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.18065378069877625
[2m[36m(func pid=45149)[0m mae:  0.13304316997528076
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.264, 0.096, 0.329, 0.105, 0.191, 0.303, 0.145, 0.143, 0.118]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5077 | Steps: 4 | Val loss: 0.3727 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=47987)[0m rmse: 0.1777123510837555
[2m[36m(func pid=47987)[0m mae:  0.13055941462516785
[2m[36m(func pid=47987)[0m rmse_per_class: [0.117, 0.265, 0.106, 0.334, 0.091, 0.188, 0.288, 0.137, 0.147, 0.105]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.6759 | Steps: 4 | Val loss: 0.5574 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=43619)[0m rmse: 0.19587913155555725
[2m[36m(func pid=43619)[0m mae:  0.13380113244056702
[2m[36m(func pid=43619)[0m rmse_per_class: [0.103, 0.219, 0.078, 0.362, 0.055, 0.272, 0.314, 0.151, 0.261, 0.144]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6597 | Steps: 4 | Val loss: 0.5312 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4494 | Steps: 4 | Val loss: 0.3479 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 16:06:38 (running for 00:37:52.51)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.508 |  0.196 |                   24 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.676 |  0.179 |                   25 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.673 |  0.181 |                   21 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.453 |  0.178 |                   10 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.17937880754470825
[2m[36m(func pid=43707)[0m mae:  0.11417074501514435
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.24, 0.036, 0.279, 0.056, 0.29, 0.265, 0.149, 0.132, 0.233]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.18062081933021545
[2m[36m(func pid=45149)[0m mae:  0.13303714990615845
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.265, 0.096, 0.329, 0.106, 0.19, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17737877368927002
[2m[36m(func pid=47987)[0m mae:  0.13027043640613556
[2m[36m(func pid=47987)[0m rmse_per_class: [0.118, 0.265, 0.106, 0.335, 0.088, 0.188, 0.286, 0.137, 0.147, 0.105]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4780 | Steps: 4 | Val loss: 0.3558 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.0112 | Steps: 4 | Val loss: 0.5086 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.6603 | Steps: 4 | Val loss: 0.5253 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=43619)[0m rmse: 0.19533520936965942
[2m[36m(func pid=43619)[0m mae:  0.13207755982875824
[2m[36m(func pid=43619)[0m rmse_per_class: [0.103, 0.258, 0.072, 0.317, 0.053, 0.262, 0.299, 0.152, 0.147, 0.291]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4430 | Steps: 4 | Val loss: 0.3425 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 16:06:44 (running for 00:37:57.77)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.478 |  0.195 |                   25 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  1.011 |  0.19  |                   26 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.66  |  0.181 |                   22 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.449 |  0.177 |                   11 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.1896505057811737
[2m[36m(func pid=43707)[0m mae:  0.10781308263540268
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.252, 0.199, 0.294, 0.056, 0.333, 0.274, 0.115, 0.138, 0.124]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.18055474758148193
[2m[36m(func pid=45149)[0m mae:  0.13298854231834412
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.265, 0.096, 0.329, 0.106, 0.191, 0.302, 0.143, 0.143, 0.117]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.177119180560112
[2m[36m(func pid=47987)[0m mae:  0.13003531098365784
[2m[36m(func pid=47987)[0m rmse_per_class: [0.119, 0.265, 0.106, 0.335, 0.085, 0.188, 0.285, 0.137, 0.147, 0.105]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4858 | Steps: 4 | Val loss: 0.3353 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.0858 | Steps: 4 | Val loss: 0.6161 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.6449 | Steps: 4 | Val loss: 0.5164 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4452 | Steps: 4 | Val loss: 0.3380 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=43619)[0m rmse: 0.17800816893577576
[2m[36m(func pid=43619)[0m mae:  0.11939147859811783
[2m[36m(func pid=43619)[0m rmse_per_class: [0.101, 0.281, 0.04, 0.269, 0.054, 0.197, 0.264, 0.15, 0.132, 0.292]
[2m[36m(func pid=43619)[0m 
== Status ==
Current time: 2024-01-07 16:06:49 (running for 00:38:03.18)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.486 |  0.178 |                   26 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  1.086 |  0.202 |                   27 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.66  |  0.181 |                   23 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.443 |  0.177 |                   12 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.20175376534461975
[2m[36m(func pid=43707)[0m mae:  0.11439383029937744
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.399, 0.137, 0.386, 0.058, 0.17, 0.318, 0.21, 0.137, 0.092]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.18049640953540802
[2m[36m(func pid=45149)[0m mae:  0.1329445242881775
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.265, 0.096, 0.329, 0.105, 0.191, 0.302, 0.144, 0.143, 0.117]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17678113281726837
[2m[36m(func pid=47987)[0m mae:  0.12974882125854492
[2m[36m(func pid=47987)[0m rmse_per_class: [0.121, 0.264, 0.105, 0.335, 0.082, 0.188, 0.284, 0.136, 0.147, 0.104]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4470 | Steps: 4 | Val loss: 0.3116 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.9682 | Steps: 4 | Val loss: 0.6106 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.6382 | Steps: 4 | Val loss: 0.5095 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4392 | Steps: 4 | Val loss: 0.3355 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=43619)[0m rmse: 0.16021683812141418
[2m[36m(func pid=43619)[0m mae:  0.10467363893985748
[2m[36m(func pid=43619)[0m rmse_per_class: [0.095, 0.285, 0.039, 0.286, 0.079, 0.169, 0.227, 0.145, 0.134, 0.143]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.21002788841724396
[2m[36m(func pid=43707)[0m mae:  0.12571223080158234
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.273, 0.049, 0.388, 0.261, 0.17, 0.32, 0.216, 0.222, 0.09]
[2m[36m(func pid=43707)[0m 
== Status ==
Current time: 2024-01-07 16:06:54 (running for 00:38:08.43)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.447 |  0.16  |                   27 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.968 |  0.21  |                   28 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.645 |  0.18  |                   24 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.445 |  0.177 |                   13 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m rmse: 0.18047864735126495
[2m[36m(func pid=45149)[0m mae:  0.13292182981967926
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.265, 0.097, 0.33, 0.104, 0.19, 0.301, 0.144, 0.143, 0.117]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17652031779289246
[2m[36m(func pid=47987)[0m mae:  0.12949883937835693
[2m[36m(func pid=47987)[0m rmse_per_class: [0.122, 0.264, 0.105, 0.335, 0.081, 0.188, 0.283, 0.136, 0.147, 0.104]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4931 | Steps: 4 | Val loss: 0.3086 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.8409 | Steps: 4 | Val loss: 0.5733 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.6278 | Steps: 4 | Val loss: 0.5016 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4420 | Steps: 4 | Val loss: 0.3337 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=43619)[0m rmse: 0.16512320935726166
[2m[36m(func pid=43619)[0m mae:  0.1031985655426979
[2m[36m(func pid=43619)[0m rmse_per_class: [0.137, 0.283, 0.047, 0.27, 0.159, 0.18, 0.223, 0.13, 0.134, 0.089]
[2m[36m(func pid=43619)[0m 
== Status ==
Current time: 2024-01-07 16:06:59 (running for 00:38:13.57)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.493 |  0.165 |                   28 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.841 |  0.209 |                   29 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.638 |  0.18  |                   25 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.439 |  0.177 |                   14 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.20927512645721436
[2m[36m(func pid=43707)[0m mae:  0.12970420718193054
[2m[36m(func pid=43707)[0m rmse_per_class: [0.326, 0.235, 0.049, 0.377, 0.301, 0.171, 0.264, 0.112, 0.167, 0.091]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.1804000288248062
[2m[36m(func pid=45149)[0m mae:  0.13285070657730103
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.33, 0.103, 0.19, 0.301, 0.144, 0.143, 0.117]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17626886069774628
[2m[36m(func pid=47987)[0m mae:  0.12923812866210938
[2m[36m(func pid=47987)[0m rmse_per_class: [0.124, 0.264, 0.104, 0.336, 0.079, 0.188, 0.282, 0.136, 0.147, 0.103]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4504 | Steps: 4 | Val loss: 0.3251 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.8172 | Steps: 4 | Val loss: 0.4631 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.6183 | Steps: 4 | Val loss: 0.4931 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4417 | Steps: 4 | Val loss: 0.3326 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=43619)[0m rmse: 0.185016468167305
[2m[36m(func pid=43619)[0m mae:  0.1147172674536705
[2m[36m(func pid=43619)[0m rmse_per_class: [0.239, 0.267, 0.047, 0.287, 0.266, 0.194, 0.222, 0.106, 0.132, 0.09]
[2m[36m(func pid=43619)[0m 
== Status ==
Current time: 2024-01-07 16:07:05 (running for 00:38:18.76)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.45  |  0.185 |                   29 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.841 |  0.209 |                   29 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.628 |  0.18  |                   26 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.442 |  0.176 |                   15 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.19208011031150818
[2m[36m(func pid=43707)[0m mae:  0.11186635494232178
[2m[36m(func pid=43707)[0m rmse_per_class: [0.23, 0.273, 0.049, 0.283, 0.114, 0.178, 0.24, 0.13, 0.139, 0.285]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.18039968609809875
[2m[36m(func pid=45149)[0m mae:  0.13284540176391602
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.264, 0.097, 0.33, 0.103, 0.19, 0.301, 0.144, 0.143, 0.117]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17581604421138763
[2m[36m(func pid=47987)[0m mae:  0.12894044816493988
[2m[36m(func pid=47987)[0m rmse_per_class: [0.122, 0.264, 0.101, 0.335, 0.08, 0.188, 0.282, 0.136, 0.147, 0.104]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.7606 | Steps: 4 | Val loss: 0.4895 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5121 | Steps: 4 | Val loss: 0.3526 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6166 | Steps: 4 | Val loss: 0.4888 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4390 | Steps: 4 | Val loss: 0.3311 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 16:07:10 (running for 00:38:24.18)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.45  |  0.185 |                   29 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.761 |  0.185 |                   31 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.618 |  0.18  |                   27 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.442 |  0.176 |                   16 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.18545255064964294
[2m[36m(func pid=43707)[0m mae:  0.1100519672036171
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.256, 0.046, 0.416, 0.051, 0.189, 0.247, 0.123, 0.14, 0.275]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.19718262553215027
[2m[36m(func pid=43619)[0m mae:  0.12541215121746063
[2m[36m(func pid=43619)[0m rmse_per_class: [0.232, 0.234, 0.047, 0.341, 0.329, 0.2, 0.248, 0.113, 0.135, 0.093]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.1802976429462433
[2m[36m(func pid=45149)[0m mae:  0.13277050852775574
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.264, 0.097, 0.33, 0.102, 0.19, 0.301, 0.144, 0.143, 0.117]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17559058964252472
[2m[36m(func pid=47987)[0m mae:  0.12877808511257172
[2m[36m(func pid=47987)[0m rmse_per_class: [0.122, 0.264, 0.1, 0.335, 0.079, 0.188, 0.281, 0.136, 0.146, 0.104]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.8846 | Steps: 4 | Val loss: 0.6154 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4298 | Steps: 4 | Val loss: 0.3910 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.6009 | Steps: 4 | Val loss: 0.4818 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4344 | Steps: 4 | Val loss: 0.3296 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 16:07:15 (running for 00:38:29.58)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.512 |  0.197 |                   30 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.885 |  0.19  |                   32 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.617 |  0.18  |                   28 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.439 |  0.176 |                   17 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.1900709569454193
[2m[36m(func pid=43707)[0m mae:  0.11250492185354233
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.284, 0.219, 0.384, 0.056, 0.206, 0.28, 0.124, 0.137, 0.099]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.19833309948444366
[2m[36m(func pid=43619)[0m mae:  0.13244672119617462
[2m[36m(func pid=43619)[0m rmse_per_class: [0.107, 0.23, 0.036, 0.373, 0.291, 0.205, 0.294, 0.158, 0.195, 0.094]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.1802646815776825
[2m[36m(func pid=45149)[0m mae:  0.13274511694908142
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.265, 0.097, 0.33, 0.102, 0.19, 0.301, 0.144, 0.143, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.1752876341342926
[2m[36m(func pid=47987)[0m mae:  0.12856413424015045
[2m[36m(func pid=47987)[0m rmse_per_class: [0.122, 0.264, 0.099, 0.335, 0.078, 0.187, 0.281, 0.136, 0.146, 0.104]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.1870 | Steps: 4 | Val loss: 0.7360 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4544 | Steps: 4 | Val loss: 0.4285 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5969 | Steps: 4 | Val loss: 0.4768 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4356 | Steps: 4 | Val loss: 0.3287 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 16:07:21 (running for 00:38:34.85)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.43  |  0.198 |                   31 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  1.187 |  0.189 |                   33 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.601 |  0.18  |                   29 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.434 |  0.175 |                   18 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.188649520277977
[2m[36m(func pid=43707)[0m mae:  0.12107561528682709
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.288, 0.029, 0.389, 0.056, 0.199, 0.283, 0.122, 0.312, 0.097]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.20480969548225403
[2m[36m(func pid=43619)[0m mae:  0.13712385296821594
[2m[36m(func pid=43619)[0m rmse_per_class: [0.099, 0.255, 0.042, 0.382, 0.221, 0.204, 0.315, 0.19, 0.246, 0.095]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.18022796511650085
[2m[36m(func pid=45149)[0m mae:  0.13272783160209656
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.264, 0.097, 0.33, 0.102, 0.19, 0.3, 0.143, 0.143, 0.117]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.1751202791929245
[2m[36m(func pid=47987)[0m mae:  0.12836207449436188
[2m[36m(func pid=47987)[0m rmse_per_class: [0.123, 0.263, 0.1, 0.335, 0.076, 0.188, 0.28, 0.136, 0.146, 0.104]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.9639 | Steps: 4 | Val loss: 0.7718 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5982 | Steps: 4 | Val loss: 0.4754 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4907 | Steps: 4 | Val loss: 0.4296 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4278 | Steps: 4 | Val loss: 0.3273 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 16:07:26 (running for 00:38:40.28)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.454 |  0.205 |                   32 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.964 |  0.21  |                   34 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.597 |  0.18  |                   30 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.436 |  0.175 |                   19 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.21014270186424255
[2m[36m(func pid=43707)[0m mae:  0.14116352796554565
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.241, 0.044, 0.389, 0.056, 0.17, 0.291, 0.12, 0.58, 0.098]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.1801179200410843
[2m[36m(func pid=45149)[0m mae:  0.13265734910964966
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.265, 0.097, 0.33, 0.102, 0.19, 0.3, 0.143, 0.144, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.2029447853565216
[2m[36m(func pid=43619)[0m mae:  0.1343763768672943
[2m[36m(func pid=43619)[0m rmse_per_class: [0.104, 0.286, 0.071, 0.383, 0.158, 0.195, 0.319, 0.197, 0.223, 0.094]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17473024129867554
[2m[36m(func pid=47987)[0m mae:  0.1280604898929596
[2m[36m(func pid=47987)[0m rmse_per_class: [0.122, 0.263, 0.099, 0.334, 0.076, 0.187, 0.28, 0.136, 0.145, 0.105]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8515 | Steps: 4 | Val loss: 0.7344 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5845 | Steps: 4 | Val loss: 0.4681 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5019 | Steps: 4 | Val loss: 0.3846 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4327 | Steps: 4 | Val loss: 0.3268 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 16:07:31 (running for 00:38:45.65)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.491 |  0.203 |                   33 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.852 |  0.197 |                   35 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.598 |  0.18  |                   31 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.428 |  0.175 |                   20 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.19687840342521667
[2m[36m(func pid=43707)[0m mae:  0.11813284456729889
[2m[36m(func pid=43707)[0m rmse_per_class: [0.258, 0.255, 0.034, 0.389, 0.056, 0.175, 0.405, 0.11, 0.138, 0.148]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.18003730475902557
[2m[36m(func pid=45149)[0m mae:  0.13259239494800568
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.264, 0.097, 0.33, 0.102, 0.19, 0.3, 0.143, 0.144, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17454175651073456
[2m[36m(func pid=47987)[0m mae:  0.12782706320285797
[2m[36m(func pid=47987)[0m rmse_per_class: [0.122, 0.263, 0.1, 0.334, 0.074, 0.187, 0.279, 0.136, 0.145, 0.105]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.19359776377677917
[2m[36m(func pid=43619)[0m mae:  0.12366695702075958
[2m[36m(func pid=43619)[0m rmse_per_class: [0.102, 0.271, 0.143, 0.381, 0.112, 0.17, 0.3, 0.183, 0.182, 0.091]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.0297 | Steps: 4 | Val loss: 0.7590 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.5844 | Steps: 4 | Val loss: 0.4675 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4319 | Steps: 4 | Val loss: 0.3267 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4560 | Steps: 4 | Val loss: 0.3313 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 16:07:37 (running for 00:38:51.10)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.502 |  0.194 |                   34 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  1.03  |  0.221 |                   36 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.584 |  0.18  |                   32 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.433 |  0.175 |                   21 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.22138555347919464
[2m[36m(func pid=43707)[0m mae:  0.13712361454963684
[2m[36m(func pid=43707)[0m rmse_per_class: [0.346, 0.244, 0.085, 0.387, 0.055, 0.235, 0.313, 0.138, 0.14, 0.27]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.1799803525209427
[2m[36m(func pid=45149)[0m mae:  0.13255909085273743
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.265, 0.097, 0.33, 0.102, 0.19, 0.3, 0.142, 0.144, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17440252006053925
[2m[36m(func pid=47987)[0m mae:  0.12773644924163818
[2m[36m(func pid=47987)[0m rmse_per_class: [0.122, 0.262, 0.099, 0.334, 0.074, 0.187, 0.279, 0.136, 0.145, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.17314179241657257
[2m[36m(func pid=43619)[0m mae:  0.10950605571269989
[2m[36m(func pid=43619)[0m rmse_per_class: [0.098, 0.215, 0.173, 0.369, 0.076, 0.187, 0.255, 0.125, 0.145, 0.087]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7432 | Steps: 4 | Val loss: 0.5073 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.5764 | Steps: 4 | Val loss: 0.4610 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4341 | Steps: 4 | Val loss: 0.3271 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4272 | Steps: 4 | Val loss: 0.3076 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 16:07:42 (running for 00:38:56.50)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.456 |  0.173 |                   35 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.743 |  0.194 |                   37 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.584 |  0.18  |                   33 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.432 |  0.174 |                   22 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.19353732466697693
[2m[36m(func pid=43707)[0m mae:  0.11220189183950424
[2m[36m(func pid=43707)[0m rmse_per_class: [0.111, 0.234, 0.202, 0.394, 0.05, 0.157, 0.34, 0.193, 0.139, 0.116]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17996007204055786
[2m[36m(func pid=45149)[0m mae:  0.13255541026592255
[2m[36m(func pid=45149)[0m rmse_per_class: [0.114, 0.265, 0.097, 0.33, 0.101, 0.19, 0.3, 0.142, 0.144, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.1744910478591919
[2m[36m(func pid=47987)[0m mae:  0.12778455018997192
[2m[36m(func pid=47987)[0m rmse_per_class: [0.124, 0.262, 0.1, 0.335, 0.072, 0.187, 0.279, 0.136, 0.145, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.1661430299282074
[2m[36m(func pid=43619)[0m mae:  0.10675905644893646
[2m[36m(func pid=43619)[0m rmse_per_class: [0.132, 0.214, 0.132, 0.351, 0.053, 0.22, 0.23, 0.098, 0.133, 0.098]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.8441 | Steps: 4 | Val loss: 0.6174 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.5774 | Steps: 4 | Val loss: 0.4601 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4345 | Steps: 4 | Val loss: 0.3262 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3747 | Steps: 4 | Val loss: 0.3049 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 16:07:48 (running for 00:39:01.79)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.427 |  0.166 |                   36 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.844 |  0.174 |                   38 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.576 |  0.18  |                   34 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.434 |  0.174 |                   23 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.1741948276758194
[2m[36m(func pid=43707)[0m mae:  0.10937833786010742
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.268, 0.036, 0.424, 0.081, 0.195, 0.258, 0.107, 0.168, 0.092]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17998650670051575
[2m[36m(func pid=45149)[0m mae:  0.13258816301822662
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.265, 0.097, 0.33, 0.101, 0.19, 0.3, 0.142, 0.144, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17451262474060059
[2m[36m(func pid=47987)[0m mae:  0.12772835791110992
[2m[36m(func pid=47987)[0m rmse_per_class: [0.126, 0.261, 0.099, 0.336, 0.071, 0.187, 0.278, 0.135, 0.145, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.16745908558368683
[2m[36m(func pid=43619)[0m mae:  0.11047442257404327
[2m[36m(func pid=43619)[0m rmse_per_class: [0.189, 0.247, 0.043, 0.326, 0.052, 0.199, 0.225, 0.122, 0.132, 0.139]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.0082 | Steps: 4 | Val loss: 0.6357 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.5693 | Steps: 4 | Val loss: 0.4556 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4278 | Steps: 4 | Val loss: 0.3250 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 16:07:53 (running for 00:39:06.99)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.375 |  0.167 |                   37 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  1.008 |  0.205 |                   39 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.577 |  0.18  |                   35 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.435 |  0.175 |                   24 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.20542237162590027
[2m[36m(func pid=43707)[0m mae:  0.11956218630075455
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.28, 0.049, 0.388, 0.254, 0.173, 0.341, 0.133, 0.234, 0.09]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4184 | Steps: 4 | Val loss: 0.3222 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=45149)[0m rmse: 0.17997051775455475
[2m[36m(func pid=45149)[0m mae:  0.13256432116031647
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.33, 0.101, 0.19, 0.299, 0.142, 0.144, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17412206530570984
[2m[36m(func pid=47987)[0m mae:  0.12748047709465027
[2m[36m(func pid=47987)[0m rmse_per_class: [0.124, 0.262, 0.097, 0.336, 0.071, 0.187, 0.277, 0.136, 0.145, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.8950 | Steps: 4 | Val loss: 0.8754 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=43619)[0m rmse: 0.17058131098747253
[2m[36m(func pid=43619)[0m mae:  0.11371065676212311
[2m[36m(func pid=43619)[0m rmse_per_class: [0.196, 0.263, 0.039, 0.289, 0.053, 0.171, 0.233, 0.138, 0.132, 0.192]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5560 | Steps: 4 | Val loss: 0.4481 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4295 | Steps: 4 | Val loss: 0.3248 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=43707)[0m rmse: 0.22901499271392822
[2m[36m(func pid=43707)[0m mae:  0.14276964962482452
[2m[36m(func pid=43707)[0m rmse_per_class: [0.103, 0.239, 0.061, 0.389, 0.332, 0.487, 0.308, 0.134, 0.137, 0.099]
[2m[36m(func pid=43707)[0m 
== Status ==
Current time: 2024-01-07 16:07:58 (running for 00:39:12.33)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.418 |  0.171 |                   38 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.895 |  0.229 |                   40 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.569 |  0.18  |                   36 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.428 |  0.174 |                   25 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m rmse: 0.17988340556621552
[2m[36m(func pid=45149)[0m mae:  0.13248470425605774
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.331, 0.1, 0.19, 0.299, 0.142, 0.144, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4504 | Steps: 4 | Val loss: 0.3184 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=47987)[0m rmse: 0.1741163432598114
[2m[36m(func pid=47987)[0m mae:  0.12741833925247192
[2m[36m(func pid=47987)[0m rmse_per_class: [0.126, 0.262, 0.096, 0.336, 0.07, 0.187, 0.277, 0.135, 0.146, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.0072 | Steps: 4 | Val loss: 1.0709 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.5591 | Steps: 4 | Val loss: 0.4434 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=43619)[0m rmse: 0.16613461077213287
[2m[36m(func pid=43619)[0m mae:  0.11235348135232925
[2m[36m(func pid=43619)[0m rmse_per_class: [0.139, 0.264, 0.044, 0.262, 0.054, 0.166, 0.246, 0.144, 0.144, 0.198]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4218 | Steps: 4 | Val loss: 0.3241 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 16:08:04 (running for 00:39:17.69)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.45  |  0.166 |                   39 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  1.007 |  0.24  |                   41 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.556 |  0.18  |                   37 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.429 |  0.174 |                   26 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.2403070032596588
[2m[36m(func pid=43707)[0m mae:  0.14523404836654663
[2m[36m(func pid=43707)[0m rmse_per_class: [0.408, 0.276, 0.047, 0.389, 0.143, 0.416, 0.296, 0.128, 0.14, 0.161]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17987385392189026
[2m[36m(func pid=45149)[0m mae:  0.13245835900306702
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.264, 0.098, 0.331, 0.1, 0.19, 0.299, 0.142, 0.144, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4422 | Steps: 4 | Val loss: 0.3127 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=47987)[0m rmse: 0.1739632785320282
[2m[36m(func pid=47987)[0m mae:  0.12726424634456635
[2m[36m(func pid=47987)[0m rmse_per_class: [0.126, 0.262, 0.095, 0.337, 0.069, 0.186, 0.277, 0.135, 0.146, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.0775 | Steps: 4 | Val loss: 0.9679 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.5603 | Steps: 4 | Val loss: 0.4437 | Batch size: 32 | lr: 0.0001 | Duration: 3.28s
[2m[36m(func pid=43619)[0m rmse: 0.1672772616147995
[2m[36m(func pid=43619)[0m mae:  0.11223249137401581
[2m[36m(func pid=43619)[0m rmse_per_class: [0.099, 0.249, 0.046, 0.262, 0.054, 0.169, 0.263, 0.144, 0.24, 0.147]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4231 | Steps: 4 | Val loss: 0.3234 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=43707)[0m rmse: 0.19732627272605896
[2m[36m(func pid=43707)[0m mae:  0.11755681037902832
[2m[36m(func pid=43707)[0m rmse_per_class: [0.255, 0.242, 0.09, 0.389, 0.054, 0.173, 0.297, 0.11, 0.14, 0.224]
[2m[36m(func pid=43707)[0m 
== Status ==
Current time: 2024-01-07 16:08:09 (running for 00:39:22.94)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.442 |  0.167 |                   40 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  1.077 |  0.197 |                   42 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.559 |  0.18  |                   38 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.422 |  0.174 |                   27 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m rmse: 0.17981423437595367
[2m[36m(func pid=45149)[0m mae:  0.13240517675876617
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.331, 0.1, 0.19, 0.298, 0.142, 0.144, 0.116]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17364588379859924
[2m[36m(func pid=47987)[0m mae:  0.1270636022090912
[2m[36m(func pid=47987)[0m rmse_per_class: [0.125, 0.262, 0.094, 0.337, 0.069, 0.186, 0.277, 0.135, 0.146, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4629 | Steps: 4 | Val loss: 0.3051 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8379 | Steps: 4 | Val loss: 0.8082 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.5497 | Steps: 4 | Val loss: 0.4373 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=43619)[0m rmse: 0.16612567007541656
[2m[36m(func pid=43619)[0m mae:  0.10951551049947739
[2m[36m(func pid=43619)[0m rmse_per_class: [0.099, 0.217, 0.044, 0.261, 0.054, 0.167, 0.26, 0.137, 0.298, 0.126]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.4212 | Steps: 4 | Val loss: 0.3222 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=43707)[0m rmse: 0.20628097653388977
[2m[36m(func pid=43707)[0m mae:  0.11714906990528107
[2m[36m(func pid=43707)[0m rmse_per_class: [0.111, 0.37, 0.133, 0.389, 0.052, 0.177, 0.247, 0.191, 0.251, 0.141]
== Status ==
Current time: 2024-01-07 16:08:14 (running for 00:39:28.15)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.463 |  0.166 |                   41 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.838 |  0.206 |                   43 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.56  |  0.18  |                   39 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.423 |  0.174 |                   28 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17976662516593933
[2m[36m(func pid=45149)[0m mae:  0.13237176835536957
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.264, 0.098, 0.331, 0.1, 0.19, 0.298, 0.142, 0.144, 0.115]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17323358356952667
[2m[36m(func pid=47987)[0m mae:  0.12694761157035828
[2m[36m(func pid=47987)[0m rmse_per_class: [0.122, 0.262, 0.092, 0.336, 0.07, 0.186, 0.277, 0.135, 0.147, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4279 | Steps: 4 | Val loss: 0.3051 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.9202 | Steps: 4 | Val loss: 0.7925 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5368 | Steps: 4 | Val loss: 0.4318 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4230 | Steps: 4 | Val loss: 0.3220 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 16:08:19 (running for 00:39:33.34)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.463 |  0.166 |                   41 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.92  |  0.213 |                   44 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.55  |  0.18  |                   40 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.421 |  0.173 |                   29 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.21332570910453796
[2m[36m(func pid=43707)[0m mae:  0.1316799521446228
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.407, 0.029, 0.389, 0.055, 0.169, 0.33, 0.213, 0.327, 0.104]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.15414468944072723
[2m[36m(func pid=43619)[0m mae:  0.10070370137691498
[2m[36m(func pid=43619)[0m rmse_per_class: [0.097, 0.238, 0.028, 0.287, 0.054, 0.162, 0.246, 0.121, 0.204, 0.104]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17974303662776947
[2m[36m(func pid=45149)[0m mae:  0.13235335052013397
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.264, 0.098, 0.331, 0.099, 0.19, 0.299, 0.142, 0.144, 0.115]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17310790717601776
[2m[36m(func pid=47987)[0m mae:  0.12683098018169403
[2m[36m(func pid=47987)[0m rmse_per_class: [0.122, 0.261, 0.091, 0.336, 0.069, 0.186, 0.277, 0.135, 0.148, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.8618 | Steps: 4 | Val loss: 0.4916 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4237 | Steps: 4 | Val loss: 0.3138 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5451 | Steps: 4 | Val loss: 0.4311 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4243 | Steps: 4 | Val loss: 0.3221 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 16:08:25 (running for 00:39:38.69)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.428 |  0.154 |                   42 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.862 |  0.16  |                   45 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.537 |  0.18  |                   41 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.423 |  0.173 |                   30 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.15965864062309265
[2m[36m(func pid=43707)[0m mae:  0.0930602103471756
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.253, 0.041, 0.287, 0.056, 0.251, 0.234, 0.108, 0.138, 0.117]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.16311681270599365
[2m[36m(func pid=43619)[0m mae:  0.10288725793361664
[2m[36m(func pid=43619)[0m rmse_per_class: [0.098, 0.387, 0.034, 0.335, 0.053, 0.16, 0.238, 0.102, 0.134, 0.091]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17963877320289612
[2m[36m(func pid=45149)[0m mae:  0.13228026032447815
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.264, 0.097, 0.331, 0.099, 0.19, 0.298, 0.142, 0.144, 0.115]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.1730298548936844
[2m[36m(func pid=47987)[0m mae:  0.12678635120391846
[2m[36m(func pid=47987)[0m rmse_per_class: [0.122, 0.261, 0.09, 0.336, 0.069, 0.186, 0.277, 0.135, 0.149, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.7789 | Steps: 4 | Val loss: 0.4990 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4274 | Steps: 4 | Val loss: 0.3285 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.5371 | Steps: 4 | Val loss: 0.4301 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 16:08:30 (running for 00:39:44.08)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.424 |  0.163 |                   43 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.779 |  0.172 |                   46 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.545 |  0.18  |                   42 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.424 |  0.173 |                   31 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.17203310132026672
[2m[36m(func pid=43707)[0m mae:  0.09875532239675522
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.23, 0.032, 0.464, 0.051, 0.16, 0.255, 0.11, 0.139, 0.167]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4214 | Steps: 4 | Val loss: 0.3227 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=43619)[0m rmse: 0.17125388979911804
[2m[36m(func pid=43619)[0m mae:  0.10721492767333984
[2m[36m(func pid=43619)[0m rmse_per_class: [0.096, 0.436, 0.049, 0.354, 0.053, 0.163, 0.243, 0.097, 0.133, 0.089]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17957518994808197
[2m[36m(func pid=45149)[0m mae:  0.13222987949848175
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.265, 0.097, 0.331, 0.099, 0.19, 0.298, 0.141, 0.144, 0.114]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17310447990894318
[2m[36m(func pid=47987)[0m mae:  0.12684109807014465
[2m[36m(func pid=47987)[0m rmse_per_class: [0.123, 0.261, 0.09, 0.337, 0.068, 0.186, 0.277, 0.135, 0.149, 0.105]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.6861 | Steps: 4 | Val loss: 0.5743 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4341 | Steps: 4 | Val loss: 0.3130 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5353 | Steps: 4 | Val loss: 0.4319 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 16:08:35 (running for 00:39:49.43)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.427 |  0.171 |                   44 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.686 |  0.215 |                   47 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.537 |  0.18  |                   43 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.421 |  0.173 |                   32 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.21454179286956787
[2m[36m(func pid=43707)[0m mae:  0.11833707988262177
[2m[36m(func pid=43707)[0m rmse_per_class: [0.321, 0.241, 0.113, 0.357, 0.168, 0.193, 0.257, 0.12, 0.141, 0.234]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4221 | Steps: 4 | Val loss: 0.3224 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=43619)[0m rmse: 0.16369760036468506
[2m[36m(func pid=43619)[0m mae:  0.10197490453720093
[2m[36m(func pid=43619)[0m rmse_per_class: [0.12, 0.322, 0.078, 0.355, 0.052, 0.161, 0.233, 0.093, 0.135, 0.088]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17962023615837097
[2m[36m(func pid=45149)[0m mae:  0.13228584825992584
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.265, 0.097, 0.331, 0.1, 0.19, 0.298, 0.141, 0.145, 0.114]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17299097776412964
[2m[36m(func pid=47987)[0m mae:  0.12673842906951904
[2m[36m(func pid=47987)[0m rmse_per_class: [0.123, 0.261, 0.09, 0.338, 0.068, 0.186, 0.276, 0.135, 0.15, 0.105]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.8008 | Steps: 4 | Val loss: 0.8608 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4144 | Steps: 4 | Val loss: 0.3058 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5377 | Steps: 4 | Val loss: 0.4311 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4259 | Steps: 4 | Val loss: 0.3223 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=43707)[0m rmse: 0.23182399570941925
[2m[36m(func pid=43707)[0m mae:  0.14198842644691467
[2m[36m(func pid=43707)[0m rmse_per_class: [0.256, 0.255, 0.139, 0.389, 0.285, 0.177, 0.27, 0.137, 0.302, 0.107]
[2m[36m(func pid=43707)[0m 
== Status ==
Current time: 2024-01-07 16:08:41 (running for 00:39:54.97)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.434 |  0.164 |                   45 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.801 |  0.232 |                   48 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.535 |  0.18  |                   44 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.422 |  0.173 |                   33 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43619)[0m rmse: 0.16144397854804993
[2m[36m(func pid=43619)[0m mae:  0.10167155414819717
[2m[36m(func pid=43619)[0m rmse_per_class: [0.214, 0.208, 0.086, 0.342, 0.051, 0.16, 0.23, 0.098, 0.135, 0.09]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17965586483478546
[2m[36m(func pid=45149)[0m mae:  0.13230955600738525
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.331, 0.101, 0.19, 0.298, 0.141, 0.145, 0.114]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17303213477134705
[2m[36m(func pid=47987)[0m mae:  0.12669780850410461
[2m[36m(func pid=47987)[0m rmse_per_class: [0.124, 0.26, 0.089, 0.339, 0.068, 0.185, 0.275, 0.135, 0.15, 0.104]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.8174 | Steps: 4 | Val loss: 0.7909 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5322 | Steps: 4 | Val loss: 0.4278 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4142 | Steps: 4 | Val loss: 0.2964 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4202 | Steps: 4 | Val loss: 0.3211 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 16:08:46 (running for 00:40:00.18)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.414 |  0.161 |                   46 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.817 |  0.207 |                   49 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.538 |  0.18  |                   45 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.426 |  0.173 |                   34 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.20658664405345917
[2m[36m(func pid=43707)[0m mae:  0.13141539692878723
[2m[36m(func pid=43707)[0m rmse_per_class: [0.108, 0.227, 0.032, 0.389, 0.328, 0.256, 0.275, 0.117, 0.23, 0.105]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17954513430595398
[2m[36m(func pid=45149)[0m mae:  0.13221696019172668
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.331, 0.1, 0.19, 0.297, 0.141, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.15899838507175446
[2m[36m(func pid=43619)[0m mae:  0.10119716823101044
[2m[36m(func pid=43619)[0m rmse_per_class: [0.193, 0.218, 0.067, 0.307, 0.052, 0.165, 0.235, 0.122, 0.134, 0.098]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17275568842887878
[2m[36m(func pid=47987)[0m mae:  0.12651826441287994
[2m[36m(func pid=47987)[0m rmse_per_class: [0.122, 0.261, 0.089, 0.338, 0.067, 0.185, 0.275, 0.135, 0.15, 0.104]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.8226 | Steps: 4 | Val loss: 0.8002 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5274 | Steps: 4 | Val loss: 0.4255 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4151 | Steps: 4 | Val loss: 0.2914 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4154 | Steps: 4 | Val loss: 0.3201 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 16:08:51 (running for 00:40:05.56)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.414 |  0.159 |                   47 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.823 |  0.224 |                   50 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.532 |  0.18  |                   46 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.42  |  0.173 |                   35 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.22409296035766602
[2m[36m(func pid=43707)[0m mae:  0.1281484067440033
[2m[36m(func pid=43707)[0m rmse_per_class: [0.11, 0.425, 0.038, 0.389, 0.264, 0.174, 0.266, 0.225, 0.137, 0.213]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17948700487613678
[2m[36m(func pid=45149)[0m mae:  0.13218942284584045
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.265, 0.098, 0.331, 0.099, 0.19, 0.297, 0.141, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.15472738444805145
[2m[36m(func pid=43619)[0m mae:  0.10030758380889893
[2m[36m(func pid=43619)[0m rmse_per_class: [0.124, 0.245, 0.041, 0.27, 0.056, 0.166, 0.252, 0.149, 0.132, 0.113]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17258882522583008
[2m[36m(func pid=47987)[0m mae:  0.12643584609031677
[2m[36m(func pid=47987)[0m rmse_per_class: [0.12, 0.261, 0.09, 0.337, 0.068, 0.185, 0.275, 0.135, 0.15, 0.105]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7811 | Steps: 4 | Val loss: 0.6098 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5317 | Steps: 4 | Val loss: 0.4224 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3833 | Steps: 4 | Val loss: 0.3006 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4206 | Steps: 4 | Val loss: 0.3203 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 16:08:57 (running for 00:40:10.87)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.415 |  0.155 |                   48 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.781 |  0.198 |                   51 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.527 |  0.179 |                   47 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.415 |  0.173 |                   36 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.19767162203788757
[2m[36m(func pid=43707)[0m mae:  0.11490879207849503
[2m[36m(func pid=43707)[0m rmse_per_class: [0.103, 0.345, 0.03, 0.389, 0.081, 0.175, 0.261, 0.271, 0.138, 0.184]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17943961918354034
[2m[36m(func pid=45149)[0m mae:  0.13213859498500824
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.331, 0.099, 0.19, 0.297, 0.141, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.1590648889541626
[2m[36m(func pid=43619)[0m mae:  0.10554119199514389
[2m[36m(func pid=43619)[0m rmse_per_class: [0.092, 0.252, 0.035, 0.258, 0.068, 0.162, 0.28, 0.145, 0.154, 0.144]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.1727769672870636
[2m[36m(func pid=47987)[0m mae:  0.12660828232765198
[2m[36m(func pid=47987)[0m rmse_per_class: [0.12, 0.261, 0.091, 0.336, 0.068, 0.186, 0.275, 0.135, 0.15, 0.105]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5618 | Steps: 4 | Val loss: 0.4549 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5179 | Steps: 4 | Val loss: 0.4181 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3963 | Steps: 4 | Val loss: 0.3086 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4172 | Steps: 4 | Val loss: 0.3203 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 16:09:02 (running for 00:40:16.38)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.383 |  0.159 |                   49 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.562 |  0.179 |                   52 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.532 |  0.179 |                   48 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.421 |  0.173 |                   37 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.17942841351032257
[2m[36m(func pid=43707)[0m mae:  0.10396888107061386
[2m[36m(func pid=43707)[0m rmse_per_class: [0.225, 0.254, 0.08, 0.326, 0.052, 0.19, 0.258, 0.112, 0.155, 0.142]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17938174307346344
[2m[36m(func pid=45149)[0m mae:  0.13208118081092834
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.331, 0.098, 0.19, 0.297, 0.141, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17292626202106476
[2m[36m(func pid=47987)[0m mae:  0.12674404680728912
[2m[36m(func pid=47987)[0m rmse_per_class: [0.12, 0.261, 0.092, 0.336, 0.068, 0.186, 0.275, 0.135, 0.151, 0.105]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.1680469661951065
[2m[36m(func pid=43619)[0m mae:  0.11160550266504288
[2m[36m(func pid=43619)[0m rmse_per_class: [0.101, 0.245, 0.043, 0.26, 0.089, 0.162, 0.291, 0.112, 0.221, 0.157]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.7069 | Steps: 4 | Val loss: 0.6450 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5193 | Steps: 4 | Val loss: 0.4179 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4188 | Steps: 4 | Val loss: 0.3208 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 16:09:08 (running for 00:40:21.75)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.396 |  0.168 |                   50 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.707 |  0.186 |                   53 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.518 |  0.179 |                   49 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.417 |  0.173 |                   38 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4032 | Steps: 4 | Val loss: 0.2960 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=43707)[0m rmse: 0.18603381514549255
[2m[36m(func pid=43707)[0m mae:  0.11986855417490005
[2m[36m(func pid=43707)[0m rmse_per_class: [0.148, 0.293, 0.05, 0.371, 0.056, 0.193, 0.32, 0.122, 0.194, 0.115]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17935709655284882
[2m[36m(func pid=45149)[0m mae:  0.13206417858600616
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.331, 0.099, 0.19, 0.297, 0.14, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17327386140823364
[2m[36m(func pid=47987)[0m mae:  0.1269451081752777
[2m[36m(func pid=47987)[0m rmse_per_class: [0.121, 0.261, 0.094, 0.336, 0.067, 0.186, 0.275, 0.135, 0.152, 0.105]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.16715092957019806
[2m[36m(func pid=43619)[0m mae:  0.10960068553686142
[2m[36m(func pid=43619)[0m rmse_per_class: [0.1, 0.227, 0.04, 0.267, 0.139, 0.165, 0.277, 0.097, 0.229, 0.13]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.6558 | Steps: 4 | Val loss: 0.4477 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5207 | Steps: 4 | Val loss: 0.4160 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4176 | Steps: 4 | Val loss: 0.3204 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 16:09:13 (running for 00:40:26.93)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.403 |  0.167 |                   51 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.656 |  0.187 |                   54 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.519 |  0.179 |                   50 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.419 |  0.173 |                   39 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.18711329996585846
[2m[36m(func pid=43707)[0m mae:  0.10971220582723618
[2m[36m(func pid=43707)[0m rmse_per_class: [0.111, 0.248, 0.05, 0.339, 0.056, 0.197, 0.263, 0.122, 0.141, 0.343]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3642 | Steps: 4 | Val loss: 0.2925 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=45149)[0m rmse: 0.17922137677669525
[2m[36m(func pid=45149)[0m mae:  0.1319466382265091
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.098, 0.19, 0.296, 0.14, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17330223321914673
[2m[36m(func pid=47987)[0m mae:  0.12694258987903595
[2m[36m(func pid=47987)[0m rmse_per_class: [0.121, 0.261, 0.094, 0.336, 0.067, 0.186, 0.275, 0.135, 0.152, 0.105]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5053 | Steps: 4 | Val loss: 0.5695 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=43619)[0m rmse: 0.15793472528457642
[2m[36m(func pid=43619)[0m mae:  0.10357312113046646
[2m[36m(func pid=43619)[0m rmse_per_class: [0.091, 0.209, 0.029, 0.285, 0.196, 0.169, 0.251, 0.093, 0.15, 0.106]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5232 | Steps: 4 | Val loss: 0.4160 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4128 | Steps: 4 | Val loss: 0.3201 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 16:09:18 (running for 00:40:32.35)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.364 |  0.158 |                   52 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.505 |  0.193 |                   55 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.521 |  0.179 |                   51 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.418 |  0.173 |                   40 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.19323068857192993
[2m[36m(func pid=43707)[0m mae:  0.11372164636850357
[2m[36m(func pid=43707)[0m rmse_per_class: [0.11, 0.311, 0.033, 0.381, 0.056, 0.186, 0.355, 0.117, 0.139, 0.243]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3495 | Steps: 4 | Val loss: 0.3025 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=45149)[0m rmse: 0.1792222410440445
[2m[36m(func pid=45149)[0m mae:  0.1319512277841568
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.265, 0.099, 0.332, 0.098, 0.19, 0.296, 0.14, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17344775795936584
[2m[36m(func pid=47987)[0m mae:  0.12706945836544037
[2m[36m(func pid=47987)[0m rmse_per_class: [0.121, 0.261, 0.095, 0.336, 0.067, 0.186, 0.276, 0.135, 0.152, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.6852 | Steps: 4 | Val loss: 0.6674 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=43619)[0m rmse: 0.16424277424812317
[2m[36m(func pid=43619)[0m mae:  0.10345105826854706
[2m[36m(func pid=43619)[0m rmse_per_class: [0.103, 0.226, 0.052, 0.318, 0.221, 0.17, 0.231, 0.102, 0.131, 0.087]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5161 | Steps: 4 | Val loss: 0.4144 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4144 | Steps: 4 | Val loss: 0.3206 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 16:09:23 (running for 00:40:37.64)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.349 |  0.164 |                   53 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.685 |  0.19  |                   56 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.523 |  0.179 |                   52 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.413 |  0.173 |                   41 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.1895039826631546
[2m[36m(func pid=43707)[0m mae:  0.11220727860927582
[2m[36m(func pid=43707)[0m rmse_per_class: [0.11, 0.391, 0.035, 0.389, 0.055, 0.171, 0.229, 0.219, 0.138, 0.159]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17924004793167114
[2m[36m(func pid=45149)[0m mae:  0.13196463882923126
[2m[36m(func pid=45149)[0m rmse_per_class: [0.115, 0.265, 0.099, 0.332, 0.098, 0.19, 0.296, 0.14, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3743 | Steps: 4 | Val loss: 0.3176 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=47987)[0m rmse: 0.1740504652261734
[2m[36m(func pid=47987)[0m mae:  0.12738186120986938
[2m[36m(func pid=47987)[0m rmse_per_class: [0.124, 0.261, 0.098, 0.337, 0.066, 0.186, 0.276, 0.134, 0.152, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5519 | Steps: 4 | Val loss: 0.6248 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=43619)[0m rmse: 0.16976235806941986
[2m[36m(func pid=43619)[0m mae:  0.10511554777622223
[2m[36m(func pid=43619)[0m rmse_per_class: [0.112, 0.237, 0.065, 0.35, 0.225, 0.164, 0.221, 0.106, 0.133, 0.085]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5145 | Steps: 4 | Val loss: 0.4127 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4154 | Steps: 4 | Val loss: 0.3214 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 16:09:29 (running for 00:40:42.92)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.374 |  0.17  |                   54 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.552 |  0.214 |                   57 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.516 |  0.179 |                   53 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.414 |  0.174 |                   42 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.2135639190673828
[2m[36m(func pid=43707)[0m mae:  0.132089763879776
[2m[36m(func pid=43707)[0m rmse_per_class: [0.167, 0.218, 0.103, 0.388, 0.057, 0.321, 0.321, 0.199, 0.179, 0.182]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17919157445430756
[2m[36m(func pid=45149)[0m mae:  0.13190589845180511
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.331, 0.098, 0.19, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4589 | Steps: 4 | Val loss: 0.3395 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=47987)[0m rmse: 0.1744004786014557
[2m[36m(func pid=47987)[0m mae:  0.12756755948066711
[2m[36m(func pid=47987)[0m rmse_per_class: [0.125, 0.261, 0.1, 0.338, 0.066, 0.186, 0.276, 0.134, 0.152, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.6145 | Steps: 4 | Val loss: 0.6527 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=43619)[0m rmse: 0.1762687861919403
[2m[36m(func pid=43619)[0m mae:  0.10943899303674698
[2m[36m(func pid=43619)[0m rmse_per_class: [0.135, 0.236, 0.091, 0.357, 0.207, 0.17, 0.223, 0.123, 0.135, 0.086]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5138 | Steps: 4 | Val loss: 0.4103 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4162 | Steps: 4 | Val loss: 0.3208 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 16:09:34 (running for 00:40:47.98)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.459 |  0.176 |                   55 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.615 |  0.226 |                   58 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.514 |  0.179 |                   54 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.415 |  0.174 |                   43 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.22579741477966309
[2m[36m(func pid=43707)[0m mae:  0.14095821976661682
[2m[36m(func pid=43707)[0m rmse_per_class: [0.332, 0.279, 0.219, 0.382, 0.158, 0.198, 0.277, 0.125, 0.172, 0.117]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17912204563617706
[2m[36m(func pid=45149)[0m mae:  0.13185231387615204
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.331, 0.098, 0.19, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4386 | Steps: 4 | Val loss: 0.3224 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=47987)[0m rmse: 0.17463012039661407
[2m[36m(func pid=47987)[0m mae:  0.12764760851860046
[2m[36m(func pid=47987)[0m rmse_per_class: [0.127, 0.261, 0.101, 0.338, 0.065, 0.186, 0.276, 0.134, 0.153, 0.106]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5960 | Steps: 4 | Val loss: 0.4852 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5084 | Steps: 4 | Val loss: 0.4075 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=43619)[0m rmse: 0.17138056457042694
[2m[36m(func pid=43619)[0m mae:  0.10638227313756943
[2m[36m(func pid=43619)[0m rmse_per_class: [0.138, 0.222, 0.093, 0.353, 0.187, 0.17, 0.22, 0.111, 0.135, 0.085]
[2m[36m(func pid=43619)[0m 
== Status ==
Current time: 2024-01-07 16:09:39 (running for 00:40:53.30)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.439 |  0.171 |                   56 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.596 |  0.199 |                   59 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.514 |  0.179 |                   55 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.416 |  0.175 |                   44 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.19907651841640472
[2m[36m(func pid=43707)[0m mae:  0.11808431148529053
[2m[36m(func pid=43707)[0m rmse_per_class: [0.119, 0.284, 0.067, 0.292, 0.325, 0.191, 0.323, 0.128, 0.134, 0.128]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4108 | Steps: 4 | Val loss: 0.3204 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=45149)[0m rmse: 0.17904385924339294
[2m[36m(func pid=45149)[0m mae:  0.1317969262599945
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.331, 0.097, 0.19, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3784 | Steps: 4 | Val loss: 0.3002 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=47987)[0m rmse: 0.17478607594966888
[2m[36m(func pid=47987)[0m mae:  0.1277059018611908
[2m[36m(func pid=47987)[0m rmse_per_class: [0.128, 0.261, 0.101, 0.338, 0.064, 0.186, 0.276, 0.134, 0.154, 0.107]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6382 | Steps: 4 | Val loss: 0.5494 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5025 | Steps: 4 | Val loss: 0.4025 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 16:09:44 (running for 00:40:58.41)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.378 |  0.165 |                   57 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.596 |  0.199 |                   59 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.508 |  0.179 |                   56 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.411 |  0.175 |                   45 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43619)[0m rmse: 0.16534852981567383
[2m[36m(func pid=43619)[0m mae:  0.10395349562168121
[2m[36m(func pid=43619)[0m rmse_per_class: [0.116, 0.215, 0.086, 0.328, 0.166, 0.185, 0.23, 0.104, 0.132, 0.09]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.20388014614582062
[2m[36m(func pid=43707)[0m mae:  0.12467782199382782
[2m[36m(func pid=43707)[0m rmse_per_class: [0.106, 0.256, 0.031, 0.471, 0.229, 0.183, 0.32, 0.121, 0.135, 0.188]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4173 | Steps: 4 | Val loss: 0.3207 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=45149)[0m rmse: 0.17898614704608917
[2m[36m(func pid=45149)[0m mae:  0.1317518949508667
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.264, 0.099, 0.332, 0.096, 0.19, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.5440 | Steps: 4 | Val loss: 0.3957 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=47987)[0m rmse: 0.17476385831832886
[2m[36m(func pid=47987)[0m mae:  0.12767890095710754
[2m[36m(func pid=47987)[0m rmse_per_class: [0.127, 0.26, 0.102, 0.338, 0.064, 0.186, 0.276, 0.134, 0.154, 0.107]
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3573 | Steps: 4 | Val loss: 0.2876 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5005 | Steps: 4 | Val loss: 0.3997 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 16:09:50 (running for 00:41:03.96)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.378 |  0.165 |                   57 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.544 |  0.172 |                   61 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.502 |  0.179 |                   57 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.417 |  0.175 |                   46 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.1722606122493744
[2m[36m(func pid=43707)[0m mae:  0.10532156378030777
[2m[36m(func pid=43707)[0m rmse_per_class: [0.102, 0.226, 0.031, 0.289, 0.065, 0.289, 0.297, 0.137, 0.138, 0.148]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.15646293759346008
[2m[36m(func pid=43619)[0m mae:  0.10183916985988617
[2m[36m(func pid=43619)[0m rmse_per_class: [0.096, 0.211, 0.048, 0.296, 0.132, 0.191, 0.25, 0.094, 0.133, 0.113]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4087 | Steps: 4 | Val loss: 0.3196 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=45149)[0m rmse: 0.17893096804618835
[2m[36m(func pid=45149)[0m mae:  0.13170699775218964
[2m[36m(func pid=45149)[0m rmse_per_class: [0.117, 0.264, 0.099, 0.332, 0.096, 0.19, 0.295, 0.14, 0.144, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5913 | Steps: 4 | Val loss: 0.5636 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=47987)[0m rmse: 0.17437121272087097
[2m[36m(func pid=47987)[0m mae:  0.12745408713817596
[2m[36m(func pid=47987)[0m rmse_per_class: [0.126, 0.26, 0.099, 0.338, 0.064, 0.186, 0.276, 0.134, 0.154, 0.107]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3360 | Steps: 4 | Val loss: 0.2877 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5021 | Steps: 4 | Val loss: 0.3998 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 16:09:55 (running for 00:41:09.37)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.357 |  0.156 |                   58 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.591 |  0.167 |                   62 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.5   |  0.179 |                   58 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.409 |  0.174 |                   47 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.16707780957221985
[2m[36m(func pid=43707)[0m mae:  0.10717557370662689
[2m[36m(func pid=43707)[0m rmse_per_class: [0.108, 0.272, 0.028, 0.382, 0.055, 0.167, 0.233, 0.137, 0.202, 0.087]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4173 | Steps: 4 | Val loss: 0.3196 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=43619)[0m rmse: 0.15780219435691833
[2m[36m(func pid=43619)[0m mae:  0.10565052181482315
[2m[36m(func pid=43619)[0m rmse_per_class: [0.091, 0.209, 0.039, 0.27, 0.097, 0.171, 0.281, 0.093, 0.184, 0.143]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17890608310699463
[2m[36m(func pid=45149)[0m mae:  0.13169164955615997
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.096, 0.19, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5730 | Steps: 4 | Val loss: 0.5468 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=47987)[0m rmse: 0.174625426530838
[2m[36m(func pid=47987)[0m mae:  0.12748396396636963
[2m[36m(func pid=47987)[0m rmse_per_class: [0.128, 0.26, 0.1, 0.338, 0.064, 0.186, 0.275, 0.134, 0.153, 0.108]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3549 | Steps: 4 | Val loss: 0.3053 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5067 | Steps: 4 | Val loss: 0.3981 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 16:10:01 (running for 00:41:14.69)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.336 |  0.158 |                   59 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.573 |  0.202 |                   63 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.502 |  0.179 |                   59 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.417 |  0.175 |                   48 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.20215901732444763
[2m[36m(func pid=43707)[0m mae:  0.1145031675696373
[2m[36m(func pid=43707)[0m rmse_per_class: [0.317, 0.264, 0.123, 0.378, 0.056, 0.171, 0.254, 0.2, 0.149, 0.111]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4061 | Steps: 4 | Val loss: 0.3189 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=43619)[0m rmse: 0.1673625111579895
[2m[36m(func pid=43619)[0m mae:  0.1125163584947586
[2m[36m(func pid=43619)[0m rmse_per_class: [0.093, 0.219, 0.048, 0.268, 0.066, 0.162, 0.296, 0.094, 0.286, 0.142]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17879115045070648
[2m[36m(func pid=45149)[0m mae:  0.1316065639257431
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.264, 0.099, 0.332, 0.095, 0.19, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17421826720237732
[2m[36m(func pid=47987)[0m mae:  0.1272723525762558
[2m[36m(func pid=47987)[0m rmse_per_class: [0.125, 0.26, 0.098, 0.338, 0.064, 0.186, 0.275, 0.134, 0.153, 0.109]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5845 | Steps: 4 | Val loss: 0.5003 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3848 | Steps: 4 | Val loss: 0.3020 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4959 | Steps: 4 | Val loss: 0.3983 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 16:10:06 (running for 00:41:20.17)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.355 |  0.167 |                   60 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.585 |  0.192 |                   64 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.507 |  0.179 |                   60 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.406 |  0.174 |                   49 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.19152316451072693
[2m[36m(func pid=43707)[0m mae:  0.11134181916713715
[2m[36m(func pid=43707)[0m rmse_per_class: [0.198, 0.245, 0.072, 0.338, 0.056, 0.171, 0.427, 0.124, 0.137, 0.149]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4123 | Steps: 4 | Val loss: 0.3188 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=45149)[0m rmse: 0.17876575887203217
[2m[36m(func pid=45149)[0m mae:  0.13158634305000305
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.095, 0.189, 0.295, 0.14, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.16344735026359558
[2m[36m(func pid=43619)[0m mae:  0.10991065204143524
[2m[36m(func pid=43619)[0m rmse_per_class: [0.089, 0.218, 0.048, 0.261, 0.055, 0.167, 0.291, 0.094, 0.277, 0.133]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.174202561378479
[2m[36m(func pid=47987)[0m mae:  0.1272689402103424
[2m[36m(func pid=47987)[0m rmse_per_class: [0.125, 0.26, 0.097, 0.338, 0.064, 0.186, 0.275, 0.134, 0.154, 0.11]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5658 | Steps: 4 | Val loss: 0.4505 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5029 | Steps: 4 | Val loss: 0.3975 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3981 | Steps: 4 | Val loss: 0.2758 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 16:10:11 (running for 00:41:25.47)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.385 |  0.163 |                   61 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.566 |  0.162 |                   65 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.496 |  0.179 |                   61 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.412 |  0.174 |                   50 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.16236090660095215
[2m[36m(func pid=43707)[0m mae:  0.09657905995845795
[2m[36m(func pid=43707)[0m rmse_per_class: [0.111, 0.264, 0.028, 0.305, 0.056, 0.213, 0.253, 0.124, 0.135, 0.133]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4096 | Steps: 4 | Val loss: 0.3169 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=45149)[0m rmse: 0.17876233160495758
[2m[36m(func pid=45149)[0m mae:  0.13159853219985962
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.098, 0.332, 0.096, 0.189, 0.294, 0.14, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.1512133777141571
[2m[36m(func pid=43619)[0m mae:  0.09835965931415558
[2m[36m(func pid=43619)[0m rmse_per_class: [0.121, 0.206, 0.043, 0.26, 0.052, 0.166, 0.261, 0.118, 0.161, 0.123]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17304253578186035
[2m[36m(func pid=47987)[0m mae:  0.12672343850135803
[2m[36m(func pid=47987)[0m rmse_per_class: [0.12, 0.26, 0.091, 0.336, 0.065, 0.185, 0.276, 0.134, 0.154, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.5986 | Steps: 4 | Val loss: 0.5021 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4950 | Steps: 4 | Val loss: 0.3942 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3461 | Steps: 4 | Val loss: 0.2710 | Batch size: 32 | lr: 0.01 | Duration: 3.29s
== Status ==
Current time: 2024-01-07 16:10:17 (running for 00:41:30.95)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.398 |  0.151 |                   62 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.599 |  0.185 |                   66 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.503 |  0.179 |                   62 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.41  |  0.173 |                   51 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.18548110127449036
[2m[36m(func pid=43707)[0m mae:  0.11820708215236664
[2m[36m(func pid=43707)[0m rmse_per_class: [0.107, 0.251, 0.032, 0.292, 0.056, 0.217, 0.334, 0.206, 0.255, 0.104]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4063 | Steps: 4 | Val loss: 0.3172 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=45149)[0m rmse: 0.1787194013595581
[2m[36m(func pid=45149)[0m mae:  0.13155241310596466
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.095, 0.189, 0.294, 0.14, 0.145, 0.113]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.14659860730171204
[2m[36m(func pid=43619)[0m mae:  0.09212346374988556
[2m[36m(func pid=43619)[0m rmse_per_class: [0.141, 0.207, 0.039, 0.275, 0.05, 0.162, 0.234, 0.117, 0.133, 0.108]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.1730075180530548
[2m[36m(func pid=47987)[0m mae:  0.1267314851284027
[2m[36m(func pid=47987)[0m rmse_per_class: [0.12, 0.26, 0.09, 0.336, 0.065, 0.185, 0.276, 0.134, 0.154, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.5655 | Steps: 4 | Val loss: 0.4651 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4879 | Steps: 4 | Val loss: 0.3955 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 16:10:22 (running for 00:41:36.35)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.346 |  0.147 |                   63 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.565 |  0.199 |                   67 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.495 |  0.179 |                   63 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.406 |  0.173 |                   52 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.19917401671409607
[2m[36m(func pid=43707)[0m mae:  0.12096673250198364
[2m[36m(func pid=43707)[0m rmse_per_class: [0.137, 0.264, 0.033, 0.345, 0.055, 0.172, 0.307, 0.25, 0.322, 0.107]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4050 | Steps: 4 | Val loss: 0.3170 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3435 | Steps: 4 | Val loss: 0.2755 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=45149)[0m rmse: 0.17880018055438995
[2m[36m(func pid=45149)[0m mae:  0.13161428272724152
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.096, 0.189, 0.294, 0.14, 0.145, 0.112]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.1728925108909607
[2m[36m(func pid=47987)[0m mae:  0.12671475112438202
[2m[36m(func pid=47987)[0m rmse_per_class: [0.119, 0.26, 0.089, 0.336, 0.066, 0.185, 0.276, 0.134, 0.153, 0.112]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.14440451562404633
[2m[36m(func pid=43619)[0m mae:  0.0909198597073555
[2m[36m(func pid=43619)[0m rmse_per_class: [0.114, 0.222, 0.043, 0.297, 0.051, 0.158, 0.221, 0.108, 0.134, 0.097]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.5310 | Steps: 4 | Val loss: 0.6612 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4925 | Steps: 4 | Val loss: 0.3927 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 16:10:27 (running for 00:41:41.66)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.344 |  0.144 |                   64 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.531 |  0.178 |                   68 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.488 |  0.179 |                   64 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.405 |  0.173 |                   53 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.17806729674339294
[2m[36m(func pid=43707)[0m mae:  0.1081397533416748
[2m[36m(func pid=43707)[0m rmse_per_class: [0.121, 0.407, 0.035, 0.388, 0.054, 0.171, 0.268, 0.111, 0.139, 0.087]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4083 | Steps: 4 | Val loss: 0.3174 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3732 | Steps: 4 | Val loss: 0.2814 | Batch size: 32 | lr: 0.01 | Duration: 3.25s
[2m[36m(func pid=45149)[0m rmse: 0.17871339619159698
[2m[36m(func pid=45149)[0m mae:  0.13157054781913757
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.095, 0.189, 0.294, 0.14, 0.145, 0.112]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.1733926385641098
[2m[36m(func pid=47987)[0m mae:  0.12691649794578552
[2m[36m(func pid=47987)[0m rmse_per_class: [0.123, 0.259, 0.091, 0.337, 0.065, 0.185, 0.276, 0.134, 0.153, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.8238 | Steps: 4 | Val loss: 0.6578 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=43619)[0m rmse: 0.145986869931221
[2m[36m(func pid=43619)[0m mae:  0.0918709859251976
[2m[36m(func pid=43619)[0m rmse_per_class: [0.096, 0.223, 0.073, 0.315, 0.051, 0.155, 0.221, 0.098, 0.136, 0.092]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4879 | Steps: 4 | Val loss: 0.3897 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 16:10:33 (running for 00:41:46.95)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.373 |  0.146 |                   65 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.824 |  0.192 |                   69 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.493 |  0.179 |                   65 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.408 |  0.173 |                   54 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.19198353588581085
[2m[36m(func pid=43707)[0m mae:  0.11573662608861923
[2m[36m(func pid=43707)[0m rmse_per_class: [0.132, 0.21, 0.131, 0.389, 0.177, 0.304, 0.229, 0.111, 0.14, 0.098]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4081 | Steps: 4 | Val loss: 0.3167 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3691 | Steps: 4 | Val loss: 0.2823 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=45149)[0m rmse: 0.1786619871854782
[2m[36m(func pid=45149)[0m mae:  0.13154202699661255
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.094, 0.189, 0.294, 0.139, 0.145, 0.112]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.1730254590511322
[2m[36m(func pid=47987)[0m mae:  0.12665946781635284
[2m[36m(func pid=47987)[0m rmse_per_class: [0.122, 0.259, 0.09, 0.336, 0.065, 0.185, 0.275, 0.134, 0.153, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6803 | Steps: 4 | Val loss: 0.6707 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=43619)[0m rmse: 0.14919663965702057
[2m[36m(func pid=43619)[0m mae:  0.0939425453543663
[2m[36m(func pid=43619)[0m rmse_per_class: [0.091, 0.21, 0.12, 0.317, 0.051, 0.166, 0.221, 0.091, 0.135, 0.091]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4887 | Steps: 4 | Val loss: 0.3894 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 16:10:38 (running for 00:41:52.33)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.369 |  0.149 |                   66 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.68  |  0.25  |                   70 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.488 |  0.179 |                   66 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.408 |  0.173 |                   55 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.25040820240974426
[2m[36m(func pid=43707)[0m mae:  0.15094402432441711
[2m[36m(func pid=43707)[0m rmse_per_class: [0.185, 0.275, 0.139, 0.374, 0.438, 0.192, 0.343, 0.198, 0.139, 0.22]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4060 | Steps: 4 | Val loss: 0.3161 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=45149)[0m rmse: 0.17864039540290833
[2m[36m(func pid=45149)[0m mae:  0.13152794539928436
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.094, 0.189, 0.294, 0.139, 0.145, 0.112]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3860 | Steps: 4 | Val loss: 0.2926 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6203 | Steps: 4 | Val loss: 0.6087 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=47987)[0m rmse: 0.17300555109977722
[2m[36m(func pid=47987)[0m mae:  0.12655474245548248
[2m[36m(func pid=47987)[0m rmse_per_class: [0.123, 0.259, 0.09, 0.336, 0.064, 0.185, 0.275, 0.134, 0.153, 0.11]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m rmse: 0.15738697350025177
[2m[36m(func pid=43619)[0m mae:  0.09924758970737457
[2m[36m(func pid=43619)[0m rmse_per_class: [0.091, 0.199, 0.161, 0.317, 0.052, 0.203, 0.228, 0.093, 0.133, 0.096]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4857 | Steps: 4 | Val loss: 0.3856 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 16:10:44 (running for 00:41:57.80)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.386 |  0.157 |                   67 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.62  |  0.221 |                   71 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.489 |  0.179 |                   67 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.406 |  0.173 |                   56 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.22147496044635773
[2m[36m(func pid=43707)[0m mae:  0.13162221014499664
[2m[36m(func pid=43707)[0m rmse_per_class: [0.149, 0.294, 0.031, 0.322, 0.227, 0.173, 0.343, 0.247, 0.26, 0.169]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4078 | Steps: 4 | Val loss: 0.3163 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=45149)[0m rmse: 0.1786106526851654
[2m[36m(func pid=45149)[0m mae:  0.13149955868721008
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.264, 0.099, 0.332, 0.094, 0.189, 0.294, 0.139, 0.145, 0.112]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3817 | Steps: 4 | Val loss: 0.2985 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=47987)[0m rmse: 0.1728908270597458
[2m[36m(func pid=47987)[0m mae:  0.1264873445034027
[2m[36m(func pid=47987)[0m rmse_per_class: [0.123, 0.259, 0.09, 0.336, 0.065, 0.185, 0.276, 0.134, 0.152, 0.11]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.7386 | Steps: 4 | Val loss: 0.5227 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=43619)[0m rmse: 0.16009566187858582
[2m[36m(func pid=43619)[0m mae:  0.103662870824337
[2m[36m(func pid=43619)[0m rmse_per_class: [0.094, 0.206, 0.13, 0.308, 0.053, 0.224, 0.246, 0.098, 0.132, 0.111]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4868 | Steps: 4 | Val loss: 0.3841 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 16:10:49 (running for 00:42:03.21)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.382 |  0.16  |                   68 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.739 |  0.181 |                   72 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.486 |  0.179 |                   68 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.408 |  0.173 |                   57 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.18104781210422516
[2m[36m(func pid=43707)[0m mae:  0.1091303601861
[2m[36m(func pid=43707)[0m rmse_per_class: [0.12, 0.292, 0.047, 0.315, 0.075, 0.173, 0.264, 0.128, 0.273, 0.123]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4037 | Steps: 4 | Val loss: 0.3161 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=45149)[0m rmse: 0.17857299745082855
[2m[36m(func pid=45149)[0m mae:  0.13147397339344025
[2m[36m(func pid=45149)[0m rmse_per_class: [0.117, 0.264, 0.1, 0.332, 0.093, 0.189, 0.294, 0.139, 0.145, 0.112]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3506 | Steps: 4 | Val loss: 0.2955 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=47987)[0m rmse: 0.17283543944358826
[2m[36m(func pid=47987)[0m mae:  0.1264687478542328
[2m[36m(func pid=47987)[0m rmse_per_class: [0.124, 0.258, 0.089, 0.336, 0.064, 0.185, 0.276, 0.134, 0.152, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.6195 | Steps: 4 | Val loss: 0.4956 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4837 | Steps: 4 | Val loss: 0.3846 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 16:10:54 (running for 00:42:08.27)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.351 |  0.159 |                   69 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.739 |  0.181 |                   72 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.487 |  0.179 |                   69 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.404 |  0.173 |                   58 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43619)[0m rmse: 0.1587068885564804
[2m[36m(func pid=43619)[0m mae:  0.1068863719701767
[2m[36m(func pid=43619)[0m rmse_per_class: [0.101, 0.215, 0.053, 0.286, 0.056, 0.202, 0.272, 0.104, 0.171, 0.128]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.1889539659023285
[2m[36m(func pid=43707)[0m mae:  0.11657571792602539
[2m[36m(func pid=43707)[0m rmse_per_class: [0.112, 0.268, 0.049, 0.357, 0.052, 0.358, 0.275, 0.106, 0.138, 0.174]
[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4046 | Steps: 4 | Val loss: 0.3157 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=45149)[0m rmse: 0.17860855162143707
[2m[36m(func pid=45149)[0m mae:  0.1314944326877594
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.264, 0.099, 0.332, 0.093, 0.189, 0.294, 0.139, 0.145, 0.112]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3498 | Steps: 4 | Val loss: 0.2981 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=47987)[0m rmse: 0.17255015671253204
[2m[36m(func pid=47987)[0m mae:  0.12636205554008484
[2m[36m(func pid=47987)[0m rmse_per_class: [0.122, 0.258, 0.088, 0.336, 0.064, 0.185, 0.276, 0.134, 0.151, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5294 | Steps: 4 | Val loss: 0.6321 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4787 | Steps: 4 | Val loss: 0.3827 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=43619)[0m rmse: 0.16376851499080658
[2m[36m(func pid=43619)[0m mae:  0.11060608923435211
[2m[36m(func pid=43619)[0m rmse_per_class: [0.112, 0.224, 0.039, 0.277, 0.063, 0.175, 0.285, 0.106, 0.225, 0.131]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=43707)[0m rmse: 0.20934124290943146
[2m[36m(func pid=43707)[0m mae:  0.12550944089889526
[2m[36m(func pid=43707)[0m rmse_per_class: [0.137, 0.472, 0.035, 0.387, 0.057, 0.214, 0.317, 0.219, 0.139, 0.115]
== Status ==
Current time: 2024-01-07 16:11:00 (running for 00:42:14.19)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.15725000575184822
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.35  |  0.164 |                   70 |
| train_10f5e_00019 | RUNNING    | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.619 |  0.189 |                   73 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.484 |  0.179 |                   70 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.405 |  0.173 |                   59 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4009 | Steps: 4 | Val loss: 0.3162 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=45149)[0m rmse: 0.17855539917945862
[2m[36m(func pid=45149)[0m mae:  0.1314581334590912
[2m[36m(func pid=45149)[0m rmse_per_class: [0.117, 0.264, 0.099, 0.333, 0.093, 0.189, 0.294, 0.139, 0.146, 0.112]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17290812730789185
[2m[36m(func pid=47987)[0m mae:  0.12652362883090973
[2m[36m(func pid=47987)[0m rmse_per_class: [0.123, 0.257, 0.091, 0.337, 0.064, 0.185, 0.277, 0.134, 0.151, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3888 | Steps: 4 | Val loss: 0.2982 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=43707)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.6513 | Steps: 4 | Val loss: 0.6591 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4793 | Steps: 4 | Val loss: 0.3848 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 16:11:05 (running for 00:42:19.56)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 3 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.35  |  0.164 |                   70 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.479 |  0.179 |                   71 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.401 |  0.173 |                   60 |
| train_10f5e_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43707)[0m rmse: 0.22208134829998016
[2m[36m(func pid=43707)[0m mae:  0.1313941478729248
[2m[36m(func pid=43707)[0m rmse_per_class: [0.148, 0.33, 0.2, 0.389, 0.064, 0.157, 0.312, 0.39, 0.135, 0.095]
[2m[36m(func pid=43619)[0m rmse: 0.16272394359111786
[2m[36m(func pid=43619)[0m mae:  0.10919637978076935
[2m[36m(func pid=43619)[0m rmse_per_class: [0.128, 0.224, 0.044, 0.272, 0.075, 0.16, 0.279, 0.103, 0.209, 0.132]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4033 | Steps: 4 | Val loss: 0.3156 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=45149)[0m rmse: 0.17863476276397705
[2m[36m(func pid=45149)[0m mae:  0.13151350617408752
[2m[36m(func pid=45149)[0m rmse_per_class: [0.116, 0.265, 0.099, 0.332, 0.094, 0.189, 0.294, 0.139, 0.146, 0.112]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17282918095588684
[2m[36m(func pid=47987)[0m mae:  0.12640587985515594
[2m[36m(func pid=47987)[0m rmse_per_class: [0.124, 0.257, 0.091, 0.337, 0.064, 0.184, 0.277, 0.134, 0.151, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3614 | Steps: 4 | Val loss: 0.2868 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4786 | Steps: 4 | Val loss: 0.3821 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4012 | Steps: 4 | Val loss: 0.3144 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=43619)[0m rmse: 0.15600459277629852
[2m[36m(func pid=43619)[0m mae:  0.10311134159564972
[2m[36m(func pid=43619)[0m rmse_per_class: [0.15, 0.213, 0.037, 0.271, 0.085, 0.158, 0.253, 0.1, 0.159, 0.135]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m rmse: 0.17857150733470917
[2m[36m(func pid=45149)[0m mae:  0.1315004527568817
[2m[36m(func pid=45149)[0m rmse_per_class: [0.117, 0.265, 0.099, 0.333, 0.093, 0.189, 0.294, 0.139, 0.146, 0.112]
[2m[36m(func pid=47987)[0m rmse: 0.1721297800540924
[2m[36m(func pid=47987)[0m mae:  0.126100555062294
[2m[36m(func pid=47987)[0m rmse_per_class: [0.12, 0.257, 0.088, 0.336, 0.064, 0.184, 0.277, 0.134, 0.151, 0.111]
== Status ==
Current time: 2024-01-07 16:11:11 (running for 00:42:25.66)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.361 |  0.156 |                   72 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.479 |  0.179 |                   72 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.403 |  0.173 |                   61 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=62337)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=62337)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=62337)[0m Configuration completed!
[2m[36m(func pid=62337)[0m New optimizer parameters:
[2m[36m(func pid=62337)[0m SGD (
[2m[36m(func pid=62337)[0m Parameter Group 0
[2m[36m(func pid=62337)[0m     dampening: 0
[2m[36m(func pid=62337)[0m     differentiable: False
[2m[36m(func pid=62337)[0m     foreach: None
[2m[36m(func pid=62337)[0m     lr: 0.01
[2m[36m(func pid=62337)[0m     maximize: False
[2m[36m(func pid=62337)[0m     momentum: 0.9
[2m[36m(func pid=62337)[0m     nesterov: False
[2m[36m(func pid=62337)[0m     weight_decay: 1e-05
[2m[36m(func pid=62337)[0m )
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3476 | Steps: 4 | Val loss: 0.2762 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 16:11:17 (running for 00:42:31.21)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.348 |  0.15  |                   73 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.479 |  0.179 |                   73 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.401 |  0.172 |                   62 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43619)[0m rmse: 0.1500202715396881
[2m[36m(func pid=43619)[0m mae:  0.09695758670568466
[2m[36m(func pid=43619)[0m rmse_per_class: [0.118, 0.208, 0.033, 0.28, 0.116, 0.16, 0.226, 0.098, 0.132, 0.129]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4800 | Steps: 4 | Val loss: 0.3823 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4016 | Steps: 4 | Val loss: 0.3143 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8899 | Steps: 4 | Val loss: 0.5979 | Batch size: 32 | lr: 0.01 | Duration: 4.54s
[2m[36m(func pid=45149)[0m rmse: 0.17857882380485535
[2m[36m(func pid=45149)[0m mae:  0.13150463998317719
[2m[36m(func pid=45149)[0m rmse_per_class: [0.117, 0.265, 0.099, 0.333, 0.094, 0.189, 0.294, 0.139, 0.146, 0.112]
[2m[36m(func pid=45149)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17225868999958038
[2m[36m(func pid=47987)[0m mae:  0.12613241374492645
[2m[36m(func pid=47987)[0m rmse_per_class: [0.12, 0.257, 0.089, 0.336, 0.064, 0.184, 0.277, 0.134, 0.151, 0.11]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3451 | Steps: 4 | Val loss: 0.2816 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=62337)[0m rmse: 0.180085226893425
[2m[36m(func pid=62337)[0m mae:  0.13254840672016144
[2m[36m(func pid=62337)[0m rmse_per_class: [0.114, 0.264, 0.104, 0.334, 0.101, 0.191, 0.295, 0.141, 0.144, 0.113]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=45149)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4793 | Steps: 4 | Val loss: 0.3793 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3997 | Steps: 4 | Val loss: 0.3138 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 16:11:23 (running for 00:42:37.04)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00018 | RUNNING    | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.345 |  0.152 |                   74 |
| train_10f5e_00020 | RUNNING    | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.48  |  0.179 |                   74 |
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.402 |  0.172 |                   63 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.89  |  0.18  |                    1 |
| train_10f5e_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=43619)[0m rmse: 0.1524648219347
[2m[36m(func pid=43619)[0m mae:  0.09628404676914215
[2m[36m(func pid=43619)[0m rmse_per_class: [0.09, 0.221, 0.058, 0.296, 0.139, 0.165, 0.218, 0.097, 0.13, 0.111]
[2m[36m(func pid=43619)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5077 | Steps: 4 | Val loss: 0.4276 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=45149)[0m rmse: 0.17854686081409454
[2m[36m(func pid=45149)[0m mae:  0.13147380948066711
[2m[36m(func pid=45149)[0m rmse_per_class: [0.117, 0.264, 0.099, 0.333, 0.093, 0.189, 0.293, 0.139, 0.146, 0.112]
[2m[36m(func pid=47987)[0m rmse: 0.17204679548740387
[2m[36m(func pid=47987)[0m mae:  0.12594160437583923
[2m[36m(func pid=47987)[0m rmse_per_class: [0.119, 0.257, 0.09, 0.336, 0.064, 0.183, 0.277, 0.134, 0.15, 0.11]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=43619)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3719 | Steps: 4 | Val loss: 0.2919 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=62337)[0m rmse: 0.1791699379682541
[2m[36m(func pid=62337)[0m mae:  0.1315942406654358
[2m[36m(func pid=62337)[0m rmse_per_class: [0.115, 0.264, 0.113, 0.336, 0.096, 0.191, 0.289, 0.137, 0.146, 0.106]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4080 | Steps: 4 | Val loss: 0.3141 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=43619)[0m rmse: 0.15973064303398132
[2m[36m(func pid=43619)[0m mae:  0.09883773326873779
[2m[36m(func pid=43619)[0m rmse_per_class: [0.096, 0.23, 0.092, 0.31, 0.157, 0.166, 0.223, 0.095, 0.13, 0.099]
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4590 | Steps: 4 | Val loss: 0.3395 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=47987)[0m rmse: 0.1726638674736023
[2m[36m(func pid=47987)[0m mae:  0.12614119052886963
[2m[36m(func pid=47987)[0m rmse_per_class: [0.121, 0.257, 0.095, 0.338, 0.063, 0.183, 0.276, 0.133, 0.151, 0.11]
[2m[36m(func pid=62337)[0m rmse: 0.17775538563728333
[2m[36m(func pid=62337)[0m mae:  0.13010480999946594
[2m[36m(func pid=62337)[0m rmse_per_class: [0.116, 0.264, 0.123, 0.34, 0.082, 0.188, 0.281, 0.135, 0.15, 0.099]
== Status ==
Current time: 2024-01-07 16:11:28 (running for 00:42:42.63)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15800000727176666
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.4   |  0.172 |                   64 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.508 |  0.179 |                    2 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 16:11:34 (running for 00:42:47.82)
Memory usage on this node: 20.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15800000727176666
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.408 |  0.173 |                   65 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.508 |  0.179 |                    2 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=63397)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=63397)[0m Configuration completed!
[2m[36m(func pid=63397)[0m New optimizer parameters:
[2m[36m(func pid=63397)[0m SGD (
[2m[36m(func pid=63397)[0m Parameter Group 0
[2m[36m(func pid=63397)[0m     dampening: 0
[2m[36m(func pid=63397)[0m     differentiable: False
[2m[36m(func pid=63397)[0m     foreach: None
[2m[36m(func pid=63397)[0m     lr: 0.1
[2m[36m(func pid=63397)[0m     maximize: False
[2m[36m(func pid=63397)[0m     momentum: 0.9
[2m[36m(func pid=63397)[0m     nesterov: False
[2m[36m(func pid=63397)[0m     weight_decay: 1e-05
[2m[36m(func pid=63397)[0m )
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4061 | Steps: 4 | Val loss: 0.3140 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4781 | Steps: 4 | Val loss: 0.3178 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.6535 | Steps: 4 | Val loss: 0.3640 | Batch size: 32 | lr: 0.1 | Duration: 4.84s
== Status ==
Current time: 2024-01-07 16:11:39 (running for 00:42:52.83)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15800000727176666
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.408 |  0.173 |                   65 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.459 |  0.178 |                    3 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=47987)[0m rmse: 0.17250144481658936
[2m[36m(func pid=47987)[0m mae:  0.1259554922580719
[2m[36m(func pid=47987)[0m rmse_per_class: [0.12, 0.257, 0.095, 0.338, 0.063, 0.183, 0.275, 0.133, 0.15, 0.11]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.17519183456897736
[2m[36m(func pid=62337)[0m mae:  0.12762081623077393
[2m[36m(func pid=62337)[0m rmse_per_class: [0.116, 0.262, 0.12, 0.34, 0.07, 0.186, 0.273, 0.136, 0.152, 0.096]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.17850805819034576
[2m[36m(func pid=63397)[0m mae:  0.12987728416919708
[2m[36m(func pid=63397)[0m rmse_per_class: [0.12, 0.266, 0.122, 0.345, 0.079, 0.189, 0.277, 0.139, 0.149, 0.1]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4910 | Steps: 4 | Val loss: 0.3154 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4008 | Steps: 4 | Val loss: 0.3127 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.7058 | Steps: 4 | Val loss: 0.3248 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 16:11:44 (running for 00:42:58.67)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15800000727176666
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.406 |  0.173 |                   66 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.491 |  0.174 |                    5 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.654 |  0.179 |                    1 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.17380289733409882
[2m[36m(func pid=62337)[0m mae:  0.1261259764432907
[2m[36m(func pid=62337)[0m rmse_per_class: [0.119, 0.26, 0.118, 0.342, 0.062, 0.184, 0.269, 0.137, 0.152, 0.095]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17156608402729034
[2m[36m(func pid=47987)[0m mae:  0.12539322674274445
[2m[36m(func pid=47987)[0m rmse_per_class: [0.116, 0.257, 0.091, 0.337, 0.064, 0.183, 0.274, 0.133, 0.149, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.1763239949941635
[2m[36m(func pid=63397)[0m mae:  0.1243249922990799
[2m[36m(func pid=63397)[0m rmse_per_class: [0.125, 0.273, 0.094, 0.35, 0.055, 0.187, 0.281, 0.144, 0.161, 0.093]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4072 | Steps: 4 | Val loss: 0.3127 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4957 | Steps: 4 | Val loss: 0.3136 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.5999 | Steps: 4 | Val loss: 0.3502 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 16:11:50 (running for 00:43:04.04)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15800000727176666
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.401 |  0.172 |                   67 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.496 |  0.173 |                    6 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.706 |  0.176 |                    2 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.17287850379943848
[2m[36m(func pid=62337)[0m mae:  0.12513162195682526
[2m[36m(func pid=62337)[0m rmse_per_class: [0.127, 0.257, 0.112, 0.341, 0.057, 0.183, 0.267, 0.139, 0.152, 0.095]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17194294929504395
[2m[36m(func pid=47987)[0m mae:  0.1253773272037506
[2m[36m(func pid=47987)[0m rmse_per_class: [0.117, 0.257, 0.096, 0.338, 0.064, 0.183, 0.273, 0.133, 0.149, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.18091866374015808
[2m[36m(func pid=63397)[0m mae:  0.13075856864452362
[2m[36m(func pid=63397)[0m rmse_per_class: [0.134, 0.26, 0.078, 0.345, 0.056, 0.182, 0.27, 0.147, 0.172, 0.165]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4022 | Steps: 4 | Val loss: 0.3119 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4383 | Steps: 4 | Val loss: 0.3069 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.5069 | Steps: 4 | Val loss: 0.3426 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=62337)[0m rmse: 0.1705673187971115
[2m[36m(func pid=62337)[0m mae:  0.12317259609699249
[2m[36m(func pid=62337)[0m rmse_per_class: [0.133, 0.254, 0.099, 0.333, 0.055, 0.183, 0.266, 0.138, 0.148, 0.096]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:11:55 (running for 00:43:09.62)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15800000727176666
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.407 |  0.172 |                   68 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.438 |  0.171 |                    7 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.6   |  0.181 |                    3 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=47987)[0m rmse: 0.171116441488266
[2m[36m(func pid=47987)[0m mae:  0.12494373321533203
[2m[36m(func pid=47987)[0m rmse_per_class: [0.112, 0.257, 0.092, 0.337, 0.065, 0.183, 0.272, 0.133, 0.148, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.16416552662849426
[2m[36m(func pid=63397)[0m mae:  0.11534140259027481
[2m[36m(func pid=63397)[0m rmse_per_class: [0.101, 0.263, 0.046, 0.3, 0.056, 0.192, 0.301, 0.126, 0.135, 0.122]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4315 | Steps: 4 | Val loss: 0.3053 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4001 | Steps: 4 | Val loss: 0.3122 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4753 | Steps: 4 | Val loss: 0.3118 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 16:12:01 (running for 00:43:15.11)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15800000727176666
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.4   |  0.171 |                   70 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.438 |  0.171 |                    7 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.507 |  0.164 |                    4 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=47987)[0m rmse: 0.17143572866916656
[2m[36m(func pid=47987)[0m mae:  0.12503063678741455
[2m[36m(func pid=47987)[0m rmse_per_class: [0.113, 0.257, 0.094, 0.338, 0.064, 0.183, 0.272, 0.133, 0.149, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.16891981661319733
[2m[36m(func pid=62337)[0m mae:  0.12166635692119598
[2m[36m(func pid=62337)[0m rmse_per_class: [0.14, 0.253, 0.085, 0.328, 0.055, 0.184, 0.265, 0.138, 0.147, 0.096]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.17587825655937195
[2m[36m(func pid=63397)[0m mae:  0.11926120519638062
[2m[36m(func pid=63397)[0m rmse_per_class: [0.099, 0.253, 0.045, 0.332, 0.055, 0.178, 0.267, 0.304, 0.133, 0.09]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4253 | Steps: 4 | Val loss: 0.3032 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3996 | Steps: 4 | Val loss: 0.3125 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5298 | Steps: 4 | Val loss: 0.3220 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 16:12:07 (running for 00:43:20.75)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15800000727176666
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.4   |  0.171 |                   70 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.425 |  0.165 |                    9 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.475 |  0.176 |                    5 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=47987)[0m rmse: 0.17172548174858093
[2m[36m(func pid=47987)[0m mae:  0.12509587407112122
[2m[36m(func pid=47987)[0m rmse_per_class: [0.114, 0.258, 0.095, 0.338, 0.064, 0.183, 0.271, 0.133, 0.15, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.16535067558288574
[2m[36m(func pid=62337)[0m mae:  0.11923154443502426
[2m[36m(func pid=62337)[0m rmse_per_class: [0.125, 0.252, 0.075, 0.317, 0.054, 0.182, 0.267, 0.135, 0.146, 0.099]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.1718066930770874
[2m[36m(func pid=63397)[0m mae:  0.12065259367227554
[2m[36m(func pid=63397)[0m rmse_per_class: [0.098, 0.27, 0.061, 0.363, 0.054, 0.216, 0.254, 0.121, 0.17, 0.111]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4088 | Steps: 4 | Val loss: 0.3129 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4132 | Steps: 4 | Val loss: 0.3025 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4224 | Steps: 4 | Val loss: 0.3369 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 16:12:12 (running for 00:43:26.05)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15800000727176666
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.4   |  0.172 |                   71 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.413 |  0.164 |                   10 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.53  |  0.172 |                    6 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.16420723497867584
[2m[36m(func pid=62337)[0m mae:  0.11886487901210785
[2m[36m(func pid=62337)[0m rmse_per_class: [0.107, 0.253, 0.071, 0.314, 0.054, 0.181, 0.27, 0.132, 0.153, 0.106]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.1719791740179062
[2m[36m(func pid=47987)[0m mae:  0.12520691752433777
[2m[36m(func pid=47987)[0m rmse_per_class: [0.115, 0.258, 0.096, 0.339, 0.063, 0.183, 0.271, 0.133, 0.151, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.1842353790998459
[2m[36m(func pid=63397)[0m mae:  0.12827715277671814
[2m[36m(func pid=63397)[0m rmse_per_class: [0.096, 0.238, 0.046, 0.352, 0.17, 0.185, 0.289, 0.119, 0.133, 0.214]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4022 | Steps: 4 | Val loss: 0.3126 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4011 | Steps: 4 | Val loss: 0.3048 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4022 | Steps: 4 | Val loss: 0.3019 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=47987)[0m rmse: 0.1716160774230957
[2m[36m(func pid=47987)[0m mae:  0.12498112767934799
[2m[36m(func pid=47987)[0m rmse_per_class: [0.114, 0.258, 0.094, 0.339, 0.063, 0.182, 0.271, 0.133, 0.15, 0.112]
[2m[36m(func pid=47987)[0m 
== Status ==
Current time: 2024-01-07 16:12:17 (running for 00:43:31.35)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15800000727176666
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.402 |  0.172 |                   73 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.413 |  0.164 |                   10 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.422 |  0.184 |                    7 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.1660711020231247
[2m[36m(func pid=62337)[0m mae:  0.12041521072387695
[2m[36m(func pid=62337)[0m rmse_per_class: [0.1, 0.253, 0.073, 0.319, 0.054, 0.18, 0.276, 0.135, 0.154, 0.116]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.16400901973247528
[2m[36m(func pid=63397)[0m mae:  0.11213773488998413
[2m[36m(func pid=63397)[0m rmse_per_class: [0.097, 0.238, 0.043, 0.295, 0.184, 0.175, 0.275, 0.11, 0.133, 0.09]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4090 | Steps: 4 | Val loss: 0.3099 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4003 | Steps: 4 | Val loss: 0.3125 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4148 | Steps: 4 | Val loss: 0.2939 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 16:12:23 (running for 00:43:36.82)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15800000727176666
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.402 |  0.172 |                   73 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.409 |  0.17  |                   12 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.402 |  0.164 |                    8 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=47987)[0m rmse: 0.17168472707271576
[2m[36m(func pid=47987)[0m mae:  0.12490832805633545
[2m[36m(func pid=47987)[0m rmse_per_class: [0.116, 0.258, 0.094, 0.339, 0.063, 0.182, 0.27, 0.133, 0.15, 0.111]
[2m[36m(func pid=47987)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.17049431800842285
[2m[36m(func pid=62337)[0m mae:  0.12323155254125595
[2m[36m(func pid=62337)[0m rmse_per_class: [0.099, 0.254, 0.088, 0.328, 0.054, 0.18, 0.282, 0.142, 0.152, 0.126]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.15940679609775543
[2m[36m(func pid=63397)[0m mae:  0.11249574273824692
[2m[36m(func pid=63397)[0m rmse_per_class: [0.106, 0.233, 0.041, 0.34, 0.057, 0.176, 0.284, 0.107, 0.162, 0.09]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=47987)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4020 | Steps: 4 | Val loss: 0.3118 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4036 | Steps: 4 | Val loss: 0.3125 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3968 | Steps: 4 | Val loss: 0.2970 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 16:12:28 (running for 00:43:42.33)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: -0.15800000727176666
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00021 | RUNNING    | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.4   |  0.172 |                   74 |
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.404 |  0.173 |                   13 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.415 |  0.159 |                    9 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.17301778495311737
[2m[36m(func pid=62337)[0m mae:  0.12507353723049164
[2m[36m(func pid=62337)[0m rmse_per_class: [0.1, 0.254, 0.088, 0.334, 0.055, 0.179, 0.286, 0.146, 0.151, 0.137]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=47987)[0m rmse: 0.17130519449710846
[2m[36m(func pid=47987)[0m mae:  0.12458308041095734
[2m[36m(func pid=47987)[0m rmse_per_class: [0.117, 0.258, 0.091, 0.339, 0.063, 0.182, 0.27, 0.133, 0.149, 0.111]
[2m[36m(func pid=63397)[0m rmse: 0.15809838473796844
[2m[36m(func pid=63397)[0m mae:  0.11001546680927277
[2m[36m(func pid=63397)[0m rmse_per_class: [0.095, 0.231, 0.052, 0.354, 0.053, 0.171, 0.255, 0.104, 0.136, 0.131]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3964 | Steps: 4 | Val loss: 0.3047 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3909 | Steps: 4 | Val loss: 0.2767 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 16:12:34 (running for 00:43:47.78)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.396 |  0.169 |                   14 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.397 |  0.158 |                   10 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.1690683513879776
[2m[36m(func pid=62337)[0m mae:  0.1224018782377243
[2m[36m(func pid=62337)[0m rmse_per_class: [0.104, 0.252, 0.072, 0.328, 0.056, 0.18, 0.28, 0.146, 0.144, 0.13]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.15350784361362457
[2m[36m(func pid=63397)[0m mae:  0.10742951929569244
[2m[36m(func pid=63397)[0m rmse_per_class: [0.099, 0.244, 0.049, 0.293, 0.053, 0.169, 0.243, 0.105, 0.148, 0.131]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3941 | Steps: 4 | Val loss: 0.2983 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3904 | Steps: 4 | Val loss: 0.2854 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=62337)[0m rmse: 0.16616520285606384
[2m[36m(func pid=62337)[0m mae:  0.11977583169937134
[2m[36m(func pid=62337)[0m rmse_per_class: [0.11, 0.25, 0.075, 0.322, 0.057, 0.18, 0.274, 0.137, 0.14, 0.116]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:12:39 (running for 00:43:53.17)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.394 |  0.166 |                   15 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.391 |  0.154 |                   11 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15863201022148132
[2m[36m(func pid=63397)[0m mae:  0.11158289760351181
[2m[36m(func pid=63397)[0m rmse_per_class: [0.105, 0.224, 0.054, 0.301, 0.085, 0.176, 0.284, 0.101, 0.155, 0.101]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3884 | Steps: 4 | Val loss: 0.2953 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3706 | Steps: 4 | Val loss: 0.2975 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 16:12:44 (running for 00:43:58.52)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.388 |  0.165 |                   16 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.39  |  0.159 |                   12 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.1648399829864502
[2m[36m(func pid=62337)[0m mae:  0.11857639253139496
[2m[36m(func pid=62337)[0m rmse_per_class: [0.109, 0.248, 0.08, 0.319, 0.058, 0.18, 0.272, 0.131, 0.14, 0.11]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.1654982715845108
[2m[36m(func pid=63397)[0m mae:  0.11312854290008545
[2m[36m(func pid=63397)[0m rmse_per_class: [0.103, 0.224, 0.048, 0.344, 0.149, 0.18, 0.238, 0.102, 0.132, 0.135]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3777 | Steps: 4 | Val loss: 0.2926 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3682 | Steps: 4 | Val loss: 0.2838 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 16:12:50 (running for 00:44:04.10)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.378 |  0.163 |                   17 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.371 |  0.165 |                   13 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.16310694813728333
[2m[36m(func pid=62337)[0m mae:  0.11754246056079865
[2m[36m(func pid=62337)[0m rmse_per_class: [0.107, 0.248, 0.072, 0.314, 0.059, 0.18, 0.27, 0.129, 0.145, 0.107]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.1580403745174408
[2m[36m(func pid=63397)[0m mae:  0.10807044804096222
[2m[36m(func pid=63397)[0m rmse_per_class: [0.098, 0.243, 0.037, 0.311, 0.104, 0.173, 0.246, 0.133, 0.133, 0.103]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3899 | Steps: 4 | Val loss: 0.2909 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3781 | Steps: 4 | Val loss: 0.2866 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 16:12:55 (running for 00:44:09.51)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.39  |  0.162 |                   18 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.368 |  0.158 |                   14 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.16169968247413635
[2m[36m(func pid=62337)[0m mae:  0.11653269827365875
[2m[36m(func pid=62337)[0m rmse_per_class: [0.112, 0.247, 0.059, 0.315, 0.061, 0.179, 0.265, 0.129, 0.145, 0.105]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.15972287952899933
[2m[36m(func pid=63397)[0m mae:  0.11132081598043442
[2m[36m(func pid=63397)[0m rmse_per_class: [0.094, 0.233, 0.08, 0.293, 0.059, 0.168, 0.276, 0.108, 0.169, 0.117]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3925 | Steps: 4 | Val loss: 0.2899 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3524 | Steps: 4 | Val loss: 0.2751 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 16:13:01 (running for 00:44:15.14)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.392 |  0.161 |                   19 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.378 |  0.16  |                   15 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.16105258464813232
[2m[36m(func pid=62337)[0m mae:  0.11575225740671158
[2m[36m(func pid=62337)[0m rmse_per_class: [0.111, 0.248, 0.055, 0.317, 0.064, 0.178, 0.261, 0.128, 0.144, 0.105]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.14995916187763214
[2m[36m(func pid=63397)[0m mae:  0.10154926776885986
[2m[36m(func pid=63397)[0m rmse_per_class: [0.113, 0.223, 0.044, 0.317, 0.053, 0.182, 0.23, 0.099, 0.132, 0.107]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3907 | Steps: 4 | Val loss: 0.2906 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3658 | Steps: 4 | Val loss: 0.2740 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 16:13:06 (running for 00:44:20.54)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.391 |  0.161 |                   20 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.352 |  0.15  |                   16 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.16139744222164154
[2m[36m(func pid=62337)[0m mae:  0.11582845449447632
[2m[36m(func pid=62337)[0m rmse_per_class: [0.104, 0.249, 0.058, 0.32, 0.068, 0.177, 0.259, 0.128, 0.143, 0.108]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.149516761302948
[2m[36m(func pid=63397)[0m mae:  0.10347254574298859
[2m[36m(func pid=63397)[0m rmse_per_class: [0.107, 0.21, 0.045, 0.277, 0.055, 0.168, 0.267, 0.105, 0.137, 0.124]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3820 | Steps: 4 | Val loss: 0.2904 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3511 | Steps: 4 | Val loss: 0.2742 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=62337)[0m rmse: 0.16125738620758057
[2m[36m(func pid=62337)[0m mae:  0.11585142463445663
[2m[36m(func pid=62337)[0m rmse_per_class: [0.098, 0.247, 0.055, 0.32, 0.075, 0.177, 0.261, 0.127, 0.14, 0.112]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:13:12 (running for 00:44:26.03)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.382 |  0.161 |                   21 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.366 |  0.15  |                   17 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15337005257606506
[2m[36m(func pid=63397)[0m mae:  0.1051357164978981
[2m[36m(func pid=63397)[0m rmse_per_class: [0.095, 0.222, 0.065, 0.3, 0.063, 0.177, 0.248, 0.101, 0.135, 0.128]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3771 | Steps: 4 | Val loss: 0.2926 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3528 | Steps: 4 | Val loss: 0.3029 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 16:13:17 (running for 00:44:31.59)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.377 |  0.163 |                   22 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.351 |  0.153 |                   18 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.16316048800945282
[2m[36m(func pid=62337)[0m mae:  0.11706842482089996
[2m[36m(func pid=62337)[0m rmse_per_class: [0.097, 0.247, 0.063, 0.321, 0.079, 0.178, 0.264, 0.128, 0.138, 0.117]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.16036655008792877
[2m[36m(func pid=63397)[0m mae:  0.11161087453365326
[2m[36m(func pid=63397)[0m rmse_per_class: [0.107, 0.218, 0.037, 0.357, 0.081, 0.168, 0.261, 0.115, 0.148, 0.111]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3829 | Steps: 4 | Val loss: 0.2945 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3609 | Steps: 4 | Val loss: 0.2934 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=62337)[0m rmse: 0.16481855511665344
[2m[36m(func pid=62337)[0m mae:  0.11788676679134369
[2m[36m(func pid=62337)[0m rmse_per_class: [0.098, 0.248, 0.071, 0.322, 0.082, 0.178, 0.267, 0.13, 0.137, 0.115]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:13:23 (running for 00:44:37.32)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.383 |  0.165 |                   23 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.353 |  0.16  |                   19 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.16521646082401276
[2m[36m(func pid=63397)[0m mae:  0.11331143230199814
[2m[36m(func pid=63397)[0m rmse_per_class: [0.1, 0.217, 0.061, 0.299, 0.121, 0.17, 0.279, 0.095, 0.133, 0.177]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3839 | Steps: 4 | Val loss: 0.2923 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3841 | Steps: 4 | Val loss: 0.2726 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 16:13:29 (running for 00:44:42.81)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.384 |  0.163 |                   24 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.361 |  0.165 |                   20 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.16348665952682495
[2m[36m(func pid=62337)[0m mae:  0.11709669977426529
[2m[36m(func pid=62337)[0m rmse_per_class: [0.099, 0.248, 0.065, 0.315, 0.08, 0.178, 0.269, 0.133, 0.137, 0.112]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.1545582264661789
[2m[36m(func pid=63397)[0m mae:  0.10338713228702545
[2m[36m(func pid=63397)[0m rmse_per_class: [0.102, 0.223, 0.074, 0.278, 0.087, 0.167, 0.248, 0.1, 0.132, 0.135]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3720 | Steps: 4 | Val loss: 0.2927 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3557 | Steps: 4 | Val loss: 0.2908 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=62337)[0m rmse: 0.16329924762248993
[2m[36m(func pid=62337)[0m mae:  0.11749991029500961
[2m[36m(func pid=62337)[0m rmse_per_class: [0.099, 0.249, 0.058, 0.316, 0.077, 0.178, 0.269, 0.132, 0.14, 0.115]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:13:34 (running for 00:44:48.44)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.372 |  0.163 |                   25 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.384 |  0.155 |                   21 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15780135989189148
[2m[36m(func pid=63397)[0m mae:  0.10986205190420151
[2m[36m(func pid=63397)[0m rmse_per_class: [0.091, 0.217, 0.043, 0.325, 0.071, 0.168, 0.273, 0.096, 0.154, 0.139]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3758 | Steps: 4 | Val loss: 0.2942 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3655 | Steps: 4 | Val loss: 0.2794 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 16:13:40 (running for 00:44:54.06)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.376 |  0.164 |                   26 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.356 |  0.158 |                   22 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.1640605330467224
[2m[36m(func pid=62337)[0m mae:  0.11809442192316055
[2m[36m(func pid=62337)[0m rmse_per_class: [0.101, 0.249, 0.059, 0.321, 0.072, 0.177, 0.267, 0.129, 0.142, 0.124]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.15454325079917908
[2m[36m(func pid=63397)[0m mae:  0.10457024723291397
[2m[36m(func pid=63397)[0m rmse_per_class: [0.13, 0.205, 0.053, 0.293, 0.081, 0.189, 0.248, 0.1, 0.144, 0.103]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3837 | Steps: 4 | Val loss: 0.2955 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3657 | Steps: 4 | Val loss: 0.2647 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 16:13:45 (running for 00:44:59.44)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.384 |  0.165 |                   27 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.365 |  0.155 |                   23 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.1649855673313141
[2m[36m(func pid=62337)[0m mae:  0.11866775900125504
[2m[36m(func pid=62337)[0m rmse_per_class: [0.102, 0.249, 0.056, 0.325, 0.069, 0.176, 0.266, 0.126, 0.141, 0.14]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.14560467004776
[2m[36m(func pid=63397)[0m mae:  0.09651356935501099
[2m[36m(func pid=63397)[0m rmse_per_class: [0.132, 0.204, 0.042, 0.271, 0.079, 0.167, 0.238, 0.096, 0.132, 0.094]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3721 | Steps: 4 | Val loss: 0.2908 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3564 | Steps: 4 | Val loss: 0.3026 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 16:13:51 (running for 00:45:04.80)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.372 |  0.162 |                   28 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.366 |  0.146 |                   24 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.16237927973270416
[2m[36m(func pid=62337)[0m mae:  0.1168571338057518
[2m[36m(func pid=62337)[0m rmse_per_class: [0.101, 0.244, 0.05, 0.315, 0.067, 0.176, 0.269, 0.124, 0.138, 0.139]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.16600549221038818
[2m[36m(func pid=63397)[0m mae:  0.11587591469287872
[2m[36m(func pid=63397)[0m rmse_per_class: [0.105, 0.212, 0.036, 0.341, 0.067, 0.178, 0.28, 0.097, 0.166, 0.178]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3816 | Steps: 4 | Val loss: 0.2875 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3487 | Steps: 4 | Val loss: 0.2681 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 16:13:56 (running for 00:45:10.41)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.382 |  0.16  |                   29 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.356 |  0.166 |                   25 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.16039545834064484
[2m[36m(func pid=62337)[0m mae:  0.11524181067943573
[2m[36m(func pid=62337)[0m rmse_per_class: [0.101, 0.243, 0.049, 0.31, 0.067, 0.176, 0.267, 0.124, 0.136, 0.132]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.14864978194236755
[2m[36m(func pid=63397)[0m mae:  0.10035146772861481
[2m[36m(func pid=63397)[0m rmse_per_class: [0.098, 0.216, 0.046, 0.275, 0.082, 0.173, 0.241, 0.113, 0.148, 0.094]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3844 | Steps: 4 | Val loss: 0.2881 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3546 | Steps: 4 | Val loss: 0.2695 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 16:14:02 (running for 00:45:15.86)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.384 |  0.161 |                   30 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.349 |  0.149 |                   26 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.16068598628044128
[2m[36m(func pid=62337)[0m mae:  0.11530021578073502
[2m[36m(func pid=62337)[0m rmse_per_class: [0.101, 0.242, 0.056, 0.314, 0.067, 0.175, 0.265, 0.123, 0.138, 0.125]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.14957545697689056
[2m[36m(func pid=63397)[0m mae:  0.10028804838657379
[2m[36m(func pid=63397)[0m rmse_per_class: [0.092, 0.209, 0.044, 0.29, 0.12, 0.172, 0.235, 0.1, 0.142, 0.093]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3721 | Steps: 4 | Val loss: 0.2893 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3530 | Steps: 4 | Val loss: 0.2850 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 16:14:07 (running for 00:45:21.39)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.384 |  0.161 |                   30 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.355 |  0.15  |                   27 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.1610005497932434
[2m[36m(func pid=62337)[0m mae:  0.11544313281774521
[2m[36m(func pid=62337)[0m rmse_per_class: [0.099, 0.241, 0.06, 0.321, 0.069, 0.175, 0.262, 0.123, 0.14, 0.12]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.15858711302280426
[2m[36m(func pid=63397)[0m mae:  0.10806518793106079
[2m[36m(func pid=63397)[0m rmse_per_class: [0.104, 0.213, 0.049, 0.323, 0.094, 0.166, 0.249, 0.097, 0.136, 0.156]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3692 | Steps: 4 | Val loss: 0.2914 | Batch size: 32 | lr: 0.01 | Duration: 3.24s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3653 | Steps: 4 | Val loss: 0.2814 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 16:14:13 (running for 00:45:26.95)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.372 |  0.161 |                   31 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.353 |  0.159 |                   28 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.16139726340770721
[2m[36m(func pid=62337)[0m mae:  0.11604976654052734
[2m[36m(func pid=62337)[0m rmse_per_class: [0.101, 0.24, 0.056, 0.328, 0.069, 0.175, 0.261, 0.123, 0.143, 0.117]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.15616309642791748
[2m[36m(func pid=63397)[0m mae:  0.1038854569196701
[2m[36m(func pid=63397)[0m rmse_per_class: [0.109, 0.21, 0.075, 0.289, 0.093, 0.173, 0.279, 0.092, 0.131, 0.111]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3718 | Steps: 4 | Val loss: 0.2917 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3496 | Steps: 4 | Val loss: 0.2713 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 16:14:18 (running for 00:45:32.54)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.369 |  0.161 |                   32 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.365 |  0.156 |                   29 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.16094538569450378
[2m[36m(func pid=62337)[0m mae:  0.11578883975744247
[2m[36m(func pid=62337)[0m rmse_per_class: [0.104, 0.239, 0.05, 0.332, 0.068, 0.175, 0.26, 0.123, 0.141, 0.116]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.15056288242340088
[2m[36m(func pid=63397)[0m mae:  0.10084602981805801
[2m[36m(func pid=63397)[0m rmse_per_class: [0.099, 0.214, 0.055, 0.294, 0.112, 0.172, 0.221, 0.094, 0.156, 0.088]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3460 | Steps: 4 | Val loss: 0.2762 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3688 | Steps: 4 | Val loss: 0.2897 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 16:14:24 (running for 00:45:37.92)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.372 |  0.161 |                   33 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.35  |  0.151 |                   30 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15360507369041443
[2m[36m(func pid=63397)[0m mae:  0.10401248931884766
[2m[36m(func pid=63397)[0m rmse_per_class: [0.112, 0.206, 0.038, 0.308, 0.11, 0.164, 0.252, 0.094, 0.133, 0.119]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.16013199090957642
[2m[36m(func pid=62337)[0m mae:  0.11481855064630508
[2m[36m(func pid=62337)[0m rmse_per_class: [0.108, 0.239, 0.047, 0.329, 0.067, 0.175, 0.26, 0.123, 0.139, 0.114]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3521 | Steps: 4 | Val loss: 0.2860 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3647 | Steps: 4 | Val loss: 0.2887 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 16:14:29 (running for 00:45:43.42)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.369 |  0.16  |                   34 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.346 |  0.154 |                   31 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.16026932001113892
[2m[36m(func pid=62337)[0m mae:  0.11456598341464996
[2m[36m(func pid=62337)[0m rmse_per_class: [0.117, 0.24, 0.047, 0.323, 0.066, 0.175, 0.263, 0.123, 0.138, 0.11]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.15592405200004578
[2m[36m(func pid=63397)[0m mae:  0.10403436422348022
[2m[36m(func pid=63397)[0m rmse_per_class: [0.093, 0.213, 0.064, 0.325, 0.08, 0.188, 0.265, 0.099, 0.134, 0.099]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3781 | Steps: 4 | Val loss: 0.2873 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3602 | Steps: 4 | Val loss: 0.2619 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 16:14:35 (running for 00:45:48.77)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.365 |  0.16  |                   35 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.352 |  0.156 |                   32 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.15959033370018005
[2m[36m(func pid=62337)[0m mae:  0.1139267235994339
[2m[36m(func pid=62337)[0m rmse_per_class: [0.115, 0.239, 0.049, 0.32, 0.065, 0.174, 0.262, 0.123, 0.138, 0.11]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.14402605593204498
[2m[36m(func pid=63397)[0m mae:  0.09665430337190628
[2m[36m(func pid=63397)[0m rmse_per_class: [0.091, 0.223, 0.036, 0.26, 0.099, 0.161, 0.239, 0.094, 0.148, 0.088]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4011 | Steps: 4 | Val loss: 0.2901 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3726 | Steps: 4 | Val loss: 0.2835 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 16:14:40 (running for 00:45:54.17)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.378 |  0.16  |                   36 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.36  |  0.144 |                   33 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.15722326934337616
[2m[36m(func pid=62337)[0m mae:  0.11234088242053986
[2m[36m(func pid=62337)[0m rmse_per_class: [0.104, 0.237, 0.05, 0.313, 0.066, 0.174, 0.263, 0.121, 0.137, 0.106]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.16242548823356628
[2m[36m(func pid=63397)[0m mae:  0.10979761928319931
[2m[36m(func pid=63397)[0m rmse_per_class: [0.132, 0.23, 0.044, 0.309, 0.118, 0.165, 0.257, 0.105, 0.14, 0.125]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3631 | Steps: 4 | Val loss: 0.3342 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3742 | Steps: 4 | Val loss: 0.2825 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 16:14:45 (running for 00:45:59.67)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.373 |  0.157 |                   37 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.401 |  0.162 |                   34 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.15672960877418518
[2m[36m(func pid=62337)[0m mae:  0.11183314025402069
[2m[36m(func pid=62337)[0m rmse_per_class: [0.102, 0.236, 0.05, 0.312, 0.07, 0.175, 0.259, 0.12, 0.137, 0.107]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.16452443599700928
[2m[36m(func pid=63397)[0m mae:  0.10982097685337067
[2m[36m(func pid=63397)[0m rmse_per_class: [0.096, 0.208, 0.078, 0.379, 0.101, 0.192, 0.252, 0.109, 0.132, 0.099]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3757 | Steps: 4 | Val loss: 0.2850 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3612 | Steps: 4 | Val loss: 0.2982 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 16:14:51 (running for 00:46:05.35)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.374 |  0.157 |                   38 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.363 |  0.165 |                   35 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.15858697891235352
[2m[36m(func pid=62337)[0m mae:  0.1129431501030922
[2m[36m(func pid=62337)[0m rmse_per_class: [0.101, 0.237, 0.056, 0.317, 0.072, 0.175, 0.256, 0.12, 0.137, 0.114]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.1622113287448883
[2m[36m(func pid=63397)[0m mae:  0.11109130084514618
[2m[36m(func pid=63397)[0m rmse_per_class: [0.135, 0.205, 0.036, 0.336, 0.078, 0.17, 0.266, 0.114, 0.152, 0.131]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3678 | Steps: 4 | Val loss: 0.2890 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3741 | Steps: 4 | Val loss: 0.2622 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 16:14:57 (running for 00:46:10.85)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.376 |  0.159 |                   39 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.361 |  0.162 |                   36 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.1432061344385147
[2m[36m(func pid=63397)[0m mae:  0.09579210728406906
[2m[36m(func pid=63397)[0m rmse_per_class: [0.093, 0.227, 0.037, 0.265, 0.06, 0.162, 0.226, 0.112, 0.152, 0.098]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.1608014553785324
[2m[36m(func pid=62337)[0m mae:  0.11478863656520844
[2m[36m(func pid=62337)[0m rmse_per_class: [0.099, 0.237, 0.055, 0.325, 0.074, 0.176, 0.255, 0.121, 0.138, 0.127]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3420 | Steps: 4 | Val loss: 0.2653 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3789 | Steps: 4 | Val loss: 0.2948 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 16:15:02 (running for 00:46:16.05)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.368 |  0.161 |                   40 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.374 |  0.143 |                   37 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.16409218311309814
[2m[36m(func pid=62337)[0m mae:  0.11730591207742691
[2m[36m(func pid=62337)[0m rmse_per_class: [0.102, 0.238, 0.058, 0.332, 0.074, 0.177, 0.259, 0.12, 0.14, 0.142]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.14689840376377106
[2m[36m(func pid=63397)[0m mae:  0.09684737026691437
[2m[36m(func pid=63397)[0m rmse_per_class: [0.1, 0.206, 0.058, 0.268, 0.087, 0.167, 0.262, 0.105, 0.129, 0.086]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3773 | Steps: 4 | Val loss: 0.2945 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3469 | Steps: 4 | Val loss: 0.2833 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 16:15:07 (running for 00:46:21.55)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.379 |  0.164 |                   41 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.342 |  0.147 |                   38 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.1579691469669342
[2m[36m(func pid=63397)[0m mae:  0.10627458244562149
[2m[36m(func pid=63397)[0m rmse_per_class: [0.119, 0.208, 0.034, 0.293, 0.106, 0.177, 0.266, 0.102, 0.132, 0.143]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.16329553723335266
[2m[36m(func pid=62337)[0m mae:  0.11670833826065063
[2m[36m(func pid=62337)[0m rmse_per_class: [0.102, 0.239, 0.051, 0.335, 0.073, 0.175, 0.258, 0.125, 0.14, 0.134]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3593 | Steps: 4 | Val loss: 0.2736 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3732 | Steps: 4 | Val loss: 0.2928 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 16:15:13 (running for 00:46:27.05)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.377 |  0.163 |                   42 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.347 |  0.158 |                   39 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15100452303886414
[2m[36m(func pid=63397)[0m mae:  0.10169360786676407
[2m[36m(func pid=63397)[0m rmse_per_class: [0.1, 0.207, 0.041, 0.292, 0.097, 0.168, 0.227, 0.097, 0.168, 0.113]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.1622665822505951
[2m[36m(func pid=62337)[0m mae:  0.11594165861606598
[2m[36m(func pid=62337)[0m rmse_per_class: [0.108, 0.24, 0.05, 0.334, 0.078, 0.175, 0.258, 0.124, 0.14, 0.118]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3349 | Steps: 4 | Val loss: 0.2608 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3722 | Steps: 4 | Val loss: 0.2918 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 16:15:18 (running for 00:46:32.64)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.373 |  0.162 |                   43 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.359 |  0.151 |                   40 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.1433688849210739
[2m[36m(func pid=63397)[0m mae:  0.09484684467315674
[2m[36m(func pid=63397)[0m rmse_per_class: [0.096, 0.214, 0.063, 0.269, 0.076, 0.164, 0.229, 0.097, 0.138, 0.089]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.16181500256061554
[2m[36m(func pid=62337)[0m mae:  0.11516781896352768
[2m[36m(func pid=62337)[0m rmse_per_class: [0.114, 0.24, 0.056, 0.333, 0.075, 0.174, 0.255, 0.12, 0.143, 0.107]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3803 | Steps: 4 | Val loss: 0.2884 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3580 | Steps: 4 | Val loss: 0.2781 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 16:15:24 (running for 00:46:38.11)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.372 |  0.162 |                   44 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.335 |  0.143 |                   41 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.1604273021221161
[2m[36m(func pid=62337)[0m mae:  0.11402909457683563
[2m[36m(func pid=62337)[0m rmse_per_class: [0.112, 0.236, 0.061, 0.325, 0.073, 0.174, 0.26, 0.117, 0.143, 0.104]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.151760533452034
[2m[36m(func pid=63397)[0m mae:  0.10431776195764542
[2m[36m(func pid=63397)[0m rmse_per_class: [0.093, 0.205, 0.031, 0.311, 0.079, 0.163, 0.269, 0.102, 0.162, 0.102]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3707 | Steps: 4 | Val loss: 0.2829 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3509 | Steps: 4 | Val loss: 0.2839 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 16:15:29 (running for 00:46:43.50)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.38  |  0.16  |                   45 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.358 |  0.152 |                   42 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.15717211365699768
[2m[36m(func pid=62337)[0m mae:  0.11171269416809082
[2m[36m(func pid=62337)[0m rmse_per_class: [0.101, 0.236, 0.053, 0.315, 0.075, 0.174, 0.259, 0.118, 0.14, 0.101]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.16070957481861115
[2m[36m(func pid=63397)[0m mae:  0.10829001665115356
[2m[36m(func pid=63397)[0m rmse_per_class: [0.118, 0.211, 0.051, 0.31, 0.105, 0.167, 0.256, 0.094, 0.134, 0.16]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3653 | Steps: 4 | Val loss: 0.2807 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3489 | Steps: 4 | Val loss: 0.2679 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 16:15:35 (running for 00:46:48.98)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.371 |  0.157 |                   46 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.351 |  0.161 |                   43 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.1560164839029312
[2m[36m(func pid=62337)[0m mae:  0.11093199253082275
[2m[36m(func pid=62337)[0m rmse_per_class: [0.1, 0.235, 0.047, 0.307, 0.08, 0.174, 0.262, 0.117, 0.136, 0.102]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.1485653519630432
[2m[36m(func pid=63397)[0m mae:  0.09606771171092987
[2m[36m(func pid=63397)[0m rmse_per_class: [0.101, 0.21, 0.073, 0.271, 0.096, 0.161, 0.238, 0.092, 0.136, 0.107]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3612 | Steps: 4 | Val loss: 0.2800 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3397 | Steps: 4 | Val loss: 0.2841 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 16:15:40 (running for 00:46:54.66)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.365 |  0.156 |                   47 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.349 |  0.149 |                   44 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15655836462974548
[2m[36m(func pid=63397)[0m mae:  0.10261546075344086
[2m[36m(func pid=63397)[0m rmse_per_class: [0.163, 0.227, 0.035, 0.325, 0.078, 0.165, 0.261, 0.09, 0.131, 0.09]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.1560381054878235
[2m[36m(func pid=62337)[0m mae:  0.11072355508804321
[2m[36m(func pid=62337)[0m rmse_per_class: [0.1, 0.236, 0.049, 0.303, 0.081, 0.174, 0.259, 0.117, 0.135, 0.106]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3465 | Steps: 4 | Val loss: 0.2857 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3657 | Steps: 4 | Val loss: 0.2791 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 16:15:46 (running for 00:47:00.18)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.361 |  0.156 |                   48 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.34  |  0.157 |                   45 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.1587534248828888
[2m[36m(func pid=63397)[0m mae:  0.1086135059595108
[2m[36m(func pid=63397)[0m rmse_per_class: [0.092, 0.213, 0.038, 0.304, 0.068, 0.163, 0.255, 0.097, 0.232, 0.125]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15569257736206055
[2m[36m(func pid=62337)[0m mae:  0.10998209565877914
[2m[36m(func pid=62337)[0m rmse_per_class: [0.101, 0.235, 0.054, 0.305, 0.078, 0.175, 0.253, 0.115, 0.135, 0.107]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3619 | Steps: 4 | Val loss: 0.2663 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3652 | Steps: 4 | Val loss: 0.2779 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 16:15:52 (running for 00:47:05.73)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.366 |  0.156 |                   49 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.346 |  0.159 |                   46 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14806556701660156
[2m[36m(func pid=63397)[0m mae:  0.09588021785020828
[2m[36m(func pid=63397)[0m rmse_per_class: [0.108, 0.224, 0.068, 0.268, 0.07, 0.157, 0.234, 0.133, 0.13, 0.088]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15498611330986023
[2m[36m(func pid=62337)[0m mae:  0.10942050069570541
[2m[36m(func pid=62337)[0m rmse_per_class: [0.102, 0.236, 0.053, 0.301, 0.074, 0.175, 0.25, 0.115, 0.135, 0.108]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3698 | Steps: 4 | Val loss: 0.2792 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3368 | Steps: 4 | Val loss: 0.2724 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 16:15:57 (running for 00:47:11.09)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.365 |  0.155 |                   50 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.362 |  0.148 |                   47 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14857041835784912
[2m[36m(func pid=63397)[0m mae:  0.09788510948419571
[2m[36m(func pid=63397)[0m rmse_per_class: [0.112, 0.23, 0.054, 0.297, 0.068, 0.159, 0.254, 0.092, 0.132, 0.088]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15565577149391174
[2m[36m(func pid=62337)[0m mae:  0.11028659343719482
[2m[36m(func pid=62337)[0m rmse_per_class: [0.101, 0.236, 0.05, 0.305, 0.075, 0.174, 0.252, 0.115, 0.137, 0.112]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3613 | Steps: 4 | Val loss: 0.2819 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3343 | Steps: 4 | Val loss: 0.2675 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 16:16:02 (running for 00:47:16.56)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.37  |  0.156 |                   51 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.337 |  0.149 |                   48 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.1570207178592682
[2m[36m(func pid=62337)[0m mae:  0.11173341423273087
[2m[36m(func pid=62337)[0m rmse_per_class: [0.1, 0.236, 0.046, 0.312, 0.079, 0.174, 0.254, 0.115, 0.139, 0.116]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.1477760374546051
[2m[36m(func pid=63397)[0m mae:  0.09950171411037445
[2m[36m(func pid=63397)[0m rmse_per_class: [0.128, 0.203, 0.034, 0.286, 0.071, 0.159, 0.24, 0.098, 0.153, 0.106]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3320 | Steps: 4 | Val loss: 0.2626 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3681 | Steps: 4 | Val loss: 0.2855 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 16:16:08 (running for 00:47:22.09)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.361 |  0.157 |                   52 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.334 |  0.148 |                   49 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14400872588157654
[2m[36m(func pid=63397)[0m mae:  0.09553318470716476
[2m[36m(func pid=63397)[0m rmse_per_class: [0.094, 0.2, 0.05, 0.283, 0.09, 0.16, 0.237, 0.093, 0.132, 0.101]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15849439799785614
[2m[36m(func pid=62337)[0m mae:  0.11326531320810318
[2m[36m(func pid=62337)[0m rmse_per_class: [0.101, 0.234, 0.047, 0.322, 0.076, 0.173, 0.254, 0.114, 0.149, 0.115]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3351 | Steps: 4 | Val loss: 0.2712 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3633 | Steps: 4 | Val loss: 0.2870 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
== Status ==
Current time: 2024-01-07 16:16:14 (running for 00:47:27.68)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.368 |  0.158 |                   53 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.332 |  0.144 |                   50 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15188249945640564
[2m[36m(func pid=63397)[0m mae:  0.10028048604726791
[2m[36m(func pid=63397)[0m rmse_per_class: [0.1, 0.206, 0.063, 0.299, 0.123, 0.158, 0.244, 0.099, 0.134, 0.092]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15833477675914764
[2m[36m(func pid=62337)[0m mae:  0.11346013844013214
[2m[36m(func pid=62337)[0m rmse_per_class: [0.099, 0.23, 0.044, 0.329, 0.073, 0.173, 0.257, 0.114, 0.146, 0.119]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3388 | Steps: 4 | Val loss: 0.2623 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3693 | Steps: 4 | Val loss: 0.2874 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 16:16:19 (running for 00:47:33.26)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.363 |  0.158 |                   54 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.339 |  0.144 |                   52 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14444604516029358
[2m[36m(func pid=63397)[0m mae:  0.09656625986099243
[2m[36m(func pid=63397)[0m rmse_per_class: [0.095, 0.207, 0.032, 0.277, 0.107, 0.16, 0.237, 0.09, 0.133, 0.107]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15783990919589996
[2m[36m(func pid=62337)[0m mae:  0.11294734477996826
[2m[36m(func pid=62337)[0m rmse_per_class: [0.1, 0.229, 0.044, 0.333, 0.071, 0.172, 0.257, 0.113, 0.139, 0.121]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3571 | Steps: 4 | Val loss: 0.2697 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3694 | Steps: 4 | Val loss: 0.2857 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 16:16:24 (running for 00:47:38.60)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.369 |  0.158 |                   55 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.357 |  0.148 |                   53 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14817288517951965
[2m[36m(func pid=63397)[0m mae:  0.099360391497612
[2m[36m(func pid=63397)[0m rmse_per_class: [0.098, 0.206, 0.036, 0.284, 0.08, 0.158, 0.263, 0.108, 0.133, 0.117]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15651710331439972
[2m[36m(func pid=62337)[0m mae:  0.1114269271492958
[2m[36m(func pid=62337)[0m rmse_per_class: [0.098, 0.229, 0.044, 0.334, 0.069, 0.173, 0.251, 0.113, 0.138, 0.116]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3300 | Steps: 4 | Val loss: 0.2896 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3634 | Steps: 4 | Val loss: 0.2855 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 16:16:30 (running for 00:47:44.19)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.369 |  0.157 |                   56 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.33  |  0.158 |                   54 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.1579994112253189
[2m[36m(func pid=63397)[0m mae:  0.10555623471736908
[2m[36m(func pid=63397)[0m rmse_per_class: [0.178, 0.202, 0.068, 0.284, 0.064, 0.154, 0.296, 0.105, 0.131, 0.099]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.1568746268749237
[2m[36m(func pid=62337)[0m mae:  0.1116238608956337
[2m[36m(func pid=62337)[0m rmse_per_class: [0.096, 0.231, 0.045, 0.332, 0.068, 0.175, 0.25, 0.113, 0.149, 0.11]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3497 | Steps: 4 | Val loss: 0.2722 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3650 | Steps: 4 | Val loss: 0.2837 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 16:16:35 (running for 00:47:49.61)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.363 |  0.157 |                   57 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.35  |  0.146 |                   55 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14593443274497986
[2m[36m(func pid=63397)[0m mae:  0.09775284677743912
[2m[36m(func pid=63397)[0m rmse_per_class: [0.091, 0.208, 0.033, 0.319, 0.073, 0.158, 0.224, 0.09, 0.14, 0.123]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.1569511592388153
[2m[36m(func pid=62337)[0m mae:  0.1115480437874794
[2m[36m(func pid=62337)[0m rmse_per_class: [0.096, 0.234, 0.044, 0.323, 0.068, 0.175, 0.253, 0.113, 0.159, 0.104]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3619 | Steps: 4 | Val loss: 0.2659 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3696 | Steps: 4 | Val loss: 0.2820 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=63397)[0m rmse: 0.14757460355758667
[2m[36m(func pid=63397)[0m mae:  0.09794813394546509
[2m[36m(func pid=63397)[0m rmse_per_class: [0.095, 0.217, 0.041, 0.267, 0.109, 0.158, 0.251, 0.09, 0.141, 0.107]
[2m[36m(func pid=63397)[0m 
== Status ==
Current time: 2024-01-07 16:16:41 (running for 00:47:54.99)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.365 |  0.157 |                   58 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.362 |  0.148 |                   56 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m rmse: 0.15619072318077087
[2m[36m(func pid=62337)[0m mae:  0.11134016513824463
[2m[36m(func pid=62337)[0m rmse_per_class: [0.1, 0.231, 0.046, 0.314, 0.067, 0.173, 0.262, 0.112, 0.154, 0.102]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3559 | Steps: 4 | Val loss: 0.2925 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3643 | Steps: 4 | Val loss: 0.2817 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=63397)[0m rmse: 0.16401846706867218
[2m[36m(func pid=63397)[0m mae:  0.11051609367132187
[2m[36m(func pid=63397)[0m rmse_per_class: [0.107, 0.212, 0.057, 0.286, 0.158, 0.161, 0.293, 0.09, 0.188, 0.089]
== Status ==
Current time: 2024-01-07 16:16:46 (running for 00:48:00.27)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.37  |  0.156 |                   59 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.356 |  0.164 |                   57 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.1563173234462738
[2m[36m(func pid=62337)[0m mae:  0.11119811236858368
[2m[36m(func pid=62337)[0m rmse_per_class: [0.107, 0.231, 0.052, 0.311, 0.068, 0.172, 0.263, 0.112, 0.144, 0.104]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3425 | Steps: 4 | Val loss: 0.3284 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3577 | Steps: 4 | Val loss: 0.2823 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 16:16:52 (running for 00:48:05.84)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.364 |  0.156 |                   60 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.342 |  0.157 |                   58 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15695375204086304
[2m[36m(func pid=63397)[0m mae:  0.10488729178905487
[2m[36m(func pid=63397)[0m rmse_per_class: [0.106, 0.207, 0.047, 0.38, 0.125, 0.161, 0.225, 0.091, 0.131, 0.096]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.156753808259964
[2m[36m(func pid=62337)[0m mae:  0.11120565980672836
[2m[36m(func pid=62337)[0m rmse_per_class: [0.114, 0.229, 0.054, 0.312, 0.068, 0.171, 0.263, 0.111, 0.14, 0.106]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3582 | Steps: 4 | Val loss: 0.3214 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3696 | Steps: 4 | Val loss: 0.2795 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 16:16:57 (running for 00:48:11.18)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.358 |  0.157 |                   61 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.358 |  0.163 |                   59 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.1632160246372223
[2m[36m(func pid=63397)[0m mae:  0.10952454805374146
[2m[36m(func pid=63397)[0m rmse_per_class: [0.126, 0.248, 0.032, 0.371, 0.081, 0.161, 0.255, 0.091, 0.137, 0.13]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15483298897743225
[2m[36m(func pid=62337)[0m mae:  0.11000294983386993
[2m[36m(func pid=62337)[0m rmse_per_class: [0.102, 0.227, 0.051, 0.31, 0.072, 0.172, 0.257, 0.111, 0.138, 0.107]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3243 | Steps: 4 | Val loss: 0.2617 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3641 | Steps: 4 | Val loss: 0.2792 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 16:17:02 (running for 00:48:16.59)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.37  |  0.155 |                   62 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.324 |  0.141 |                   60 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.1414051204919815
[2m[36m(func pid=63397)[0m mae:  0.09419772028923035
[2m[36m(func pid=63397)[0m rmse_per_class: [0.091, 0.211, 0.045, 0.261, 0.067, 0.158, 0.259, 0.092, 0.132, 0.099]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15441764891147614
[2m[36m(func pid=62337)[0m mae:  0.10972621291875839
[2m[36m(func pid=62337)[0m rmse_per_class: [0.096, 0.226, 0.051, 0.314, 0.072, 0.172, 0.251, 0.111, 0.144, 0.108]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3290 | Steps: 4 | Val loss: 0.2634 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3684 | Steps: 4 | Val loss: 0.2814 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 16:17:08 (running for 00:48:21.89)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.364 |  0.154 |                   63 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.329 |  0.144 |                   61 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14364422857761383
[2m[36m(func pid=63397)[0m mae:  0.09496919810771942
[2m[36m(func pid=63397)[0m rmse_per_class: [0.097, 0.21, 0.056, 0.283, 0.074, 0.16, 0.223, 0.095, 0.131, 0.107]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15543851256370544
[2m[36m(func pid=62337)[0m mae:  0.11029621213674545
[2m[36m(func pid=62337)[0m rmse_per_class: [0.096, 0.226, 0.054, 0.321, 0.069, 0.174, 0.243, 0.111, 0.152, 0.108]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3354 | Steps: 4 | Val loss: 0.2919 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3660 | Steps: 4 | Val loss: 0.2793 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 16:17:13 (running for 00:48:27.15)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.368 |  0.155 |                   64 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.335 |  0.155 |                   62 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15549814701080322
[2m[36m(func pid=63397)[0m mae:  0.10408975929021835
[2m[36m(func pid=63397)[0m rmse_per_class: [0.094, 0.24, 0.051, 0.338, 0.08, 0.158, 0.254, 0.09, 0.158, 0.092]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.154170423746109
[2m[36m(func pid=62337)[0m mae:  0.10938720405101776
[2m[36m(func pid=62337)[0m rmse_per_class: [0.096, 0.225, 0.049, 0.319, 0.07, 0.174, 0.243, 0.111, 0.148, 0.108]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3278 | Steps: 4 | Val loss: 0.2681 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3610 | Steps: 4 | Val loss: 0.2749 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 16:17:18 (running for 00:48:32.61)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.366 |  0.154 |                   65 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.328 |  0.147 |                   63 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14658385515213013
[2m[36m(func pid=63397)[0m mae:  0.09848561882972717
[2m[36m(func pid=63397)[0m rmse_per_class: [0.107, 0.197, 0.034, 0.284, 0.079, 0.159, 0.25, 0.096, 0.171, 0.089]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15200325846672058
[2m[36m(func pid=62337)[0m mae:  0.10757134109735489
[2m[36m(func pid=62337)[0m rmse_per_class: [0.096, 0.227, 0.045, 0.305, 0.069, 0.172, 0.246, 0.11, 0.142, 0.108]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3343 | Steps: 4 | Val loss: 0.2568 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3745 | Steps: 4 | Val loss: 0.2728 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 16:17:24 (running for 00:48:38.03)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.361 |  0.152 |                   66 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.334 |  0.138 |                   64 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.13840344548225403
[2m[36m(func pid=63397)[0m mae:  0.09231188148260117
[2m[36m(func pid=63397)[0m rmse_per_class: [0.09, 0.202, 0.033, 0.268, 0.08, 0.162, 0.233, 0.092, 0.132, 0.092]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15099799633026123
[2m[36m(func pid=62337)[0m mae:  0.1066957339644432
[2m[36m(func pid=62337)[0m rmse_per_class: [0.095, 0.229, 0.042, 0.296, 0.071, 0.171, 0.251, 0.111, 0.14, 0.104]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3436 | Steps: 4 | Val loss: 0.2901 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3595 | Steps: 4 | Val loss: 0.2744 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 16:17:29 (running for 00:48:43.43)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.374 |  0.151 |                   67 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.344 |  0.164 |                   65 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.16445331275463104
[2m[36m(func pid=63397)[0m mae:  0.10814257711172104
[2m[36m(func pid=63397)[0m rmse_per_class: [0.128, 0.204, 0.087, 0.31, 0.102, 0.163, 0.266, 0.144, 0.135, 0.106]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15223169326782227
[2m[36m(func pid=62337)[0m mae:  0.10735766589641571
[2m[36m(func pid=62337)[0m rmse_per_class: [0.097, 0.23, 0.052, 0.297, 0.07, 0.171, 0.253, 0.11, 0.14, 0.103]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3273 | Steps: 4 | Val loss: 0.2804 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3621 | Steps: 4 | Val loss: 0.2755 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 16:17:35 (running for 00:48:48.90)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.36  |  0.152 |                   68 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.327 |  0.153 |                   66 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15346860885620117
[2m[36m(func pid=63397)[0m mae:  0.10432429611682892
[2m[36m(func pid=63397)[0m rmse_per_class: [0.099, 0.199, 0.035, 0.311, 0.092, 0.163, 0.251, 0.091, 0.183, 0.112]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15309172868728638
[2m[36m(func pid=62337)[0m mae:  0.1079825907945633
[2m[36m(func pid=62337)[0m rmse_per_class: [0.099, 0.226, 0.058, 0.298, 0.07, 0.171, 0.256, 0.109, 0.141, 0.101]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3491 | Steps: 4 | Val loss: 0.2556 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3622 | Steps: 4 | Val loss: 0.2770 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=63397)[0m rmse: 0.13966169953346252
[2m[36m(func pid=63397)[0m mae:  0.0903630182147026
[2m[36m(func pid=63397)[0m rmse_per_class: [0.099, 0.198, 0.043, 0.269, 0.07, 0.173, 0.231, 0.098, 0.129, 0.086]
== Status ==
Current time: 2024-01-07 16:17:40 (running for 00:48:54.26)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.362 |  0.153 |                   69 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.349 |  0.14  |                   67 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=63397)[0m 

[2m[36m(func pid=62337)[0m rmse: 0.1538441926240921
[2m[36m(func pid=62337)[0m mae:  0.10875562578439713
[2m[36m(func pid=62337)[0m rmse_per_class: [0.1, 0.225, 0.057, 0.306, 0.072, 0.171, 0.256, 0.11, 0.14, 0.1]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3503 | Steps: 4 | Val loss: 0.2738 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3614 | Steps: 4 | Val loss: 0.2794 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 16:17:46 (running for 00:48:59.79)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.362 |  0.154 |                   70 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.35  |  0.155 |                   68 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.1546599566936493
[2m[36m(func pid=63397)[0m mae:  0.10062233358621597
[2m[36m(func pid=63397)[0m rmse_per_class: [0.114, 0.215, 0.081, 0.27, 0.087, 0.161, 0.268, 0.128, 0.128, 0.095]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15466511249542236
[2m[36m(func pid=62337)[0m mae:  0.1092953234910965
[2m[36m(func pid=62337)[0m rmse_per_class: [0.101, 0.226, 0.056, 0.317, 0.071, 0.171, 0.251, 0.111, 0.143, 0.099]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3475 | Steps: 4 | Val loss: 0.2771 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3645 | Steps: 4 | Val loss: 0.2831 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 16:17:51 (running for 00:49:05.30)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.361 |  0.155 |                   71 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.347 |  0.153 |                   69 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15304434299468994
[2m[36m(func pid=63397)[0m mae:  0.10327521711587906
[2m[36m(func pid=63397)[0m rmse_per_class: [0.103, 0.198, 0.04, 0.294, 0.099, 0.181, 0.233, 0.093, 0.156, 0.134]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.1560986340045929
[2m[36m(func pid=62337)[0m mae:  0.11055850982666016
[2m[36m(func pid=62337)[0m rmse_per_class: [0.109, 0.225, 0.051, 0.326, 0.072, 0.172, 0.25, 0.111, 0.146, 0.099]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3360 | Steps: 4 | Val loss: 0.2582 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3587 | Steps: 4 | Val loss: 0.2862 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 16:17:57 (running for 00:49:10.81)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.364 |  0.156 |                   72 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.336 |  0.142 |                   70 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.141511932015419
[2m[36m(func pid=63397)[0m mae:  0.09142497181892395
[2m[36m(func pid=63397)[0m rmse_per_class: [0.093, 0.199, 0.043, 0.26, 0.097, 0.166, 0.222, 0.097, 0.129, 0.11]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15717898309230804
[2m[36m(func pid=62337)[0m mae:  0.11178622394800186
[2m[36m(func pid=62337)[0m rmse_per_class: [0.105, 0.227, 0.045, 0.332, 0.073, 0.172, 0.25, 0.116, 0.152, 0.099]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3346 | Steps: 4 | Val loss: 0.2869 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3597 | Steps: 4 | Val loss: 0.2864 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 16:18:02 (running for 00:49:16.09)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.359 |  0.157 |                   73 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.335 |  0.162 |                   71 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.16181246936321259
[2m[36m(func pid=63397)[0m mae:  0.10698169469833374
[2m[36m(func pid=63397)[0m rmse_per_class: [0.128, 0.237, 0.087, 0.288, 0.094, 0.174, 0.267, 0.09, 0.132, 0.121]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m rmse: 0.15727166831493378
[2m[36m(func pid=62337)[0m mae:  0.11205185949802399
[2m[36m(func pid=62337)[0m rmse_per_class: [0.106, 0.224, 0.043, 0.332, 0.076, 0.172, 0.256, 0.115, 0.147, 0.102]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3312 | Steps: 4 | Val loss: 0.2883 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 16:18:07 (running for 00:49:21.50)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15850000455975533
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.36  |  0.157 |                   74 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.331 |  0.154 |                   72 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15439201891422272
[2m[36m(func pid=63397)[0m mae:  0.1038537248969078
[2m[36m(func pid=63397)[0m rmse_per_class: [0.123, 0.198, 0.034, 0.343, 0.094, 0.164, 0.243, 0.096, 0.146, 0.103]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3618 | Steps: 4 | Val loss: 0.2826 | Batch size: 32 | lr: 0.01 | Duration: 3.34s
[2m[36m(func pid=62337)[0m rmse: 0.15585318207740784
[2m[36m(func pid=62337)[0m mae:  0.1108403205871582
[2m[36m(func pid=62337)[0m rmse_per_class: [0.099, 0.223, 0.042, 0.323, 0.081, 0.171, 0.259, 0.114, 0.139, 0.108]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3544 | Steps: 4 | Val loss: 0.2671 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 16:18:13 (running for 00:49:26.76)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.362 |  0.156 |                   75 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.354 |  0.145 |                   73 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14486117660999298
[2m[36m(func pid=63397)[0m mae:  0.09553830325603485
[2m[36m(func pid=63397)[0m rmse_per_class: [0.103, 0.209, 0.03, 0.306, 0.085, 0.156, 0.236, 0.096, 0.138, 0.09]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3595 | Steps: 4 | Val loss: 0.2807 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=62337)[0m rmse: 0.155262291431427
[2m[36m(func pid=62337)[0m mae:  0.11024560034275055
[2m[36m(func pid=62337)[0m rmse_per_class: [0.097, 0.222, 0.049, 0.316, 0.078, 0.172, 0.259, 0.111, 0.139, 0.111]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3196 | Steps: 4 | Val loss: 0.2706 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 16:18:18 (running for 00:49:32.18)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1575000062584877
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.359 |  0.155 |                   76 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.32  |  0.147 |                   74 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14729011058807373
[2m[36m(func pid=63397)[0m mae:  0.09846045076847076
[2m[36m(func pid=63397)[0m rmse_per_class: [0.094, 0.225, 0.038, 0.261, 0.074, 0.158, 0.261, 0.092, 0.135, 0.134]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3568 | Steps: 4 | Val loss: 0.2781 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=62337)[0m rmse: 0.15405166149139404
[2m[36m(func pid=62337)[0m mae:  0.10886536538600922
[2m[36m(func pid=62337)[0m rmse_per_class: [0.095, 0.222, 0.053, 0.313, 0.074, 0.171, 0.255, 0.11, 0.137, 0.111]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3247 | Steps: 4 | Val loss: 0.2769 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=63397)[0m rmse: 0.15219458937644958
[2m[36m(func pid=63397)[0m mae:  0.0999387800693512
[2m[36m(func pid=63397)[0m rmse_per_class: [0.131, 0.199, 0.047, 0.321, 0.084, 0.181, 0.25, 0.089, 0.131, 0.09]
== Status ==
Current time: 2024-01-07 16:18:24 (running for 00:49:37.75)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.357 |  0.154 |                   77 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.325 |  0.152 |                   75 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=63397)[0m 

[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3614 | Steps: 4 | Val loss: 0.2775 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=62337)[0m rmse: 0.15349054336547852
[2m[36m(func pid=62337)[0m mae:  0.10815007984638214
[2m[36m(func pid=62337)[0m rmse_per_class: [0.097, 0.223, 0.053, 0.314, 0.074, 0.171, 0.252, 0.109, 0.136, 0.106]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3574 | Steps: 4 | Val loss: 0.3028 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 16:18:29 (running for 00:49:43.24)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.361 |  0.153 |                   78 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.357 |  0.162 |                   76 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.16242925822734833
[2m[36m(func pid=63397)[0m mae:  0.11009813845157623
[2m[36m(func pid=63397)[0m rmse_per_class: [0.112, 0.202, 0.031, 0.352, 0.101, 0.159, 0.256, 0.091, 0.227, 0.093]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3765 | Steps: 4 | Val loss: 0.2798 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=62337)[0m rmse: 0.15505270659923553
[2m[36m(func pid=62337)[0m mae:  0.10936079919338226
[2m[36m(func pid=62337)[0m rmse_per_class: [0.105, 0.224, 0.051, 0.317, 0.077, 0.171, 0.253, 0.108, 0.139, 0.105]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3442 | Steps: 4 | Val loss: 0.2698 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 16:18:34 (running for 00:49:48.60)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.377 |  0.155 |                   79 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.344 |  0.15  |                   77 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15007148683071136
[2m[36m(func pid=63397)[0m mae:  0.09907347708940506
[2m[36m(func pid=63397)[0m rmse_per_class: [0.09, 0.228, 0.06, 0.259, 0.086, 0.162, 0.242, 0.09, 0.134, 0.151]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3783 | Steps: 4 | Val loss: 0.2790 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3470 | Steps: 4 | Val loss: 0.2627 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=62337)[0m rmse: 0.15455085039138794
[2m[36m(func pid=62337)[0m mae:  0.10874921083450317
[2m[36m(func pid=62337)[0m rmse_per_class: [0.101, 0.227, 0.045, 0.319, 0.085, 0.17, 0.247, 0.109, 0.14, 0.103]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:18:40 (running for 00:49:53.85)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.378 |  0.155 |                   80 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.347 |  0.145 |                   78 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14526937901973724
[2m[36m(func pid=63397)[0m mae:  0.09311804175376892
[2m[36m(func pid=63397)[0m rmse_per_class: [0.113, 0.214, 0.043, 0.26, 0.068, 0.174, 0.251, 0.099, 0.131, 0.101]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3596 | Steps: 4 | Val loss: 0.2811 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=62337)[0m rmse: 0.1563788652420044
[2m[36m(func pid=62337)[0m mae:  0.10986766964197159
[2m[36m(func pid=62337)[0m rmse_per_class: [0.098, 0.229, 0.056, 0.319, 0.09, 0.171, 0.246, 0.108, 0.144, 0.103]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3419 | Steps: 4 | Val loss: 0.2662 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 16:18:45 (running for 00:49:59.37)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.36  |  0.156 |                   81 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.342 |  0.145 |                   79 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.1449567675590515
[2m[36m(func pid=63397)[0m mae:  0.09616973996162415
[2m[36m(func pid=63397)[0m rmse_per_class: [0.107, 0.203, 0.04, 0.289, 0.067, 0.158, 0.257, 0.104, 0.136, 0.088]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3559 | Steps: 4 | Val loss: 0.2786 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=62337)[0m rmse: 0.1551552712917328
[2m[36m(func pid=62337)[0m mae:  0.10886929929256439
[2m[36m(func pid=62337)[0m rmse_per_class: [0.096, 0.227, 0.055, 0.313, 0.093, 0.171, 0.246, 0.109, 0.141, 0.101]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3371 | Steps: 4 | Val loss: 0.2766 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=63397)[0m rmse: 0.15311335027217865
[2m[36m(func pid=63397)[0m mae:  0.10256089270114899
[2m[36m(func pid=63397)[0m rmse_per_class: [0.092, 0.211, 0.073, 0.299, 0.071, 0.16, 0.233, 0.091, 0.141, 0.161]
[2m[36m(func pid=63397)[0m 
== Status ==
Current time: 2024-01-07 16:18:51 (running for 00:50:04.93)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.356 |  0.155 |                   82 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.337 |  0.153 |                   80 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3640 | Steps: 4 | Val loss: 0.2763 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=62337)[0m rmse: 0.15381436049938202
[2m[36m(func pid=62337)[0m mae:  0.10761399567127228
[2m[36m(func pid=62337)[0m rmse_per_class: [0.099, 0.227, 0.055, 0.308, 0.086, 0.171, 0.244, 0.109, 0.139, 0.099]
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3256 | Steps: 4 | Val loss: 0.2765 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.15491148829460144
[2m[36m(func pid=63397)[0m mae:  0.10052746534347534
[2m[36m(func pid=63397)[0m rmse_per_class: [0.14, 0.198, 0.035, 0.302, 0.111, 0.191, 0.259, 0.09, 0.129, 0.094]
== Status ==
Current time: 2024-01-07 16:18:56 (running for 00:50:10.10)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.364 |  0.154 |                   83 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.326 |  0.155 |                   81 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3607 | Steps: 4 | Val loss: 0.2717 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3304 | Steps: 4 | Val loss: 0.2747 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=62337)[0m rmse: 0.15045848488807678
[2m[36m(func pid=62337)[0m mae:  0.10542625188827515
[2m[36m(func pid=62337)[0m rmse_per_class: [0.098, 0.223, 0.046, 0.298, 0.081, 0.17, 0.246, 0.107, 0.136, 0.099]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:19:01 (running for 00:50:15.54)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.361 |  0.15  |                   84 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.33  |  0.151 |                   82 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15086612105369568
[2m[36m(func pid=63397)[0m mae:  0.10064289718866348
[2m[36m(func pid=63397)[0m rmse_per_class: [0.098, 0.199, 0.05, 0.292, 0.104, 0.159, 0.262, 0.09, 0.166, 0.089]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3518 | Steps: 4 | Val loss: 0.2709 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3274 | Steps: 4 | Val loss: 0.2577 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=62337)[0m rmse: 0.14979809522628784
[2m[36m(func pid=62337)[0m mae:  0.10521592199802399
[2m[36m(func pid=62337)[0m rmse_per_class: [0.103, 0.221, 0.042, 0.295, 0.076, 0.171, 0.25, 0.106, 0.136, 0.099]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:19:07 (running for 00:50:20.99)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.352 |  0.15  |                   85 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.327 |  0.141 |                   83 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14123229682445526
[2m[36m(func pid=63397)[0m mae:  0.09200167655944824
[2m[36m(func pid=63397)[0m rmse_per_class: [0.098, 0.21, 0.029, 0.262, 0.096, 0.16, 0.229, 0.091, 0.128, 0.11]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3549 | Steps: 4 | Val loss: 0.2722 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=62337)[0m rmse: 0.15051905810832977
[2m[36m(func pid=62337)[0m mae:  0.10580410808324814
[2m[36m(func pid=62337)[0m rmse_per_class: [0.108, 0.221, 0.041, 0.299, 0.073, 0.17, 0.252, 0.106, 0.136, 0.099]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3161 | Steps: 4 | Val loss: 0.2825 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=63397)[0m rmse: 0.15967854857444763
[2m[36m(func pid=63397)[0m mae:  0.10318158566951752
[2m[36m(func pid=63397)[0m rmse_per_class: [0.106, 0.222, 0.039, 0.275, 0.153, 0.168, 0.258, 0.09, 0.131, 0.154]
[2m[36m(func pid=63397)[0m 
== Status ==
Current time: 2024-01-07 16:19:12 (running for 00:50:26.59)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.355 |  0.151 |                   86 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.316 |  0.16  |                   84 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3522 | Steps: 4 | Val loss: 0.2734 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=62337)[0m rmse: 0.1511087417602539
[2m[36m(func pid=62337)[0m mae:  0.10643001645803452
[2m[36m(func pid=62337)[0m rmse_per_class: [0.108, 0.221, 0.042, 0.303, 0.07, 0.169, 0.252, 0.106, 0.136, 0.103]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3365 | Steps: 4 | Val loss: 0.2786 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 16:19:18 (running for 00:50:32.20)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.352 |  0.151 |                   87 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.337 |  0.155 |                   85 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15498410165309906
[2m[36m(func pid=63397)[0m mae:  0.1028740257024765
[2m[36m(func pid=63397)[0m rmse_per_class: [0.104, 0.201, 0.045, 0.311, 0.128, 0.163, 0.246, 0.092, 0.135, 0.126]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3575 | Steps: 4 | Val loss: 0.2755 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=62337)[0m rmse: 0.152289479970932
[2m[36m(func pid=62337)[0m mae:  0.10746847093105316
[2m[36m(func pid=62337)[0m rmse_per_class: [0.109, 0.221, 0.044, 0.309, 0.07, 0.169, 0.251, 0.106, 0.138, 0.105]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3170 | Steps: 4 | Val loss: 0.2736 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 16:19:24 (running for 00:50:37.87)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.357 |  0.152 |                   88 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.317 |  0.148 |                   86 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14761023223400116
[2m[36m(func pid=63397)[0m mae:  0.09929485619068146
[2m[36m(func pid=63397)[0m rmse_per_class: [0.089, 0.198, 0.031, 0.312, 0.077, 0.164, 0.25, 0.127, 0.137, 0.09]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3558 | Steps: 4 | Val loss: 0.2760 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3114 | Steps: 4 | Val loss: 0.2557 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=62337)[0m rmse: 0.15231671929359436
[2m[36m(func pid=62337)[0m mae:  0.10777254402637482
[2m[36m(func pid=62337)[0m rmse_per_class: [0.1, 0.223, 0.045, 0.311, 0.07, 0.169, 0.248, 0.106, 0.148, 0.104]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:19:29 (running for 00:50:43.42)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.356 |  0.152 |                   89 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.311 |  0.137 |                   87 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.13667520880699158
[2m[36m(func pid=63397)[0m mae:  0.0889589861035347
[2m[36m(func pid=63397)[0m rmse_per_class: [0.098, 0.192, 0.035, 0.26, 0.071, 0.162, 0.236, 0.09, 0.129, 0.095]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3502 | Steps: 4 | Val loss: 0.2772 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=62337)[0m rmse: 0.15340784192085266
[2m[36m(func pid=62337)[0m mae:  0.10845582187175751
[2m[36m(func pid=62337)[0m rmse_per_class: [0.095, 0.223, 0.052, 0.31, 0.069, 0.169, 0.25, 0.106, 0.151, 0.109]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3203 | Steps: 4 | Val loss: 0.2772 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 16:19:34 (running for 00:50:48.61)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.35  |  0.153 |                   90 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.32  |  0.154 |                   88 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15354040265083313
[2m[36m(func pid=63397)[0m mae:  0.10294687747955322
[2m[36m(func pid=63397)[0m rmse_per_class: [0.099, 0.204, 0.041, 0.282, 0.074, 0.162, 0.236, 0.12, 0.15, 0.168]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3633 | Steps: 4 | Val loss: 0.2782 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3326 | Steps: 4 | Val loss: 0.2984 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=62337)[0m rmse: 0.1538964807987213
[2m[36m(func pid=62337)[0m mae:  0.10860886424779892
[2m[36m(func pid=62337)[0m rmse_per_class: [0.095, 0.222, 0.057, 0.314, 0.068, 0.169, 0.252, 0.106, 0.148, 0.109]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:19:40 (running for 00:50:54.17)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.363 |  0.154 |                   91 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.333 |  0.163 |                   89 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.1626056730747223
[2m[36m(func pid=63397)[0m mae:  0.10819926112890244
[2m[36m(func pid=63397)[0m rmse_per_class: [0.104, 0.21, 0.071, 0.318, 0.103, 0.174, 0.289, 0.096, 0.167, 0.093]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3651 | Steps: 4 | Val loss: 0.2789 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3635 | Steps: 4 | Val loss: 0.2677 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=62337)[0m rmse: 0.15429963171482086
[2m[36m(func pid=62337)[0m mae:  0.10896341502666473
[2m[36m(func pid=62337)[0m rmse_per_class: [0.096, 0.221, 0.057, 0.315, 0.07, 0.169, 0.256, 0.105, 0.144, 0.11]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.146758034825325
[2m[36m(func pid=63397)[0m mae:  0.0957312136888504
[2m[36m(func pid=63397)[0m rmse_per_class: [0.099, 0.195, 0.04, 0.301, 0.099, 0.157, 0.239, 0.122, 0.129, 0.087]
[2m[36m(func pid=63397)[0m 
== Status ==
Current time: 2024-01-07 16:19:45 (running for 00:50:59.56)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.365 |  0.154 |                   92 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.363 |  0.147 |                   90 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3668 | Steps: 4 | Val loss: 0.2785 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3438 | Steps: 4 | Val loss: 0.2869 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=62337)[0m rmse: 0.15401367843151093
[2m[36m(func pid=62337)[0m mae:  0.10872058570384979
[2m[36m(func pid=62337)[0m rmse_per_class: [0.101, 0.221, 0.053, 0.314, 0.071, 0.169, 0.256, 0.105, 0.143, 0.107]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.15900523960590363
[2m[36m(func pid=63397)[0m mae:  0.10764855146408081
[2m[36m(func pid=63397)[0m rmse_per_class: [0.102, 0.214, 0.032, 0.285, 0.1, 0.161, 0.272, 0.093, 0.154, 0.176]
== Status ==
Current time: 2024-01-07 16:19:51 (running for 00:51:04.84)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.367 |  0.154 |                   93 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.344 |  0.159 |                   91 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=63397)[0m 

[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3566 | Steps: 4 | Val loss: 0.2767 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3167 | Steps: 4 | Val loss: 0.2730 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=62337)[0m rmse: 0.15332622826099396
[2m[36m(func pid=62337)[0m mae:  0.10778979957103729
[2m[36m(func pid=62337)[0m rmse_per_class: [0.106, 0.219, 0.053, 0.308, 0.075, 0.169, 0.258, 0.105, 0.139, 0.101]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:19:56 (running for 00:51:10.41)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.357 |  0.153 |                   94 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.317 |  0.151 |                   92 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.151342511177063
[2m[36m(func pid=63397)[0m mae:  0.09842957556247711
[2m[36m(func pid=63397)[0m rmse_per_class: [0.095, 0.2, 0.085, 0.291, 0.101, 0.168, 0.261, 0.089, 0.135, 0.089]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3550 | Steps: 4 | Val loss: 0.2747 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3135 | Steps: 4 | Val loss: 0.2662 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=62337)[0m rmse: 0.15221402049064636
[2m[36m(func pid=62337)[0m mae:  0.10633064806461334
[2m[36m(func pid=62337)[0m rmse_per_class: [0.107, 0.219, 0.051, 0.309, 0.078, 0.17, 0.25, 0.105, 0.136, 0.098]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:20:02 (running for 00:51:15.96)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.355 |  0.152 |                   95 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.313 |  0.149 |                   93 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14883527159690857
[2m[36m(func pid=63397)[0m mae:  0.09462841600179672
[2m[36m(func pid=63397)[0m rmse_per_class: [0.174, 0.193, 0.046, 0.295, 0.086, 0.158, 0.225, 0.093, 0.129, 0.089]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3525 | Steps: 4 | Val loss: 0.2736 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3212 | Steps: 4 | Val loss: 0.2574 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=62337)[0m rmse: 0.1508711278438568
[2m[36m(func pid=62337)[0m mae:  0.10502491146326065
[2m[36m(func pid=62337)[0m rmse_per_class: [0.101, 0.219, 0.047, 0.313, 0.079, 0.17, 0.245, 0.106, 0.134, 0.096]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:20:07 (running for 00:51:21.09)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.353 |  0.151 |                   96 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.321 |  0.14  |                   94 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.13977155089378357
[2m[36m(func pid=63397)[0m mae:  0.09161672741174698
[2m[36m(func pid=63397)[0m rmse_per_class: [0.09, 0.205, 0.032, 0.259, 0.072, 0.159, 0.231, 0.098, 0.138, 0.113]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3644 | Steps: 4 | Val loss: 0.2743 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3526 | Steps: 4 | Val loss: 0.2669 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=62337)[0m rmse: 0.15078362822532654
[2m[36m(func pid=62337)[0m mae:  0.10538260638713837
[2m[36m(func pid=62337)[0m rmse_per_class: [0.095, 0.218, 0.045, 0.316, 0.079, 0.17, 0.247, 0.106, 0.133, 0.099]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:20:12 (running for 00:51:26.66)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.364 |  0.151 |                   97 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.353 |  0.148 |                   95 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.14811453223228455
[2m[36m(func pid=63397)[0m mae:  0.09603513777256012
[2m[36m(func pid=63397)[0m rmse_per_class: [0.097, 0.196, 0.085, 0.287, 0.067, 0.175, 0.244, 0.09, 0.129, 0.112]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3550 | Steps: 4 | Val loss: 0.2774 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3332 | Steps: 4 | Val loss: 0.3013 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=62337)[0m rmse: 0.1530671864748001
[2m[36m(func pid=62337)[0m mae:  0.10750342905521393
[2m[36m(func pid=62337)[0m rmse_per_class: [0.096, 0.22, 0.052, 0.317, 0.078, 0.17, 0.252, 0.106, 0.137, 0.102]
[2m[36m(func pid=62337)[0m 
== Status ==
Current time: 2024-01-07 16:20:18 (running for 00:51:32.12)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.355 |  0.153 |                   98 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.333 |  0.159 |                   96 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15948715806007385
[2m[36m(func pid=63397)[0m mae:  0.10567395389080048
[2m[36m(func pid=63397)[0m rmse_per_class: [0.168, 0.195, 0.034, 0.355, 0.093, 0.18, 0.248, 0.093, 0.14, 0.089]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3511 | Steps: 4 | Val loss: 0.2801 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3453 | Steps: 4 | Val loss: 0.2871 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=62337)[0m rmse: 0.15518265962600708
[2m[36m(func pid=62337)[0m mae:  0.10956107079982758
[2m[36m(func pid=62337)[0m rmse_per_class: [0.1, 0.222, 0.051, 0.316, 0.082, 0.17, 0.256, 0.105, 0.145, 0.104]
[2m[36m(func pid=62337)[0m 
[2m[36m(func pid=63397)[0m rmse: 0.15535269677639008
[2m[36m(func pid=63397)[0m mae:  0.10193963348865509
[2m[36m(func pid=63397)[0m rmse_per_class: [0.093, 0.227, 0.038, 0.292, 0.119, 0.161, 0.245, 0.103, 0.142, 0.133]== Status ==
Current time: 2024-01-07 16:20:23 (running for 00:51:37.46)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00022 | RUNNING    | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.351 |  0.155 |                   99 |
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.345 |  0.155 |                   97 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)



[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=62337)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3510 | Steps: 4 | Val loss: 0.2846 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3446 | Steps: 4 | Val loss: 0.2731 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=62337)[0m rmse: 0.1580314189195633
[2m[36m(func pid=62337)[0m mae:  0.11200866848230362
[2m[36m(func pid=62337)[0m rmse_per_class: [0.1, 0.227, 0.05, 0.317, 0.08, 0.17, 0.26, 0.105, 0.164, 0.107]
== Status ==
Current time: 2024-01-07 16:20:29 (running for 00:51:42.95)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.345 |  0.152 |                   98 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
| train_10f5e_00018 | TERMINATED | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.372 |  0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15173974633216858
[2m[36m(func pid=63397)[0m mae:  0.09674572199583054
[2m[36m(func pid=63397)[0m rmse_per_class: [0.104, 0.214, 0.073, 0.263, 0.102, 0.16, 0.27, 0.104, 0.131, 0.096]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3515 | Steps: 4 | Val loss: 0.2870 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 16:20:34 (running for 00:51:47.96)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.345 |  0.152 |                   98 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
| train_10f5e_00018 | TERMINATED | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.372 |  0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=63397)[0m rmse: 0.15475521981716156
[2m[36m(func pid=63397)[0m mae:  0.10228623449802399
[2m[36m(func pid=63397)[0m rmse_per_class: [0.102, 0.227, 0.043, 0.332, 0.089, 0.166, 0.248, 0.097, 0.159, 0.083]
[2m[36m(func pid=63397)[0m 
[2m[36m(func pid=63397)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3385 | Steps: 4 | Val loss: 0.2696 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 16:20:39 (running for 00:51:53.23)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00023 | RUNNING    | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.352 |  0.155 |                   99 |
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
| train_10f5e_00018 | TERMINATED | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.372 |  0.16  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 16:20:40 (running for 00:51:53.89)
Memory usage on this node: 16.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: -0.15675000473856926
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/119.96 GiB heap, 0.0/55.4 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_10f5e_00000 | TERMINATED | 192.168.7.53:138220 | 0.0001 |       0.99 |         0      |  0.429 |  0.178 |                   75 |
| train_10f5e_00001 | TERMINATED | 192.168.7.53:138601 | 0.001  |       0.99 |         0      |  0.374 |  0.154 |                  100 |
| train_10f5e_00002 | TERMINATED | 192.168.7.53:139034 | 0.01   |       0.99 |         0      |  0.323 |  0.156 |                  100 |
| train_10f5e_00003 | TERMINATED | 192.168.7.53:139456 | 0.1    |       0.99 |         0      |  0.968 |  0.18  |                   75 |
| train_10f5e_00004 | TERMINATED | 192.168.7.53:157242 | 0.0001 |       0.9  |         0      |  0.474 |  0.178 |                   75 |
| train_10f5e_00005 | TERMINATED | 192.168.7.53:157825 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   75 |
| train_10f5e_00006 | TERMINATED | 192.168.7.53:162590 | 0.01   |       0.9  |         0      |  0.356 |  0.151 |                  100 |
| train_10f5e_00007 | TERMINATED | 192.168.7.53:163121 | 0.1    |       0.9  |         0      |  0.343 |  0.153 |                  100 |
| train_10f5e_00008 | TERMINATED | 192.168.7.53:176262 | 0.0001 |       0.99 |         0.0001 |  0.429 |  0.178 |                   75 |
| train_10f5e_00009 | TERMINATED | 192.168.7.53:176351 | 0.001  |       0.99 |         0.0001 |  0.381 |  0.168 |                   75 |
| train_10f5e_00010 | TERMINATED | 192.168.7.53:186938 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.155 |                  100 |
| train_10f5e_00011 | TERMINATED | 192.168.7.53:187535 | 0.1    |       0.99 |         0.0001 |  0.691 |  0.189 |                   75 |
| train_10f5e_00012 | TERMINATED | 192.168.7.53:7413   | 0.0001 |       0.9  |         0.0001 |  0.474 |  0.178 |                   75 |
| train_10f5e_00013 | TERMINATED | 192.168.7.53:7482   | 0.001  |       0.9  |         0.0001 |  0.399 |  0.17  |                   75 |
| train_10f5e_00014 | TERMINATED | 192.168.7.53:19000  | 0.01   |       0.9  |         0.0001 |  0.354 |  0.153 |                  100 |
| train_10f5e_00015 | TERMINATED | 192.168.7.53:23101  | 0.1    |       0.9  |         0.0001 |  0.329 |  0.144 |                  100 |
| train_10f5e_00016 | TERMINATED | 192.168.7.53:25864  | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.178 |                   75 |
| train_10f5e_00017 | TERMINATED | 192.168.7.53:26476  | 0.001  |       0.99 |         1e-05  |  0.384 |  0.165 |                   75 |
| train_10f5e_00018 | TERMINATED | 192.168.7.53:43619  | 0.01   |       0.99 |         1e-05  |  0.372 |  0.16  |                   75 |
| train_10f5e_00019 | TERMINATED | 192.168.7.53:43707  | 0.1    |       0.99 |         1e-05  |  0.651 |  0.222 |                   75 |
| train_10f5e_00020 | TERMINATED | 192.168.7.53:45149  | 0.0001 |       0.9  |         1e-05  |  0.479 |  0.179 |                   75 |
| train_10f5e_00021 | TERMINATED | 192.168.7.53:47987  | 0.001  |       0.9  |         1e-05  |  0.402 |  0.171 |                   75 |
| train_10f5e_00022 | TERMINATED | 192.168.7.53:62337  | 0.01   |       0.9  |         1e-05  |  0.351 |  0.158 |                  100 |
| train_10f5e_00023 | TERMINATED | 192.168.7.53:63397  | 0.1    |       0.9  |         1e-05  |  0.338 |  0.15  |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+


2024-01-07 16:20:40,223	INFO tune.py:798 -- Total run time: 3114.72 seconds (3113.86 seconds for the tuning loop).
[2m[36m(func pid=63397)[0m rmse: 0.15048888325691223
[2m[36m(func pid=63397)[0m mae:  0.09731574356555939
[2m[36m(func pid=63397)[0m rmse_per_class: [0.094, 0.194, 0.047, 0.276, 0.076, 0.164, 0.23, 0.102, 0.134, 0.187]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341351.1 ON aap04 CANCELLED AT 2024-01-07T16:20:46 ***
srun: error: aap04: task 0: Exited with exit code 1
